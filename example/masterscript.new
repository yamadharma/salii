#!/bin/sh
#
# SARA install script for LISA nodes: 
# Initial Authors: Bas van der Vlies and Jaap Dijkshoorn
#
# SVN Info:
#	$Id: sara_install 3863 2009-06-10 12:37:52Z bas $
#	$URL: https://subtrac.sara.nl/hpcv/svn/beowulf/trunk/systemimager/boot_scripts/sara_install $

# This master autoinstall script was created with SystemImager v3.7.6

# Pull in variables left behind by the linuxrc script.
# This information is passed from the linuxrc script on the autoinstall media 
# via /tmp/variables.txt.  Apparently the shell we use in BOEL is not 
# intelligent enough to take a "set -a" parameter.
#
#set -x

. /tmp/variables.txt

# Load functions and other variables
. /etc/init.d/functions

## Checking if we are using grub legacy or grub2.
## if grub2 is used the disk are set with label gpt

if [ "$GRUB2" == "yes" ]; then
	DISKLABEL=gpt
else
	DISKLABEL=msdos
fi

get_arch

NO_LISTING=yes
if [ -z $NO_LISTING ]; then
    VERBOSE_OPT="v"
else
    VERBOSE_OPT=""
fi

# Remove .master from script name
#
SARA_SCRIPT=`echo $0 | cut -d. -f1`

echo $0
echo $IMAGENAME

[ -z $IMAGENAME ] && IMAGENAME=`basename $SARA_SCRIPT`
[ -z $OVERRIDES ] && OVERRIDES=`basename $SARA_SCRIPT`

echo $IMAGENAME

### BEGIN Check to be sure this not run from a working machine ###
# Test for mounted SCSI or IDE disks
mount | grep [hs]d[a-z][1-9] > /dev/null 2>&1
[ $? -eq 0 ] &&  logmsg Sorry.  Must not run on a working machine... && shellout

# Test for mounted software RAID devices
mount | grep md[0-9] > /dev/null 2>&1
[ $? -eq 0 ] &&  logmsg Sorry.  Must not run on a working machine... && shellout

# Test for mounted hardware RAID disks
mount | grep c[0-9]+d[0-9]+p > /dev/null 2>&1
[ $? -eq 0 ] &&  logmsg Sorry.  Must not run on a working machine... && shellout
### END Check to be sure this not run from a working machine ###


################################################################################
#
#   Stop RAID devices before partitioning begins
#
# Q1) Why did they get started in the first place?  
# A1) So we can pull a local.cfg file off a root mounted software RAID system.
#     They may not be started on your system -- they would only be started if
#     you did the stuff in Q3 below.
#
# Q2) Why didn't my local.cfg on my root mounted software RAID work for me 
#     with the standard kernel flavour?
# A2) The standard kernel flavour uses modules for the software RAID drivers --
#     therefore, software RAID is not available at the point in the boot process
#     where BOEL needs to read the local.cfg file.  They are only pulled over 
#     when this script is run, which is, of course, only runnable if it was
#     pulled over the network using the settings that you would have wanted it
#     to get from the local.cfg file, which it couldn't.  Right?
#
# Q3) Whatever.  So how do I make it work with a local.cfg file on my root
#     mounted software RAID?  
# A3) Compile an autoinstall kernel with software RAID, and any other drivers 
#     you might need built in (filesystem, SCSI drivers, etc.).
#
if [ -f /proc/mdstat ]; then
  RAID_DEVICES=` cat /proc/mdstat | grep ^md | sed 's/ .*$//g' `

  # Turn dem pesky raid devices off!
  for RAID_DEVICE in ${RAID_DEVICES}
  do
    DEV="/dev/${RAID_DEVICE}"
    logmsg "mdadm --manage ${DEV} --stop"
    mdadm --manage ${DEV} --stop
  done
fi

#
# disk enumeration

disk_enumerate "sd hd"

## Begin disk creation and mounting
# Create disk label.  This ensures that all remnants of the old label, whatever
# type it was, are removed and that we're starting with a clean label.
logmsg "set_disklabel  $DISK0 $DISKLABEL"
set_disklabel $DISK0 $DISKLABEL

if [ "$DISKLABEL" == "msdos" ]; then
	logmsg "partition $DISK0 1024:ext2.128:/boot:boot 1:none 4096:swap::swap 0:xfs:/:root"
	partition $DISK0 1024:ext2.128:/boot:boot 1:none 4096:swap::swap 0:xfs:/:root
else
	logmsg "partition $DISK0 1024:ext2:/boot:boot 1:grub2 4096:swap::swap 0:xfs:/:root"
	partition $DISK0 1024:ext2:/boot:boot 1:grub2 4096:swap::swap 0:xfs:/:root
fi

if [ ! -z $DISK1 ]
then
	logmsg "set_disklabel  $DISK1 $DISKLABEL"
	set_disklabel $DISK1 $DISKLABEL

	logmsg "partition $DISK1 0:xfs:/scratch:scratch"
	partition $DISK1 0:xfs:/scratch:scratch
fi

logmsg "mount_disks"
mount_disks

## End disk creation and mounting

### BEGIN mount proc in image for tools like System Configurator ###
### BEGIN mount sysfs in image for tools that might be run during chroot ### 
logmsg "mount_system_fs"
mount_system_fs
### END mount proc in image for tools like System Configurator ###
### END mount sysfs in image for tools that might be run during chroot ### 

################################################################################
#
#   Lay the image down on the freshly formatted disk(s)
#
if [ ! -z $MONITOR_SERVER ]; then
    start_report_task
fi

## Let's get the image
# TODO: rewrite this so we can use PROTOCOL instead of using BITTORRENT yes/no
logmsg "getimage"
getimage

#
################################################################################

# Leave notice of which image is installed on the client
echo $IMAGENAME > /a/etc/systemimager/IMAGE_LAST_SYNCED_TO || shellout

#
################################################################################

##################################################################
#
# Uncomment the line below to leave your hostname blank.
# Certain distributions use this as an indication to take on the
# hostname provided by a DHCP server.  The default is to have
# SystemConfigurator assign your clients the hostname that
# corresponds to the IP address the use during the install.
# (If you used to use the static_dhcp option, this is your man.)
#
#HOSTNAME=""


################################################################################
#
#   Post Install Scripts
#
export DISK0
run_post_install_scripts
#
################################################################################

################################################################################
#
#   Save virtual console session in the imaged client
#
if [ ! -z $MONITOR_SERVER ]; then
    if [ "x$MONITOR_CONSOLE" = "xyes" ]; then 
        [ ! -d /a/root ] && mkdir -p /a/root
        cp -f /tmp/si_monitor.log /a/root/si_monitor.log
    fi
fi

#
################################################################################

################################################################################
#
#   Unmount filesystems
#

logmsg "umount_system_fs"
umount_system_fs

logmsg "umount_disks"
umount_disks

#
################################################################################


################################################################################
#
#   Tell the image server we are done
#   
rsync $IMAGESERVER::scripts/imaging_complete > /dev/null 2>&1
logmsg "Imaging completed"
#
################################################################################

if [ ! -z $MONITOR_SERVER ]; then
    # Report the 'imaged' state to the monitor server.
    send_monitor_msg "status=100:speed=0"
    if [ "x$MONITOR_CONSOLE" = "xyes" ]; then 
        # Print some empty lines and sleep some seconds to give time to
        # the virtual console to get last messages.
        # XXX: this is a dirty solution, we should find a better way to
        # sync last messages... -AR-
        logmsg ""
        logmsg ""
        logmsg ""
        sleep 10
    fi
    # Report the post-install action.
    send_monitor_msg "status=104:speed=0"
fi


# Take network interface down
[ -z $DEVICE ] && DEVICE=eth0
ifconfig $DEVICE down || shellout

reboot
