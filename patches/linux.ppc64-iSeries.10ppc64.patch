diff -uNr -X /home/engebret/dontdiff linux-2.4.19/Documentation/Configure.help linuxppc64_2_4/Documentation/Configure.help
--- linux-2.4.19/Documentation/Configure.help	Fri Aug  2 19:39:42 2002
+++ linuxppc64_2_4/Documentation/Configure.help	Fri Aug  9 12:35:57 2002
@@ -232,6 +232,13 @@
   CPU and the single-board computers built around it, targeted for
   network and embedded applications.  For more information see the
   Axis Communication site, <http://developer.axis.com/>.
+PowerPC64 processor
+CONFIG_PPC64
+  The PowerPC architecture was designed for both 32 bit and 64 bit
+  processor implementations. 64 bit PowerPC processors are in many
+  ways a superset of their 32 bit PowerPC cousins. Each 64 bit PowerPC
+  processor also has a 32 bit mode to allow for 32 bit compatibility.
+  The home of the PowerPC 64 Linux project is at <http://linuxppc64.org>
 
 Multiquad support for NUMA systems
 CONFIG_MULTIQUAD
@@ -15375,6 +15382,20 @@
   hard drives and ADFS-formatted floppy disks. This is experimental
   codes, so if you're unsure, say N.
 
+JFS filesystem support
+CONFIG_JFS_FS
+  This is a port of IBM's Journaled Filesystem .  More information is
+  available in the file Documentation/filesystems/jfs.txt.
+
+  If you do not intend to use the JFS filesystem, say N.
+
+JFS Debugging
+CONFIG_JFS_DEBUG
+  If you are experiencing any problems with the JFS filesystem, say
+  Y here.  This will result in additional debugging messages to be
+  written to the system log.  Under normal circumstances, this
+  results in very little overhead.
+
 /dev/pts file system for Unix98 PTYs
 CONFIG_DEVPTS_FS
   You should say Y here if you said Y to "Unix98 PTY support" above.
@@ -16544,6 +16565,19 @@
   <file:Documentation/modules.txt>.  The module will be called
   isicom.o.
 
+IBM Multiport Serial Adapter
+CONFIG_ICOM
+   This driver is for a family of multiport serial adapters including
+   2 port RVX (iSeries  2745),  2 port modem (iSeries 
+   2772) and  1 port RVX + 1 port modem (iSeries 2771).  The
+  module is called iCom.o
+CONFIG_ICOM_MODEM_CC
+  This field entry enables the device driver to configure the modem
+  for appropriate operations based on country code.  If you do not
+  have an internal modem card then a blank entry is recommended.
+  If you do  have an internal modem card, look for the comment in iCom.c
+  indicating which value relates to your country.
+
 Unix98 PTY support
 CONFIG_UNIX98_PTYS
   A pseudo terminal (PTY) is a software device consisting of two
@@ -19613,6 +19647,47 @@
   keys are documented in <file:Documentation/sysrq.txt>. Don't say Y
   unless you really know what this hack does.
 
+Kernel Debugging support
+CONFIG_KDB
+  This option provides a built-in kernel debugger.  The built-in
+  kernel debugger contains commands which allow memory to be examined,
+  instructions to be disassembled and breakpoints to be set.  For details,
+  see Documentation/kdb/kdb.mm and the manual pages kdb_bt, kdb_ss, etc.
+  Kdb can also be used via the serial port.  Set up the system to
+  have a serial console (see Documentation/serial-console.txt).
+  The Control-A key sequence on the serial port will cause the
+  kernel debugger to be entered with input from the serial port and
+  output to the serial console.  Selecting this option will
+  automatically set CONFIG_KALLSYMS. If unsure, say N.
+
+KDB modules
+CONFIG_KDB_MODULES
+  KDB can be extended by adding your own modules, in directory
+  kdb/modules.  This option selects the way that these modules should
+  be compiled, as free standing modules (select M) or built into the
+  kernel (select Y).  If unsure say M.
+
+KDB off by default
+CONFIG_KDB_OFF
+  Normally kdb is activated by default, as long as CONFIG_KDB is set.
+  If you want to ship a kernel with kdb support but only have kdb
+  turned on when the user requests it then select this option.  When
+  compiled with CONFIG_KDB_OFF, kdb ignores all events unless you boot
+  with kdb=on or you echo "1" > /proc/sys/kernel/kdb.  This option also
+  works in reverse, if kdb is normally activated, you can boot with
+  kdb=off or echo "0" > /proc/sys/kernel/kdb to deactivate kdb. If
+  unsure, say N.
+
+Load all symbols for debugging
+CONFIG_KALLSYMS
+  Normally only exported symbols are available to modules. For
+  debugging you may want all symbols, not just the exported ones. If
+  you say Y here then extra data is added to the kernel and modules,
+  this data lists all the non-stack symbols in the kernel or module
+  and can be used by any debugger.  You need modutils >= 2.3.11 to use
+  this option. See "man kallsyms" for the data format, it adds 10-20%
+  to the size of the kernel and the loaded modules. If unsure, say N.
+
 ISDN support
 CONFIG_ISDN
   ISDN ("Integrated Services Digital Networks", called RNIS in France)
@@ -21365,6 +21440,12 @@
   Select APUS if configuring for a PowerUP Amiga.
   More information is available at:
   <http://linux-apus.sourceforge.net/>.
+# Choice: i or p
+Platform support
+CONFIG_PPC_ISERIES
+  Linux runs on certain models of the IBM AS/400, now known as the 
+  IBM iSeries. Generally if you can run LPAR (Logical Partitioning)
+  on your iSeries you can run Linux in a partition on your machine.
 
 AltiVec kernel support
 CONFIG_ALTIVEC
@@ -21428,6 +21509,16 @@
   You may also want to compile the dma sound driver as a module and
   have it autoloaded. The act of removing the module shuts down the
   sound hardware for more power savings.
+Platform support
+CONFIG_PPC_PSERIES
+  Linux runs on most models of IBM pSeries hardware. (pSeries used
+  to be known as the RS/6000)
+  
+  See <http://linuxppc64.org> for exact model information for the 
+  64 bit PowerPC kernel.
+  
+  pSeries Linux information from IBM can be found at:
+  <http://www.ibm.com/servers/eserver/pseries/linux/>
 
 APM emulation
 CONFIG_PMAC_APM_EMU
@@ -21716,6 +21807,12 @@
     Date of Release: early 2001 (?)
     End of life: -
     URL: <http://www.microsys.de/html/cu824.html>
+Support for Large Memory
+CONFIG_MSCHUNKS
+  MsChunks stands for Main Store Chunks and specifically allows the 
+  64 bit PowerPC Linux kernel to optimize for machines with sparse 
+  discontiguous memory. iSeries kernels need to have this on. 
+  It is recommended that for pSeries hardware that you answer N.
 
 ADB raw keycode support
 CONFIG_MAC_ADBKEYCODES
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/Documentation/filesystems/changelog.jfs linuxppc64_2_4/Documentation/filesystems/changelog.jfs
--- linux-2.4.19/Documentation/filesystems/changelog.jfs	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/Documentation/filesystems/changelog.jfs	Tue Apr 23 11:14:24 2002
@@ -0,0 +1,234 @@
+IBM's Journaled File System (JFS) for Linux version 1.0.17
+Team members
+Steve Best        sbest@us.ibm.com
+Dave Kleikamp     shaggy@austin.ibm.com  
+Barry Arndt       barndt@us.ibm.com
+Christoph Hellwig hch@infradead.org
+
+
+Release April 2, 2002 (version 1.0.17)
+
+This is our fifty-fifth release of IBM's Enterprise JFS technology port to Linux.
+Beta 1 was release 0.1.0 on 12/8/2000, Beta 2 was release 0.2.0 on 3/7/2001, 
+Beta 3 was release 0.3.0 on 4/30/2001, and release 1.0.0 on 6/28/2001.
+
+
+Function and Fixes in drop 55 (1.0.17)
+   - Call sb_set_blocksize instead of set_blocksize in 2.5 (Christoph Hellwig)
+   - Replace strtok by strsep (Christoph Hellwig)
+   - Store entire device number in log superblock rather than just the minor.
+   - Include file clean (Christoph Hellwig)
+   - Fix race introduced by thread handling cleanups (Christoph Hellwig)
+   - Detect dtree corruption to avoid infinite loop
+   - JFS needs to include completion.h
+   - Support external log(journal) device file system work part 1 (Christoph Hellwig)
+
+Function and Fixes in drop 54 (1.0.16)
+   - Limit readdir offset to signed integer for NFSv2 (Christoph Hellwig)
+   - missing static in jfs_imap.c (Christoph Hellwig)
+   - Fix infinite loop in jfs_readdir
+     weren't updating the directory index table completely (bug # 2591)
+   - Sync up 2.4 tree with 2.5 -- (Christoph Hellwig & Shaggy)
+     move to completions, provide back-compact for pre-2.4.7
+     remove dead code
+     add kdev_t conversion, that should have been in 2.4 anyway
+     move one-time inode initialization into slab constructor
+   - Remove non-core files from CVS
+
+Function and Fixes in drop 53 (1.0.15)
+   - Fix trap when appending to very large file
+   - Moving jfs headers into fs/jfs at Linus' request
+   - Move up to linux-2.5.4
+   - Fix file size limit on 32-bit (Andi Kleen)
+   - make changelog more read-able and include only 1.0.0 and above (Christoph Hellwig)
+   - Don't allocate metadata pages from high memory. JFS keeps them kmapped too long causing deadlock.
+   - Fix xtree corruption when creating file with >= 64 GB of physically contiguous dasd
+   - Replace semaphore with struct completion for thread startup/shutdown (Benedikt Spranger)
+   - cleanup Tx alloc/free (Christoph Hellwig)
+   - Move up to linux-2.5.3
+   - thread cleanups (Christoph Hellwig)
+   - First step toward making tblocks and tlocks dynamically allocated. Intro tid_t and lid_t to
+     insulate the majority of the code from future changes. Also hide TxBlock and TxLock arrays
+     by using macros to get from tids and lids to real structures.
+   - minor list-handling cleanup (Christoph Hellwig)
+   - Replace altnext and altprev with struct list_head
+   - Clean up the debugging code and add support for collecting statistics (Christoph Hellwig)
+   
+  
+Function and Fixes in drop 52 (1.0.14)
+   - Fix hang in invalidate_metapages when jfs.o is built as a module
+   - Fix anon_list removal logic in txLock
+
+Function and Fixes in drop 51 (1.0.13)
+   - chmod changes on newly created directories are lost after umount (bug 2535)
+   - Page locking race fixes
+   - Improve metapage locking
+   - Fix timing window. Lock page while metapage is active to avoid page going 
+     away before the metadata is released. (Fixed crash during mount/umount testing)
+   - Make changes for 2.5.2 kernel
+   - Fix race condition truncating large files
+         
+Function and Fixes in drop50 (1.0.12)
+   - Add O_DIRECT support
+   - Add support for 2.4.17 kernel
+   - Make sure COMMIT_STALE gets reset before the inode is unlocked. Fixing
+     this gets rid of XT_GETPAGE errors
+   - Remove invalid __exit keyword from metapage_exit and txExit.
+   - fix assert(log->cqueue.head == NULL by waiting longer
+   
+Function and Fixes in drop49 (1.0.11)
+   - Readdir was not handling multibyte codepages correctly.
+   - Make mount option parsing more robust.
+   - Add iocharset mount option.
+   - Journalling of symlinks incorrect, resulting in logredo failure of -265.
+   - Add jfsutils information to Changes file
+   - Improve recoverability of the file system when metadata corruption is detected.
+   - Fix kernel OOPS when root inode is corrupted 
+   
+Function and Fixes in drop48 (1.0.10)
+   - put inodes later on hash queues
+   - Fix boundary case in xtTruncate
+   - When invalidating metadata, try to flush the dirty buffers rather than sync them.
+   - Add another sanity check to avoid trapping when imap is corrupt
+   - Fix file truncate while removing large file (assert(cmp == 0))
+   - read_cache_page returns ERR_PTR, not NULL on error
+   - Add dtSearchNode and dtRelocate
+   - JFS needs to use generic_file_open & generic_file_llseek
+   - Remove lazyQwait, etc. It created an unnecessary bottleneck in TxBegin.
+
+Function and Fixes in drop47 (1.0.9)
+   - Fix data corruption problem when creating files while deleting others. (jitterbug 183)
+   - Make sure all metadata is written before finalizing the log
+   - Fix serialization problem in shutdown by setting i_size of directory sooner. (bugzilla #334)
+   - JFS should quit whining when special files are marked dirty during read-only mount.
+   - Must always check rc after DT_GETPAGE
+   - Add diExtendFS
+   - Removing defconfig form JFS source - not really needed
+   
+Function and Fixes in drop46 (1.0.8)
+   - Synclist was being built backwards causing logredo to quit too early
+   - jfs_compat.h needs to include module.h
+   - uncomment EXPORTS_NO_SYMBOLS in super.c
+   - Minor code cleanup
+   - xtree of zero-truncated file not being logged
+   - Fix logging on file truncate
+   - remove unused metapage fields
+
+Function and Fixes in drop45 (1.0.7)
+   - cleanup remove IS_KIOBUFIO define.
+   - cleanup remove TRUNC_NO_TOSS define. 
+   - have jFYI's use the name directly from dentry  
+   - Remove nul _ALLOC and _FREE macros and also make spinlocks static. 
+   - cleanup add externs where needed in the header files  
+   - jfs_write_inode is a bad place to call iput.  Also limit warnings.
+   - More truncate cleanup 
+   - Truncate cleanup 
+   - Add missing statics in jfs_metapage.c 
+   - fsync fixes   
+   - Clean up symlink code - use page_symlink_inode_operations 
+   - unicode handling cleanup   
+   - cleanup replace UniChar with wchar_t
+   - Get rid of CDLL_* macros - use list.h instead 
+   - 2.4.11-prex mount problem Call new_inode instead of get_empty_inode 
+   - use kernel min/max macros 
+   - Add MODULE_LICENSE stub for older kernels 
+   - IA64/gcc3 fixes 
+   - Log Manager fixes, introduce __SLEEP_COND macro 
+   - Mark superblock dirty when some errors detected (forcing fsck to be run).
+   - More robust remounting from r/o to r/w. 
+   - Misc. cleanup add static where appropriate 
+   - small cleanup in jfs_umount_rw 
+   - add MODULE_ stuff 
+   - Set *dropped_lock in alloc_metapage 
+   - Get rid of unused log list 
+   - cleanup jfs_imap.c to remove _OLD_STUFF and _NO_MORE_MOUNT_INODE defines 
+   - Log manager cleanup  
+   - Transaction manager cleanup 
+   - correct memory allocations flags 
+   - Better handling of iterative truncation
+   - Change continue to break, otherwise we don't re-acquire LAZY_LOCK
+
+Function and Fixes in drop44 (1.0.6)
+   - Create jfs_incore.h which merges linux/jfs_fs.h, linux/jfs_fs_i.h, and jfs_fs_sb.h
+   - Create a configuration option to handle JFS_DEBUG define
+   - Fixed a few cases where positive error codes were returned to the VFS.
+   - Replace jfs_dir_read by generic_read_dir.
+   - jfs_fsync_inode is only called by jfs_fsync_file, merge the two and rename to jfs_fsync.
+   - Add a bunch of missing externs.
+   - jfs_rwlock_lock is unused, nuke it.
+   - Always use atomic set/test_bit operations to protect jfs_ip->cflag 
+   - Combine jfs_ip->flag with jfs_ip->cflag
+   - Fixed minor format errors reported by fsck 
+   - cflags should be long so bitops always works correctly
+   - Use GFP_NOFS for runtime memory allocations 
+   - Support  VM changes in 2.4.10 of the kernel
+   - Remove ifdefs supporting older 2.4 kernels. JFS now requires at least 2.4.3 or 2.4.2-ac2
+   - Simplify and remove one use of IWRITE_TRYLOCK
+   - jfs_truncate was not passing tid to xtTruncate
+   - removed obsolete extent_page workaround
+   - correct recovery from failed diAlloc call (disk full)
+   - In write_metapage, don't call commit_write if prepare_write failed   
+   
+Function and Fixes in drop43 (1.0.5)
+   - Allow separate allocation of JFS-private superblock/inode data.
+   - Remove checks in namei.c that are already done by the VFS.
+   - Remove redundant mutex defines.
+   - Replace all occurrences of #include <linux/malloc.h> with #include <linux/slab.h>
+   - Work around race condition in remount -fixes OOPS during shutdown
+   - Truncate large files incrementally ( affects directories too)
+
+Function and Fixes in drop42 (1.0.4)
+   - Fixed compiler warnings in the FS when building on 64 bits systems
+   - Fixed deadlock where jfsCommit hung in hold_metapage
+   - Fixed problems with remount
+   - Reserve metapages for jfsCommit thread 
+   - Get rid of buggy invalidate_metapage & use discard_metapage 
+   - Don't hand metapages to jfsIOthread (too many context switches) (jitterbug 125, bugzilla 238)
+   - Fix error message in jfs_strtoUCS
+
+Function and Fixes in drop41 (1.0.3)
+   - Patch to move from previous release to latest release needs to update the version number in super.c 
+   - Jitterbug problems (134,140,152) removing files have been fixed
+   - Set rc=ENOSPC if ialloc fails in jfs_create and jfs_mkdir
+   - Fixed jfs_txnmgr.c 775! assert
+   - Fixed jfs_txnmgr.c 884! assert(mp->nohomeok==0)
+   - Fix hang - prevent tblocks from being exhausted
+   - Fix oops trying to mount reiserfs
+   - Fail more gracefully in jfs_imap.c
+   - Print more information when char2uni fails
+   - Fix timing problem between Block map and metapage cache - jitterbug 139
+   - Code Cleanup (removed many ifdef's, obsolete code, ran code through indent) Mostly 2.4 tree
+   - Split source tree (Now have a separate source tree for 2.2, 2.4, and jfsutils)  
+
+Function and Fixes in drop40 (1.0.2)
+   - Fixed multiple truncate hang
+   - Fixed hang on unlink a file and sync happening at the same time
+   - Improved handling of kmalloc error conditions
+   - Fixed hang in blk_get_queue and SMP deadlock: bh_end_io call generic_make_request
+     (jitterbug 145 and 146)
+   - stbl was not set correctly set in dtDelete  
+   - changed trap to printk in dbAllocAG to avoid system hang
+
+Function and Fixes in drop 39 (1.0.1)
+   - Fixed hang during copying files on 2.2.x series
+   - Fixed TxLock compile problem
+   - Fixed to correctly update the number of blocks for directories (this was causing the FS 
+     to show fsck error after compiling mozilla).
+   - Fixed to prevent old data from being written to disk from the page cache. 
+
+Function and Fixes in drop 38 (1.0.0)
+   - Fixed some general log problems   
+
+Please send bugs, comments, cards and letters to linuxjfs@us.ibm.com.
+
+The JFS mailing list can be subscribed to by using the link labeled "Mail list Subscribe"
+at our web page http://oss.software.ibm.com/jfs/.
+
+
+
+
+
+
+
+
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/Documentation/filesystems/jfs.txt linuxppc64_2_4/Documentation/filesystems/jfs.txt
--- linux-2.4.19/Documentation/filesystems/jfs.txt	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/Documentation/filesystems/jfs.txt	Tue Apr 23 11:14:24 2002
@@ -0,0 +1,10 @@
+IBM's Journaled File System (JFS) for Linux
+
+The JFS utilities can be found at the JFS homepage at
+http://oss.software.ibm.com/jfs
+
+Team members
+Steve Best         sbest@us.ibm.com
+Dave Kleikamp      shaggy@austin.ibm.com  
+Barry Arndt        barndt@us.ibm.com
+Christoph Hellwig  hch@infradead.org
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/Makefile linuxppc64_2_4/Makefile
--- linux-2.4.19/Makefile	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/Makefile	Mon Aug 19 09:15:51 2002
@@ -5,7 +5,8 @@
 
 KERNELRELEASE=$(VERSION).$(PATCHLEVEL).$(SUBLEVEL)$(EXTRAVERSION)
 
-ARCH := $(shell uname -m | sed -e s/i.86/i386/ -e s/sun4u/sparc64/ -e s/arm.*/arm/ -e s/sa110/arm/)
+#ARCH := $(shell uname -m | sed -e s/i.86/i386/ -e s/sun4u/sparc64/ -e s/arm.*/arm/ -e s/sa110/arm/)
+ARCH := ppc64
 KERNELPATH=kernel-$(shell echo $(KERNELRELEASE) | sed -e "s/-//g")
 
 CONFIG_SHELL := $(shell if [ -x "$$BASH" ]; then echo $$BASH; \
@@ -19,7 +20,7 @@
 HOSTCC  	= gcc
 HOSTCFLAGS	= -Wall -Wstrict-prototypes -O2 -fomit-frame-pointer
 
-CROSS_COMPILE 	=
+CROSS_COMPILE 	= /opt/cross/bin/powerpc64-linux-
 
 #
 # Include the make variables (CC, etc...)
@@ -37,13 +38,16 @@
 MAKEFILES	= $(TOPDIR)/.config
 GENKSYMS	= /sbin/genksyms
 DEPMOD		= /sbin/depmod
+KALLSYMS	= /sbin/kallsyms
 MODFLAGS	= -DMODULE
 CFLAGS_KERNEL	=
 PERL		= perl
+AWK		= awk
+TMPPREFIX	=
 
 export	VERSION PATCHLEVEL SUBLEVEL EXTRAVERSION KERNELRELEASE ARCH \
 	CONFIG_SHELL TOPDIR HPATH HOSTCC HOSTCFLAGS CROSS_COMPILE AS LD CC \
-	CPP AR NM STRIP OBJCOPY OBJDUMP MAKE MAKEFILES GENKSYMS MODFLAGS PERL
+	CPP AR NM STRIP OBJCOPY OBJDUMP MAKE MAKEFILES GENKSYMS MODFLAGS PERL AWK
 
 all:	do-it-all
 
@@ -87,6 +91,7 @@
 #
 
 CPPFLAGS := -D__KERNEL__ -I$(HPATH)
+CPPFLAGS += $(patsubst %,-I%,$(CROSS_COMPILE_INC))
 
 CFLAGS := $(CPPFLAGS) -Wall -Wstrict-prototypes -Wno-trigraphs -O2 \
 	  -fno-strict-aliasing -fno-common
@@ -127,6 +132,11 @@
 LIBS		=$(TOPDIR)/lib/lib.a
 SUBDIRS		=kernel drivers mm fs net ipc lib
 
+ifeq ($(CONFIG_KDB),y)
+CORE_FILES	+= kdb/kdb.o
+SUBDIRS		+= kdb
+endif
+
 DRIVERS-n :=
 DRIVERS-y :=
 DRIVERS-m :=
@@ -153,7 +163,9 @@
 DRIVERS-$(CONFIG_FC4) += drivers/fc4/fc4.a
 DRIVERS-$(CONFIG_SCSI) += drivers/scsi/scsidrv.o
 DRIVERS-$(CONFIG_FUSION_BOOT) += drivers/message/fusion/fusion.o
+DRIVERS-$(CONFIG_DUMP) += drivers/dump/dumpdrv.o
 DRIVERS-$(CONFIG_IEEE1394) += drivers/ieee1394/ieee1394drv.o
+DRIVERS-$(CONFIG_PPC_ISERIES) += drivers/iseries/iseries.o
 
 ifneq ($(CONFIG_CD_NO_IDESCSI)$(CONFIG_BLK_DEV_IDECD)$(CONFIG_BLK_DEV_SR)$(CONFIG_PARIDE_PCD),)
 DRIVERS-y += drivers/cdrom/driver.o
@@ -170,8 +182,8 @@
 DRIVERS-$(CONFIG_SBUS) += drivers/sbus/sbus_all.o
 DRIVERS-$(CONFIG_ZORRO) += drivers/zorro/driver.o
 DRIVERS-$(CONFIG_FC4) += drivers/fc4/fc4.a
-DRIVERS-$(CONFIG_PPC) += drivers/macintosh/macintosh.o
 DRIVERS-$(CONFIG_MAC) += drivers/macintosh/macintosh.o
+DRIVERS-$(CONFIG_PPC32) += drivers/macintosh/macintosh.o
 DRIVERS-$(CONFIG_ISAPNP) += drivers/pnp/pnp.o
 DRIVERS-$(CONFIG_SGI_IP22) += drivers/sgi/sgi.a
 DRIVERS-$(CONFIG_VT) += drivers/video/video.o
@@ -196,7 +208,7 @@
 CLEAN_FILES = \
 	kernel/ksyms.lst include/linux/compile.h \
 	vmlinux System.map \
-	.tmp* \
+	$(TMPPREFIX).tmp* Kerntypes \
 	drivers/char/consolemap_deftbl.c drivers/video/promcon_tbl.c \
 	drivers/char/conmakehash \
 	drivers/char/drm/*-mod.c \
@@ -239,6 +251,7 @@
 	scripts/lxdialog/*.o scripts/lxdialog/lxdialog \
 	.menuconfig.log \
 	include/asm \
+	kdb/gen-kdb_cmds.c \
 	.hdepend scripts/mkdep scripts/split-include scripts/docproc \
 	$(TOPDIR)/include/linux/modversions.h \
 	kernel.spec
@@ -276,16 +289,53 @@
 boot: vmlinux
 	@$(MAKE) CFLAGS="$(CFLAGS) $(CFLAGS_KERNEL)" -C arch/$(ARCH)/boot
 
-vmlinux: include/linux/version.h $(CONFIGURATION) init/main.o init/version.o init/do_mounts.o linuxsubdirs
-	$(LD) $(LINKFLAGS) $(HEAD) init/main.o init/version.o init/do_mounts.o \
-		--start-group \
-		$(CORE_FILES) \
-		$(DRIVERS) \
-		$(NETWORKS) \
-		$(LIBS) \
-		--end-group \
-		-o vmlinux
+LD_VMLINUX    := $(LD) $(LINKFLAGS) $(HEAD) init/main.o init/version.o init/do_mounts.o\
+                      --start-group \
+                      $(CORE_FILES) \
+                      $(DRIVERS) \
+                      $(NETWORKS) \
+                      $(LIBS) \
+                      --end-group
+ifeq ($(CONFIG_KALLSYMS),y)
+LD_VMLINUX_KALLSYMS   := $(TMPPREFIX).tmp_kallsyms3.o
+else
+LD_VMLINUX_KALLSYMS   :=
+endif
+
+
+vmlinux: include/linux/version.h $(CONFIGURATION) init/main.o init/version.o init/do_mounts.o Kerntypes linuxsubdirs
+	@$(MAKE) CFLAGS="$(CFLAGS) $(CFLAGS_KERNEL)" kallsyms
+
+.PHONY:       kallsyms
+
+kallsyms:
+ifeq ($(CONFIG_KALLSYMS),y)
+	@echo kallsyms pass 1
+	@if [ `/sbin/kallsyms -V  2>&1 | sed s/kallsyms\ version\ 2.4.//` -lt 17 ]; \
+	then \
+	  echo "WARNING! KDB requires modutils (kallsyms) version 2.4.17 or newer to work properly!"  ;\
+	  echo "WARNING! KDB requires modutils (kallsyms) version 2.4.17 or newer to work properly!"  ; fi 
+	$(LD_VMLINUX) -o $(TMPPREFIX).tmp_vmlinux1
+	@$(KALLSYMS) $(TMPPREFIX).tmp_vmlinux1 > $(TMPPREFIX).tmp_kallsyms1.o
+	@echo kallsyms pass 2
+	@$(LD_VMLINUX) $(TMPPREFIX).tmp_kallsyms1.o -o $(TMPPREFIX).tmp_vmlinux2
+	@$(KALLSYMS) $(TMPPREFIX).tmp_vmlinux2 > $(TMPPREFIX).tmp_kallsyms2.o
+	@echo kallsyms pass 3
+	@$(LD_VMLINUX) $(TMPPREFIX).tmp_kallsyms2.o -o $(TMPPREFIX).tmp_vmlinux3
+	@$(KALLSYMS) $(TMPPREFIX).tmp_vmlinux3 > $(TMPPREFIX).tmp_kallsyms3.o
+endif
+
+	$(LD_VMLINUX) $(LD_VMLINUX_KALLSYMS) -o $(TMPPREFIX)vmlinux
+ifneq ($(TMPPREFIX),)
+	mv $(TMPPREFIX)vmlinux vmlinux
+endif
 	$(NM) vmlinux | grep -v '\(compiled\)\|\(\.o$$\)\|\( [aUw] \)\|\(\.\.ng$$\)\|\(LASH[RL]DI\)' | sort > System.map
+	@rm -f $(TMPPREFIX).tmp_vmlinux* $(TMPPREFIX).tmp_kallsyms*
+
+Kerntypes: init/kerntypes.o
+	@if [ -f init/kerntypes.o ]; then \
+		mv init/kerntypes.o Kerntypes; \
+	fi
 
 symlinks:
 	rm -f include/asm
@@ -324,29 +374,25 @@
 	. scripts/mkversion > .tmpversion
 	@mv -f .tmpversion .version
 
-uts_len		:= 64
-uts_truncate	:= sed -e 's/\(.\{1,$(uts_len)\}\).*/\1/'
-
 include/linux/compile.h: $(CONFIGURATION) include/linux/version.h newversion
-	@echo -n \#`cat .version` > .ver1
-	@if [ -n "$(CONFIG_SMP)" ] ; then echo -n " SMP" >> .ver1; fi
-	@if [ -f .name ]; then  echo -n \-`cat .name` >> .ver1; fi
-	@LANG=C echo ' '`date` >> .ver1
-	@echo \#define UTS_VERSION \"`cat .ver1 | $(uts_truncate)`\" > .ver
-	@LANG=C echo \#define LINUX_COMPILE_TIME \"`date +%T`\" >> .ver
+	@echo -n \#define UTS_VERSION \"\#`cat .version` > .ver
+	@if [ -n "$(CONFIG_SMP)" ] ; then echo -n " SMP" >> .ver; fi
+	@if [ -f .name ]; then  echo -n \-`cat .name` >> .ver; fi
+	@echo ' '`date`'"' >> .ver
+	@echo \#define LINUX_COMPILE_TIME \"`date +%T`\" >> .ver
 	@echo \#define LINUX_COMPILE_BY \"`whoami`\" >> .ver
-	@echo \#define LINUX_COMPILE_HOST \"`hostname | $(uts_truncate)`\" >> .ver
-	@([ -x /bin/dnsdomainname ] && /bin/dnsdomainname > .ver1) || \
-	 ([ -x /bin/domainname ] && /bin/domainname > .ver1) || \
-	 echo > .ver1
-	@echo \#define LINUX_COMPILE_DOMAIN \"`cat .ver1 | $(uts_truncate)`\" >> .ver
+	@echo \#define LINUX_COMPILE_HOST \"`hostname`\" >> .ver
+	@if [ -x /bin/dnsdomainname ]; then \
+	   echo \#define LINUX_COMPILE_DOMAIN \"`dnsdomainname`\"; \
+	 elif [ -x /bin/domainname ]; then \
+	   echo \#define LINUX_COMPILE_DOMAIN \"`domainname`\"; \
+	 else \
+	   echo \#define LINUX_COMPILE_DOMAIN ; \
+	 fi >> .ver
 	@echo \#define LINUX_COMPILER \"`$(CC) $(CFLAGS) -v 2>&1 | tail -1`\" >> .ver
 	@mv -f .ver $@
-	@rm -f .ver1
 
 include/linux/version.h: ./Makefile
-	@expr length "$(KERNELRELEASE)" \<= $(uts_len) > /dev/null || \
-	  (echo KERNELRELEASE \"$(KERNELRELEASE)\" exceeds $(uts_len) characters >&2; false)
 	@echo \#define UTS_RELEASE \"$(KERNELRELEASE)\" > .ver
 	@echo \#define LINUX_VERSION_CODE `expr $(VERSION) \\* 65536 + $(PATCHLEVEL) \\* 256 + $(SUBLEVEL)` >> .ver
 	@echo '#define KERNEL_VERSION(a,b,c) (((a) << 16) + ((b) << 8) + (c))' >>.ver
@@ -357,11 +403,14 @@
 init/version.o: init/version.c include/linux/compile.h include/config/MARKER
 	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) -DUTS_MACHINE='"$(ARCH)"' -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -c -o init/version.o init/version.c
 
+init/kerntypes.o: init/kerntypes.c include/config/MARKER
+	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) $(PROFILING) -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -gstabs -c -o $*.o $<
+
 init/main.o: init/main.c include/config/MARKER
-	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) $(PROFILING) -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -c -o $@ $<
+	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) $(PROFILING) -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -c -o $*.o $<
 
 init/do_mounts.o: init/do_mounts.c include/config/MARKER
-	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) $(PROFILING) -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -c -o $@ $<
+	$(CC) $(CFLAGS) $(CFLAGS_KERNEL) $(PROFILING) -DKBUILD_BASENAME=$(subst $(comma),_,$(subst -,_,$(*F))) -c -o $*.o $<
 
 fs lib mm ipc kernel drivers net: dummy
 	$(MAKE) CFLAGS="$(CFLAGS) $(CFLAGS_KERNEL)" $(subst $@, _dir_$@, $@)
@@ -369,7 +418,7 @@
 TAGS: dummy
 	{ find include/asm-${ARCH} -name '*.h' -print ; \
 	find include -type d \( -name "asm-*" -o -name config \) -prune -o -name '*.h' -print ; \
-	find $(SUBDIRS) init arch/${ARCH} -name '*.[chS]' ; } | grep -v SCCS | etags -
+	find $(SUBDIRS) init -name '*.[ch]' ; } | grep -v SCCS | etags -
 
 # Exuberant ctags works better with -I
 tags: dummy
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/dec/ioasic-irq.c linuxppc64_2_4/arch/mips/dec/ioasic-irq.c
--- linux-2.4.19/arch/mips/dec/ioasic-irq.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/dec/ioasic-irq.c	Wed Dec 31 18:00:00 1969
@@ -1,160 +0,0 @@
-/*
- *	linux/arch/mips/dec/ioasic-irq.c
- *
- *	DEC I/O ASIC interrupts.
- *
- *	Copyright (c) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-
-#include <linux/init.h>
-#include <linux/irq.h>
-#include <linux/spinlock.h>
-#include <linux/types.h>
-
-#include <asm/dec/ioasic.h>
-#include <asm/dec/ioasic_addrs.h>
-#include <asm/dec/ioasic_ints.h>
-
-
-static spinlock_t ioasic_lock = SPIN_LOCK_UNLOCKED;
-
-static int ioasic_irq_base;
-
-
-static inline void unmask_ioasic_irq(unsigned int irq)
-{
-	u32 simr;
-
-	simr = ioasic_read(SIMR);
-	simr |= (1 << (irq - ioasic_irq_base));
-	ioasic_write(SIMR, simr);
-}
-
-static inline void mask_ioasic_irq(unsigned int irq)
-{
-	u32 simr;
-
-	simr = ioasic_read(SIMR);
-	simr &= ~(1 << (irq - ioasic_irq_base));
-	ioasic_write(SIMR, simr);
-}
-
-static inline void clear_ioasic_irq(unsigned int irq)
-{
-	u32 sir;
-
-	sir = ~(1 << (irq - ioasic_irq_base));
-	ioasic_write(SIR, sir);
-}
-
-static inline void enable_ioasic_irq(unsigned int irq)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ioasic_lock, flags);
-	unmask_ioasic_irq(irq);
-	spin_unlock_irqrestore(&ioasic_lock, flags);
-}
-
-static inline void disable_ioasic_irq(unsigned int irq)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ioasic_lock, flags);
-	mask_ioasic_irq(irq);
-	spin_unlock_irqrestore(&ioasic_lock, flags);
-}
-
-
-static inline unsigned int startup_ioasic_irq(unsigned int irq)
-{
-	enable_ioasic_irq(irq);
-	return 0;
-}
-
-#define shutdown_ioasic_irq disable_ioasic_irq
-
-static inline void ack_ioasic_irq(unsigned int irq)
-{
-	spin_lock(&ioasic_lock);
-	mask_ioasic_irq(irq);
-	spin_unlock(&ioasic_lock);
-}
-
-static inline void end_ioasic_irq(unsigned int irq)
-{
-	if (!(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS)))
-		enable_ioasic_irq(irq);
-}
-
-#define set_ioasic_affinity NULL
-
-static struct hw_interrupt_type ioasic_irq_type = {
-	"IO-ASIC",
-	startup_ioasic_irq,
-	shutdown_ioasic_irq,
-	enable_ioasic_irq,
-	disable_ioasic_irq,
-	ack_ioasic_irq,
-	end_ioasic_irq,
-	set_ioasic_affinity,
-};
-
-
-#define startup_ioasic_dma_irq startup_ioasic_irq
-
-#define shutdown_ioasic_dma_irq shutdown_ioasic_irq
-
-#define enable_ioasic_dma_irq enable_ioasic_irq
-
-#define disable_ioasic_dma_irq disable_ioasic_irq
-
-#define ack_ioasic_dma_irq ack_ioasic_irq
-
-static inline void end_ioasic_dma_irq(unsigned int irq)
-{
-	clear_ioasic_irq(irq);
-	end_ioasic_irq(irq);
-}
-
-#define set_ioasic_dma_affinity set_ioasic_affinity
-
-static struct hw_interrupt_type ioasic_dma_irq_type = {
-	"IO-ASIC-DMA",
-	startup_ioasic_dma_irq,
-	shutdown_ioasic_dma_irq,
-	enable_ioasic_dma_irq,
-	disable_ioasic_dma_irq,
-	ack_ioasic_dma_irq,
-	end_ioasic_dma_irq,
-	set_ioasic_dma_affinity,
-};
-
-
-void __init init_ioasic_irqs(int base)
-{
-	int i;
-
-	/* Mask interrupts. */
-	ioasic_write(SIMR, 0);
-
-	for (i = base; i < base + IO_INR_DMA; i++) {
-		irq_desc[i].status = IRQ_DISABLED;
-		irq_desc[i].action = 0;
-		irq_desc[i].depth = 1;
-		irq_desc[i].handler = &ioasic_irq_type;
-	}
-	for (; i < base + IO_IRQ_LINES; i++) {
-		irq_desc[i].status = IRQ_DISABLED;
-		irq_desc[i].action = 0;
-		irq_desc[i].depth = 1;
-		irq_desc[i].handler = &ioasic_dma_irq_type;
-	}
-
-	ioasic_irq_base = base;
-}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/dec/kn02-irq.c linuxppc64_2_4/arch/mips/dec/kn02-irq.c
--- linux-2.4.19/arch/mips/dec/kn02-irq.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/dec/kn02-irq.c	Wed Dec 31 18:00:00 1969
@@ -1,125 +0,0 @@
-/*
- *	linux/arch/mips/dec/kn02-irq.c
- *
- *	DECstation 5000/200 (KN02) Control and Status Register
- *	interrupts.
- *
- *	Copyright (c) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-
-#include <linux/init.h>
-#include <linux/irq.h>
-#include <linux/spinlock.h>
-#include <linux/types.h>
-
-#include <asm/dec/kn02.h>
-
-
-/*
- * Bits 7:0 of the Control Register are write-only -- the
- * corresponding bits of the Status Register have a different
- * meaning.  Hence we use a cache.  It speeds up things a bit
- * as well.
- *
- * There is no default value -- it has to be initialized.
- */
-u32 cached_kn02_csr;
-spinlock_t kn02_lock = SPIN_LOCK_UNLOCKED;
-
-
-static int kn02_irq_base;
-
-
-static inline void unmask_kn02_irq(unsigned int irq)
-{
-	volatile u32 *csr = (volatile u32 *)KN02_CSR_ADDR;
-
-	cached_kn02_csr |= (1 << (irq - kn02_irq_base + 16));
-	*csr = cached_kn02_csr;
-}
-
-static inline void mask_kn02_irq(unsigned int irq)
-{
-	volatile u32 *csr = (volatile u32 *)KN02_CSR_ADDR;
-
-	cached_kn02_csr &= ~(1 << (irq - kn02_irq_base + 16));
-	*csr = cached_kn02_csr;
-}
-
-static inline void enable_kn02_irq(unsigned int irq)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&kn02_lock, flags);
-	unmask_kn02_irq(irq);
-	spin_unlock_irqrestore(&kn02_lock, flags);
-}
-
-static inline void disable_kn02_irq(unsigned int irq)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&kn02_lock, flags);
-	mask_kn02_irq(irq);
-	spin_unlock_irqrestore(&kn02_lock, flags);
-}
-
-
-static unsigned int startup_kn02_irq(unsigned int irq)
-{
-	enable_kn02_irq(irq);
-	return 0;
-}
-
-#define shutdown_kn02_irq disable_kn02_irq
-
-static void ack_kn02_irq(unsigned int irq)
-{
-	spin_lock(&kn02_lock);
-	mask_kn02_irq(irq);
-	spin_unlock(&kn02_lock);
-}
-
-static void end_kn02_irq(unsigned int irq)
-{
-	if (!(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS)))
-		enable_kn02_irq(irq);
-}
-
-#define set_kn02_affinity NULL
-
-static struct hw_interrupt_type kn02_irq_type = {
-	"KN02-CSR",
-	startup_kn02_irq,
-	shutdown_kn02_irq,
-	enable_kn02_irq,
-	disable_kn02_irq,
-	ack_kn02_irq,
-	end_kn02_irq,
-	set_kn02_affinity,
-};
-
-
-void __init init_kn02_irqs(int base)
-{
-	volatile u32 *csr = (volatile u32 *)KN02_CSR_ADDR;
-	int i;
-
-	/* Mask interrupts and preset write-only bits. */
-	cached_kn02_csr = (*csr & ~0xff0000) | 0xff;
-	*csr = cached_kn02_csr;
-
-	for (i = base; i < base + KN02_IRQ_LINES; i++) {
-		irq_desc[i].status = IRQ_DISABLED;
-		irq_desc[i].action = 0;
-		irq_desc[i].depth = 1;
-		irq_desc[i].handler = &kn02_irq_type;
-	}
-
-	kn02_irq_base = base;
-}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/sgi-ip22/ip22-berr.c linuxppc64_2_4/arch/mips/sgi-ip22/ip22-berr.c
--- linux-2.4.19/arch/mips/sgi-ip22/ip22-berr.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/sgi-ip22/ip22-berr.c	Wed Dec 31 18:00:00 1969
@@ -1,78 +0,0 @@
-/*
- * ip22-berr.c: Bus error handling.
- *
- * Copyright (C) 2002 Ladislav Michl
- */
-
-#include <linux/init.h>
-#include <linux/kernel.h>
-#include <linux/sched.h>
-
-#include <asm/addrspace.h>
-#include <asm/system.h>
-#include <asm/traps.h>
-#include <asm/branch.h>
-#include <asm/sgi/sgimc.h>
-#include <asm/sgi/sgihpc.h>
-
-unsigned int cpu_err_stat;	/* Status reg for CPU */
-unsigned int gio_err_stat;	/* Status reg for GIO */
-unsigned int cpu_err_addr;	/* Error address reg for CPU */
-unsigned int gio_err_addr;	/* Error address reg for GIO */
-
-volatile int nofault;
-
-static void save_and_clear_buserr(void)
-{
-	/* save memory controler's error status registers */ 
-	cpu_err_addr = mcmisc_regs->cerr;
-	cpu_err_stat = mcmisc_regs->cstat;
-	gio_err_addr = mcmisc_regs->gerr;
-	gio_err_stat = mcmisc_regs->gstat;
-
-	mcmisc_regs->cstat = mcmisc_regs->gstat = 0;
-}
-
-/*
- * MC sends an interrupt whenever bus or parity errors occur. In addition, 
- * if the error happened during a CPU read, it also asserts the bus error 
- * pin on the R4K. Code in bus error handler save the MC bus error registers
- * and then clear the interrupt when this happens.
- */
-
-void be_ip22_interrupt(int irq, struct pt_regs *regs)
-{
-	save_and_clear_buserr();
-	printk(KERN_ALERT "Bus error, epc == %08lx, ra == %08lx\n",
-	       regs->cp0_epc, regs->regs[31]);
-	die_if_kernel("Oops", regs);
-	force_sig(SIGBUS, current);
-}
-
-int be_ip22_handler(struct pt_regs *regs, int is_fixup)
-{
-	save_and_clear_buserr();
-	if (nofault) {
-		nofault = 0;
-		compute_return_epc(regs);
-		return MIPS_BE_DISCARD;
-	}
-	return MIPS_BE_FIXUP;
-}
-
-int ip22_baddr(unsigned int *val, unsigned long addr)
-{
-	nofault = 1;
-	*val = *(volatile unsigned int *) addr;
-	__asm__ __volatile__("nop;nop;nop;nop");
-	if (nofault) {
-		nofault = 0;
-		return 0;
-	}
-	return -EFAULT;
-}
-
-void __init bus_error_init(void)
-{
-	be_board_handler = be_ip22_handler;
-}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/sgi-ip22/ip22-gio.c linuxppc64_2_4/arch/mips/sgi-ip22/ip22-gio.c
--- linux-2.4.19/arch/mips/sgi-ip22/ip22-gio.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/sgi-ip22/ip22-gio.c	Wed Dec 31 18:00:00 1969
@@ -1,152 +0,0 @@
-/* 
- * ip22-gio.c: Support for GIO64 bus (inspired by PCI code)
- * 
- * Copyright (C) 2002 Ladislav Michl
- */
-
-#include <linux/kernel.h>
-#include <linux/types.h>
-#include <linux/slab.h>
-#include <linux/init.h>
-#include <linux/proc_fs.h>
-
-#include <asm/sgi/sgimc.h>
-#include <asm/sgi/sgigio.h>
-
-#define GIO_PIO_MAP_BASE	0x1f000000L
-#define GIO_PIO_MAP_SIZE	(16 * 1024*1024)
-
-#define GIO_ADDR_GFX		0x1f000000L
-#define GIO_ADDR_GIO1		0x1f400000L
-#define GIO_ADDR_GIO2		0x1f600000L
-
-#define GIO_GFX_MAP_SIZE	(4 * 1024*1024)
-#define GIO_GIO1_MAP_SIZE	(2 * 1024*1024)
-#define GIO_GIO2_MAP_SIZE	(4 * 1024*1024)
-
-#define GIO_NO_DEVICE		0x80
-
-static struct gio_dev gio_slot[GIO_NUM_SLOTS] = {
-	{
-		0,
-		0,
-		0,
-		GIO_NO_DEVICE,
-		GIO_SLOT_GFX,
-		GIO_ADDR_GFX,
-		GIO_GFX_MAP_SIZE,
-		NULL,
-		"GFX"
-	},
-	{
-		0,
-		0,
-		0,
-		GIO_NO_DEVICE,
-		GIO_SLOT_GIO1,
-		GIO_ADDR_GIO1,
-		GIO_GIO1_MAP_SIZE,
-		NULL,
-		"EXP0"
-	},
-	{
-		0,
-		0,
-		0,
-		GIO_NO_DEVICE,
-		GIO_SLOT_GIO2,
-		GIO_ADDR_GIO2,
-		GIO_GIO2_MAP_SIZE,
-		NULL,
-		"EXP1"
-	}
-};
-
-static int gio_read_proc(char *buf, char **start, off_t off,
-			 int count, int *eof, void *data)
-{
-	int i;
-	char *p = buf;
-	
-	p += sprintf(p, "GIO devices found:\n");
-	for (i = 0; i < GIO_NUM_SLOTS; i++) {
-		if (gio_slot[i].flags & GIO_NO_DEVICE)
-			continue;
-		p += sprintf(p, "  Slot %s, DeviceId 0x%02x\n",
-			     gio_slot[i].slot_name, gio_slot[i].device);
-		p += sprintf(p, "    BaseAddr 0x%08lx, MapSize 0x%08x\n",
-			     gio_slot[i].base_addr, gio_slot[i].map_size);
-	}
-	
-	return p - buf;
-}
-
-void create_gio_proc_entry(void)
-{
-	create_proc_read_entry("gio", 0, NULL, gio_read_proc, NULL);
-}
-
-/**
- * gio_find_device - begin or continue searching for a GIO device by device id
- * @device: GIO device id to match, or %GIO_ANY_ID to match all device ids
- * @from: Previous GIO device found in search, or %NULL for new search.
- *
- * Iterates through the list of known GIO devices. If a GIO device is found
- * with a matching @device, a pointer to its device structure is returned. 
- * Otherwise, %NULL is returned.
- * A new search is initiated by passing %NULL to the @from argument.
- * Otherwise if @from is not %NULL, searches continue from next device.
- */
-struct gio_dev *
-gio_find_device(unsigned char device, const struct gio_dev *from)
-{
-	int i;
-	
-	for (i = (from) ? from->slot_number : 0; i < GIO_NUM_SLOTS; i++)
-		if (!(gio_slot[i].flags & GIO_NO_DEVICE) && 
-		   (device == GIO_ANY_ID || device == gio_slot[i].device))
-			return &gio_slot[i];
-	
-	return NULL;
-}
-
-#define GIO_IDCODE(x)		(x & 0x7f)
-#define GIO_ALL_BITS_VALID	0x80
-#define GIO_REV(x)		((x >> 8) & 0xff)
-#define GIO_GIO_SIZE_64		0x10000
-#define GIO_ROM_PRESENT		0x20000
-#define GIO_VENDOR_CODE(x)	((x >> 18) & 0x3fff)
-
-extern int ip22_baddr(unsigned int *val, unsigned long addr);
-
-/** 
- * sgigio_init - scan the GIO space and figure out what hardware is actually
- * present.
- */
-void __init sgigio_init(void)
-{
-	unsigned int i, id, found = 0;
-
-	printk("GIO: Scanning for GIO cards...\n");
-	for (i = 0; i < GIO_NUM_SLOTS; i++) {
-		if (ip22_baddr(&id, KSEG1ADDR(gio_slot[i].base_addr)))
-			continue;
-
-		found = 1;
-		gio_slot[i].device = GIO_IDCODE(id);
-		if (id & GIO_ALL_BITS_VALID) {
-			gio_slot[i].revision = GIO_REV(id);
-			gio_slot[i].vendor = GIO_VENDOR_CODE(id);
-			gio_slot[i].flags =
-				(id & GIO_GIO_SIZE_64) ? GIO_IFACE_64 : 0 |
-				(id & GIO_ROM_PRESENT) ? GIO_HAS_ROM : 0;
-		} else
-			gio_slot[i].flags = GIO_VALID_ID_ONLY;
-
-		printk("GIO: Card 0x%02x @ 0x%08lx\n", gio_slot[i].device,
-			gio_slot[i].base_addr);
-	}
-	
-	if (!found)
-		printk("GIO: No GIO cards present.\n");
-}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/sibyte/swarm/dbg_io.c linuxppc64_2_4/arch/mips/sibyte/swarm/dbg_io.c
--- linux-2.4.19/arch/mips/sibyte/swarm/dbg_io.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/sibyte/swarm/dbg_io.c	Wed Dec 31 18:00:00 1969
@@ -1,78 +0,0 @@
-/*
- * kgdb debug routines for swarm board.
- *
- * Copyright (C) 2001 MontaVista Software Inc.
- * Author: Jun Sun, jsun@mvista.com or jsun@junsun.net
- *
- * This program is free software; you can redistribute  it and/or modify it
- * under  the terms of  the GNU General  Public License as published by the
- * Free Software Foundation;  either version 2 of the  License, or (at your
- * option) any later version.
- *
- */
-
-/* -------------------- BEGINNING OF CONFIG --------------------- */
-
-#include <linux/delay.h>
-#include <asm/sibyte/sb1250.h>
-#include <asm/sibyte/sb1250_regs.h>
-#include <asm/sibyte/sb1250_uart.h>
-#include <asm/sibyte/sb1250_int.h>
-#include <asm/sibyte/64bit.h>
-#include <asm/addrspace.h>
-
-/*
- * We use the second serial port for kgdb traffic.
- * 	115200, 8, N, 1.
- */
-
-#define	BAUD_RATE		115200
-#define	CLK_DIVISOR		V_DUART_BAUD_RATE(BAUD_RATE)
-#define	DATA_BITS		V_DUART_BITS_PER_CHAR_8		/* or 7    */
-#define	PARITY			V_DUART_PARITY_MODE_NONE	/* or even */
-#define	STOP_BITS		M_DUART_STOP_BIT_LEN_1		/* or 2    */
-
-static int duart_initialized = 0;	/* 0: need to be init'ed by kgdb */
-
-/* -------------------- END OF CONFIG --------------------- */
-
-
-#define	duart_out(reg, val)	out64(val, KSEG1 + A_DUART_CHANREG(1,reg))
-#define duart_in(reg)		in64(KSEG1 + A_DUART_CHANREG(1,reg))
-
-extern void set_async_breakpoint(unsigned int epc);
-
-void putDebugChar(unsigned char c);
-unsigned char getDebugChar(void);
-static void
-duart_init(int clk_divisor, int data, int parity, int stop)
-{
-	duart_out(R_DUART_MODE_REG_1, data | parity);
-	duart_out(R_DUART_MODE_REG_2, stop);
-	duart_out(R_DUART_CLK_SEL, clk_divisor);
-
-	duart_out(R_DUART_CMD, M_DUART_RX_EN | M_DUART_TX_EN);	/* enable rx and tx */
-}
-
-void
-putDebugChar(unsigned char c)
-{
-	if (!duart_initialized) {
-		duart_initialized = 1;
-		duart_init(CLK_DIVISOR, DATA_BITS, PARITY, STOP_BITS);
-	}
-	while ((duart_in(R_DUART_STATUS) & M_DUART_TX_RDY) == 0);
-	duart_out(R_DUART_TX_HOLD, c);
-}
-
-unsigned char
-getDebugChar(void)
-{
-	if (!duart_initialized) {
-		duart_initialized = 1;
-		duart_init(CLK_DIVISOR, DATA_BITS, PARITY, STOP_BITS);
-	}
-	while ((duart_in(R_DUART_STATUS) & M_DUART_RX_RDY) == 0) ;
-	return duart_in(R_DUART_RX_HOLD);
-}
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/mips/sibyte/swarm/rtc_xicor1241.c linuxppc64_2_4/arch/mips/sibyte/swarm/rtc_xicor1241.c
--- linux-2.4.19/arch/mips/sibyte/swarm/rtc_xicor1241.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/mips/sibyte/swarm/rtc_xicor1241.c	Wed Dec 31 18:00:00 1969
@@ -1,202 +0,0 @@
-/*
- * Copyright (C) 2000, 2001 Broadcom Corporation
- *
- * Copyright (C) 2002 MontaVista Software Inc.
- * Author: jsun@mvista.com or jsun@junsun.net
- *
- * This program is free software; you can redistribute  it and/or modify it
- * under  the terms of  the GNU General  Public License as published by the
- * Free Software Foundation;  either version 2 of the  License, or (at your
- * option) any later version.
- *
- */
-
-#include <linux/types.h>
-#include <linux/time.h>
-
-#include <asm/time.h>
-#include <asm/addrspace.h>
-
-#include <asm/sibyte/64bit.h>
-#include <asm/sibyte/sb1250.h>
-#include <asm/sibyte/sb1250_regs.h>
-#include <asm/sibyte/sb1250_smbus.h>
-
-
-/* Xicor 1241 definitions */
-
-/*
- * Register bits
- */
-
-#define X1241REG_SR_BAT	0x80		/* currently on battery power */
-#define X1241REG_SR_RWEL 0x04		/* r/w latch is enabled, can write RTC */
-#define X1241REG_SR_WEL 0x02		/* r/w latch is unlocked, can enable r/w now */
-#define X1241REG_SR_RTCF 0x01		/* clock failed */
-#define X1241REG_BL_BP2 0x80		/* block protect 2 */
-#define X1241REG_BL_BP1 0x40		/* block protect 1 */
-#define X1241REG_BL_BP0 0x20		/* block protect 0 */
-#define X1241REG_BL_WD1	0x10
-#define X1241REG_BL_WD0	0x08
-#define X1241REG_HR_MIL 0x80		/* military time format */
-
-/*
- * Register numbers
- */
-
-#define X1241REG_BL	0x10		/* block protect bits */
-#define X1241REG_INT	0x11		/*  */
-#define X1241REG_SC	0x30		/* Seconds */
-#define X1241REG_MN	0x31		/* Minutes */
-#define X1241REG_HR	0x32		/* Hours */
-#define X1241REG_DT	0x33		/* Day of month */
-#define X1241REG_MO	0x34		/* Month */
-#define X1241REG_YR	0x35		/* Year */
-#define X1241REG_DW	0x36		/* Day of Week */
-#define X1241REG_Y2K	0x37		/* Year 2K */
-#define X1241REG_SR	0x3F		/* Status register */
-
-#define X1241_CCR_ADDRESS	0x6F
-
-#define SMB_CSR(reg) (KSEG1 | A_SMB_REGISTER(1, reg))
-
-static int xicor_read(uint8_t addr)
-{
-        while (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_BUSY)
-                ;
-
-	out64((addr >> 8) & 0x7, SMB_CSR(R_SMB_CMD));
-	out64((addr & 0xff), SMB_CSR(R_SMB_DATA));
-	out64((V_SMB_ADDR(X1241_CCR_ADDRESS) | V_SMB_TT_WR2BYTE), SMB_CSR(R_SMB_START));
-
-        while (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_BUSY)
-                ;
-
-	out64((V_SMB_ADDR(X1241_CCR_ADDRESS) | V_SMB_TT_RD1BYTE), SMB_CSR(R_SMB_START));
-
-        while (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_BUSY)
-                ;
-
-        if (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_ERROR) {
-                /* Clear error bit by writing a 1 */
-                out64(M_SMB_ERROR, SMB_CSR(R_SMB_STATUS));
-                return -1;
-        }
-
-	return (in64(SMB_CSR(R_SMB_DATA)) & 0xff);
-}
-
-static int xicor_write(uint8_t addr, int b)
-{
-        while (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_BUSY)
-                ;
-
-	out64(addr, SMB_CSR(R_SMB_CMD));
-	out64((addr & 0xff) | ((b & 0xff) << 8), SMB_CSR(R_SMB_DATA));
-	out64(V_SMB_ADDR(X1241_CCR_ADDRESS) | V_SMB_TT_WR3BYTE,
-	      SMB_CSR(R_SMB_START));
-
-        while (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_BUSY)
-                ;
-
-        if (in64(SMB_CSR(R_SMB_STATUS)) & M_SMB_ERROR) {
-                /* Clear error bit by writing a 1 */
-                out64(M_SMB_ERROR, SMB_CSR(R_SMB_STATUS));
-                return -1;
-        } else {
-		return 0;
-	}
-}
-
-#define BCD_TO_BIN(val) ((val)=((val)&15) + ((val)>>4)*10)
-#define BIN_TO_BCD(val) ((val)=(((val)/10)<<4) + (val)%10)
-
-int xicor_set_time(unsigned long t)
-{
-	struct rtc_time tm;
-	int tmp;
-
-	to_tm(t, &tm);
-
-	/* unlock writes to the CCR */
-	xicor_write(X1241REG_SR, X1241REG_SR_WEL);
-	xicor_write(X1241REG_SR, X1241REG_SR_WEL | X1241REG_SR_RWEL);
-
-	/* trivial ones */
-	BIN_TO_BCD(tm.tm_sec);
-	xicor_write(X1241REG_SC, tm.tm_sec);
-	
-	BIN_TO_BCD(tm.tm_min);
-	xicor_write(X1241REG_MN, tm.tm_min);
-
-	BIN_TO_BCD(tm.tm_mday);
-	xicor_write(X1241REG_DT, tm.tm_mday);
-
-	/* tm_mon starts from 0, *ick* */
-	tm.tm_mon ++;
-	BIN_TO_BCD(tm.tm_mon);
-	xicor_write(X1241REG_MO, tm.tm_mon);
-
-	/* year is split */
-	tmp = tm.tm_year / 100;
-	tm.tm_year %= 100;
-	xicor_write(X1241REG_YR, tm.tm_year);
-	xicor_write(X1241REG_Y2K, tmp);
-
-	/* hour is the most tricky one */
-	tmp = xicor_read(X1241REG_HR);
-	if (tmp & X1241REG_HR_MIL) {
-		/* 24 hour format */
-		BIN_TO_BCD(tm.tm_hour);
-		tmp = (tmp & ~0x3f) | (tm.tm_hour & 0x3f);
-	} else {
-		/* 12 hour format, with 0x2 for pm */
-		tmp = tmp & ~0x3f;
-		if (tm.tm_hour >= 12) {
-			tmp |= 0x20;
-			tm.tm_hour -= 12;
-		}
-		BIN_TO_BCD(tm.tm_hour);
-		tmp |= tm.tm_hour;
-	}
-	xicor_write(X1241REG_HR, tmp);
-
-	xicor_write(X1241REG_SR, 0);
-
-	return 0;
-}
-
-unsigned long xicor_get_time(void)
-{
-	unsigned int year, mon, day, hour, min, sec, y2k;
-
-	sec = xicor_read(X1241REG_SC);
-	min = xicor_read(X1241REG_MN);
-	hour = xicor_read(X1241REG_HR);
-
-	if (hour & X1241REG_HR_MIL) {
-		hour &= 0x3f;
-	} else {
-		if (hour & 0x20)
-			hour = (hour & 0xf) + 0x12;
-	}
-
-	BCD_TO_BIN(sec);
-	BCD_TO_BIN(min);
-	BCD_TO_BIN(hour);
-
-	day = xicor_read(X1241REG_DT);
-	mon = xicor_read(X1241REG_MO);
-	year = xicor_read(X1241REG_YR);
-	y2k = xicor_read(X1241REG_Y2K);
- 
-	BCD_TO_BIN(day);
-	BCD_TO_BIN(mon);
-	BCD_TO_BIN(year);
-	BCD_TO_BIN(y2k);
-
-	year += (y2k * 100);
-
-	return mktime(year, mon, day, hour, min, sec);
-}
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/Makefile linuxppc64_2_4/arch/ppc64/Makefile
--- linux-2.4.19/arch/ppc64/Makefile	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/Makefile	Mon Jul 29 11:15:40 2002
@@ -22,7 +22,8 @@
 LINKFLAGS	= -T arch/ppc64/vmlinux.lds -Bstatic \
 		-e $(KERNELLOAD) -Ttext $(KERNELLOAD)
 CFLAGS		:= $(CFLAGS) -fsigned-char -msoft-float -pipe \
-		-Wno-uninitialized -mminimal-toc -fno-builtin
+		-Wno-uninitialized -mminimal-toc -fno-builtin \
+		-mtraceback=full
 CPP		= $(CC) -E $(CFLAGS)
 
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/boot/addRamDisk.c linuxppc64_2_4/arch/ppc64/boot/addRamDisk.c
--- linux-2.4.19/arch/ppc64/boot/addRamDisk.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/boot/addRamDisk.c	Thu May 30 09:04:06 2002
@@ -25,7 +25,7 @@
 
 void death(const char *msg, FILE *fdesc, const char *fname) 
 {
-	printf(msg);
+	fprintf(stderr, msg);
 	fclose(fdesc);
 	unlink(fname);
 	exit(1);
@@ -66,47 +66,47 @@
   
   
 	if (argc < 2) {
-		printf("Name of RAM disk file missing.\n");
+		fprintf(stderr, "Name of RAM disk file missing.\n");
 		exit(1);
 	}
 
 	if (argc < 3) {
-		printf("Name of System Map input file is missing.\n");
+		fprintf(stderr, "Name of System Map input file is missing.\n");
 		exit(1);
 	}
   
 	if (argc < 4) {
-		printf("Name of vmlinux file missing.\n");
+		fprintf(stderr, "Name of vmlinux file missing.\n");
 		exit(1);
 	}
 
 	if (argc < 5) {
-		printf("Name of vmlinux output file missing.\n");
+		fprintf(stderr, "Name of vmlinux output file missing.\n");
 		exit(1);
 	}
 
 
 	ramDisk = fopen(argv[1], "r");
 	if ( ! ramDisk ) {
-		printf("RAM disk file \"%s\" failed to open.\n", argv[1]);
+		fprintf(stderr, "RAM disk file \"%s\" failed to open.\n", argv[1]);
 		exit(1);
 	}
 
 	sysmap = fopen(argv[2], "r");
 	if ( ! sysmap ) {
-		printf("System Map file \"%s\" failed to open.\n", argv[2]);
+		fprintf(stderr, "System Map file \"%s\" failed to open.\n", argv[2]);
 		exit(1);
 	}
   
 	inputVmlinux = fopen(argv[3], "r");
 	if ( ! inputVmlinux ) {
-		printf("vmlinux file \"%s\" failed to open.\n", argv[3]);
+		fprintf(stderr, "vmlinux file \"%s\" failed to open.\n", argv[3]);
 		exit(1);
 	}
   
 	outputVmlinux = fopen(argv[4], "w+");
 	if ( ! outputVmlinux ) {
-		printf("output vmlinux file \"%s\" failed to open.\n", argv[4]);
+		fprintf(stderr, "output vmlinux file \"%s\" failed to open.\n", argv[4]);
 		exit(1);
 	}
   
@@ -118,7 +118,7 @@
 	fseek(inputVmlinux, 0, SEEK_SET);
 	printf("kernel file size = %d\n", kernelLen);
 	if ( kernelLen == 0 ) {
-		printf("You must have a linux kernel specified as argv[3]\n");
+		fprintf(stderr, "You must have a linux kernel specified as argv[3]\n");
 		exit(1);
 	}
 
@@ -160,9 +160,9 @@
 	/* search for _end in the last page of the system map */
 	ptr_end = strstr(inbuf, " _end");
 	if (!ptr_end) {
-		printf("Unable to find _end in the sysmap file \n");
-		printf("inbuf: \n");
-		printf("%s \n", inbuf);
+		fprintf(stderr, "Unable to find _end in the sysmap file \n");
+		fprintf(stderr, "inbuf: \n");
+		fprintf(stderr, "%s \n", inbuf);
 		exit(1);
 	}
 	printf("Found _end in the last page of the sysmap - backing up 10 characters it looks like %s", ptr_end-10);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/boot/addSystemMap.c linuxppc64_2_4/arch/ppc64/boot/addSystemMap.c
--- linux-2.4.19/arch/ppc64/boot/addSystemMap.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/boot/addSystemMap.c	Thu May 30 09:04:06 2002
@@ -64,38 +64,38 @@
 	long padPages = 0;
 	if ( argc < 2 )
 	{
-		printf("Name of System Map file missing.\n");
+		fprintf(stderr, "Name of System Map file missing.\n");
 		exit(1);
 	}
 
 	if ( argc < 3 )
 	{
-		printf("Name of vmlinux file missing.\n");
+		fprintf(stderr, "Name of vmlinux file missing.\n");
 		exit(1);
 	}
 
 	if ( argc < 4 )
 	{
-		printf("Name of vmlinux output file missing.\n");
+		fprintf(stderr, "Name of vmlinux output file missing.\n");
 		exit(1);
 	}
 
 	sysmap = fopen(argv[1], "r");
 	if ( ! sysmap )
 	{
-		printf("System Map file \"%s\" failed to open.\n", argv[1]);
+		fprintf(stderr, "System Map file \"%s\" failed to open.\n", argv[1]);
 		exit(1);
 	}
 	inputVmlinux = fopen(argv[2], "r");
 	if ( ! inputVmlinux )
 	{
-		printf("vmlinux file \"%s\" failed to open.\n", argv[2]);
+		fprintf(stderr, "vmlinux file \"%s\" failed to open.\n", argv[2]);
 		exit(1);
 	}
 	outputVmlinux = fopen(argv[3], "w");
 	if ( ! outputVmlinux )
 	{
-		printf("output vmlinux file \"%s\" failed to open.\n", argv[3]);
+		fprintf(stderr, "output vmlinux file \"%s\" failed to open.\n", argv[3]);
 		exit(1);
 	}
 
@@ -107,7 +107,7 @@
 	printf("kernel file size = %ld\n", kernelLen);
 	if ( kernelLen == 0 )
 	{
-		printf("You must have a linux kernel specified as argv[2]\n");
+		fprintf(stderr, "You must have a linux kernel specified as argv[2]\n");
 		exit(1);
 	}
 
@@ -154,9 +154,9 @@
 	ptr_end = strstr(inbuf, " _end");
 	if (!ptr_end)
 	{
-		printf("Unable to find _end in the sysmap file \n");
-		printf("inbuf: \n");
-		printf("%s \n", inbuf);
+		fprintf(stderr, "Unable to find _end in the sysmap file \n");
+		fprintf(stderr, "inbuf: \n");
+		fprintf(stderr, "%s \n", inbuf);
 		exit(1);
 	}
 	printf("Found _end in the last page of the sysmap - backing up 10 characters it looks like %s", ptr_end-10);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/boot/zImage.c linuxppc64_2_4/arch/ppc64/boot/zImage.c
--- linux-2.4.19/arch/ppc64/boot/zImage.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/boot/zImage.c	Tue Apr 30 09:19:01 2002
@@ -225,7 +225,7 @@
 
 	rec = bi_rec_alloc(rec, 2);
 	rec->tag = BI_MACHTYPE;
-	rec->data[0] = _MACH_pSeries;
+	rec->data[0] = PLATFORM_PSERIES;
 	rec->data[1] = 1;
 
 	if ( initrd.size > 0 ) {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/config.in linuxppc64_2_4/arch/ppc64/config.in
--- linux-2.4.19/arch/ppc64/config.in	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/config.in	Mon Aug 19 09:15:52 2002
@@ -37,6 +37,7 @@
    define_bool CONFIG_MSCHUNKS y
 else
 bool 'MsChunks Physical to Absolute address translation support' CONFIG_MSCHUNKS
+tristate 'Firmware flash interface' CONFIG_RTAS_FLASH
 fi
 
 
@@ -59,6 +60,7 @@
 define_bool CONFIG_MCA n
 define_bool CONFIG_EISA n
 define_bool CONFIG_PCI y
+define_bool CONFIG_PCMCIA n
 
 bool 'Networking support' CONFIG_NET
 bool 'Sysctl support' CONFIG_SYSCTL
@@ -70,9 +72,9 @@
    define_bool CONFIG_KCORE_ELF y
 fi
 
-bool 'Kernel Support for 64 bit ELF binaries' CONFIG_BINFMT_ELF
+bool 'Kernel support for 64 bit ELF binaries' CONFIG_BINFMT_ELF
 
-tristate 'Kernel support for 32 bit binaries' CONFIG_BINFMT_ELF32
+tristate 'Kernel support for 32 bit ELF binaries' CONFIG_BINFMT_ELF32
 
 tristate 'Kernel support for MISC binaries' CONFIG_BINFMT_MISC
 
@@ -80,12 +82,6 @@
 
 bool 'Support for hot-pluggable devices' CONFIG_HOTPLUG
 
-if [ "$CONFIG_HOTPLUG" = "y" ]; then
-   source drivers/pcmcia/Config.in
-else
-   define_bool CONFIG_PCMCIA n
-fi
-
 source drivers/parport/Config.in
 
 if [ "$CONFIG_PPC_ISERIES" != "y" ]; then
@@ -93,12 +89,15 @@
    bool 'Support for frame buffer devices' CONFIG_FB
 
    bool 'Support for Open Firmware device tree in /proc' CONFIG_PROC_DEVICETREE
+
+   bool 'Default bootloader kernel arguments' CONFIG_CMDLINE_BOOL
+   if [ "$CONFIG_CMDLINE_BOOL" = "y" ] ; then
+     string 'Initial kernel command string' CONFIG_CMDLINE "console=ttyS0,9600 console=tty0 root=/dev/sda2"
+   fi
 fi
 
 endmenu
 
-source drivers/mtd/Config.in
-source drivers/pnp/Config.in
 source drivers/block/Config.in
 source drivers/md/Config.in
 
@@ -145,8 +144,6 @@
 
 source net/ax25/Config.in
 
-source net/irda/Config.in
-
 mainmenu_option next_comment
 comment 'ISDN subsystem'
 
@@ -165,12 +162,14 @@
 fi
 endmenu
 
+if [ "$CONFIG_PPC_ISERIES" != "y" ]; then
 mainmenu_option next_comment
 comment 'Console drivers'
 source drivers/video/Config.in
 endmenu
 
 source drivers/input/Config.in
+fi
 
 if [ "$CONFIG_PPC_ISERIES" = "y" ]; then
 mainmenu_option next_comment
@@ -214,6 +213,7 @@
 source drivers/char/Config.in
 source fs/Config.in
 
+if [ "$CONFIG_PPC_ISERIES" != "y" ]; then
 mainmenu_option next_comment
 comment 'Sound'
 tristate 'Sound card support' CONFIG_SOUND
@@ -224,7 +224,9 @@
 
 endmenu
 
+source drivers/media/Config.in
 source drivers/usb/Config.in
+fi
 
 mainmenu_option next_comment
 comment 'Kernel hacking'
@@ -232,10 +234,21 @@
 bool 'Magic SysRq key' CONFIG_MAGIC_SYSRQ
 bool 'Include kgdb kernel debugger' CONFIG_KGDB
 bool 'Include xmon kernel debugger' CONFIG_XMON
-#bool 'Include kdb kernel debugger' CONFIG_KDB
-#if [ "$CONFIG_KDB" = "y" ]; then
-#  bool '  KDB off by default' CONFIG_KDB_OFF
-#  define_bool CONFIG_KALLSYMS y
-#fi
+bool 'Include kdb kernel debugger' CONFIG_KDB
+if [ "$CONFIG_KDB" = "y" ]; then
+  bool '  KDB off by default' CONFIG_KDB_OFF
+  define_bool CONFIG_KALLSYMS y
+  define_bool CONFIG_XMON n
+fi
+if [ "$CONFIG_XMON" = "y" ]; then
+  define_bool CONFIG_KDB n
+  define_bool CONFIG_KALLSYMS n
+fi
 bool 'Include PPCDBG realtime debugging' CONFIG_PPCDBG
+
+tristate 'Linux Kernel Crash Dump (LKCD) Support' CONFIG_DUMP
+if [ "$CONFIG_DUMP" = "y" -o "$CONFIG_DUMP" = "m" ]; then
+   dep_bool '  LKCD RLE compression' CONFIG_DUMP_COMPRESS_RLE $CONFIG_DUMP
+   dep_bool '  LKCD GZIP compression' CONFIG_DUMP_COMPRESS_GZIP $CONFIG_DUMP
+fi
 endmenu
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/configs/iSeries_devfs_defconfig linuxppc64_2_4/arch/ppc64/configs/iSeries_devfs_defconfig
--- linux-2.4.19/arch/ppc64/configs/iSeries_devfs_defconfig	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/configs/iSeries_devfs_defconfig	Mon Jul  8 15:34:40 2002
@@ -696,4 +696,10 @@
 # CONFIG_MAGIC_SYSRQ is not set
 # CONFIG_KGDB is not set
 # CONFIG_XMON is not set
+# CONFIG_KDB is not set
+# CONFIG_KDB_OFF is not set
+# CONFIG_KALLSYMS is not set
 # CONFIG_PPCDBG is not set
+CONFIG_DUMP=y
+CONFIG_DUMP_COMPRESS_RLE=y
+CONFIG_DUMP_COMPRESS_GZIP=y
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/configs/iSeries_nodevfs_ideemul_defconfig linuxppc64_2_4/arch/ppc64/configs/iSeries_nodevfs_ideemul_defconfig
--- linux-2.4.19/arch/ppc64/configs/iSeries_nodevfs_ideemul_defconfig	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/configs/iSeries_nodevfs_ideemul_defconfig	Mon Jul  8 15:34:40 2002
@@ -67,7 +67,6 @@
 #
 # CONFIG_PNP is not set
 # CONFIG_ISAPNP is not set
-# CONFIG_PNPBIOS is not set
 
 #
 # Block devices
@@ -77,6 +76,7 @@
 # CONFIG_PARIDE is not set
 # CONFIG_BLK_CPQ_DA is not set
 # CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_CISS_SCSI_TAPE is not set
 # CONFIG_BLK_DEV_DAC960 is not set
 CONFIG_BLK_DEV_LOOP=y
 # CONFIG_BLK_DEV_NBD is not set
@@ -101,8 +101,6 @@
 #
 CONFIG_PACKET=y
 # CONFIG_PACKET_MMAP is not set
-CONFIG_NETLINK=y
-CONFIG_RTNETLINK=y
 # CONFIG_NETLINK_DEV is not set
 # CONFIG_NETFILTER is not set
 CONFIG_FILTER=y
@@ -110,8 +108,6 @@
 CONFIG_INET=y
 CONFIG_IP_MULTICAST=y
 CONFIG_IP_ADVANCED_ROUTER=y
-CONFIG_RTNETLINK=y
-CONFIG_NETLINK=y
 CONFIG_IP_MULTIPLE_TABLES=y
 CONFIG_IP_ROUTE_NAT=y
 CONFIG_IP_ROUTE_MULTIPATH=y
@@ -134,12 +130,18 @@
 # CONFIG_IPV6 is not set
 # CONFIG_KHTTPD is not set
 # CONFIG_ATM is not set
+# CONFIG_VLAN_8021Q is not set
 
 #
 #  
 #
 # CONFIG_IPX is not set
 # CONFIG_ATALK is not set
+
+#
+# Appletalk devices
+#
+# CONFIG_DEV_APPLETALK is not set
 # CONFIG_DECNET is not set
 # CONFIG_BRIDGE is not set
 # CONFIG_X25 is not set
@@ -155,6 +157,16 @@
 # QoS and/or fair queueing
 #
 # CONFIG_NET_SCHED is not set
+
+#
+# Network testing
+#
+# CONFIG_NET_PKTGEN is not set
+
+#
+# ATA/IDE/MFM/RLL support
+#
+# CONFIG_IDE is not set
 # CONFIG_BLK_DEV_IDE_MODES is not set
 # CONFIG_BLK_DEV_HD is not set
 
@@ -192,6 +204,7 @@
 # CONFIG_SCSI_AHA152X is not set
 # CONFIG_SCSI_AHA1542 is not set
 # CONFIG_SCSI_AHA1740 is not set
+# CONFIG_SCSI_AACRAID is not set
 # CONFIG_SCSI_AIC7XXX is not set
 # CONFIG_SCSI_AIC7XXX_OLD is not set
 # CONFIG_SCSI_DPT_I2O is not set
@@ -213,6 +226,7 @@
 # CONFIG_SCSI_INIA100 is not set
 # CONFIG_SCSI_NCR53C406A is not set
 # CONFIG_SCSI_NCR53C7xx is not set
+# CONFIG_SCSI_SYM53C8XX_2 is not set
 # CONFIG_SCSI_NCR53C8XX is not set
 # CONFIG_SCSI_SYM53C8XX is not set
 # CONFIG_SCSI_PAS16 is not set
@@ -229,8 +243,6 @@
 # CONFIG_SCSI_T128 is not set
 # CONFIG_SCSI_U14_34F is not set
 # CONFIG_SCSI_DEBUG is not set
-# CONFIG_SCSI_MESH is not set
-# CONFIG_SCSI_MAC53C94 is not set
 
 #
 # IEEE 1394 (FireWire) support (EXPERIMENTAL)
@@ -259,12 +271,10 @@
 # CONFIG_MACE is not set
 # CONFIG_BMAC is not set
 # CONFIG_GMAC is not set
-# CONFIG_OAKNET is not set
 # CONFIG_SUNLANCE is not set
 # CONFIG_HAPPYMEAL is not set
 # CONFIG_SUNBMAC is not set
 # CONFIG_SUNQE is not set
-# CONFIG_SUNLANCE is not set
 # CONFIG_SUNGEM is not set
 # CONFIG_NET_VENDOR_3COM is not set
 # CONFIG_LANCE is not set
@@ -278,6 +288,7 @@
 # CONFIG_APRICOT is not set
 # CONFIG_CS89x0 is not set
 # CONFIG_TULIP is not set
+# CONFIG_TC35815 is not set
 # CONFIG_DE4X5 is not set
 # CONFIG_DGRS is not set
 # CONFIG_DM9102 is not set
@@ -293,11 +304,13 @@
 # CONFIG_8139TOO_PIO is not set
 # CONFIG_8139TOO_TUNE_TWISTER is not set
 # CONFIG_8139TOO_8129 is not set
+# CONFIG_8139_NEW_RX_RESET is not set
 # CONFIG_SIS900 is not set
 # CONFIG_EPIC100 is not set
 # CONFIG_SUNDANCE is not set
 # CONFIG_TLAN is not set
 # CONFIG_VIA_RHINE is not set
+# CONFIG_VIA_RHINE_MMIO is not set
 # CONFIG_WINBOND_840 is not set
 # CONFIG_NET_POCKET is not set
 
@@ -312,6 +325,7 @@
 # CONFIG_HAMACHI is not set
 # CONFIG_YELLOWFIN is not set
 # CONFIG_SK98LIN is not set
+# CONFIG_TIGON3 is not set
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
 # CONFIG_PLIP is not set
@@ -329,6 +343,7 @@
 CONFIG_TR=y
 CONFIG_IBMOL=m
 # CONFIG_IBMLS is not set
+# CONFIG_3C359 is not set
 # CONFIG_TMS380TR is not set
 # CONFIG_NET_FC is not set
 # CONFIG_RCPCI is not set
@@ -380,6 +395,15 @@
 # CONFIG_FB is not set
 
 #
+# Input core support
+#
+# CONFIG_INPUT is not set
+# CONFIG_INPUT_KEYBDEV is not set
+# CONFIG_INPUT_MOUSEDEV is not set
+# CONFIG_INPUT_JOYDEV is not set
+# CONFIG_INPUT_EVDEV is not set
+
+#
 # iSeries device drivers
 #
 CONFIG_VIOCONS=y
@@ -435,43 +459,20 @@
 # Joysticks
 #
 # CONFIG_INPUT_GAMEPORT is not set
-# CONFIG_INPUT_NS558 is not set
-# CONFIG_INPUT_LIGHTNING is not set
-# CONFIG_INPUT_PCIGAME is not set
-# CONFIG_INPUT_CS461X is not set
-# CONFIG_INPUT_EMU10K1 is not set
-# CONFIG_INPUT_SERIO is not set
-# CONFIG_INPUT_SERPORT is not set
 
 #
-# Joysticks
+# Input core support is needed for gameports
+#
+
+#
+# Input core support is needed for joysticks
 #
-# CONFIG_INPUT_ANALOG is not set
-# CONFIG_INPUT_A3D is not set
-# CONFIG_INPUT_ADI is not set
-# CONFIG_INPUT_COBRA is not set
-# CONFIG_INPUT_GF2K is not set
-# CONFIG_INPUT_GRIP is not set
-# CONFIG_INPUT_INTERACT is not set
-# CONFIG_INPUT_TMDC is not set
-# CONFIG_INPUT_SIDEWINDER is not set
-# CONFIG_INPUT_IFORCE_USB is not set
-# CONFIG_INPUT_IFORCE_232 is not set
-# CONFIG_INPUT_WARRIOR is not set
-# CONFIG_INPUT_MAGELLAN is not set
-# CONFIG_INPUT_SPACEORB is not set
-# CONFIG_INPUT_SPACEBALL is not set
-# CONFIG_INPUT_STINGER is not set
-# CONFIG_INPUT_DB9 is not set
-# CONFIG_INPUT_GAMECON is not set
-# CONFIG_INPUT_TURBOGRAFX is not set
 # CONFIG_QIC02_TAPE is not set
 
 #
 # Watchdog Cards
 #
 # CONFIG_WATCHDOG is not set
-# CONFIG_INTEL_RNG is not set
 # CONFIG_NVRAM is not set
 # CONFIG_RTC is not set
 # CONFIG_DTLK is not set
@@ -484,7 +485,6 @@
 # CONFIG_FTAPE is not set
 # CONFIG_AGP is not set
 # CONFIG_DRM is not set
-# CONFIG_MWAVE is not set
 
 #
 # File systems
@@ -494,11 +494,15 @@
 CONFIG_AUTOFS4_FS=y
 CONFIG_REISERFS_FS=y
 # CONFIG_REISERFS_CHECK is not set
+# CONFIG_REISERFS_PROC_INFO is not set
 # CONFIG_ADFS_FS is not set
 # CONFIG_ADFS_FS_RW is not set
 # CONFIG_AFFS_FS is not set
 # CONFIG_HFS_FS is not set
 # CONFIG_BFS_FS is not set
+CONFIG_EXT3_FS=y
+CONFIG_JBD=y
+# CONFIG_JBD_DEBUG is not set
 # CONFIG_FAT_FS is not set
 # CONFIG_MSDOS_FS is not set
 # CONFIG_UMSDOS_FS is not set
@@ -511,8 +515,10 @@
 CONFIG_RAMFS=y
 CONFIG_ISO9660_FS=y
 CONFIG_JOLIET=y
+# CONFIG_ZISOFS is not set
 # CONFIG_MINIX_FS is not set
 # CONFIG_JFS_FS is not set
+# CONFIG_JFS_DEBUG is not set
 # CONFIG_VXFS_FS is not set
 # CONFIG_NTFS_FS is not set
 # CONFIG_NTFS_RW is not set
@@ -536,6 +542,7 @@
 # Network File Systems
 #
 # CONFIG_CODA_FS is not set
+# CONFIG_INTERMEZZO_FS is not set
 CONFIG_NFS_FS=y
 CONFIG_NFS_V3=y
 # CONFIG_ROOT_NFS is not set
@@ -555,6 +562,8 @@
 CONFIG_NCPFS_SMALLDOS=y
 CONFIG_NCPFS_NLS=y
 CONFIG_NCPFS_EXTRAS=y
+# CONFIG_ZISOFS_FS is not set
+# CONFIG_ZLIB_FS_INFLATE is not set
 
 #
 # Partition Types
@@ -589,6 +598,7 @@
 # CONFIG_NLS_CODEPAGE_949 is not set
 # CONFIG_NLS_CODEPAGE_874 is not set
 # CONFIG_NLS_ISO8859_8 is not set
+# CONFIG_NLS_CODEPAGE_1250 is not set
 # CONFIG_NLS_CODEPAGE_1251 is not set
 CONFIG_NLS_ISO8859_1=y
 # CONFIG_NLS_ISO8859_2 is not set
@@ -616,109 +626,15 @@
 # CONFIG_USB is not set
 
 #
-# USB Controllers
-#
-# CONFIG_USB_UHCI is not set
-# CONFIG_USB_UHCI_ALT is not set
-# CONFIG_USB_OHCI is not set
-
-#
-# USB Device Class drivers
-#
-# CONFIG_USB_AUDIO is not set
-# CONFIG_USB_BLUETOOTH is not set
-# CONFIG_USB_STORAGE is not set
-# CONFIG_USB_STORAGE_DEBUG is not set
-# CONFIG_USB_STORAGE_DATAFAB is not set
-# CONFIG_USB_STORAGE_FREECOM is not set
-# CONFIG_USB_STORAGE_ISD200 is not set
-# CONFIG_USB_STORAGE_DPCM is not set
-# CONFIG_USB_STORAGE_HP8200e is not set
-# CONFIG_USB_STORAGE_SDDR09 is not set
-# CONFIG_USB_STORAGE_JUMPSHOT is not set
-# CONFIG_USB_ACM is not set
-# CONFIG_USB_PRINTER is not set
-
-#
-# USB Human Interface Devices (HID)
-#
-# CONFIG_USB_HID is not set
-# CONFIG_USB_HIDDEV is not set
-# CONFIG_USB_KBD is not set
-# CONFIG_USB_MOUSE is not set
-# CONFIG_USB_WACOM is not set
-
-#
-# USB Imaging devices
-#
-# CONFIG_USB_DC2XX is not set
-# CONFIG_USB_MDC800 is not set
-# CONFIG_USB_SCANNER is not set
-# CONFIG_USB_MICROTEK is not set
-# CONFIG_USB_HPUSBSCSI is not set
-
-#
-# USB Multimedia devices
-#
-# CONFIG_USB_IBMCAM is not set
-# CONFIG_USB_OV511 is not set
-# CONFIG_USB_PWC is not set
-# CONFIG_USB_SE401 is not set
-# CONFIG_USB_DSBR is not set
-# CONFIG_USB_DABUSB is not set
-
-#
-# USB Network adaptors
-#
-# CONFIG_USB_PEGASUS is not set
-# CONFIG_USB_KAWETH is not set
-# CONFIG_USB_CATC is not set
-# CONFIG_USB_CDCETHER is not set
-# CONFIG_USB_USBNET is not set
-
-#
-# USB port drivers
-#
-# CONFIG_USB_USS720 is not set
-
-#
-# USB Serial Converter support
-#
-# CONFIG_USB_SERIAL is not set
-# CONFIG_USB_SERIAL_GENERIC is not set
-# CONFIG_USB_SERIAL_BELKIN is not set
-# CONFIG_USB_SERIAL_WHITEHEAT is not set
-# CONFIG_USB_SERIAL_DIGI_ACCELEPORT is not set
-# CONFIG_USB_SERIAL_EMPEG is not set
-# CONFIG_USB_SERIAL_FTDI_SIO is not set
-# CONFIG_USB_SERIAL_VISOR is not set
-# CONFIG_USB_SERIAL_IR is not set
-# CONFIG_USB_SERIAL_EDGEPORT is not set
-# CONFIG_USB_SERIAL_KEYSPAN_PDA is not set
-# CONFIG_USB_SERIAL_KEYSPAN is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XA is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XB is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA18X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19W is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA49W is not set
-# CONFIG_USB_SERIAL_MCT_U232 is not set
-# CONFIG_USB_SERIAL_PL2303 is not set
-# CONFIG_USB_SERIAL_CYBERJACK is not set
-# CONFIG_USB_SERIAL_XIRCOM is not set
-# CONFIG_USB_SERIAL_OMNINET is not set
-
-#
-# USB Miscellaneous drivers
-#
-# CONFIG_USB_RIO500 is not set
-
-#
 # Kernel hacking
 #
 CONFIG_MAGIC_SYSRQ=y
 # CONFIG_KGDB is not set
 # CONFIG_XMON is not set
+# CONFIG_KDB is not set
+# CONFIG_KDB_OFF is not set
+# CONFIG_KALLSYMS is not set
 # CONFIG_PPCDBG is not set
+CONFIG_DUMP=y
+CONFIG_DUMP_COMPRESS_RLE=y
+CONFIG_DUMP_COMPRESS_GZIP=y
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/configs/pSeries_defconfig linuxppc64_2_4/arch/ppc64/configs/pSeries_defconfig
--- linux-2.4.19/arch/ppc64/configs/pSeries_defconfig	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/configs/pSeries_defconfig	Fri Aug  9 14:48:53 2002
@@ -25,6 +25,7 @@
 CONFIG_IRQ_ALL_CPUS=y
 # CONFIG_HMT is not set
 # CONFIG_MSCHUNKS is not set
+CONFIG_RTAS_FLASH=m
 
 #
 # Loadable module support
@@ -61,6 +62,9 @@
 CONFIG_FB=y
 CONFIG_PROC_DEVICETREE=y
 
+CONFIG_CMDLINE_BOOL=n
+CONFIG_CMDLINE="console=ttyS0,9600 console=tty0 root=/dev/sda2"
+
 #
 # Memory Technology Devices (MTD)
 #
@@ -80,7 +84,9 @@
 # CONFIG_PARIDE is not set
 # CONFIG_BLK_CPQ_DA is not set
 # CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_CISS_SCSI_TAPE is not set
 # CONFIG_BLK_DEV_DAC960 is not set
+# CONFIG_BLK_DEV_UMEM is not set
 CONFIG_BLK_DEV_LOOP=y
 CONFIG_BLK_DEV_NBD=y
 CONFIG_BLK_DEV_RAM=y
@@ -128,6 +134,11 @@
 #
 # CONFIG_IPX is not set
 # CONFIG_ATALK is not set
+
+#
+# Appletalk devices
+#
+# CONFIG_DEV_APPLETALK is not set
 # CONFIG_DECNET is not set
 # CONFIG_BRIDGE is not set
 # CONFIG_X25 is not set
@@ -145,6 +156,11 @@
 # CONFIG_NET_SCHED is not set
 
 #
+# Network testing
+#
+# CONFIG_NET_PKTGEN is not set
+
+#
 # ATA/IDE/MFM/RLL support
 #
 CONFIG_IDE=y
@@ -161,6 +177,7 @@
 # CONFIG_BLK_DEV_HD is not set
 # CONFIG_BLK_DEV_IDEDISK is not set
 # CONFIG_IDEDISK_MULTI_MODE is not set
+# CONFIG_IDEDISK_STROKE is not set
 # CONFIG_BLK_DEV_IDEDISK_VENDOR is not set
 # CONFIG_BLK_DEV_IDEDISK_FUJITSU is not set
 # CONFIG_BLK_DEV_IDEDISK_IBM is not set
@@ -175,6 +192,7 @@
 # CONFIG_BLK_DEV_IDETAPE is not set
 # CONFIG_BLK_DEV_IDEFLOPPY is not set
 # CONFIG_BLK_DEV_IDESCSI is not set
+# CONFIG_IDE_TASK_IOCTL is not set
 
 #
 # IDE chipset support/bugfixes
@@ -186,12 +204,15 @@
 CONFIG_BLK_DEV_IDEPCI=y
 # CONFIG_IDEPCI_SHARE_IRQ is not set
 CONFIG_BLK_DEV_IDEDMA_PCI=y
-CONFIG_BLK_DEV_ADMA=y
 # CONFIG_BLK_DEV_OFFBOARD is not set
+# CONFIG_BLK_DEV_IDEDMA_FORCED is not set
 # CONFIG_IDEDMA_PCI_AUTO is not set
+# CONFIG_IDEDMA_ONLYDISK is not set
 CONFIG_BLK_DEV_IDEDMA=y
 # CONFIG_IDEDMA_PCI_WIP is not set
+# CONFIG_BLK_DEV_IDEDMA_TIMEOUT is not set
 # CONFIG_IDEDMA_NEW_DRIVE_LISTINGS is not set
+CONFIG_BLK_DEV_ADMA=y
 # CONFIG_BLK_DEV_AEC62XX is not set
 # CONFIG_AEC62XX_TUNING is not set
 # CONFIG_BLK_DEV_ALI15X3 is not set
@@ -199,6 +220,7 @@
 # CONFIG_BLK_DEV_AMD74XX is not set
 # CONFIG_AMD74XX_OVERRIDE is not set
 # CONFIG_BLK_DEV_CMD64X is not set
+# CONFIG_BLK_DEV_CMD680 is not set
 # CONFIG_BLK_DEV_CY82C693 is not set
 # CONFIG_BLK_DEV_CS5530 is not set
 # CONFIG_BLK_DEV_HPT34X is not set
@@ -349,6 +371,7 @@
 # CONFIG_APRICOT is not set
 # CONFIG_CS89x0 is not set
 # CONFIG_TULIP is not set
+# CONFIG_TC35815 is not set
 # CONFIG_DE4X5 is not set
 # CONFIG_DGRS is not set
 # CONFIG_DM9102 is not set
@@ -380,12 +403,12 @@
 CONFIG_ACENIC=y
 # CONFIG_ACENIC_OMIT_TIGON_I is not set
 # CONFIG_DL2K is not set
-CONFIG_E1000=y
 # CONFIG_MYRI_SBUS is not set
 # CONFIG_NS83820 is not set
 # CONFIG_HAMACHI is not set
 # CONFIG_YELLOWFIN is not set
 # CONFIG_SK98LIN is not set
+# CONFIG_TIGON3 is not set
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
 # CONFIG_PLIP is not set
@@ -403,6 +426,7 @@
 CONFIG_TR=y
 CONFIG_IBMOL=y
 # CONFIG_IBMLS is not set
+# CONFIG_3C359 is not set
 # CONFIG_TMS380TR is not set
 # CONFIG_NET_FC is not set
 # CONFIG_RCPCI is not set
@@ -445,6 +469,7 @@
 # CONFIG_FB_RIVA is not set
 # CONFIG_FB_CLGEN is not set
 # CONFIG_FB_PM2 is not set
+# CONFIG_FB_PM3 is not set
 # CONFIG_FB_CYBER2000 is not set
 CONFIG_FB_OF=y
 # CONFIG_FB_CONTROL is not set
@@ -464,6 +489,7 @@
 # CONFIG_FB_RADEON is not set
 # CONFIG_FB_ATY128 is not set
 # CONFIG_FB_SIS is not set
+# CONFIG_FB_NEOMAGIC is not set
 # CONFIG_FB_3DFX is not set
 # CONFIG_FB_VOODOO1 is not set
 # CONFIG_FB_TRIDENT is not set
@@ -517,6 +543,7 @@
 CONFIG_PSMOUSE=y
 # CONFIG_82C710_MOUSE is not set
 # CONFIG_PC110_PAD is not set
+# CONFIG_MK712_MOUSE is not set
 
 #
 # Joysticks
@@ -536,7 +563,6 @@
 # Watchdog Cards
 #
 # CONFIG_WATCHDOG is not set
-# CONFIG_INTEL_RNG is not set
 # CONFIG_NVRAM is not set
 # CONFIG_RTC is not set
 # CONFIG_RTC is not set
@@ -577,7 +603,7 @@
 # CONFIG_JFFS2_FS is not set
 # CONFIG_CRAMFS is not set
 # CONFIG_TMPFS is not set
-# CONFIG_RAMFS is not set
+CONFIG_RAMFS=y
 CONFIG_ISO9660_FS=y
 # CONFIG_JOLIET is not set
 # CONFIG_ZISOFS is not set
@@ -691,112 +717,16 @@
 # CONFIG_USB is not set
 
 #
-# USB Controllers
-#
-# CONFIG_USB_UHCI is not set
-# CONFIG_USB_UHCI_ALT is not set
-# CONFIG_USB_OHCI is not set
-
-#
-# USB Device Class drivers
-#
-# CONFIG_USB_AUDIO is not set
-# CONFIG_USB_BLUETOOTH is not set
-# CONFIG_USB_STORAGE is not set
-# CONFIG_USB_STORAGE_DEBUG is not set
-# CONFIG_USB_STORAGE_DATAFAB is not set
-# CONFIG_USB_STORAGE_FREECOM is not set
-# CONFIG_USB_STORAGE_ISD200 is not set
-# CONFIG_USB_STORAGE_DPCM is not set
-# CONFIG_USB_STORAGE_HP8200e is not set
-# CONFIG_USB_STORAGE_SDDR09 is not set
-# CONFIG_USB_STORAGE_JUMPSHOT is not set
-# CONFIG_USB_ACM is not set
-# CONFIG_USB_PRINTER is not set
-
-#
-# USB Human Interface Devices (HID)
-#
-
-#
-#   Input core support is needed for USB HID
-#
-
-#
-# USB Imaging devices
-#
-# CONFIG_USB_DC2XX is not set
-# CONFIG_USB_MDC800 is not set
-# CONFIG_USB_SCANNER is not set
-# CONFIG_USB_MICROTEK is not set
-# CONFIG_USB_HPUSBSCSI is not set
-
-#
-# USB Multimedia devices
-#
-# CONFIG_USB_IBMCAM is not set
-# CONFIG_USB_OV511 is not set
-# CONFIG_USB_PWC is not set
-# CONFIG_USB_SE401 is not set
-# CONFIG_USB_STV680 is not set
-# CONFIG_USB_VICAM is not set
-# CONFIG_USB_DSBR is not set
-# CONFIG_USB_DABUSB is not set
-
-#
-# USB Network adaptors
-#
-# CONFIG_USB_PEGASUS is not set
-# CONFIG_USB_KAWETH is not set
-# CONFIG_USB_CATC is not set
-# CONFIG_USB_CDCETHER is not set
-# CONFIG_USB_USBNET is not set
-
-#
-# USB port drivers
-#
-# CONFIG_USB_USS720 is not set
-
-#
-# USB Serial Converter support
-#
-# CONFIG_USB_SERIAL is not set
-# CONFIG_USB_SERIAL_GENERIC is not set
-# CONFIG_USB_SERIAL_BELKIN is not set
-# CONFIG_USB_SERIAL_WHITEHEAT is not set
-# CONFIG_USB_SERIAL_DIGI_ACCELEPORT is not set
-# CONFIG_USB_SERIAL_EMPEG is not set
-# CONFIG_USB_SERIAL_FTDI_SIO is not set
-# CONFIG_USB_SERIAL_VISOR is not set
-# CONFIG_USB_SERIAL_IPAQ is not set
-# CONFIG_USB_SERIAL_IR is not set
-# CONFIG_USB_SERIAL_EDGEPORT is not set
-# CONFIG_USB_SERIAL_KEYSPAN_PDA is not set
-# CONFIG_USB_SERIAL_KEYSPAN is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XA is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XB is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA18X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19W is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA49W is not set
-# CONFIG_USB_SERIAL_MCT_U232 is not set
-# CONFIG_USB_SERIAL_KLSI is not set
-# CONFIG_USB_SERIAL_PL2303 is not set
-# CONFIG_USB_SERIAL_CYBERJACK is not set
-# CONFIG_USB_SERIAL_XIRCOM is not set
-# CONFIG_USB_SERIAL_OMNINET is not set
-
-#
-# USB Miscellaneous drivers
-#
-# CONFIG_USB_RIO500 is not set
-
-#
 # Kernel hacking
 #
 CONFIG_MAGIC_SYSRQ=y
 # CONFIG_KGDB is not set
-CONFIG_XMON=y
+# CONFIG_XMON is not set
+CONFIG_KDB=y
+# CONFIG_KDB_OFF is not set
+# CONFIG_KALLSYMS is not set
 CONFIG_PPCDBG=y
+
+CONFIG_DUMP=y
+CONFIG_DUMP_COMPRESS_RLE=y
+CONFIG_DUMP_COMPRESS_GZIP=y
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/defconfig linuxppc64_2_4/arch/ppc64/defconfig
--- linux-2.4.19/arch/ppc64/defconfig	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/defconfig	Sat Aug 17 05:06:10 2002
@@ -25,6 +25,7 @@
 CONFIG_IRQ_ALL_CPUS=y
 # CONFIG_HMT is not set
 # CONFIG_MSCHUNKS is not set
+CONFIG_RTAS_FLASH=m
 
 #
 # Loadable module support
@@ -61,6 +62,9 @@
 CONFIG_FB=y
 CONFIG_PROC_DEVICETREE=y
 
+CONFIG_CMDLINE_BOOL=n
+CONFIG_CMDLINE="console=ttyS0,9600 console=tty0 root=/dev/sda2"
+
 #
 # Memory Technology Devices (MTD)
 #
@@ -80,7 +84,9 @@
 # CONFIG_PARIDE is not set
 # CONFIG_BLK_CPQ_DA is not set
 # CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_CISS_SCSI_TAPE is not set
 # CONFIG_BLK_DEV_DAC960 is not set
+# CONFIG_BLK_DEV_UMEM is not set
 CONFIG_BLK_DEV_LOOP=y
 CONFIG_BLK_DEV_NBD=y
 CONFIG_BLK_DEV_RAM=y
@@ -128,6 +134,11 @@
 #
 # CONFIG_IPX is not set
 # CONFIG_ATALK is not set
+
+#
+# Appletalk devices
+#
+# CONFIG_DEV_APPLETALK is not set
 # CONFIG_DECNET is not set
 # CONFIG_BRIDGE is not set
 # CONFIG_X25 is not set
@@ -145,6 +156,11 @@
 # CONFIG_NET_SCHED is not set
 
 #
+# Network testing
+#
+# CONFIG_NET_PKTGEN is not set
+
+#
 # ATA/IDE/MFM/RLL support
 #
 CONFIG_IDE=y
@@ -161,6 +177,7 @@
 # CONFIG_BLK_DEV_HD is not set
 # CONFIG_BLK_DEV_IDEDISK is not set
 # CONFIG_IDEDISK_MULTI_MODE is not set
+# CONFIG_IDEDISK_STROKE is not set
 # CONFIG_BLK_DEV_IDEDISK_VENDOR is not set
 # CONFIG_BLK_DEV_IDEDISK_FUJITSU is not set
 # CONFIG_BLK_DEV_IDEDISK_IBM is not set
@@ -175,6 +192,7 @@
 # CONFIG_BLK_DEV_IDETAPE is not set
 # CONFIG_BLK_DEV_IDEFLOPPY is not set
 # CONFIG_BLK_DEV_IDESCSI is not set
+# CONFIG_IDE_TASK_IOCTL is not set
 
 #
 # IDE chipset support/bugfixes
@@ -186,12 +204,15 @@
 CONFIG_BLK_DEV_IDEPCI=y
 # CONFIG_IDEPCI_SHARE_IRQ is not set
 CONFIG_BLK_DEV_IDEDMA_PCI=y
-CONFIG_BLK_DEV_ADMA=y
 # CONFIG_BLK_DEV_OFFBOARD is not set
+# CONFIG_BLK_DEV_IDEDMA_FORCED is not set
 # CONFIG_IDEDMA_PCI_AUTO is not set
+# CONFIG_IDEDMA_ONLYDISK is not set
 CONFIG_BLK_DEV_IDEDMA=y
 # CONFIG_IDEDMA_PCI_WIP is not set
+# CONFIG_BLK_DEV_IDEDMA_TIMEOUT is not set
 # CONFIG_IDEDMA_NEW_DRIVE_LISTINGS is not set
+CONFIG_BLK_DEV_ADMA=y
 # CONFIG_BLK_DEV_AEC62XX is not set
 # CONFIG_AEC62XX_TUNING is not set
 # CONFIG_BLK_DEV_ALI15X3 is not set
@@ -199,6 +220,7 @@
 # CONFIG_BLK_DEV_AMD74XX is not set
 # CONFIG_AMD74XX_OVERRIDE is not set
 # CONFIG_BLK_DEV_CMD64X is not set
+# CONFIG_BLK_DEV_CMD680 is not set
 # CONFIG_BLK_DEV_CY82C693 is not set
 # CONFIG_BLK_DEV_CS5530 is not set
 # CONFIG_BLK_DEV_HPT34X is not set
@@ -349,6 +371,7 @@
 # CONFIG_APRICOT is not set
 # CONFIG_CS89x0 is not set
 # CONFIG_TULIP is not set
+# CONFIG_TC35815 is not set
 # CONFIG_DE4X5 is not set
 # CONFIG_DGRS is not set
 # CONFIG_DM9102 is not set
@@ -378,14 +401,14 @@
 # Ethernet (1000 Mbit)
 #
 CONFIG_ACENIC=y
-# CONFIG_ACENIC_OMIT_TIGON_I is not set
+CONFIG_ACENIC_OMIT_TIGON_I=y
 # CONFIG_DL2K is not set
-CONFIG_E1000=y
 # CONFIG_MYRI_SBUS is not set
 # CONFIG_NS83820 is not set
 # CONFIG_HAMACHI is not set
 # CONFIG_YELLOWFIN is not set
 # CONFIG_SK98LIN is not set
+# CONFIG_TIGON3 is not set
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
 # CONFIG_PLIP is not set
@@ -403,6 +426,7 @@
 CONFIG_TR=y
 CONFIG_IBMOL=y
 # CONFIG_IBMLS is not set
+# CONFIG_3C359 is not set
 # CONFIG_TMS380TR is not set
 # CONFIG_NET_FC is not set
 # CONFIG_RCPCI is not set
@@ -445,6 +469,7 @@
 # CONFIG_FB_RIVA is not set
 # CONFIG_FB_CLGEN is not set
 # CONFIG_FB_PM2 is not set
+# CONFIG_FB_PM3 is not set
 # CONFIG_FB_CYBER2000 is not set
 CONFIG_FB_OF=y
 # CONFIG_FB_CONTROL is not set
@@ -464,6 +489,7 @@
 # CONFIG_FB_RADEON is not set
 # CONFIG_FB_ATY128 is not set
 # CONFIG_FB_SIS is not set
+# CONFIG_FB_NEOMAGIC is not set
 # CONFIG_FB_3DFX is not set
 # CONFIG_FB_VOODOO1 is not set
 # CONFIG_FB_TRIDENT is not set
@@ -517,6 +543,7 @@
 CONFIG_PSMOUSE=y
 # CONFIG_82C710_MOUSE is not set
 # CONFIG_PC110_PAD is not set
+# CONFIG_MK712_MOUSE is not set
 
 #
 # Joysticks
@@ -536,7 +563,6 @@
 # Watchdog Cards
 #
 # CONFIG_WATCHDOG is not set
-# CONFIG_INTEL_RNG is not set
 # CONFIG_NVRAM is not set
 # CONFIG_RTC is not set
 # CONFIG_RTC is not set
@@ -577,7 +603,7 @@
 # CONFIG_JFFS2_FS is not set
 # CONFIG_CRAMFS is not set
 # CONFIG_TMPFS is not set
-# CONFIG_RAMFS is not set
+CONFIG_RAMFS=y
 CONFIG_ISO9660_FS=y
 # CONFIG_JOLIET is not set
 # CONFIG_ZISOFS is not set
@@ -691,112 +717,16 @@
 # CONFIG_USB is not set
 
 #
-# USB Controllers
-#
-# CONFIG_USB_UHCI is not set
-# CONFIG_USB_UHCI_ALT is not set
-# CONFIG_USB_OHCI is not set
-
-#
-# USB Device Class drivers
-#
-# CONFIG_USB_AUDIO is not set
-# CONFIG_USB_BLUETOOTH is not set
-# CONFIG_USB_STORAGE is not set
-# CONFIG_USB_STORAGE_DEBUG is not set
-# CONFIG_USB_STORAGE_DATAFAB is not set
-# CONFIG_USB_STORAGE_FREECOM is not set
-# CONFIG_USB_STORAGE_ISD200 is not set
-# CONFIG_USB_STORAGE_DPCM is not set
-# CONFIG_USB_STORAGE_HP8200e is not set
-# CONFIG_USB_STORAGE_SDDR09 is not set
-# CONFIG_USB_STORAGE_JUMPSHOT is not set
-# CONFIG_USB_ACM is not set
-# CONFIG_USB_PRINTER is not set
-
-#
-# USB Human Interface Devices (HID)
-#
-
-#
-#   Input core support is needed for USB HID
-#
-
-#
-# USB Imaging devices
-#
-# CONFIG_USB_DC2XX is not set
-# CONFIG_USB_MDC800 is not set
-# CONFIG_USB_SCANNER is not set
-# CONFIG_USB_MICROTEK is not set
-# CONFIG_USB_HPUSBSCSI is not set
-
-#
-# USB Multimedia devices
-#
-# CONFIG_USB_IBMCAM is not set
-# CONFIG_USB_OV511 is not set
-# CONFIG_USB_PWC is not set
-# CONFIG_USB_SE401 is not set
-# CONFIG_USB_STV680 is not set
-# CONFIG_USB_VICAM is not set
-# CONFIG_USB_DSBR is not set
-# CONFIG_USB_DABUSB is not set
-
-#
-# USB Network adaptors
-#
-# CONFIG_USB_PEGASUS is not set
-# CONFIG_USB_KAWETH is not set
-# CONFIG_USB_CATC is not set
-# CONFIG_USB_CDCETHER is not set
-# CONFIG_USB_USBNET is not set
-
-#
-# USB port drivers
-#
-# CONFIG_USB_USS720 is not set
-
-#
-# USB Serial Converter support
-#
-# CONFIG_USB_SERIAL is not set
-# CONFIG_USB_SERIAL_GENERIC is not set
-# CONFIG_USB_SERIAL_BELKIN is not set
-# CONFIG_USB_SERIAL_WHITEHEAT is not set
-# CONFIG_USB_SERIAL_DIGI_ACCELEPORT is not set
-# CONFIG_USB_SERIAL_EMPEG is not set
-# CONFIG_USB_SERIAL_FTDI_SIO is not set
-# CONFIG_USB_SERIAL_VISOR is not set
-# CONFIG_USB_SERIAL_IPAQ is not set
-# CONFIG_USB_SERIAL_IR is not set
-# CONFIG_USB_SERIAL_EDGEPORT is not set
-# CONFIG_USB_SERIAL_KEYSPAN_PDA is not set
-# CONFIG_USB_SERIAL_KEYSPAN is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XA is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA28XB is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19 is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA18X is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA19W is not set
-# CONFIG_USB_SERIAL_KEYSPAN_USA49W is not set
-# CONFIG_USB_SERIAL_MCT_U232 is not set
-# CONFIG_USB_SERIAL_KLSI is not set
-# CONFIG_USB_SERIAL_PL2303 is not set
-# CONFIG_USB_SERIAL_CYBERJACK is not set
-# CONFIG_USB_SERIAL_XIRCOM is not set
-# CONFIG_USB_SERIAL_OMNINET is not set
-
-#
-# USB Miscellaneous drivers
-#
-# CONFIG_USB_RIO500 is not set
-
-#
 # Kernel hacking
 #
 CONFIG_MAGIC_SYSRQ=y
 # CONFIG_KGDB is not set
-CONFIG_XMON=y
+# CONFIG_XMON is not set
+CONFIG_KDB=y
+# CONFIG_KDB_OFF is not set
+# CONFIG_KALLSYMS is not set
 CONFIG_PPCDBG=y
+
+CONFIG_DUMP=y
+CONFIG_DUMP_COMPRESS_RLE=y
+CONFIG_DUMP_COMPRESS_GZIP=y
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/Makefile linuxppc64_2_4/arch/ppc64/kdb/Makefile
--- linux-2.4.19/arch/ppc64/kdb/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/Makefile	Fri May 24 15:08:51 2002
@@ -0,0 +1,16 @@
+O_TARGET	:= kdba.o
+obj-y		:= kdba_bt.o kdba_bp.o kdba_id.o kdba_io.o ppc-dis.o ppc-opc.o kdbasupport.o  start.o kdba_utils.o
+
+# Warning: running with a minimal-toc means that kdb_setjmp will break
+# due to saving the wrong r30. A solution would be to move it into setjmp.S
+EXTRA_CFLAGS = -mno-minimal-toc
+
+# override CFLAGS := $(CFLAGS) -g
+
+override CFLAGS := $(CFLAGS) -I.
+
+# Translate to Rules.make lists.
+O_OBJS		:= $(filter-out	$(export-objs), $(obj-y))
+OX_OBJS		:= $(filter	$(export-objs), $(obj-y))
+
+include $(TOPDIR)/Rules.make
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/ansidecl.h linuxppc64_2_4/arch/ppc64/kdb/ansidecl.h
--- linux-2.4.19/arch/ppc64/kdb/ansidecl.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/ansidecl.h	Fri May 24 15:08:51 2002
@@ -0,0 +1,198 @@
+/* ANSI and traditional C compatability macros
+   Copyright 1991, 1992, 1996, 1999 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+/* ANSI and traditional C compatibility macros
+
+   ANSI C is assumed if __STDC__ is #defined.
+
+   Macro	ANSI C definition	Traditional C definition
+   -----	---- - ----------	----------- - ----------
+   PTR		`void *'		`char *'
+   LONG_DOUBLE	`long double'		`double'
+   VOLATILE	`volatile'		`'
+   SIGNED	`signed'		`'
+   PTRCONST	`void *const'		`char *'
+   ANSI_PROTOTYPES  1			not defined
+
+   CONST is also defined, but is obsolete.  Just use const.
+
+   obsolete --     DEFUN (name, arglist, args)
+
+	Defines function NAME.
+
+	ARGLIST lists the arguments, separated by commas and enclosed in
+	parentheses.  ARGLIST becomes the argument list in traditional C.
+
+	ARGS list the arguments with their types.  It becomes a prototype in
+	ANSI C, and the type declarations in traditional C.  Arguments should
+	be separated with `AND'.  For functions with a variable number of
+	arguments, the last thing listed should be `DOTS'.
+
+   obsolete --     DEFUN_VOID (name)
+
+	Defines a function NAME, which takes no arguments.
+
+   obsolete --     EXFUN (name, (prototype))	-- obsolete.
+
+	Replaced by PARAMS.  Do not use; will disappear someday soon.
+	Was used in external function declarations.
+	In ANSI C it is `NAME PROTOTYPE' (so PROTOTYPE should be enclosed in
+	parentheses).  In traditional C it is `NAME()'.
+	For a function that takes no arguments, PROTOTYPE should be `(void)'.
+
+   obsolete --     PROTO (type, name, (prototype)    -- obsolete.
+
+	This one has also been replaced by PARAMS.  Do not use.
+
+   PARAMS ((args))
+
+	We could use the EXFUN macro to handle prototype declarations, but
+	the name is misleading and the result is ugly.  So we just define a
+	simple macro to handle the parameter lists, as in:
+
+	      static int foo PARAMS ((int, char));
+
+	This produces:  `static int foo();' or `static int foo (int, char);'
+
+	EXFUN would have done it like this:
+
+	      static int EXFUN (foo, (int, char));
+
+	but the function is not external...and it's hard to visually parse
+	the function name out of the mess.   EXFUN should be considered
+	obsolete; new code should be written to use PARAMS.
+
+   DOTS is also obsolete.
+
+   Examples:
+
+	extern int printf PARAMS ((const char *format, ...));
+*/
+
+#ifndef	_ANSIDECL_H
+
+#define	_ANSIDECL_H	1
+
+
+/* Every source file includes this file,
+   so they will all get the switch for lint.  */
+/* LINTLIBRARY */
+
+
+#if defined (__STDC__) || defined (_AIX) || (defined (__mips) && defined (_SYSTYPE_SVR4)) || defined(_WIN32)
+/* All known AIX compilers implement these things (but don't always
+   define __STDC__).  The RISC/OS MIPS compiler defines these things
+   in SVR4 mode, but does not define __STDC__.  */
+
+#define	PTR		void *
+#define	PTRCONST	void *CONST
+#define	LONG_DOUBLE	long double
+
+#ifndef IN_GCC
+#define	AND		,
+#define	NOARGS		void
+#define	VOLATILE	volatile
+#define	SIGNED		signed
+#endif /* ! IN_GCC */
+
+#ifndef PARAMS
+#define PARAMS(paramlist)		paramlist
+#endif
+#define ANSI_PROTOTYPES			1
+
+#define VPARAMS(ARGS)			ARGS
+#define VA_START(va_list,var)		va_start(va_list,var)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST				const
+#define DOTS				, ...
+#define PROTO(type, name, arglist)	type name arglist
+#define EXFUN(name, proto)		name proto
+#define DEFUN(name, arglist, args)	name(args)
+#define DEFUN_VOID(name)		name(void)
+#endif /* ! IN_GCC */
+
+#else	/* Not ANSI C.  */
+
+#define	PTR		char *
+#define	PTRCONST	PTR
+#define	LONG_DOUBLE	double
+
+#ifndef IN_GCC
+#define	AND		;
+#define	NOARGS
+#define	VOLATILE
+#define	SIGNED
+#endif /* !IN_GCC */
+
+#ifndef const /* some systems define it in header files for non-ansi mode */
+#define	const
+#endif
+
+#define PARAMS(paramlist)		()
+
+#define VPARAMS(ARGS)			(va_alist) va_dcl
+#define VA_START(va_list,var)		va_start(va_list)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST
+#define DOTS
+#define PROTO(type, name, arglist)	type name ()
+#define EXFUN(name, proto)		name()
+#define DEFUN(name, arglist, args)	name arglist args;
+#define DEFUN_VOID(name)		name()
+#endif /* ! IN_GCC */
+
+#endif	/* ANSI C.  */
+
+/* Define macros for some gcc attributes.  This permits us to use the
+   macros freely, and know that they will come into play for the
+   version of gcc in which they are supported.  */
+
+#if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 7)
+# define __attribute__(x)
+#endif
+
+#ifndef ATTRIBUTE_UNUSED_LABEL
+# if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 93)
+#  define ATTRIBUTE_UNUSED_LABEL
+# else
+#  define ATTRIBUTE_UNUSED_LABEL ATTRIBUTE_UNUSED
+# endif /* GNUC < 2.93 */
+#endif /* ATTRIBUTE_UNUSED_LABEL */
+
+#ifndef ATTRIBUTE_UNUSED
+#define ATTRIBUTE_UNUSED __attribute__ ((__unused__))
+#endif /* ATTRIBUTE_UNUSED */
+
+#ifndef ATTRIBUTE_NORETURN
+#define ATTRIBUTE_NORETURN __attribute__ ((__noreturn__))
+#endif /* ATTRIBUTE_NORETURN */
+
+#ifndef ATTRIBUTE_PRINTF
+#define ATTRIBUTE_PRINTF(m, n) __attribute__ ((format (__printf__, m, n)))
+#define ATTRIBUTE_PRINTF_1 ATTRIBUTE_PRINTF(1, 2)
+#define ATTRIBUTE_PRINTF_2 ATTRIBUTE_PRINTF(2, 3)
+#define ATTRIBUTE_PRINTF_3 ATTRIBUTE_PRINTF(3, 4)
+#define ATTRIBUTE_PRINTF_4 ATTRIBUTE_PRINTF(4, 5)
+#define ATTRIBUTE_PRINTF_5 ATTRIBUTE_PRINTF(5, 6)
+#endif /* ATTRIBUTE_PRINTF */
+
+#endif	/* ansidecl.h	*/
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/bfd.h linuxppc64_2_4/arch/ppc64/kdb/bfd.h
--- linux-2.4.19/arch/ppc64/kdb/bfd.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/bfd.h	Fri May 24 15:08:51 2002
@@ -0,0 +1,3706 @@
+/* DO NOT EDIT!  -*- buffer-read-only: t -*-  This file is automatically 
+   generated from "bfd-in.h", "init.c", "opncls.c", "libbfd.c", 
+   "section.c", "archures.c", "reloc.c", "syms.c", "bfd.c", "archive.c", 
+   "corefile.c", "targets.c" and "format.c".
+   Run "make headers" in your build bfd/ to regenerate.  */
+
+/* Main header file for the bfd library -- portable access to object files.
+   Copyright 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+   2000, 2001
+   Free Software Foundation, Inc.
+   Contributed by Cygnus Support.
+
+This file is part of BFD, the Binary File Descriptor library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef __BFD_H_SEEN__
+#define __BFD_H_SEEN__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "ansidecl.h"
+#include "symcat.h"
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#ifndef SABER
+/* This hack is to avoid a problem with some strict ANSI C preprocessors.
+   The problem is, "32_" is not a valid preprocessing token, and we don't
+   want extra underscores (e.g., "nlm_32_").  The XCONCAT2 macro will
+   cause the inner CONCAT2 macros to be evaluated first, producing
+   still-valid pp-tokens.  Then the final concatenation can be done.  */
+#undef CONCAT4
+#define CONCAT4(a,b,c,d) XCONCAT2(CONCAT2(a,b),CONCAT2(c,d))
+#endif
+#endif
+
+#define BFD_VERSION 211920007
+#define BFD_VERSION_DATE 20011016
+#define BFD_VERSION_STRING "2.11.92.0.7 20011016 Debian\/GNU Linux"
+
+/* The word size used by BFD on the host.  This may be 64 with a 32
+   bit target if the host is 64 bit, or if other 64 bit targets have
+   been selected with --enable-targets, or if --enable-64-bit-bfd.  */
+#define BFD_ARCH_SIZE 64
+
+/* The word size of the default bfd target.  */
+#define BFD_DEFAULT_TARGET_SIZE 32
+
+#define BFD_HOST_64BIT_LONG 1
+#define BFD_HOST_64_BIT long
+#define BFD_HOST_U_64_BIT unsigned long
+
+#if BFD_ARCH_SIZE >= 64
+#define BFD64
+#endif
+
+#ifndef INLINE
+#if __GNUC__ >= 2
+#define INLINE __inline__
+#else
+#define INLINE
+#endif
+#endif
+
+/* forward declaration */
+typedef struct _bfd bfd;
+
+/* To squelch erroneous compiler warnings ("illegal pointer
+   combination") from the SVR3 compiler, we would like to typedef
+   boolean to int (it doesn't like functions which return boolean.
+   Making sure they are never implicitly declared to return int
+   doesn't seem to help).  But this file is not configured based on
+   the host.  */
+/* General rules: functions which are boolean return true on success
+   and false on failure (unless they're a predicate).   -- bfd.doc */
+/* I'm sure this is going to break something and someone is going to
+   force me to change it.  */
+/* typedef enum boolean {false, true} boolean; */
+/* Yup, SVR4 has a "typedef enum boolean" in <sys/types.h>  -fnf */
+/* It gets worse if the host also defines a true/false enum... -sts */
+/* And even worse if your compiler has built-in boolean types... -law */
+#if defined (__GNUG__) && (__GNUC__ > 2 || (__GNUC__ == 2 && __GNUC_MINOR__ >= 6))
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif
+#ifdef MPW
+/* Pre-emptive strike - get the file with the enum.  */
+#include <Types.h>
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif /* MPW */
+#ifndef TRUE_FALSE_ALREADY_DEFINED
+typedef enum bfd_boolean {false, true} boolean;
+#define BFD_TRUE_FALSE
+#else
+/* Use enum names that will appear nowhere else.  */
+typedef enum bfd_boolean {bfd_fffalse, bfd_tttrue} boolean;
+#endif
+
+/* Support for different sizes of target format ints and addresses.
+   If the type `long' is at least 64 bits, BFD_HOST_64BIT_LONG will be
+   set to 1 above.  Otherwise, if gcc is being used, this code will
+   use gcc's "long long" type.  Otherwise, BFD_HOST_64_BIT must be
+   defined above.  */
+
+#ifndef BFD_HOST_64_BIT
+# if BFD_HOST_64BIT_LONG
+#  define BFD_HOST_64_BIT long
+#  define BFD_HOST_U_64_BIT unsigned long
+# else
+#  ifdef __GNUC__
+#   if __GNUC__ >= 2
+#    define BFD_HOST_64_BIT long long
+#    define BFD_HOST_U_64_BIT unsigned long long
+#   endif /* __GNUC__ >= 2 */
+#  endif /* ! defined (__GNUC__) */
+# endif /* ! BFD_HOST_64BIT_LONG */
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+#ifdef BFD64
+
+#ifndef BFD_HOST_64_BIT
+ #error No 64 bit integer type available
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+typedef BFD_HOST_U_64_BIT bfd_vma;
+typedef BFD_HOST_64_BIT bfd_signed_vma;
+typedef BFD_HOST_U_64_BIT bfd_size_type;
+typedef BFD_HOST_U_64_BIT symvalue;
+
+#ifndef fprintf_vma
+#if BFD_HOST_64BIT_LONG
+#define sprintf_vma(s,x) sprintf (s, "%016lx", x)
+#define fprintf_vma(f,x) fprintf (f, "%016lx", x)
+#else
+#define _bfd_int64_low(x) ((unsigned long) (((x) & 0xffffffff)))
+#define _bfd_int64_high(x) ((unsigned long) (((x) >> 32) & 0xffffffff))
+#define fprintf_vma(s,x) \
+  fprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#define sprintf_vma(s,x) \
+  sprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#endif
+#endif
+
+#else /* not BFD64  */
+
+/* Represent a target address.  Also used as a generic unsigned type
+   which is guaranteed to be big enough to hold any arithmetic types
+   we need to deal with.  */
+typedef unsigned long bfd_vma;
+
+/* A generic signed type which is guaranteed to be big enough to hold any
+   arithmetic types we need to deal with.  Can be assumed to be compatible
+   with bfd_vma in the same way that signed and unsigned ints are compatible
+   (as parameters, in assignment, etc).  */
+typedef long bfd_signed_vma;
+
+typedef unsigned long symvalue;
+typedef unsigned long bfd_size_type;
+
+/* Print a bfd_vma x on stream s.  */
+#define fprintf_vma(s,x) fprintf (s, "%08lx", x)
+#define sprintf_vma(s,x) sprintf (s, "%08lx", x)
+
+#endif /* not BFD64  */
+
+/* A pointer to a position in a file.  */
+/* FIXME:  This should be using off_t from <sys/types.h>.
+   For now, try to avoid breaking stuff by not including <sys/types.h> here.
+   This will break on systems with 64-bit file offsets (e.g. 4.4BSD).
+   Probably the best long-term answer is to avoid using file_ptr AND off_t
+   in this header file, and to handle this in the BFD implementation
+   rather than in its interface.  */
+/* typedef off_t	file_ptr; */
+typedef bfd_signed_vma file_ptr;
+typedef bfd_vma ufile_ptr;
+
+extern void bfd_sprintf_vma PARAMS ((bfd *, char *, bfd_vma));
+extern void bfd_fprintf_vma PARAMS ((bfd *, PTR, bfd_vma));
+
+#define printf_vma(x) fprintf_vma(stdout,x)
+#define bfd_printf_vma(abfd,x) bfd_fprintf_vma (abfd,stdout,x)
+
+typedef unsigned int flagword;	/* 32 bits of flags */
+typedef unsigned char bfd_byte;
+
+/** File formats */
+
+typedef enum bfd_format {
+	      bfd_unknown = 0,	/* file format is unknown */
+	      bfd_object,	/* linker/assember/compiler output */
+	      bfd_archive,	/* object archive file */
+	      bfd_core,		/* core dump */
+	      bfd_type_end}	/* marks the end; don't use it! */
+         bfd_format;
+
+/* Values that may appear in the flags field of a BFD.  These also
+   appear in the object_flags field of the bfd_target structure, where
+   they indicate the set of flags used by that backend (not all flags
+   are meaningful for all object file formats) (FIXME: at the moment,
+   the object_flags values have mostly just been copied from backend
+   to another, and are not necessarily correct).  */
+
+/* No flags.  */
+#define BFD_NO_FLAGS   	0x00
+
+/* BFD contains relocation entries.  */
+#define HAS_RELOC   	0x01
+
+/* BFD is directly executable.  */
+#define EXEC_P      	0x02
+
+/* BFD has line number information (basically used for F_LNNO in a
+   COFF header).  */
+#define HAS_LINENO  	0x04
+
+/* BFD has debugging information.  */
+#define HAS_DEBUG   	0x08
+
+/* BFD has symbols.  */
+#define HAS_SYMS    	0x10
+
+/* BFD has local symbols (basically used for F_LSYMS in a COFF
+   header).  */
+#define HAS_LOCALS  	0x20
+
+/* BFD is a dynamic object.  */
+#define DYNAMIC     	0x40
+
+/* Text section is write protected (if D_PAGED is not set, this is
+   like an a.out NMAGIC file) (the linker sets this by default, but
+   clears it for -r or -N).  */
+#define WP_TEXT     	0x80
+
+/* BFD is dynamically paged (this is like an a.out ZMAGIC file) (the
+   linker sets this by default, but clears it for -r or -n or -N).  */
+#define D_PAGED     	0x100
+
+/* BFD is relaxable (this means that bfd_relax_section may be able to
+   do something) (sometimes bfd_relax_section can do something even if
+   this is not set).  */
+#define BFD_IS_RELAXABLE 0x200
+
+/* This may be set before writing out a BFD to request using a
+   traditional format.  For example, this is used to request that when
+   writing out an a.out object the symbols not be hashed to eliminate
+   duplicates.  */
+#define BFD_TRADITIONAL_FORMAT 0x400
+
+/* This flag indicates that the BFD contents are actually cached in
+   memory.  If this is set, iostream points to a bfd_in_memory struct.  */
+#define BFD_IN_MEMORY 0x800
+
+/* symbols and relocation */
+
+/* A count of carsyms (canonical archive symbols).  */
+typedef unsigned long symindex;
+
+/* How to perform a relocation.  */
+typedef const struct reloc_howto_struct reloc_howto_type;
+
+#define BFD_NO_MORE_SYMBOLS ((symindex) ~0)
+
+/* General purpose part of a symbol X;
+   target specific parts are in libcoff.h, libaout.h, etc.  */
+
+#define bfd_get_section(x) ((x)->section)
+#define bfd_get_output_section(x) ((x)->section->output_section)
+#define bfd_set_section(x,y) ((x)->section) = (y)
+#define bfd_asymbol_base(x) ((x)->section->vma)
+#define bfd_asymbol_value(x) (bfd_asymbol_base(x) + (x)->value)
+#define bfd_asymbol_name(x) ((x)->name)
+/*Perhaps future: #define bfd_asymbol_bfd(x) ((x)->section->owner)*/
+#define bfd_asymbol_bfd(x) ((x)->the_bfd)
+#define bfd_asymbol_flavour(x) (bfd_asymbol_bfd(x)->xvec->flavour)
+
+/* A canonical archive symbol.  */
+/* This is a type pun with struct ranlib on purpose! */
+typedef struct carsym {
+  char *name;
+  file_ptr file_offset;		/* look here to find the file */
+} carsym;			/* to make these you call a carsymogen */
+
+/* Used in generating armaps (archive tables of contents).
+   Perhaps just a forward definition would do? */
+struct orl {			/* output ranlib */
+  char **name;			/* symbol name */
+  union {
+    file_ptr pos;
+    bfd *abfd;
+  } u;				/* bfd* or file position */
+  int namidx;			/* index into string table */
+};
+
+/* Linenumber stuff */
+typedef struct lineno_cache_entry {
+  unsigned int line_number;	/* Linenumber from start of function*/
+  union {
+    struct symbol_cache_entry *sym; /* Function name */
+    bfd_vma offset;	    /* Offset into section */
+  } u;
+} alent;
+
+/* object and core file sections */
+
+#define	align_power(addr, align)	\
+	( ((addr) + ((1<<(align))-1)) & (-1 << (align)))
+
+typedef struct sec *sec_ptr;
+
+#define bfd_get_section_name(bfd, ptr) ((ptr)->name + 0)
+#define bfd_get_section_vma(bfd, ptr) ((ptr)->vma + 0)
+#define bfd_get_section_alignment(bfd, ptr) ((ptr)->alignment_power + 0)
+#define bfd_section_name(bfd, ptr) ((ptr)->name)
+#define bfd_section_size(bfd, ptr) (bfd_get_section_size_before_reloc(ptr))
+#define bfd_section_vma(bfd, ptr) ((ptr)->vma)
+#define bfd_section_lma(bfd, ptr) ((ptr)->lma)
+#define bfd_section_alignment(bfd, ptr) ((ptr)->alignment_power)
+#define bfd_get_section_flags(bfd, ptr) ((ptr)->flags + 0)
+#define bfd_get_section_userdata(bfd, ptr) ((ptr)->userdata)
+
+#define bfd_is_com_section(ptr) (((ptr)->flags & SEC_IS_COMMON) != 0)
+
+#define bfd_set_section_vma(bfd, ptr, val) (((ptr)->vma = (ptr)->lma= (val)), ((ptr)->user_set_vma = (boolean)true), true)
+#define bfd_set_section_alignment(bfd, ptr, val) (((ptr)->alignment_power = (val)),true)
+#define bfd_set_section_userdata(bfd, ptr, val) (((ptr)->userdata = (val)),true)
+
+typedef struct stat stat_type;
+
+typedef enum bfd_print_symbol
+{
+  bfd_print_symbol_name,
+  bfd_print_symbol_more,
+  bfd_print_symbol_all
+} bfd_print_symbol_type;
+
+/* Information about a symbol that nm needs.  */
+
+typedef struct _symbol_info
+{
+  symvalue value;
+  char type;
+  const char *name;            /* Symbol name.  */
+  unsigned char stab_type;     /* Stab type.  */
+  char stab_other;             /* Stab other.  */
+  short stab_desc;             /* Stab desc.  */
+  const char *stab_name;       /* String for stab type.  */
+} symbol_info;
+
+/* Get the name of a stabs type code.  */
+
+extern const char *bfd_get_stab_name PARAMS ((int));
+
+/* Hash table routines.  There is no way to free up a hash table.  */
+
+/* An element in the hash table.  Most uses will actually use a larger
+   structure, and an instance of this will be the first field.  */
+
+struct bfd_hash_entry
+{
+  /* Next entry for this hash code.  */
+  struct bfd_hash_entry *next;
+  /* String being hashed.  */
+  const char *string;
+  /* Hash code.  This is the full hash code, not the index into the
+     table.  */
+  unsigned long hash;
+};
+
+/* A hash table.  */
+
+struct bfd_hash_table
+{
+  /* The hash array.  */
+  struct bfd_hash_entry **table;
+  /* The number of slots in the hash table.  */
+  unsigned int size;
+  /* A function used to create new elements in the hash table.  The
+     first entry is itself a pointer to an element.  When this
+     function is first invoked, this pointer will be NULL.  However,
+     having the pointer permits a hierarchy of method functions to be
+     built each of which calls the function in the superclass.  Thus
+     each function should be written to allocate a new block of memory
+     only if the argument is NULL.  */
+  struct bfd_hash_entry *(*newfunc) PARAMS ((struct bfd_hash_entry *,
+					     struct bfd_hash_table *,
+					     const char *));
+   /* An objalloc for this hash table.  This is a struct objalloc *,
+     but we use PTR to avoid requiring the inclusion of objalloc.h.  */
+  PTR memory;
+};
+
+/* Initialize a hash table.  */
+extern boolean bfd_hash_table_init
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *)));
+
+/* Initialize a hash table specifying a size.  */
+extern boolean bfd_hash_table_init_n
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *),
+	   unsigned int size));
+
+/* Free up a hash table.  */
+extern void bfd_hash_table_free PARAMS ((struct bfd_hash_table *));
+
+/* Look up a string in a hash table.  If CREATE is true, a new entry
+   will be created for this string if one does not already exist.  The
+   COPY argument must be true if this routine should copy the string
+   into newly allocated memory when adding an entry.  */
+extern struct bfd_hash_entry *bfd_hash_lookup
+  PARAMS ((struct bfd_hash_table *, const char *, boolean create,
+	   boolean copy));
+
+/* Replace an entry in a hash table.  */
+extern void bfd_hash_replace
+  PARAMS ((struct bfd_hash_table *, struct bfd_hash_entry *old,
+	   struct bfd_hash_entry *nw));
+
+/* Base method for creating a hash table entry.  */
+extern struct bfd_hash_entry *bfd_hash_newfunc
+  PARAMS ((struct bfd_hash_entry *, struct bfd_hash_table *,
+	   const char *));
+
+/* Grab some space for a hash table entry.  */
+extern PTR bfd_hash_allocate PARAMS ((struct bfd_hash_table *,
+				      unsigned int));
+
+/* Traverse a hash table in a random order, calling a function on each
+   element.  If the function returns false, the traversal stops.  The
+   INFO argument is passed to the function.  */
+extern void bfd_hash_traverse PARAMS ((struct bfd_hash_table *,
+				       boolean (*) (struct bfd_hash_entry *,
+						    PTR),
+				       PTR info));
+
+#define COFF_SWAP_TABLE (PTR) &bfd_coff_std_swap_table
+
+/* User program access to BFD facilities */
+
+/* Direct I/O routines, for programs which know more about the object
+   file than BFD does.  Use higher level routines if possible.  */
+
+extern bfd_size_type bfd_bread PARAMS ((PTR, bfd_size_type, bfd *));
+extern bfd_size_type bfd_bwrite PARAMS ((const PTR, bfd_size_type, bfd *));
+extern int bfd_seek PARAMS ((bfd *, file_ptr, int));
+extern ufile_ptr bfd_tell PARAMS ((bfd *));
+extern int bfd_flush PARAMS ((bfd *));
+extern int bfd_stat PARAMS ((bfd *, struct stat *));
+
+/* Deprecated old routines.  */
+#if __GNUC__
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#else
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", (const char *) 0, 0, (const char *) 0), \
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", (const char *) 0, 0, (const char *) 0),\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#endif
+extern void warn_deprecated
+  PARAMS ((const char *, const char *, int, const char *));
+
+/* Cast from const char * to char * so that caller can assign to
+   a char * without a warning.  */
+#define bfd_get_filename(abfd) ((char *) (abfd)->filename)
+#define bfd_get_cacheable(abfd) ((abfd)->cacheable)
+#define bfd_get_format(abfd) ((abfd)->format)
+#define bfd_get_target(abfd) ((abfd)->xvec->name)
+#define bfd_get_flavour(abfd) ((abfd)->xvec->flavour)
+#define bfd_family_coff(abfd) \
+  (bfd_get_flavour (abfd) == bfd_target_coff_flavour || \
+   bfd_get_flavour (abfd) == bfd_target_xcoff_flavour)
+#define bfd_big_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_BIG)
+#define bfd_little_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_header_big_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_BIG)
+#define bfd_header_little_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_get_file_flags(abfd) ((abfd)->flags)
+#define bfd_applicable_file_flags(abfd) ((abfd)->xvec->object_flags)
+#define bfd_applicable_section_flags(abfd) ((abfd)->xvec->section_flags)
+#define bfd_my_archive(abfd) ((abfd)->my_archive)
+#define bfd_has_map(abfd) ((abfd)->has_armap)
+
+#define bfd_valid_reloc_types(abfd) ((abfd)->xvec->valid_reloc_types)
+#define bfd_usrdata(abfd) ((abfd)->usrdata)
+
+#define bfd_get_start_address(abfd) ((abfd)->start_address)
+#define bfd_get_symcount(abfd) ((abfd)->symcount)
+#define bfd_get_outsymbols(abfd) ((abfd)->outsymbols)
+#define bfd_count_sections(abfd) ((abfd)->section_count)
+
+#define bfd_get_symbol_leading_char(abfd) ((abfd)->xvec->symbol_leading_char)
+
+#define bfd_set_cacheable(abfd,bool) (((abfd)->cacheable = (boolean) (bool)), true)
+
+extern boolean bfd_cache_close PARAMS ((bfd *abfd));
+/* NB: This declaration should match the autogenerated one in libbfd.h.  */
+
+extern boolean bfd_record_phdr
+  PARAMS ((bfd *, unsigned long, boolean, flagword, boolean, bfd_vma,
+	   boolean, boolean, unsigned int, struct sec **));
+
+/* Byte swapping routines.  */
+
+bfd_vma		bfd_getb64	   PARAMS ((const unsigned char *));
+bfd_vma 	bfd_getl64	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_64 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_64 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb32	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl32	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_32 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_32 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb16	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl16	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_16 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_16 PARAMS ((const unsigned char *));
+void		bfd_putb64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb16	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl16	   PARAMS ((bfd_vma, unsigned char *));
+
+/* Byte swapping routines which take size and endiannes as arguments.  */
+
+bfd_vma         bfd_get_bits       PARAMS ((bfd_byte *, int, boolean));
+void            bfd_put_bits       PARAMS ((bfd_vma, bfd_byte *, int, boolean));
+
+/* Externally visible ECOFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct ecoff_debug_info;
+struct ecoff_debug_swap;
+struct ecoff_extr;
+struct symbol_cache_entry;
+struct bfd_link_info;
+struct bfd_link_hash_entry;
+struct bfd_elf_version_tree;
+#endif
+extern bfd_vma bfd_ecoff_get_gp_value PARAMS ((bfd * abfd));
+extern boolean bfd_ecoff_set_gp_value PARAMS ((bfd *abfd, bfd_vma gp_value));
+extern boolean bfd_ecoff_set_regmasks
+  PARAMS ((bfd *abfd, unsigned long gprmask, unsigned long fprmask,
+	   unsigned long *cprmask));
+extern PTR bfd_ecoff_debug_init
+  PARAMS ((bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern void bfd_ecoff_debug_free
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   bfd *input_bfd, struct ecoff_debug_info *input_debug,
+	   const struct ecoff_debug_swap *input_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate_other
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap, bfd *input_bfd,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_externals
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   boolean relocateable,
+	   boolean (*get_extr) (struct symbol_cache_entry *,
+				struct ecoff_extr *),
+	   void (*set_index) (struct symbol_cache_entry *,
+			      bfd_size_type)));
+extern boolean bfd_ecoff_debug_one_external
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   const char *name, struct ecoff_extr *esym));
+extern bfd_size_type bfd_ecoff_debug_size
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap));
+extern boolean bfd_ecoff_write_debug
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap, file_ptr where));
+extern boolean bfd_ecoff_write_accumulated_debug
+  PARAMS ((PTR handle, bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   struct bfd_link_info *info, file_ptr where));
+extern boolean bfd_mips_ecoff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* Externally visible ELF routines.  */
+
+struct bfd_link_needed_list
+{
+  struct bfd_link_needed_list *next;
+  bfd *by;
+  const char *name;
+};
+
+extern boolean bfd_elf32_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern boolean bfd_elf64_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern struct bfd_link_needed_list *bfd_elf_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_elf_get_bfd_needed_list
+  PARAMS ((bfd *, struct bfd_link_needed_list **));
+extern boolean bfd_elf32_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern boolean bfd_elf64_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern void bfd_elf_set_dt_needed_name PARAMS ((bfd *, const char *));
+extern void bfd_elf_set_dt_needed_soname PARAMS ((bfd *, const char *));
+extern const char *bfd_elf_get_dt_soname PARAMS ((bfd *));
+extern struct bfd_link_needed_list *bfd_elf_get_runpath_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* Return an upper bound on the number of bytes required to store a
+   copy of ABFD's program header table entries.  Return -1 if an error
+   occurs; bfd_get_error will return an appropriate code.  */
+extern long bfd_get_elf_phdr_upper_bound PARAMS ((bfd *abfd));
+
+/* Copy ABFD's program header table entries to *PHDRS.  The entries
+   will be stored as an array of Elf_Internal_Phdr structures, as
+   defined in include/elf/internal.h.  To find out how large the
+   buffer needs to be, call bfd_get_elf_phdr_upper_bound.
+
+   Return the number of program header table entries read, or -1 if an
+   error occurs; bfd_get_error will return an appropriate code.  */
+extern int bfd_get_elf_phdrs PARAMS ((bfd *abfd, void *phdrs));
+
+/* Return the arch_size field of an elf bfd, or -1 if not elf.  */
+extern int bfd_get_arch_size PARAMS ((bfd *));
+
+/* Return true if address "naturally" sign extends, or -1 if not elf.  */
+extern int bfd_get_sign_extend_vma PARAMS ((bfd *));
+
+extern boolean bfd_m68k_elf32_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* SunOS shared library support routines for the linker.  */
+
+extern struct bfd_link_needed_list *bfd_sunos_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sunos_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_sunos_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec **, struct sec **,
+	   struct sec **));
+
+/* Linux shared library support routines for the linker.  */
+
+extern boolean bfd_i386linux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_m68klinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sparclinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* mmap hacks */
+
+struct _bfd_window_internal;
+typedef struct _bfd_window_internal bfd_window_internal;
+
+typedef struct _bfd_window {
+  /* What the user asked for.  */
+  PTR data;
+  bfd_size_type size;
+  /* The actual window used by BFD.  Small user-requested read-only
+     regions sharing a page may share a single window into the object
+     file.  Read-write versions shouldn't until I've fixed things to
+     keep track of which portions have been claimed by the
+     application; don't want to give the same region back when the
+     application wants two writable copies!  */
+  struct _bfd_window_internal *i;
+} bfd_window;
+
+extern void bfd_init_window PARAMS ((bfd_window *));
+extern void bfd_free_window PARAMS ((bfd_window *));
+extern boolean bfd_get_file_window
+  PARAMS ((bfd *, file_ptr, bfd_size_type, bfd_window *, boolean));
+
+/* XCOFF support routines for the linker.  */
+
+extern boolean bfd_xcoff_link_record_set
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_size_type));
+extern boolean bfd_xcoff_import_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_vma, const char *, const char *, const char *, unsigned int));
+extern boolean bfd_xcoff_export_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *));
+extern boolean bfd_xcoff_link_count_reloc
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, const char *,
+	   unsigned long, unsigned long, unsigned long, boolean,
+	   int, boolean, boolean, struct sec **));
+
+/* Externally visible COFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct internal_syment;
+union internal_auxent;
+#endif
+
+extern boolean bfd_coff_get_syment
+  PARAMS ((bfd *, struct symbol_cache_entry *, struct internal_syment *));
+
+extern boolean bfd_coff_get_auxent
+  PARAMS ((bfd *, struct symbol_cache_entry *, int, union internal_auxent *));
+
+extern boolean bfd_coff_set_symbol_class
+  PARAMS ((bfd *, struct symbol_cache_entry *, unsigned int));
+
+extern boolean bfd_m68k_coff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* PE ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_pe_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_pe_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_pe_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* ELF ARM Interworking support.  Called from linker.  */
+extern boolean bfd_elf32_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_elf32_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_elf32_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* TI COFF load page support.  */
+extern void bfd_ticoff_set_section_load_page
+  PARAMS ((struct sec *, int));
+
+extern int bfd_ticoff_get_section_load_page
+  PARAMS ((struct sec *));
+
+/* And more from the source.  */
+void
+bfd_init PARAMS ((void));
+
+bfd *
+bfd_openr PARAMS ((const char *filename, const char *target));
+
+bfd *
+bfd_fdopenr PARAMS ((const char *filename, const char *target, int fd));
+
+bfd *
+bfd_openstreamr PARAMS ((const char *, const char *, PTR));
+
+bfd *
+bfd_openw PARAMS ((const char *filename, const char *target));
+
+boolean
+bfd_close PARAMS ((bfd *abfd));
+
+boolean
+bfd_close_all_done PARAMS ((bfd *));
+
+bfd *
+bfd_create PARAMS ((const char *filename, bfd *templ));
+
+boolean
+bfd_make_writable PARAMS ((bfd *abfd));
+
+boolean
+bfd_make_readable PARAMS ((bfd *abfd));
+
+
+/* Byte swapping macros for user section data.  */
+
+#define bfd_put_8(abfd, val, ptr) \
+                ((void) (*((unsigned char *) (ptr)) = (unsigned char) (val)))
+#define bfd_put_signed_8 \
+               bfd_put_8
+#define bfd_get_8(abfd, ptr) \
+                (*(unsigned char *) (ptr) & 0xff)
+#define bfd_get_signed_8(abfd, ptr) \
+               (((*(unsigned char *) (ptr) & 0xff) ^ 0x80) - 0x80)
+
+#define bfd_put_16(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx16, ((val),(ptr)))
+#define bfd_put_signed_16 \
+                bfd_put_16
+#define bfd_get_16(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx16, (ptr))
+#define bfd_get_signed_16(abfd, ptr) \
+                BFD_SEND (abfd, bfd_getx_signed_16, (ptr))
+
+#define bfd_put_32(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx32, ((val),(ptr)))
+#define bfd_put_signed_32 \
+                bfd_put_32
+#define bfd_get_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx32, (ptr))
+#define bfd_get_signed_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_32, (ptr))
+
+#define bfd_put_64(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx64, ((val), (ptr)))
+#define bfd_put_signed_64 \
+                bfd_put_64
+#define bfd_get_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx64, (ptr))
+#define bfd_get_signed_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_64, (ptr))
+
+#define bfd_get(bits, abfd, ptr)                               \
+                ( (bits) ==  8 ? (bfd_vma) bfd_get_8 (abfd, ptr)       \
+                : (bits) == 16 ? bfd_get_16 (abfd, ptr)        \
+                : (bits) == 32 ? bfd_get_32 (abfd, ptr)        \
+                : (bits) == 64 ? bfd_get_64 (abfd, ptr)        \
+                : (abort (), (bfd_vma) - 1))
+
+#define bfd_put(bits, abfd, val, ptr)                          \
+                ( (bits) ==  8 ? bfd_put_8  (abfd, val, ptr)   \
+                : (bits) == 16 ? bfd_put_16 (abfd, val, ptr)   \
+                : (bits) == 32 ? bfd_put_32 (abfd, val, ptr)   \
+                : (bits) == 64 ? bfd_put_64 (abfd, val, ptr)   \
+                : (abort (), (void) 0))
+
+
+/* Byte swapping macros for file header data.  */
+
+#define bfd_h_put_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_put_signed_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_get_8(abfd, ptr) \
+  bfd_get_8 (abfd, ptr)
+#define bfd_h_get_signed_8(abfd, ptr) \
+  bfd_get_signed_8 (abfd, ptr)
+
+#define bfd_h_put_16(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx16, (val, ptr))
+#define bfd_h_put_signed_16 \
+  bfd_h_put_16
+#define bfd_h_get_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx16, (ptr))
+#define bfd_h_get_signed_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_16, (ptr))
+
+#define bfd_h_put_32(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx32, (val, ptr))
+#define bfd_h_put_signed_32 \
+  bfd_h_put_32
+#define bfd_h_get_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx32, (ptr))
+#define bfd_h_get_signed_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_32, (ptr))
+
+#define bfd_h_put_64(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx64, (val, ptr))
+#define bfd_h_put_signed_64 \
+  bfd_h_put_64
+#define bfd_h_get_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx64, (ptr))
+#define bfd_h_get_signed_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_64, (ptr))
+
+/* Refinements on the above, which should eventually go away.  Save
+   cluttering the source with (bfd_vma) and (bfd_byte *) casts.  */
+
+#define H_PUT_64(abfd, val, where) \
+  bfd_h_put_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_32(abfd, val, where) \
+  bfd_h_put_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_16(abfd, val, where) \
+  bfd_h_put_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_8 bfd_h_put_8
+
+#define H_PUT_S64(abfd, val, where) \
+  bfd_h_put_signed_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S32(abfd, val, where) \
+  bfd_h_put_signed_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S16(abfd, val, where) \
+  bfd_h_put_signed_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S8 bfd_h_put_signed_8
+
+#define H_GET_64(abfd, where) \
+  bfd_h_get_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_32(abfd, where) \
+  bfd_h_get_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_16(abfd, where) \
+  bfd_h_get_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_8 bfd_h_get_8
+
+#define H_GET_S64(abfd, where) \
+  bfd_h_get_signed_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S32(abfd, where) \
+  bfd_h_get_signed_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S16(abfd, where) \
+  bfd_h_get_signed_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S8 bfd_h_get_signed_8
+
+
+/* This structure is used for a comdat section, as in PE.  A comdat
+   section is associated with a particular symbol.  When the linker
+   sees a comdat section, it keeps only one of the sections with a
+   given name and associated with a given symbol.  */
+
+struct bfd_comdat_info
+{
+  /* The name of the symbol associated with a comdat section.  */
+  const char *name;
+
+  /* The local symbol table index of the symbol associated with a
+     comdat section.  This is only meaningful to the object file format
+     specific code; it is not an index into the list returned by
+     bfd_canonicalize_symtab.  */
+  long symbol;
+};
+
+typedef struct sec
+{
+  /* The name of the section; the name isn't a copy, the pointer is
+     the same as that passed to bfd_make_section.  */
+
+  const char *name;
+
+  /* A unique sequence number.  */
+
+  int id;
+
+  /* Which section in the bfd; 0..n-1 as sections are created in a bfd.  */
+
+  int index;
+
+  /* The next section in the list belonging to the BFD, or NULL.  */
+
+  struct sec *next;
+
+  /* The field flags contains attributes of the section. Some
+     flags are read in from the object file, and some are
+     synthesized from other information.  */
+
+  flagword flags;
+
+#define SEC_NO_FLAGS   0x000
+
+  /* Tells the OS to allocate space for this section when loading.
+     This is clear for a section containing debug information only.  */
+#define SEC_ALLOC      0x001
+
+  /* Tells the OS to load the section from the file when loading.
+     This is clear for a .bss section.  */
+#define SEC_LOAD       0x002
+
+  /* The section contains data still to be relocated, so there is
+     some relocation information too.  */
+#define SEC_RELOC      0x004
+
+  /* ELF reserves 4 processor specific bits and 8 operating system
+     specific bits in sh_flags; at present we can get away with just
+     one in communicating between the assembler and BFD, but this
+     isn't a good long-term solution.  */
+#define SEC_ARCH_BIT_0 0x008
+
+  /* A signal to the OS that the section contains read only data.  */
+#define SEC_READONLY   0x010
+
+  /* The section contains code only.  */
+#define SEC_CODE       0x020
+
+  /* The section contains data only.  */
+#define SEC_DATA       0x040
+
+  /* The section will reside in ROM.  */
+#define SEC_ROM        0x080
+
+  /* The section contains constructor information. This section
+     type is used by the linker to create lists of constructors and
+     destructors used by <<g++>>. When a back end sees a symbol
+     which should be used in a constructor list, it creates a new
+     section for the type of name (e.g., <<__CTOR_LIST__>>), attaches
+     the symbol to it, and builds a relocation. To build the lists
+     of constructors, all the linker has to do is catenate all the
+     sections called <<__CTOR_LIST__>> and relocate the data
+     contained within - exactly the operations it would peform on
+     standard data.  */
+#define SEC_CONSTRUCTOR 0x100
+
+  /* The section is a constructor, and should be placed at the
+     end of the text, data, or bss section(?).  */
+#define SEC_CONSTRUCTOR_TEXT 0x1100
+#define SEC_CONSTRUCTOR_DATA 0x2100
+#define SEC_CONSTRUCTOR_BSS  0x3100
+
+  /* The section has contents - a data section could be
+     <<SEC_ALLOC>> | <<SEC_HAS_CONTENTS>>; a debug section could be
+     <<SEC_HAS_CONTENTS>>  */
+#define SEC_HAS_CONTENTS 0x200
+
+  /* An instruction to the linker to not output the section
+     even if it has information which would normally be written.  */
+#define SEC_NEVER_LOAD 0x400
+
+  /* The section is a COFF shared library section.  This flag is
+     only for the linker.  If this type of section appears in
+     the input file, the linker must copy it to the output file
+     without changing the vma or size.  FIXME: Although this
+     was originally intended to be general, it really is COFF
+     specific (and the flag was renamed to indicate this).  It
+     might be cleaner to have some more general mechanism to
+     allow the back end to control what the linker does with
+     sections.  */
+#define SEC_COFF_SHARED_LIBRARY 0x800
+
+  /* The section has GOT references.  This flag is only for the
+     linker, and is currently only used by the elf32-hppa back end.
+     It will be set if global offset table references were detected
+     in this section, which indicate to the linker that the section
+     contains PIC code, and must be handled specially when doing a
+     static link.  */
+#define SEC_HAS_GOT_REF 0x4000
+
+  /* The section contains common symbols (symbols may be defined
+     multiple times, the value of a symbol is the amount of
+     space it requires, and the largest symbol value is the one
+     used).  Most targets have exactly one of these (which we
+     translate to bfd_com_section_ptr), but ECOFF has two.  */
+#define SEC_IS_COMMON 0x8000
+
+  /* The section contains only debugging information.  For
+     example, this is set for ELF .debug and .stab sections.
+     strip tests this flag to see if a section can be
+     discarded.  */
+#define SEC_DEBUGGING 0x10000
+
+  /* The contents of this section are held in memory pointed to
+     by the contents field.  This is checked by bfd_get_section_contents,
+     and the data is retrieved from memory if appropriate.  */
+#define SEC_IN_MEMORY 0x20000
+
+  /* The contents of this section are to be excluded by the
+     linker for executable and shared objects unless those
+     objects are to be further relocated.  */
+#define SEC_EXCLUDE 0x40000
+
+  /* The contents of this section are to be sorted based on the sum of
+     the symbol and addend values specified by the associated relocation
+     entries.  Entries without associated relocation entries will be
+     appended to the end of the section in an unspecified order.  */
+#define SEC_SORT_ENTRIES 0x80000
+
+  /* When linking, duplicate sections of the same name should be
+     discarded, rather than being combined into a single section as
+     is usually done.  This is similar to how common symbols are
+     handled.  See SEC_LINK_DUPLICATES below.  */
+#define SEC_LINK_ONCE 0x100000
+
+  /* If SEC_LINK_ONCE is set, this bitfield describes how the linker
+     should handle duplicate sections.  */
+#define SEC_LINK_DUPLICATES 0x600000
+
+  /* This value for SEC_LINK_DUPLICATES means that duplicate
+     sections with the same name should simply be discarded.  */
+#define SEC_LINK_DUPLICATES_DISCARD 0x0
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if there are any duplicate sections, although
+     it should still only link one copy.  */
+#define SEC_LINK_DUPLICATES_ONE_ONLY 0x200000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections are a different size.  */
+#define SEC_LINK_DUPLICATES_SAME_SIZE 0x400000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections contain different
+     contents.  */
+#define SEC_LINK_DUPLICATES_SAME_CONTENTS 0x600000
+
+  /* This section was created by the linker as part of dynamic
+     relocation or other arcane processing.  It is skipped when
+     going through the first-pass output, trusting that someone
+     else up the line will take care of it later.  */
+#define SEC_LINKER_CREATED 0x800000
+
+  /* This section should not be subject to garbage collection.  */
+#define SEC_KEEP 0x1000000
+
+  /* This section contains "short" data, and should be placed
+     "near" the GP.  */
+#define SEC_SMALL_DATA 0x2000000
+
+  /* This section contains data which may be shared with other
+     executables or shared objects.  */
+#define SEC_SHARED 0x4000000
+
+  /* When a section with this flag is being linked, then if the size of
+     the input section is less than a page, it should not cross a page
+     boundary.  If the size of the input section is one page or more, it
+     should be aligned on a page boundary.  */
+#define SEC_BLOCK 0x8000000
+
+  /* Conditionally link this section; do not link if there are no
+     references found to any symbol in the section.  */
+#define SEC_CLINK 0x10000000
+
+  /* Attempt to merge identical entities in the section.
+     Entity size is given in the entsize field.  */
+#define SEC_MERGE 0x20000000
+
+  /* If given with SEC_MERGE, entities to merge are zero terminated
+     strings where entsize specifies character size instead of fixed
+     size entries.  */
+#define SEC_STRINGS 0x40000000
+
+  /* This section contains data about section groups.  */
+#define SEC_GROUP 0x80000000
+
+  /*  End of section flags.  */
+
+  /* Some internal packed boolean fields.  */
+
+  /* See the vma field.  */
+  unsigned int user_set_vma : 1;
+
+  /* Whether relocations have been processed.  */
+  unsigned int reloc_done : 1;
+
+  /* A mark flag used by some of the linker backends.  */
+  unsigned int linker_mark : 1;
+
+  /* Another mark flag used by some of the linker backends.  Set for
+     output sections that have an input section.  */
+  unsigned int linker_has_input : 1;
+
+  /* A mark flag used by some linker backends for garbage collection.  */
+  unsigned int gc_mark : 1;
+
+  /* Used by the ELF code to mark sections which have been allocated
+     to segments.  */
+  unsigned int segment_mark : 1;
+
+  /* End of internal packed boolean fields.  */
+
+  /*  The virtual memory address of the section - where it will be
+      at run time.  The symbols are relocated against this.  The
+      user_set_vma flag is maintained by bfd; if it's not set, the
+      backend can assign addresses (for example, in <<a.out>>, where
+      the default address for <<.data>> is dependent on the specific
+      target and various flags).  */
+
+  bfd_vma vma;
+
+  /*  The load address of the section - where it would be in a
+      rom image; really only used for writing section header
+      information. */
+
+  bfd_vma lma;
+
+  /* The size of the section in octets, as it will be output.
+     Contains a value even if the section has no contents (e.g., the
+     size of <<.bss>>).  This will be filled in after relocation.  */
+
+  bfd_size_type _cooked_size;
+
+  /* The original size on disk of the section, in octets.  Normally this
+     value is the same as the size, but if some relaxing has
+     been done, then this value will be bigger.  */
+
+  bfd_size_type _raw_size;
+
+  /* If this section is going to be output, then this value is the
+     offset in *bytes* into the output section of the first byte in the
+     input section (byte ==> smallest addressable unit on the
+     target).  In most cases, if this was going to start at the
+     100th octet (8-bit quantity) in the output section, this value
+     would be 100.  However, if the target byte size is 16 bits
+     (bfd_octets_per_byte is "2"), this value would be 50.  */
+
+  bfd_vma output_offset;
+
+  /* The output section through which to map on output.  */
+
+  struct sec *output_section;
+
+  /* The alignment requirement of the section, as an exponent of 2 -
+     e.g., 3 aligns to 2^3 (or 8).  */
+
+  unsigned int alignment_power;
+
+  /* If an input section, a pointer to a vector of relocation
+     records for the data in this section.  */
+
+  struct reloc_cache_entry *relocation;
+
+  /* If an output section, a pointer to a vector of pointers to
+     relocation records for the data in this section.  */
+
+  struct reloc_cache_entry **orelocation;
+
+  /* The number of relocation records in one of the above  */
+
+  unsigned reloc_count;
+
+  /* Information below is back end specific - and not always used
+     or updated.  */
+
+  /* File position of section data.  */
+
+  file_ptr filepos;
+
+  /* File position of relocation info.  */
+
+  file_ptr rel_filepos;
+
+  /* File position of line data.  */
+
+  file_ptr line_filepos;
+
+  /* Pointer to data for applications.  */
+
+  PTR userdata;
+
+  /* If the SEC_IN_MEMORY flag is set, this points to the actual
+     contents.  */
+  unsigned char *contents;
+
+  /* Attached line number information.  */
+
+  alent *lineno;
+
+  /* Number of line number records.  */
+
+  unsigned int lineno_count;
+
+  /* Entity size for merging purposes.  */
+
+  unsigned int entsize;
+
+  /* Optional information about a COMDAT entry; NULL if not COMDAT.  */
+
+  struct bfd_comdat_info *comdat;
+
+  /* When a section is being output, this value changes as more
+     linenumbers are written out.  */
+
+  file_ptr moving_line_filepos;
+
+  /* What the section number is in the target world.  */
+
+  int target_index;
+
+  PTR used_by_bfd;
+
+  /* If this is a constructor section then here is a list of the
+     relocations created to relocate items within it.  */
+
+  struct relent_chain *constructor_chain;
+
+  /* The BFD which owns the section.  */
+
+  bfd *owner;
+
+  /* A symbol which points at this section only */
+  struct symbol_cache_entry *symbol;
+  struct symbol_cache_entry **symbol_ptr_ptr;
+
+  struct bfd_link_order *link_order_head;
+  struct bfd_link_order *link_order_tail;
+} asection ;
+
+/* These sections are global, and are managed by BFD.  The application
+   and target back end are not permitted to change the values in
+   these sections.  New code should use the section_ptr macros rather
+   than referring directly to the const sections.  The const sections
+   may eventually vanish.  */
+#define BFD_ABS_SECTION_NAME "*ABS*"
+#define BFD_UND_SECTION_NAME "*UND*"
+#define BFD_COM_SECTION_NAME "*COM*"
+#define BFD_IND_SECTION_NAME "*IND*"
+
+/* the absolute section */
+extern const asection bfd_abs_section;
+#define bfd_abs_section_ptr ((asection *) &bfd_abs_section)
+#define bfd_is_abs_section(sec) ((sec) == bfd_abs_section_ptr)
+/* Pointer to the undefined section */
+extern const asection bfd_und_section;
+#define bfd_und_section_ptr ((asection *) &bfd_und_section)
+#define bfd_is_und_section(sec) ((sec) == bfd_und_section_ptr)
+/* Pointer to the common section */
+extern const asection bfd_com_section;
+#define bfd_com_section_ptr ((asection *) &bfd_com_section)
+/* Pointer to the indirect section */
+extern const asection bfd_ind_section;
+#define bfd_ind_section_ptr ((asection *) &bfd_ind_section)
+#define bfd_is_ind_section(sec) ((sec) == bfd_ind_section_ptr)
+
+extern const struct symbol_cache_entry * const bfd_abs_symbol;
+extern const struct symbol_cache_entry * const bfd_com_symbol;
+extern const struct symbol_cache_entry * const bfd_und_symbol;
+extern const struct symbol_cache_entry * const bfd_ind_symbol;
+#define bfd_get_section_size_before_reloc(section) \
+     ((section)->reloc_done ? (abort (), (bfd_size_type) 1) \
+                            : (section)->_raw_size)
+#define bfd_get_section_size_after_reloc(section) \
+     ((section)->reloc_done ? (section)->_cooked_size \
+                            : (abort (), (bfd_size_type) 1))
+asection *
+bfd_get_section_by_name PARAMS ((bfd *abfd, const char *name));
+
+char *
+bfd_get_unique_section_name PARAMS ((bfd *abfd,
+    const char *templat,
+    int *count));
+
+asection *
+bfd_make_section_old_way PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section_anyway PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section PARAMS ((bfd *, const char *name));
+
+boolean
+bfd_set_section_flags PARAMS ((bfd *abfd, asection *sec, flagword flags));
+
+void
+bfd_map_over_sections PARAMS ((bfd *abfd,
+    void (*func) (bfd *abfd,
+    asection *sect,
+    PTR obj),
+    PTR obj));
+
+boolean
+bfd_set_section_size PARAMS ((bfd *abfd, asection *sec, bfd_size_type val));
+
+boolean
+bfd_set_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR data, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_get_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR location, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_copy_private_section_data PARAMS ((bfd *ibfd, asection *isec,
+    bfd *obfd, asection *osec));
+
+#define bfd_copy_private_section_data(ibfd, isection, obfd, osection) \
+     BFD_SEND (obfd, _bfd_copy_private_section_data, \
+               (ibfd, isection, obfd, osection))
+void
+_bfd_strip_section_from_output PARAMS ((struct bfd_link_info *info, asection *section));
+
+enum bfd_architecture
+{
+  bfd_arch_unknown,   /* File arch not known */
+  bfd_arch_obscure,   /* Arch known, not one of these */
+  bfd_arch_m68k,      /* Motorola 68xxx */
+#define bfd_mach_m68000 1
+#define bfd_mach_m68008 2
+#define bfd_mach_m68010 3
+#define bfd_mach_m68020 4
+#define bfd_mach_m68030 5
+#define bfd_mach_m68040 6
+#define bfd_mach_m68060 7
+#define bfd_mach_cpu32  8
+#define bfd_mach_mcf5200  9
+#define bfd_mach_mcf5206e 10
+#define bfd_mach_mcf5307  11
+#define bfd_mach_mcf5407  12
+  bfd_arch_vax,       /* DEC Vax */
+  bfd_arch_i960,      /* Intel 960 */
+    /* The order of the following is important.
+       lower number indicates a machine type that
+       only accepts a subset of the instructions
+       available to machines with higher numbers.
+       The exception is the "ca", which is
+       incompatible with all other machines except
+       "core". */
+
+#define bfd_mach_i960_core      1
+#define bfd_mach_i960_ka_sa     2
+#define bfd_mach_i960_kb_sb     3
+#define bfd_mach_i960_mc        4
+#define bfd_mach_i960_xa        5
+#define bfd_mach_i960_ca        6
+#define bfd_mach_i960_jx        7
+#define bfd_mach_i960_hx        8
+
+  bfd_arch_a29k,      /* AMD 29000 */
+  bfd_arch_sparc,     /* SPARC */
+#define bfd_mach_sparc                 1
+/* The difference between v8plus and v9 is that v9 is a true 64 bit env.  */
+#define bfd_mach_sparc_sparclet        2
+#define bfd_mach_sparc_sparclite       3
+#define bfd_mach_sparc_v8plus          4
+#define bfd_mach_sparc_v8plusa         5 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_sparclite_le    6
+#define bfd_mach_sparc_v9              7
+#define bfd_mach_sparc_v9a             8 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_v8plusb         9 /* with cheetah add'ns */
+#define bfd_mach_sparc_v9b             10 /* with cheetah add'ns */
+/* Nonzero if MACH has the v9 instruction set.  */
+#define bfd_mach_sparc_v9_p(mach) \
+  ((mach) >= bfd_mach_sparc_v8plus && (mach) <= bfd_mach_sparc_v9b \
+   && (mach) != bfd_mach_sparc_sparclite_le)
+  bfd_arch_mips,      /* MIPS Rxxxx */
+#define bfd_mach_mips3000              3000
+#define bfd_mach_mips3900              3900
+#define bfd_mach_mips4000              4000
+#define bfd_mach_mips4010              4010
+#define bfd_mach_mips4100              4100
+#define bfd_mach_mips4111              4111
+#define bfd_mach_mips4300              4300
+#define bfd_mach_mips4400              4400
+#define bfd_mach_mips4600              4600
+#define bfd_mach_mips4650              4650
+#define bfd_mach_mips5000              5000
+#define bfd_mach_mips6000              6000
+#define bfd_mach_mips8000              8000
+#define bfd_mach_mips10000             10000
+#define bfd_mach_mips12000             12000
+#define bfd_mach_mips16                16
+#define bfd_mach_mips5                 5
+#define bfd_mach_mips_sb1              12310201 /* octal 'SB', 01 */
+#define bfd_mach_mipsisa32             32
+#define bfd_mach_mipsisa64             64
+  bfd_arch_i386,      /* Intel 386 */
+#define bfd_mach_i386_i386 0
+#define bfd_mach_i386_i8086 1
+#define bfd_mach_i386_i386_intel_syntax 2
+#define bfd_mach_x86_64 3
+#define bfd_mach_x86_64_intel_syntax 4
+  bfd_arch_we32k,     /* AT&T WE32xxx */
+  bfd_arch_tahoe,     /* CCI/Harris Tahoe */
+  bfd_arch_i860,      /* Intel 860 */
+  bfd_arch_i370,      /* IBM 360/370 Mainframes */
+  bfd_arch_romp,      /* IBM ROMP PC/RT */
+  bfd_arch_alliant,   /* Alliant */
+  bfd_arch_convex,    /* Convex */
+  bfd_arch_m88k,      /* Motorola 88xxx */
+  bfd_arch_pyramid,   /* Pyramid Technology */
+  bfd_arch_h8300,     /* Hitachi H8/300 */
+#define bfd_mach_h8300   1
+#define bfd_mach_h8300h  2
+#define bfd_mach_h8300s  3
+  bfd_arch_pdp11,     /* DEC PDP-11 */
+  bfd_arch_powerpc,   /* PowerPC */
+#define bfd_mach_ppc           0
+#define bfd_mach_ppc_403       403
+#define bfd_mach_ppc_403gc     4030
+#define bfd_mach_ppc_505       505
+#define bfd_mach_ppc_601       601
+#define bfd_mach_ppc_602       602
+#define bfd_mach_ppc_603       603
+#define bfd_mach_ppc_ec603e    6031
+#define bfd_mach_ppc_604       604
+#define bfd_mach_ppc_620       620
+#define bfd_mach_ppc_630       630
+#define bfd_mach_ppc_750       750
+#define bfd_mach_ppc_860       860
+#define bfd_mach_ppc_a35       35
+#define bfd_mach_ppc_rs64ii    642
+#define bfd_mach_ppc_rs64iii   643
+#define bfd_mach_ppc_7400      7400
+  bfd_arch_rs6000,    /* IBM RS/6000 */
+#define bfd_mach_rs6k          0
+#define bfd_mach_rs6k_rs1      6001
+#define bfd_mach_rs6k_rsc      6003
+#define bfd_mach_rs6k_rs2      6002
+  bfd_arch_hppa,      /* HP PA RISC */
+  bfd_arch_d10v,      /* Mitsubishi D10V */
+#define bfd_mach_d10v          0
+#define bfd_mach_d10v_ts2      2
+#define bfd_mach_d10v_ts3      3
+  bfd_arch_d30v,      /* Mitsubishi D30V */
+  bfd_arch_m68hc11,   /* Motorola 68HC11 */
+  bfd_arch_m68hc12,   /* Motorola 68HC12 */
+  bfd_arch_z8k,       /* Zilog Z8000 */
+#define bfd_mach_z8001         1
+#define bfd_mach_z8002         2
+  bfd_arch_h8500,     /* Hitachi H8/500 */
+  bfd_arch_sh,        /* Hitachi SH */
+#define bfd_mach_sh            0
+#define bfd_mach_sh2        0x20
+#define bfd_mach_sh_dsp     0x2d
+#define bfd_mach_sh3        0x30
+#define bfd_mach_sh3_dsp    0x3d
+#define bfd_mach_sh3e       0x3e
+#define bfd_mach_sh4        0x40
+  bfd_arch_alpha,     /* Dec Alpha */
+#define bfd_mach_alpha_ev4  0x10
+#define bfd_mach_alpha_ev5  0x20
+#define bfd_mach_alpha_ev6  0x30
+  bfd_arch_arm,       /* Advanced Risc Machines ARM */
+#define bfd_mach_arm_2         1
+#define bfd_mach_arm_2a        2
+#define bfd_mach_arm_3         3
+#define bfd_mach_arm_3M        4
+#define bfd_mach_arm_4         5
+#define bfd_mach_arm_4T        6
+#define bfd_mach_arm_5         7
+#define bfd_mach_arm_5T        8
+#define bfd_mach_arm_5TE       9
+#define bfd_mach_arm_XScale    10
+  bfd_arch_ns32k,     /* National Semiconductors ns32000 */
+  bfd_arch_w65,       /* WDC 65816 */
+  bfd_arch_tic30,     /* Texas Instruments TMS320C30 */
+  bfd_arch_tic54x,    /* Texas Instruments TMS320C54X */
+  bfd_arch_tic80,     /* TI TMS320c80 (MVP) */
+  bfd_arch_v850,      /* NEC V850 */
+#define bfd_mach_v850          0
+#define bfd_mach_v850e         'E'
+#define bfd_mach_v850ea        'A'
+  bfd_arch_arc,       /* ARC Cores */
+#define bfd_mach_arc_5         0
+#define bfd_mach_arc_6         1
+#define bfd_mach_arc_7         2
+#define bfd_mach_arc_8         3
+  bfd_arch_m32r,      /* Mitsubishi M32R/D */
+#define bfd_mach_m32r          0 /* backwards compatibility */
+#define bfd_mach_m32rx         'x'
+  bfd_arch_mn10200,   /* Matsushita MN10200 */
+  bfd_arch_mn10300,   /* Matsushita MN10300 */
+#define bfd_mach_mn10300               300
+#define bfd_mach_am33          330
+  bfd_arch_fr30,
+#define bfd_mach_fr30          0x46523330
+  bfd_arch_mcore,
+  bfd_arch_ia64,      /* HP/Intel ia64 */
+#define bfd_mach_ia64_elf64    0
+#define bfd_mach_ia64_elf32    1
+  bfd_arch_pj,
+  bfd_arch_avr,       /* Atmel AVR microcontrollers */
+#define bfd_mach_avr1          1
+#define bfd_mach_avr2          2
+#define bfd_mach_avr3          3
+#define bfd_mach_avr4          4
+#define bfd_mach_avr5          5
+  bfd_arch_cris,      /* Axis CRIS */
+  bfd_arch_s390,      /* IBM s390 */
+#define bfd_mach_s390_esa      0
+#define bfd_mach_s390_esame    1
+  bfd_arch_openrisc,  /* OpenRISC */
+  bfd_arch_last
+  };
+
+typedef struct bfd_arch_info
+{
+  int bits_per_word;
+  int bits_per_address;
+  int bits_per_byte;
+  enum bfd_architecture arch;
+  unsigned long mach;
+  const char *arch_name;
+  const char *printable_name;
+  unsigned int section_align_power;
+  /* True if this is the default machine for the architecture.  */
+  boolean the_default;
+  const struct bfd_arch_info * (*compatible)
+       PARAMS ((const struct bfd_arch_info *a,
+                const struct bfd_arch_info *b));
+
+  boolean (*scan) PARAMS ((const struct bfd_arch_info *, const char *));
+
+  const struct bfd_arch_info *next;
+} bfd_arch_info_type;
+const char *
+bfd_printable_name PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_scan_arch PARAMS ((const char *string));
+
+const char **
+bfd_arch_list PARAMS ((void));
+
+const bfd_arch_info_type *
+bfd_arch_get_compatible PARAMS ((
+    const bfd *abfd,
+    const bfd *bbfd));
+
+void
+bfd_set_arch_info PARAMS ((bfd *abfd, const bfd_arch_info_type *arg));
+
+enum bfd_architecture
+bfd_get_arch PARAMS ((bfd *abfd));
+
+unsigned long
+bfd_get_mach PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_address PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_get_arch_info PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_lookup_arch PARAMS ((enum bfd_architecture
+    arch,
+    unsigned long machine));
+
+const char *
+bfd_printable_arch_mach PARAMS ((enum bfd_architecture arch, unsigned long machine));
+
+unsigned int
+bfd_octets_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_mach_octets_per_byte PARAMS ((enum bfd_architecture arch,
+    unsigned long machine));
+
+typedef enum bfd_reloc_status
+{
+  /* No errors detected */
+  bfd_reloc_ok,
+
+  /* The relocation was performed, but there was an overflow. */
+  bfd_reloc_overflow,
+
+  /* The address to relocate was not within the section supplied. */
+  bfd_reloc_outofrange,
+
+  /* Used by special functions */
+  bfd_reloc_continue,
+
+  /* Unsupported relocation size requested. */
+  bfd_reloc_notsupported,
+
+  /* Unused */
+  bfd_reloc_other,
+
+  /* The symbol to relocate against was undefined. */
+  bfd_reloc_undefined,
+
+  /* The relocation was performed, but may not be ok - presently
+     generated only when linking i960 coff files with i960 b.out
+     symbols.  If this type is returned, the error_message argument
+     to bfd_perform_relocation will be set.  */
+  bfd_reloc_dangerous
+ }
+ bfd_reloc_status_type;
+
+
+typedef struct reloc_cache_entry
+{
+  /* A pointer into the canonical table of pointers  */
+  struct symbol_cache_entry **sym_ptr_ptr;
+
+  /* offset in section */
+  bfd_size_type address;
+
+  /* addend for relocation value */
+  bfd_vma addend;
+
+  /* Pointer to how to perform the required relocation */
+  reloc_howto_type *howto;
+
+} arelent;
+enum complain_overflow
+{
+  /* Do not complain on overflow. */
+  complain_overflow_dont,
+
+  /* Complain if the bitfield overflows, whether it is considered
+     as signed or unsigned. */
+  complain_overflow_bitfield,
+
+  /* Complain if the value overflows when considered as signed
+     number. */
+  complain_overflow_signed,
+
+  /* Complain if the value overflows when considered as an
+     unsigned number. */
+  complain_overflow_unsigned
+};
+
+struct reloc_howto_struct
+{
+  /*  The type field has mainly a documentary use - the back end can
+      do what it wants with it, though normally the back end's
+      external idea of what a reloc number is stored
+      in this field.  For example, a PC relative word relocation
+      in a coff environment has the type 023 - because that's
+      what the outside world calls a R_PCRWORD reloc.  */
+  unsigned int type;
+
+  /*  The value the final relocation is shifted right by.  This drops
+      unwanted data from the relocation.  */
+  unsigned int rightshift;
+
+  /*  The size of the item to be relocated.  This is *not* a
+      power-of-two measure.  To get the number of bytes operated
+      on by a type of relocation, use bfd_get_reloc_size.  */
+  int size;
+
+  /*  The number of bits in the item to be relocated.  This is used
+      when doing overflow checking.  */
+  unsigned int bitsize;
+
+  /*  Notes that the relocation is relative to the location in the
+      data section of the addend.  The relocation function will
+      subtract from the relocation value the address of the location
+      being relocated.  */
+  boolean pc_relative;
+
+  /*  The bit position of the reloc value in the destination.
+      The relocated value is left shifted by this amount.  */
+  unsigned int bitpos;
+
+  /* What type of overflow error should be checked for when
+     relocating.  */
+  enum complain_overflow complain_on_overflow;
+
+  /* If this field is non null, then the supplied function is
+     called rather than the normal function.  This allows really
+     strange relocation methods to be accomodated (e.g., i960 callj
+     instructions).  */
+  bfd_reloc_status_type (*special_function)
+    PARAMS ((bfd *, arelent *, struct symbol_cache_entry *, PTR, asection *,
+             bfd *, char **));
+
+  /* The textual name of the relocation type.  */
+  char *name;
+
+  /* Some formats record a relocation addend in the section contents
+     rather than with the relocation.  For ELF formats this is the
+     distinction between USE_REL and USE_RELA (though the code checks
+     for USE_REL == 1/0).  The value of this field is TRUE if the
+     addend is recorded with the section contents; when performing a
+     partial link (ld -r) the section contents (the data) will be
+     modified.  The value of this field is FALSE if addends are
+     recorded with the relocation (in arelent.addend); when performing
+     a partial link the relocation will be modified.
+     All relocations for all ELF USE_RELA targets should set this field
+     to FALSE (values of TRUE should be looked on with suspicion).
+     However, the converse is not true: not all relocations of all ELF
+     USE_REL targets set this field to TRUE.  Why this is so is peculiar
+     to each particular target.  For relocs that aren't used in partial
+     links (e.g. GOT stuff) it doesn't matter what this is set to.  */
+  boolean partial_inplace;
+
+  /* The src_mask selects which parts of the read in data
+     are to be used in the relocation sum.  E.g., if this was an 8 bit
+     byte of data which we read and relocated, this would be
+     0x000000ff.  When we have relocs which have an addend, such as
+     sun4 extended relocs, the value in the offset part of a
+     relocating field is garbage so we never use it.  In this case
+     the mask would be 0x00000000.  */
+  bfd_vma src_mask;
+
+  /* The dst_mask selects which parts of the instruction are replaced
+     into the instruction.  In most cases src_mask == dst_mask,
+     except in the above special case, where dst_mask would be
+     0x000000ff, and src_mask would be 0x00000000.  */
+  bfd_vma dst_mask;
+
+  /* When some formats create PC relative instructions, they leave
+     the value of the pc of the place being relocated in the offset
+     slot of the instruction, so that a PC relative relocation can
+     be made just by adding in an ordinary offset (e.g., sun3 a.out).
+     Some formats leave the displacement part of an instruction
+     empty (e.g., m88k bcs); this flag signals the fact.  */
+  boolean pcrel_offset;
+};
+#define HOWTO(C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC) \
+  { (unsigned) C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC }
+#define NEWHOWTO(FUNCTION, NAME, SIZE, REL, IN) \
+  HOWTO (0, 0, SIZE, 0, REL, 0, complain_overflow_dont, FUNCTION, \
+         NAME, false, 0, 0, IN)
+
+#define EMPTY_HOWTO(C) \
+  HOWTO ((C), 0, 0, 0, false, 0, complain_overflow_dont, NULL, \
+         NULL, false, 0, 0, false)
+
+#define HOWTO_PREPARE(relocation, symbol)               \
+  {                                                     \
+    if (symbol != (asymbol *) NULL)                     \
+      {                                                 \
+        if (bfd_is_com_section (symbol->section))       \
+          {                                             \
+            relocation = 0;                             \
+          }                                             \
+        else                                            \
+          {                                             \
+            relocation = symbol->value;                 \
+          }                                             \
+      }                                                 \
+  }
+unsigned int
+bfd_get_reloc_size PARAMS ((reloc_howto_type *));
+
+typedef struct relent_chain
+{
+  arelent relent;
+  struct relent_chain *next;
+} arelent_chain;
+bfd_reloc_status_type
+bfd_check_overflow PARAMS ((enum complain_overflow how,
+    unsigned int bitsize,
+    unsigned int rightshift,
+    unsigned int addrsize,
+    bfd_vma relocation));
+
+bfd_reloc_status_type
+bfd_perform_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data,
+    asection *input_section,
+    bfd *output_bfd,
+    char **error_message));
+
+bfd_reloc_status_type
+bfd_install_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data, bfd_vma data_start,
+    asection *input_section,
+    char **error_message));
+
+enum bfd_reloc_code_real {
+  _dummy_first_bfd_reloc_code_real,
+
+
+/* Basic absolute relocations of N bits. */
+  BFD_RELOC_64,
+  BFD_RELOC_32,
+  BFD_RELOC_26,
+  BFD_RELOC_24,
+  BFD_RELOC_16,
+  BFD_RELOC_14,
+  BFD_RELOC_8,
+
+/* PC-relative relocations.  Sometimes these are relative to the address
+of the relocation itself; sometimes they are relative to the start of
+the section containing the relocation.  It depends on the specific target.
+
+The 24-bit relocation is used in some Intel 960 configurations. */
+  BFD_RELOC_64_PCREL,
+  BFD_RELOC_32_PCREL,
+  BFD_RELOC_24_PCREL,
+  BFD_RELOC_16_PCREL,
+  BFD_RELOC_12_PCREL,
+  BFD_RELOC_8_PCREL,
+
+/* For ELF. */
+  BFD_RELOC_32_GOT_PCREL,
+  BFD_RELOC_16_GOT_PCREL,
+  BFD_RELOC_8_GOT_PCREL,
+  BFD_RELOC_32_GOTOFF,
+  BFD_RELOC_16_GOTOFF,
+  BFD_RELOC_LO16_GOTOFF,
+  BFD_RELOC_HI16_GOTOFF,
+  BFD_RELOC_HI16_S_GOTOFF,
+  BFD_RELOC_8_GOTOFF,
+  BFD_RELOC_64_PLT_PCREL,
+  BFD_RELOC_32_PLT_PCREL,
+  BFD_RELOC_24_PLT_PCREL,
+  BFD_RELOC_16_PLT_PCREL,
+  BFD_RELOC_8_PLT_PCREL,
+  BFD_RELOC_64_PLTOFF,
+  BFD_RELOC_32_PLTOFF,
+  BFD_RELOC_16_PLTOFF,
+  BFD_RELOC_LO16_PLTOFF,
+  BFD_RELOC_HI16_PLTOFF,
+  BFD_RELOC_HI16_S_PLTOFF,
+  BFD_RELOC_8_PLTOFF,
+
+/* Relocations used by 68K ELF. */
+  BFD_RELOC_68K_GLOB_DAT,
+  BFD_RELOC_68K_JMP_SLOT,
+  BFD_RELOC_68K_RELATIVE,
+
+/* Linkage-table relative. */
+  BFD_RELOC_32_BASEREL,
+  BFD_RELOC_16_BASEREL,
+  BFD_RELOC_LO16_BASEREL,
+  BFD_RELOC_HI16_BASEREL,
+  BFD_RELOC_HI16_S_BASEREL,
+  BFD_RELOC_8_BASEREL,
+  BFD_RELOC_RVA,
+
+/* Absolute 8-bit relocation, but used to form an address like 0xFFnn. */
+  BFD_RELOC_8_FFnn,
+
+/* These PC-relative relocations are stored as word displacements --
+i.e., byte displacements shifted right two bits.  The 30-bit word
+displacement (<<32_PCREL_S2>> -- 32 bits, shifted 2) is used on the
+SPARC.  (SPARC tools generally refer to this as <<WDISP30>>.)  The
+signed 16-bit displacement is used on the MIPS, and the 23-bit
+displacement is used on the Alpha. */
+  BFD_RELOC_32_PCREL_S2,
+  BFD_RELOC_16_PCREL_S2,
+  BFD_RELOC_23_PCREL_S2,
+
+/* High 22 bits and low 10 bits of 32-bit value, placed into lower bits of
+the target word.  These are used on the SPARC. */
+  BFD_RELOC_HI22,
+  BFD_RELOC_LO10,
+
+/* For systems that allocate a Global Pointer register, these are
+displacements off that register.  These relocation types are
+handled specially, because the value the register will have is
+decided relatively late. */
+  BFD_RELOC_GPREL16,
+  BFD_RELOC_GPREL32,
+
+/* Reloc types used for i960/b.out. */
+  BFD_RELOC_I960_CALLJ,
+
+/* SPARC ELF relocations.  There is probably some overlap with other
+relocation types already defined. */
+  BFD_RELOC_NONE,
+  BFD_RELOC_SPARC_WDISP22,
+  BFD_RELOC_SPARC22,
+  BFD_RELOC_SPARC13,
+  BFD_RELOC_SPARC_GOT10,
+  BFD_RELOC_SPARC_GOT13,
+  BFD_RELOC_SPARC_GOT22,
+  BFD_RELOC_SPARC_PC10,
+  BFD_RELOC_SPARC_PC22,
+  BFD_RELOC_SPARC_WPLT30,
+  BFD_RELOC_SPARC_COPY,
+  BFD_RELOC_SPARC_GLOB_DAT,
+  BFD_RELOC_SPARC_JMP_SLOT,
+  BFD_RELOC_SPARC_RELATIVE,
+  BFD_RELOC_SPARC_UA16,
+  BFD_RELOC_SPARC_UA32,
+  BFD_RELOC_SPARC_UA64,
+
+/* I think these are specific to SPARC a.out (e.g., Sun 4). */
+  BFD_RELOC_SPARC_BASE13,
+  BFD_RELOC_SPARC_BASE22,
+
+/* SPARC64 relocations */
+#define BFD_RELOC_SPARC_64 BFD_RELOC_64
+  BFD_RELOC_SPARC_10,
+  BFD_RELOC_SPARC_11,
+  BFD_RELOC_SPARC_OLO10,
+  BFD_RELOC_SPARC_HH22,
+  BFD_RELOC_SPARC_HM10,
+  BFD_RELOC_SPARC_LM22,
+  BFD_RELOC_SPARC_PC_HH22,
+  BFD_RELOC_SPARC_PC_HM10,
+  BFD_RELOC_SPARC_PC_LM22,
+  BFD_RELOC_SPARC_WDISP16,
+  BFD_RELOC_SPARC_WDISP19,
+  BFD_RELOC_SPARC_7,
+  BFD_RELOC_SPARC_6,
+  BFD_RELOC_SPARC_5,
+#define BFD_RELOC_SPARC_DISP64 BFD_RELOC_64_PCREL
+  BFD_RELOC_SPARC_PLT64,
+  BFD_RELOC_SPARC_HIX22,
+  BFD_RELOC_SPARC_LOX10,
+  BFD_RELOC_SPARC_H44,
+  BFD_RELOC_SPARC_M44,
+  BFD_RELOC_SPARC_L44,
+  BFD_RELOC_SPARC_REGISTER,
+
+/* SPARC little endian relocation */
+  BFD_RELOC_SPARC_REV32,
+
+/* Alpha ECOFF and ELF relocations.  Some of these treat the symbol or
+"addend" in some special way.
+For GPDISP_HI16 ("gpdisp") relocations, the symbol is ignored when
+writing; when reading, it will be the absolute section symbol.  The
+addend is the displacement in bytes of the "lda" instruction from
+the "ldah" instruction (which is at the address of this reloc). */
+  BFD_RELOC_ALPHA_GPDISP_HI16,
+
+/* For GPDISP_LO16 ("ignore") relocations, the symbol is handled as
+with GPDISP_HI16 relocs.  The addend is ignored when writing the
+relocations out, and is filled in with the file's GP value on
+reading, for convenience. */
+  BFD_RELOC_ALPHA_GPDISP_LO16,
+
+/* The ELF GPDISP relocation is exactly the same as the GPDISP_HI16
+relocation except that there is no accompanying GPDISP_LO16
+relocation. */
+  BFD_RELOC_ALPHA_GPDISP,
+
+/* The Alpha LITERAL/LITUSE relocs are produced by a symbol reference;
+the assembler turns it into a LDQ instruction to load the address of
+the symbol, and then fills in a register in the real instruction.
+
+The LITERAL reloc, at the LDQ instruction, refers to the .lita
+section symbol.  The addend is ignored when writing, but is filled
+in with the file's GP value on reading, for convenience, as with the
+GPDISP_LO16 reloc.
+
+The ELF_LITERAL reloc is somewhere between 16_GOTOFF and GPDISP_LO16.
+It should refer to the symbol to be referenced, as with 16_GOTOFF,
+but it generates output not based on the position within the .got
+section, but relative to the GP value chosen for the file during the
+final link stage.
+
+The LITUSE reloc, on the instruction using the loaded address, gives
+information to the linker that it might be able to use to optimize
+away some literal section references.  The symbol is ignored (read
+as the absolute section symbol), and the "addend" indicates the type
+of instruction using the register:
+1 - "memory" fmt insn
+2 - byte-manipulation (byte offset reg)
+3 - jsr (target of branch) */
+  BFD_RELOC_ALPHA_LITERAL,
+  BFD_RELOC_ALPHA_ELF_LITERAL,
+  BFD_RELOC_ALPHA_LITUSE,
+
+/* The HINT relocation indicates a value that should be filled into the
+"hint" field of a jmp/jsr/ret instruction, for possible branch-
+prediction logic which may be provided on some processors. */
+  BFD_RELOC_ALPHA_HINT,
+
+/* The LINKAGE relocation outputs a linkage pair in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_LINKAGE,
+
+/* The CODEADDR relocation outputs a STO_CA in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_CODEADDR,
+
+/* The GPREL_HI/LO relocations together form a 32-bit offset from the
+GP register. */
+  BFD_RELOC_ALPHA_GPREL_HI16,
+  BFD_RELOC_ALPHA_GPREL_LO16,
+
+/* Bits 27..2 of the relocation address shifted right 2 bits;
+simple reloc otherwise. */
+  BFD_RELOC_MIPS_JMP,
+
+/* The MIPS16 jump instruction. */
+  BFD_RELOC_MIPS16_JMP,
+
+/* MIPS16 GP relative reloc. */
+  BFD_RELOC_MIPS16_GPREL,
+
+/* High 16 bits of 32-bit value; simple reloc. */
+  BFD_RELOC_HI16,
+
+/* High 16 bits of 32-bit value but the low 16 bits will be sign
+extended and added to form the final result.  If the low 16
+bits form a negative number, we need to add one to the high value
+to compensate for the borrow when the low bits are added. */
+  BFD_RELOC_HI16_S,
+
+/* Low 16 bits. */
+  BFD_RELOC_LO16,
+
+/* Like BFD_RELOC_HI16_S, but PC relative. */
+  BFD_RELOC_PCREL_HI16_S,
+
+/* Like BFD_RELOC_LO16, but PC relative. */
+  BFD_RELOC_PCREL_LO16,
+
+/* Relocation relative to the global pointer. */
+#define BFD_RELOC_MIPS_GPREL BFD_RELOC_GPREL16
+
+/* Relocation against a MIPS literal section. */
+  BFD_RELOC_MIPS_LITERAL,
+
+/* MIPS ELF relocations. */
+  BFD_RELOC_MIPS_GOT16,
+  BFD_RELOC_MIPS_CALL16,
+#define BFD_RELOC_MIPS_GPREL32 BFD_RELOC_GPREL32
+  BFD_RELOC_MIPS_GOT_HI16,
+  BFD_RELOC_MIPS_GOT_LO16,
+  BFD_RELOC_MIPS_CALL_HI16,
+  BFD_RELOC_MIPS_CALL_LO16,
+  BFD_RELOC_MIPS_SUB,
+  BFD_RELOC_MIPS_GOT_PAGE,
+  BFD_RELOC_MIPS_GOT_OFST,
+  BFD_RELOC_MIPS_GOT_DISP,
+  BFD_RELOC_MIPS_SHIFT5,
+  BFD_RELOC_MIPS_SHIFT6,
+  BFD_RELOC_MIPS_INSERT_A,
+  BFD_RELOC_MIPS_INSERT_B,
+  BFD_RELOC_MIPS_DELETE,
+  BFD_RELOC_MIPS_HIGHEST,
+  BFD_RELOC_MIPS_HIGHER,
+  BFD_RELOC_MIPS_SCN_DISP,
+  BFD_RELOC_MIPS_REL16,
+  BFD_RELOC_MIPS_RELGOT,
+  BFD_RELOC_MIPS_JALR,
+
+
+/* i386/elf relocations */
+  BFD_RELOC_386_GOT32,
+  BFD_RELOC_386_PLT32,
+  BFD_RELOC_386_COPY,
+  BFD_RELOC_386_GLOB_DAT,
+  BFD_RELOC_386_JUMP_SLOT,
+  BFD_RELOC_386_RELATIVE,
+  BFD_RELOC_386_GOTOFF,
+  BFD_RELOC_386_GOTPC,
+
+/* x86-64/elf relocations */
+  BFD_RELOC_X86_64_GOT32,
+  BFD_RELOC_X86_64_PLT32,
+  BFD_RELOC_X86_64_COPY,
+  BFD_RELOC_X86_64_GLOB_DAT,
+  BFD_RELOC_X86_64_JUMP_SLOT,
+  BFD_RELOC_X86_64_RELATIVE,
+  BFD_RELOC_X86_64_GOTPCREL,
+  BFD_RELOC_X86_64_32S,
+
+/* ns32k relocations */
+  BFD_RELOC_NS32K_IMM_8,
+  BFD_RELOC_NS32K_IMM_16,
+  BFD_RELOC_NS32K_IMM_32,
+  BFD_RELOC_NS32K_IMM_8_PCREL,
+  BFD_RELOC_NS32K_IMM_16_PCREL,
+  BFD_RELOC_NS32K_IMM_32_PCREL,
+  BFD_RELOC_NS32K_DISP_8,
+  BFD_RELOC_NS32K_DISP_16,
+  BFD_RELOC_NS32K_DISP_32,
+  BFD_RELOC_NS32K_DISP_8_PCREL,
+  BFD_RELOC_NS32K_DISP_16_PCREL,
+  BFD_RELOC_NS32K_DISP_32_PCREL,
+
+/* PDP11 relocations */
+  BFD_RELOC_PDP11_DISP_8_PCREL,
+  BFD_RELOC_PDP11_DISP_6_PCREL,
+
+/* Picojava relocs.  Not all of these appear in object files. */
+  BFD_RELOC_PJ_CODE_HI16,
+  BFD_RELOC_PJ_CODE_LO16,
+  BFD_RELOC_PJ_CODE_DIR16,
+  BFD_RELOC_PJ_CODE_DIR32,
+  BFD_RELOC_PJ_CODE_REL16,
+  BFD_RELOC_PJ_CODE_REL32,
+
+/* Power(rs6000) and PowerPC relocations. */
+  BFD_RELOC_PPC_B26,
+  BFD_RELOC_PPC_BA26,
+  BFD_RELOC_PPC_TOC16,
+  BFD_RELOC_PPC_B16,
+  BFD_RELOC_PPC_B16_BRTAKEN,
+  BFD_RELOC_PPC_B16_BRNTAKEN,
+  BFD_RELOC_PPC_BA16,
+  BFD_RELOC_PPC_BA16_BRTAKEN,
+  BFD_RELOC_PPC_BA16_BRNTAKEN,
+  BFD_RELOC_PPC_COPY,
+  BFD_RELOC_PPC_GLOB_DAT,
+  BFD_RELOC_PPC_JMP_SLOT,
+  BFD_RELOC_PPC_RELATIVE,
+  BFD_RELOC_PPC_LOCAL24PC,
+  BFD_RELOC_PPC_EMB_NADDR32,
+  BFD_RELOC_PPC_EMB_NADDR16,
+  BFD_RELOC_PPC_EMB_NADDR16_LO,
+  BFD_RELOC_PPC_EMB_NADDR16_HI,
+  BFD_RELOC_PPC_EMB_NADDR16_HA,
+  BFD_RELOC_PPC_EMB_SDAI16,
+  BFD_RELOC_PPC_EMB_SDA2I16,
+  BFD_RELOC_PPC_EMB_SDA2REL,
+  BFD_RELOC_PPC_EMB_SDA21,
+  BFD_RELOC_PPC_EMB_MRKREF,
+  BFD_RELOC_PPC_EMB_RELSEC16,
+  BFD_RELOC_PPC_EMB_RELST_LO,
+  BFD_RELOC_PPC_EMB_RELST_HI,
+  BFD_RELOC_PPC_EMB_RELST_HA,
+  BFD_RELOC_PPC_EMB_BIT_FLD,
+  BFD_RELOC_PPC_EMB_RELSDA,
+  BFD_RELOC_PPC64_HIGHER,
+  BFD_RELOC_PPC64_HIGHER_S,
+  BFD_RELOC_PPC64_HIGHEST,
+  BFD_RELOC_PPC64_HIGHEST_S,
+  BFD_RELOC_PPC64_TOC16_LO,
+  BFD_RELOC_PPC64_TOC16_HI,
+  BFD_RELOC_PPC64_TOC16_HA,
+  BFD_RELOC_PPC64_TOC,
+  BFD_RELOC_PPC64_PLTGOT16,
+  BFD_RELOC_PPC64_PLTGOT16_LO,
+  BFD_RELOC_PPC64_PLTGOT16_HI,
+  BFD_RELOC_PPC64_PLTGOT16_HA,
+  BFD_RELOC_PPC64_ADDR16_DS,
+  BFD_RELOC_PPC64_ADDR16_LO_DS,
+  BFD_RELOC_PPC64_GOT16_DS,
+  BFD_RELOC_PPC64_GOT16_LO_DS,
+  BFD_RELOC_PPC64_PLT16_LO_DS,
+  BFD_RELOC_PPC64_SECTOFF_DS,
+  BFD_RELOC_PPC64_SECTOFF_LO_DS,
+  BFD_RELOC_PPC64_TOC16_DS,
+  BFD_RELOC_PPC64_TOC16_LO_DS,
+  BFD_RELOC_PPC64_PLTGOT16_DS,
+  BFD_RELOC_PPC64_PLTGOT16_LO_DS,
+
+/* IBM 370/390 relocations */
+  BFD_RELOC_I370_D12,
+
+/* The type of reloc used to build a contructor table - at the moment
+probably a 32 bit wide absolute relocation, but the target can choose.
+It generally does map to one of the other relocation types. */
+  BFD_RELOC_CTOR,
+
+/* ARM 26 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction. */
+  BFD_RELOC_ARM_PCREL_BRANCH,
+
+/* ARM 26 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_ARM_PCREL_BLX,
+
+/* Thumb 22 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BLX,
+
+/* These relocs are only used within the ARM assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_ARM_IMMEDIATE,
+  BFD_RELOC_ARM_ADRL_IMMEDIATE,
+  BFD_RELOC_ARM_OFFSET_IMM,
+  BFD_RELOC_ARM_SHIFT_IMM,
+  BFD_RELOC_ARM_SWI,
+  BFD_RELOC_ARM_MULTI,
+  BFD_RELOC_ARM_CP_OFF_IMM,
+  BFD_RELOC_ARM_ADR_IMM,
+  BFD_RELOC_ARM_LDR_IMM,
+  BFD_RELOC_ARM_LITERAL,
+  BFD_RELOC_ARM_IN_POOL,
+  BFD_RELOC_ARM_OFFSET_IMM8,
+  BFD_RELOC_ARM_HWLITERAL,
+  BFD_RELOC_ARM_THUMB_ADD,
+  BFD_RELOC_ARM_THUMB_IMM,
+  BFD_RELOC_ARM_THUMB_SHIFT,
+  BFD_RELOC_ARM_THUMB_OFFSET,
+  BFD_RELOC_ARM_GOT12,
+  BFD_RELOC_ARM_GOT32,
+  BFD_RELOC_ARM_JUMP_SLOT,
+  BFD_RELOC_ARM_COPY,
+  BFD_RELOC_ARM_GLOB_DAT,
+  BFD_RELOC_ARM_PLT32,
+  BFD_RELOC_ARM_RELATIVE,
+  BFD_RELOC_ARM_GOTOFF,
+  BFD_RELOC_ARM_GOTPC,
+
+/* Hitachi SH relocs.  Not all of these appear in object files. */
+  BFD_RELOC_SH_PCDISP8BY2,
+  BFD_RELOC_SH_PCDISP12BY2,
+  BFD_RELOC_SH_IMM4,
+  BFD_RELOC_SH_IMM4BY2,
+  BFD_RELOC_SH_IMM4BY4,
+  BFD_RELOC_SH_IMM8,
+  BFD_RELOC_SH_IMM8BY2,
+  BFD_RELOC_SH_IMM8BY4,
+  BFD_RELOC_SH_PCRELIMM8BY2,
+  BFD_RELOC_SH_PCRELIMM8BY4,
+  BFD_RELOC_SH_SWITCH16,
+  BFD_RELOC_SH_SWITCH32,
+  BFD_RELOC_SH_USES,
+  BFD_RELOC_SH_COUNT,
+  BFD_RELOC_SH_ALIGN,
+  BFD_RELOC_SH_CODE,
+  BFD_RELOC_SH_DATA,
+  BFD_RELOC_SH_LABEL,
+  BFD_RELOC_SH_LOOP_START,
+  BFD_RELOC_SH_LOOP_END,
+  BFD_RELOC_SH_COPY,
+  BFD_RELOC_SH_GLOB_DAT,
+  BFD_RELOC_SH_JMP_SLOT,
+  BFD_RELOC_SH_RELATIVE,
+  BFD_RELOC_SH_GOTPC,
+
+/* Thumb 23-, 12- and 9-bit pc-relative branches.  The lowest bit must
+be zero and is not stored in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BRANCH9,
+  BFD_RELOC_THUMB_PCREL_BRANCH12,
+  BFD_RELOC_THUMB_PCREL_BRANCH23,
+
+/* ARC Cores relocs.
+ARC 22 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction.  The high 20 bits are installed in bits 26
+through 7 of the instruction. */
+  BFD_RELOC_ARC_B22_PCREL,
+
+/* ARC 26 bit absolute branch.  The lowest two bits must be zero and are not
+stored in the instruction.  The high 24 bits are installed in bits 23
+through 0. */
+  BFD_RELOC_ARC_B26,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_10_PCREL_R,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0.  This is the same as the previous reloc
+except it is in the left container, i.e.,
+shifted left 15 bits. */
+  BFD_RELOC_D10V_10_PCREL_L,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18_PCREL,
+
+/* Mitsubishi D30V relocs.
+This is a 6-bit absolute reloc. */
+  BFD_RELOC_D30V_6,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_9_PCREL,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_9_PCREL_R,
+
+/* This is a 12-bit absolute reloc with the
+right 3 bitsassumed to be 0. */
+  BFD_RELOC_D30V_15,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_15_PCREL,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_15_PCREL_R,
+
+/* This is an 18-bit absolute reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21_PCREL,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_21_PCREL_R,
+
+/* This is a 32-bit absolute reloc. */
+  BFD_RELOC_D30V_32,
+
+/* This is a 32-bit pc-relative reloc. */
+  BFD_RELOC_D30V_32_PCREL,
+
+/* Mitsubishi M32R relocs.
+This is a 24 bit absolute address. */
+  BFD_RELOC_M32R_24,
+
+/* This is a 10-bit pc-relative reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_10_PCREL,
+
+/* This is an 18-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_18_PCREL,
+
+/* This is a 26-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_26_PCREL,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as unsigned. */
+  BFD_RELOC_M32R_HI16_ULO,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as signed. */
+  BFD_RELOC_M32R_HI16_SLO,
+
+/* This is a 16-bit reloc containing the lower 16 bits of an address. */
+  BFD_RELOC_M32R_LO16,
+
+/* This is a 16-bit reloc containing the small data area offset for use in
+add3, load, and store instructions. */
+  BFD_RELOC_M32R_SDA16,
+
+/* This is a 9-bit reloc */
+  BFD_RELOC_V850_9_PCREL,
+
+/* This is a 22-bit reloc */
+  BFD_RELOC_V850_22_PCREL,
+
+/* This is a 16 bit offset from the short data area pointer. */
+  BFD_RELOC_V850_SDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+short data area pointer. */
+  BFD_RELOC_V850_SDA_15_16_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer. */
+  BFD_RELOC_V850_ZDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+zero data area pointer. */
+  BFD_RELOC_V850_ZDA_15_16_OFFSET,
+
+/* This is an 8 bit offset (of which only 6 bits are used) from the
+tiny data area pointer. */
+  BFD_RELOC_V850_TDA_6_8_OFFSET,
+
+/* This is an 8bit offset (of which only 7 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_7_8_OFFSET,
+
+/* This is a 7 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_7_7_OFFSET,
+
+/* This is a 16 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_16_16_OFFSET,
+
+/* This is a 5 bit offset (of which only 4 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_4_5_OFFSET,
+
+/* This is a 4 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_4_4_OFFSET,
+
+/* This is a 16 bit offset from the short data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_SDA_16_16_SPLIT_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_ZDA_16_16_SPLIT_OFFSET,
+
+/* This is a 6 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_6_7_OFFSET,
+
+/* This is a 16 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_16_16_OFFSET,
+
+
+/* This is a 32bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_32_PCREL,
+
+/* This is a 16bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_16_PCREL,
+
+/* This is a 8bit DP reloc for the tms320c30, where the most
+significant 8 bits of a 24 bit word are placed into the least
+significant 8 bits of the opcode. */
+  BFD_RELOC_TIC30_LDP,
+
+/* This is a 7bit reloc for the tms320c54x, where the least
+significant 7 bits of a 16 bit word are placed into the least
+significant 7 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTLS7,
+
+/* This is a 9bit DP reloc for the tms320c54x, where the most
+significant 9 bits of a 16 bit word are placed into the least
+significant 9 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTMS9,
+
+/* This is an extended address 23-bit reloc for the tms320c54x. */
+  BFD_RELOC_TIC54X_23,
+
+/* This is a 16-bit reloc for the tms320c54x, where the least
+significant 16 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_16_OF_23,
+
+/* This is a reloc for the tms320c54x, where the most
+significant 7 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_MS7_OF_23,
+
+/* This is a 48 bit reloc for the FR30 that stores 32 bits. */
+  BFD_RELOC_FR30_48,
+
+/* This is a 32 bit reloc for the FR30 that stores 20 bits split up into
+two sections. */
+  BFD_RELOC_FR30_20,
+
+/* This is a 16 bit reloc for the FR30 that stores a 6 bit word offset in
+4 bits. */
+  BFD_RELOC_FR30_6_IN_4,
+
+/* This is a 16 bit reloc for the FR30 that stores an 8 bit byte offset
+into 8 bits. */
+  BFD_RELOC_FR30_8_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit short offset
+into 8 bits. */
+  BFD_RELOC_FR30_9_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 10 bit word offset
+into 8 bits. */
+  BFD_RELOC_FR30_10_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit pc relative
+short offset into 8 bits. */
+  BFD_RELOC_FR30_9_PCREL,
+
+/* This is a 16 bit reloc for the FR30 that stores a 12 bit pc relative
+short offset into 11 bits. */
+  BFD_RELOC_FR30_12_PCREL,
+
+/* Motorola Mcore relocations. */
+  BFD_RELOC_MCORE_PCREL_IMM8BY4,
+  BFD_RELOC_MCORE_PCREL_IMM11BY2,
+  BFD_RELOC_MCORE_PCREL_IMM4BY2,
+  BFD_RELOC_MCORE_PCREL_32,
+  BFD_RELOC_MCORE_PCREL_JSR_IMM11BY2,
+  BFD_RELOC_MCORE_RVA,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit pc relative
+short offset into 7 bits. */
+  BFD_RELOC_AVR_7_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 13 bit pc relative
+short offset into 12 bits. */
+  BFD_RELOC_AVR_13_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 17 bit value (usually
+program memory address) into 16 bits. */
+  BFD_RELOC_AVR_16_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of program memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually data memory address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of data memory address) into 8 bit immediate value of
+SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(most high 8 bit of program memory address) into 8 bit immediate value
+of LDI or SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually command address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of 16 bit command address) into 8 bit immediate value
+of SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 6 bit of 22 bit command address) into 8 bit immediate
+value of SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM_NEG,
+
+/* This is a 32 bit reloc for the AVR that stores 23 bit value
+into 22 bits. */
+  BFD_RELOC_AVR_CALL,
+
+/* Direct 12 bit. */
+  BFD_RELOC_390_12,
+
+/* 12 bit GOT offset. */
+  BFD_RELOC_390_GOT12,
+
+/* 32 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT32,
+
+/* Copy symbol at runtime. */
+  BFD_RELOC_390_COPY,
+
+/* Create GOT entry. */
+  BFD_RELOC_390_GLOB_DAT,
+
+/* Create PLT entry. */
+  BFD_RELOC_390_JMP_SLOT,
+
+/* Adjust by program base. */
+  BFD_RELOC_390_RELATIVE,
+
+/* 32 bit PC relative offset to GOT. */
+  BFD_RELOC_390_GOTPC,
+
+/* 16 bit GOT offset. */
+  BFD_RELOC_390_GOT16,
+
+/* PC relative 16 bit shifted by 1. */
+  BFD_RELOC_390_PC16DBL,
+
+/* 16 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT16DBL,
+
+/* PC relative 32 bit shifted by 1. */
+  BFD_RELOC_390_PC32DBL,
+
+/* 32 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT32DBL,
+
+/* 32 bit PC rel. GOT shifted by 1. */
+  BFD_RELOC_390_GOTPCDBL,
+
+/* 64 bit GOT offset. */
+  BFD_RELOC_390_GOT64,
+
+/* 64 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT64,
+
+/* 32 bit rel. offset to GOT entry. */
+  BFD_RELOC_390_GOTENT,
+
+/* These two relocations are used by the linker to determine which of
+the entries in a C++ virtual function table are actually used.  When
+the --gc-sections option is given, the linker will zero out the entries
+that are not used, so that the code for those functions need not be
+included in the output.
+
+VTABLE_INHERIT is a zero-space relocation used to describe to the
+linker the inheritence tree of a C++ virtual function table.  The
+relocation's symbol should be the parent class' vtable, and the
+relocation should be located at the child vtable.
+
+VTABLE_ENTRY is a zero-space relocation that describes the use of a
+virtual function table entry.  The reloc's symbol should refer to the
+table of the class mentioned in the code.  Off of that base, an offset
+describes the entry that is being used.  For Rela hosts, this offset
+is stored in the reloc's addend.  For Rel hosts, we are forced to put
+this offset in the reloc's section offset. */
+  BFD_RELOC_VTABLE_INHERIT,
+  BFD_RELOC_VTABLE_ENTRY,
+
+/* Intel IA64 Relocations. */
+  BFD_RELOC_IA64_IMM14,
+  BFD_RELOC_IA64_IMM22,
+  BFD_RELOC_IA64_IMM64,
+  BFD_RELOC_IA64_DIR32MSB,
+  BFD_RELOC_IA64_DIR32LSB,
+  BFD_RELOC_IA64_DIR64MSB,
+  BFD_RELOC_IA64_DIR64LSB,
+  BFD_RELOC_IA64_GPREL22,
+  BFD_RELOC_IA64_GPREL64I,
+  BFD_RELOC_IA64_GPREL32MSB,
+  BFD_RELOC_IA64_GPREL32LSB,
+  BFD_RELOC_IA64_GPREL64MSB,
+  BFD_RELOC_IA64_GPREL64LSB,
+  BFD_RELOC_IA64_LTOFF22,
+  BFD_RELOC_IA64_LTOFF64I,
+  BFD_RELOC_IA64_PLTOFF22,
+  BFD_RELOC_IA64_PLTOFF64I,
+  BFD_RELOC_IA64_PLTOFF64MSB,
+  BFD_RELOC_IA64_PLTOFF64LSB,
+  BFD_RELOC_IA64_FPTR64I,
+  BFD_RELOC_IA64_FPTR32MSB,
+  BFD_RELOC_IA64_FPTR32LSB,
+  BFD_RELOC_IA64_FPTR64MSB,
+  BFD_RELOC_IA64_FPTR64LSB,
+  BFD_RELOC_IA64_PCREL21B,
+  BFD_RELOC_IA64_PCREL21BI,
+  BFD_RELOC_IA64_PCREL21M,
+  BFD_RELOC_IA64_PCREL21F,
+  BFD_RELOC_IA64_PCREL22,
+  BFD_RELOC_IA64_PCREL60B,
+  BFD_RELOC_IA64_PCREL64I,
+  BFD_RELOC_IA64_PCREL32MSB,
+  BFD_RELOC_IA64_PCREL32LSB,
+  BFD_RELOC_IA64_PCREL64MSB,
+  BFD_RELOC_IA64_PCREL64LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR22,
+  BFD_RELOC_IA64_LTOFF_FPTR64I,
+  BFD_RELOC_IA64_LTOFF_FPTR32MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR32LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64LSB,
+  BFD_RELOC_IA64_SEGREL32MSB,
+  BFD_RELOC_IA64_SEGREL32LSB,
+  BFD_RELOC_IA64_SEGREL64MSB,
+  BFD_RELOC_IA64_SEGREL64LSB,
+  BFD_RELOC_IA64_SECREL32MSB,
+  BFD_RELOC_IA64_SECREL32LSB,
+  BFD_RELOC_IA64_SECREL64MSB,
+  BFD_RELOC_IA64_SECREL64LSB,
+  BFD_RELOC_IA64_REL32MSB,
+  BFD_RELOC_IA64_REL32LSB,
+  BFD_RELOC_IA64_REL64MSB,
+  BFD_RELOC_IA64_REL64LSB,
+  BFD_RELOC_IA64_LTV32MSB,
+  BFD_RELOC_IA64_LTV32LSB,
+  BFD_RELOC_IA64_LTV64MSB,
+  BFD_RELOC_IA64_LTV64LSB,
+  BFD_RELOC_IA64_IPLTMSB,
+  BFD_RELOC_IA64_IPLTLSB,
+  BFD_RELOC_IA64_COPY,
+  BFD_RELOC_IA64_TPREL22,
+  BFD_RELOC_IA64_TPREL64MSB,
+  BFD_RELOC_IA64_TPREL64LSB,
+  BFD_RELOC_IA64_LTOFF_TP22,
+  BFD_RELOC_IA64_LTOFF22X,
+  BFD_RELOC_IA64_LDXMOV,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits high part of an absolute address. */
+  BFD_RELOC_M68HC11_HI8,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits low part of an absolute address. */
+  BFD_RELOC_M68HC11_LO8,
+
+/* Motorola 68HC11 reloc.
+This is the 3 bits of a value. */
+  BFD_RELOC_M68HC11_3B,
+
+/* These relocs are only used within the CRIS assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_CRIS_BDISP8,
+  BFD_RELOC_CRIS_UNSIGNED_5,
+  BFD_RELOC_CRIS_SIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_4,
+
+/* Relocs used in ELF shared libraries for CRIS. */
+  BFD_RELOC_CRIS_COPY,
+  BFD_RELOC_CRIS_GLOB_DAT,
+  BFD_RELOC_CRIS_JUMP_SLOT,
+  BFD_RELOC_CRIS_RELATIVE,
+
+/* 32-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_32_GOT,
+
+/* 16-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_16_GOT,
+
+/* 32-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_32_GOTPLT,
+
+/* 16-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_16_GOTPLT,
+
+/* 32-bit offset to symbol, relative to GOT. */
+  BFD_RELOC_CRIS_32_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to GOT. */
+  BFD_RELOC_CRIS_32_PLT_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to this relocation. */
+  BFD_RELOC_CRIS_32_PLT_PCREL,
+
+/* Intel i860 Relocations. */
+  BFD_RELOC_860_COPY,
+  BFD_RELOC_860_GLOB_DAT,
+  BFD_RELOC_860_JUMP_SLOT,
+  BFD_RELOC_860_RELATIVE,
+  BFD_RELOC_860_PC26,
+  BFD_RELOC_860_PLT26,
+  BFD_RELOC_860_PC16,
+  BFD_RELOC_860_LOW0,
+  BFD_RELOC_860_SPLIT0,
+  BFD_RELOC_860_LOW1,
+  BFD_RELOC_860_SPLIT1,
+  BFD_RELOC_860_LOW2,
+  BFD_RELOC_860_SPLIT2,
+  BFD_RELOC_860_LOW3,
+  BFD_RELOC_860_LOGOT0,
+  BFD_RELOC_860_SPGOT0,
+  BFD_RELOC_860_LOGOT1,
+  BFD_RELOC_860_SPGOT1,
+  BFD_RELOC_860_LOGOTOFF0,
+  BFD_RELOC_860_SPGOTOFF0,
+  BFD_RELOC_860_LOGOTOFF1,
+  BFD_RELOC_860_SPGOTOFF1,
+  BFD_RELOC_860_LOGOTOFF2,
+  BFD_RELOC_860_LOGOTOFF3,
+  BFD_RELOC_860_LOPC,
+  BFD_RELOC_860_HIGHADJ,
+  BFD_RELOC_860_HAGOT,
+  BFD_RELOC_860_HAGOTOFF,
+  BFD_RELOC_860_HAPC,
+  BFD_RELOC_860_HIGH,
+  BFD_RELOC_860_HIGOT,
+  BFD_RELOC_860_HIGOTOFF,
+
+/* OpenRISC Relocations. */
+  BFD_RELOC_OPENRISC_ABS_26,
+  BFD_RELOC_OPENRISC_REL_26,
+
+/* H8 elf Relocations. */
+  BFD_RELOC_H8_DIR16A8,
+  BFD_RELOC_H8_DIR16R8,
+  BFD_RELOC_H8_DIR24A8,
+  BFD_RELOC_H8_DIR24R8,
+  BFD_RELOC_H8_DIR32A16,
+  BFD_RELOC_UNUSED };
+typedef enum bfd_reloc_code_real bfd_reloc_code_real_type;
+reloc_howto_type *
+bfd_reloc_type_lookup PARAMS ((bfd *abfd, bfd_reloc_code_real_type code));
+
+const char *
+bfd_get_reloc_code_name PARAMS ((bfd_reloc_code_real_type code));
+
+
+typedef struct symbol_cache_entry
+{
+       /* A pointer to the BFD which owns the symbol. This information
+          is necessary so that a back end can work out what additional
+          information (invisible to the application writer) is carried
+          with the symbol.
+
+          This field is *almost* redundant, since you can use section->owner
+          instead, except that some symbols point to the global sections
+          bfd_{abs,com,und}_section.  This could be fixed by making
+          these globals be per-bfd (or per-target-flavor).  FIXME. */
+
+  struct _bfd *the_bfd; /* Use bfd_asymbol_bfd(sym) to access this field. */
+
+       /* The text of the symbol. The name is left alone, and not copied; the
+          application may not alter it. */
+  const char *name;
+
+       /* The value of the symbol.  This really should be a union of a
+          numeric value with a pointer, since some flags indicate that
+          a pointer to another symbol is stored here.  */
+  symvalue value;
+
+       /* Attributes of a symbol: */
+
+#define BSF_NO_FLAGS    0x00
+
+       /* The symbol has local scope; <<static>> in <<C>>. The value
+          is the offset into the section of the data. */
+#define BSF_LOCAL      0x01
+
+       /* The symbol has global scope; initialized data in <<C>>. The
+          value is the offset into the section of the data. */
+#define BSF_GLOBAL     0x02
+
+       /* The symbol has global scope and is exported. The value is
+          the offset into the section of the data. */
+#define BSF_EXPORT     BSF_GLOBAL /* no real difference */
+
+       /* A normal C symbol would be one of:
+          <<BSF_LOCAL>>, <<BSF_FORT_COMM>>,  <<BSF_UNDEFINED>> or
+          <<BSF_GLOBAL>> */
+
+       /* The symbol is a debugging record. The value has an arbitary
+          meaning, unless BSF_DEBUGGING_RELOC is also set.  */
+#define BSF_DEBUGGING  0x08
+
+       /* The symbol denotes a function entry point.  Used in ELF,
+          perhaps others someday.  */
+#define BSF_FUNCTION    0x10
+
+       /* Used by the linker. */
+#define BSF_KEEP        0x20
+#define BSF_KEEP_G      0x40
+
+       /* A weak global symbol, overridable without warnings by
+          a regular global symbol of the same name.  */
+#define BSF_WEAK        0x80
+
+       /* This symbol was created to point to a section, e.g. ELF's
+          STT_SECTION symbols.  */
+#define BSF_SECTION_SYM 0x100
+
+       /* The symbol used to be a common symbol, but now it is
+          allocated. */
+#define BSF_OLD_COMMON  0x200
+
+       /* The default value for common data. */
+#define BFD_FORT_COMM_DEFAULT_VALUE 0
+
+       /* In some files the type of a symbol sometimes alters its
+          location in an output file - ie in coff a <<ISFCN>> symbol
+          which is also <<C_EXT>> symbol appears where it was
+          declared and not at the end of a section.  This bit is set
+          by the target BFD part to convey this information. */
+
+#define BSF_NOT_AT_END    0x400
+
+       /* Signal that the symbol is the label of constructor section. */
+#define BSF_CONSTRUCTOR   0x800
+
+       /* Signal that the symbol is a warning symbol.  The name is a
+          warning.  The name of the next symbol is the one to warn about;
+          if a reference is made to a symbol with the same name as the next
+          symbol, a warning is issued by the linker. */
+#define BSF_WARNING       0x1000
+
+       /* Signal that the symbol is indirect.  This symbol is an indirect
+          pointer to the symbol with the same name as the next symbol. */
+#define BSF_INDIRECT      0x2000
+
+       /* BSF_FILE marks symbols that contain a file name.  This is used
+          for ELF STT_FILE symbols.  */
+#define BSF_FILE          0x4000
+
+       /* Symbol is from dynamic linking information.  */
+#define BSF_DYNAMIC       0x8000
+
+       /* The symbol denotes a data object.  Used in ELF, and perhaps
+          others someday.  */
+#define BSF_OBJECT        0x10000
+
+       /* This symbol is a debugging symbol.  The value is the offset
+          into the section of the data.  BSF_DEBUGGING should be set
+          as well.  */
+#define BSF_DEBUGGING_RELOC 0x20000
+
+  flagword flags;
+
+       /* A pointer to the section to which this symbol is
+          relative.  This will always be non NULL, there are special
+          sections for undefined and absolute symbols.  */
+  struct sec *section;
+
+       /* Back end special data.  */
+  union
+    {
+      PTR p;
+      bfd_vma i;
+    } udata;
+
+} asymbol;
+#define bfd_get_symtab_upper_bound(abfd) \
+     BFD_SEND (abfd, _bfd_get_symtab_upper_bound, (abfd))
+boolean
+bfd_is_local_label PARAMS ((bfd *abfd, asymbol *sym));
+
+boolean
+bfd_is_local_label_name PARAMS ((bfd *abfd, const char *name));
+
+#define bfd_is_local_label_name(abfd, name) \
+     BFD_SEND (abfd, _bfd_is_local_label_name, (abfd, name))
+#define bfd_canonicalize_symtab(abfd, location) \
+     BFD_SEND (abfd, _bfd_canonicalize_symtab,\
+                  (abfd, location))
+boolean
+bfd_set_symtab PARAMS ((bfd *abfd, asymbol **location, unsigned int count));
+
+void
+bfd_print_symbol_vandf PARAMS ((bfd *abfd, PTR file, asymbol *symbol));
+
+#define bfd_make_empty_symbol(abfd) \
+     BFD_SEND (abfd, _bfd_make_empty_symbol, (abfd))
+#define bfd_make_debug_symbol(abfd,ptr,size) \
+        BFD_SEND (abfd, _bfd_make_debug_symbol, (abfd, ptr, size))
+int
+bfd_decode_symclass PARAMS ((asymbol *symbol));
+
+boolean
+bfd_is_undefined_symclass PARAMS ((int symclass));
+
+void
+bfd_symbol_info PARAMS ((asymbol *symbol, symbol_info *ret));
+
+boolean
+bfd_copy_private_symbol_data PARAMS ((bfd *ibfd, asymbol *isym, bfd *obfd, asymbol *osym));
+
+#define bfd_copy_private_symbol_data(ibfd, isymbol, obfd, osymbol) \
+     BFD_SEND (obfd, _bfd_copy_private_symbol_data, \
+               (ibfd, isymbol, obfd, osymbol))
+struct _bfd
+{
+    /* The filename the application opened the BFD with.  */
+    const char *filename;
+
+    /* A pointer to the target jump table.             */
+    const struct bfd_target *xvec;
+
+    /* To avoid dragging too many header files into every file that
+       includes `<<bfd.h>>', IOSTREAM has been declared as a "char
+       *", and MTIME as a "long".  Their correct types, to which they
+       are cast when used, are "FILE *" and "time_t".    The iostream
+       is the result of an fopen on the filename.  However, if the
+       BFD_IN_MEMORY flag is set, then iostream is actually a pointer
+       to a bfd_in_memory struct.  */
+    PTR iostream;
+
+    /* Is the file descriptor being cached?  That is, can it be closed as
+       needed, and re-opened when accessed later?  */
+
+    boolean cacheable;
+
+    /* Marks whether there was a default target specified when the
+       BFD was opened. This is used to select which matching algorithm
+       to use to choose the back end. */
+
+    boolean target_defaulted;
+
+    /* The caching routines use these to maintain a
+       least-recently-used list of BFDs */
+
+    struct _bfd *lru_prev, *lru_next;
+
+    /* When a file is closed by the caching routines, BFD retains
+       state information on the file here: */
+
+    ufile_ptr where;
+
+    /* and here: (``once'' means at least once) */
+
+    boolean opened_once;
+
+    /* Set if we have a locally maintained mtime value, rather than
+       getting it from the file each time: */
+
+    boolean mtime_set;
+
+    /* File modified time, if mtime_set is true: */
+
+    long mtime;
+
+    /* Reserved for an unimplemented file locking extension.*/
+
+    int ifd;
+
+    /* The format which belongs to the BFD. (object, core, etc.) */
+
+    bfd_format format;
+
+    /* The direction the BFD was opened with*/
+
+    enum bfd_direction {no_direction = 0,
+                        read_direction = 1,
+                        write_direction = 2,
+                        both_direction = 3} direction;
+
+    /* Format_specific flags*/
+
+    flagword flags;
+
+    /* Currently my_archive is tested before adding origin to
+       anything. I believe that this can become always an add of
+       origin, with origin set to 0 for non archive files.   */
+
+    ufile_ptr origin;
+
+    /* Remember when output has begun, to stop strange things
+       from happening. */
+    boolean output_has_begun;
+
+    /* Pointer to linked list of sections*/
+    struct sec  *sections;
+
+    /* The number of sections */
+    unsigned int section_count;
+
+    /* Stuff only useful for object files:
+       The start address. */
+    bfd_vma start_address;
+
+    /* Used for input and output*/
+    unsigned int symcount;
+
+    /* Symbol table for output BFD (with symcount entries) */
+    struct symbol_cache_entry  **outsymbols;
+
+    /* Pointer to structure which contains architecture information*/
+    const struct bfd_arch_info *arch_info;
+
+    /* Stuff only useful for archives:*/
+    PTR arelt_data;
+    struct _bfd *my_archive;     /* The containing archive BFD.  */
+    struct _bfd *next;           /* The next BFD in the archive.  */
+    struct _bfd *archive_head;   /* The first BFD in the archive.  */
+    boolean has_armap;
+
+    /* A chain of BFD structures involved in a link.  */
+    struct _bfd *link_next;
+
+    /* A field used by _bfd_generic_link_add_archive_symbols.  This will
+       be used only for archive elements.  */
+    int archive_pass;
+
+    /* Used by the back end to hold private data. */
+
+    union
+      {
+      struct aout_data_struct *aout_data;
+      struct artdata *aout_ar_data;
+      struct _oasys_data *oasys_obj_data;
+      struct _oasys_ar_data *oasys_ar_data;
+      struct coff_tdata *coff_obj_data;
+      struct pe_tdata *pe_obj_data;
+      struct xcoff_tdata *xcoff_obj_data;
+      struct ecoff_tdata *ecoff_obj_data;
+      struct ieee_data_struct *ieee_data;
+      struct ieee_ar_data_struct *ieee_ar_data;
+      struct srec_data_struct *srec_data;
+      struct ihex_data_struct *ihex_data;
+      struct tekhex_data_struct *tekhex_data;
+      struct elf_obj_tdata *elf_obj_data;
+      struct nlm_obj_tdata *nlm_obj_data;
+      struct bout_data_struct *bout_data;
+      struct sun_core_struct *sun_core_data;
+      struct sco5_core_struct *sco5_core_data;
+      struct trad_core_struct *trad_core_data;
+      struct som_data_struct *som_data;
+      struct hpux_core_struct *hpux_core_data;
+      struct hppabsd_core_struct *hppabsd_core_data;
+      struct sgi_core_struct *sgi_core_data;
+      struct lynx_core_struct *lynx_core_data;
+      struct osf_core_struct *osf_core_data;
+      struct cisco_core_struct *cisco_core_data;
+      struct versados_data_struct *versados_data;
+      struct netbsd_core_struct *netbsd_core_data;
+      PTR any;
+      } tdata;
+
+    /* Used by the application to hold private data*/
+    PTR usrdata;
+
+  /* Where all the allocated stuff under this BFD goes.  This is a
+     struct objalloc *, but we use PTR to avoid requiring the inclusion of
+     objalloc.h.  */
+    PTR memory;
+};
+
+typedef enum bfd_error
+{
+  bfd_error_no_error = 0,
+  bfd_error_system_call,
+  bfd_error_invalid_target,
+  bfd_error_wrong_format,
+  bfd_error_wrong_object_format,
+  bfd_error_invalid_operation,
+  bfd_error_no_memory,
+  bfd_error_no_symbols,
+  bfd_error_no_armap,
+  bfd_error_no_more_archived_files,
+  bfd_error_malformed_archive,
+  bfd_error_file_not_recognized,
+  bfd_error_file_ambiguously_recognized,
+  bfd_error_no_contents,
+  bfd_error_nonrepresentable_section,
+  bfd_error_no_debug_section,
+  bfd_error_bad_value,
+  bfd_error_file_truncated,
+  bfd_error_file_too_big,
+  bfd_error_invalid_error_code
+} bfd_error_type;
+
+bfd_error_type
+bfd_get_error PARAMS ((void));
+
+void
+bfd_set_error PARAMS ((bfd_error_type error_tag));
+
+const char *
+bfd_errmsg PARAMS ((bfd_error_type error_tag));
+
+void
+bfd_perror PARAMS ((const char *message));
+
+typedef void (*bfd_error_handler_type) PARAMS ((const char *, ...));
+
+bfd_error_handler_type
+bfd_set_error_handler PARAMS ((bfd_error_handler_type));
+
+void
+bfd_set_error_program_name PARAMS ((const char *));
+
+bfd_error_handler_type
+bfd_get_error_handler PARAMS ((void));
+
+const char *
+bfd_archive_filename PARAMS ((bfd *));
+
+long
+bfd_get_reloc_upper_bound PARAMS ((bfd *abfd, asection *sect));
+
+long
+bfd_canonicalize_reloc PARAMS ((bfd *abfd,
+    asection *sec,
+    arelent **loc,
+    asymbol **syms));
+
+void
+bfd_set_reloc PARAMS ((bfd *abfd, asection *sec, arelent **rel, unsigned int count)
+    
+    );
+
+boolean
+bfd_set_file_flags PARAMS ((bfd *abfd, flagword flags));
+
+int
+bfd_get_arch_size PARAMS ((bfd *abfd));
+
+int
+bfd_get_sign_extend_vma PARAMS ((bfd *abfd));
+
+boolean
+bfd_set_start_address PARAMS ((bfd *abfd, bfd_vma vma));
+
+long
+bfd_get_mtime PARAMS ((bfd *abfd));
+
+long
+bfd_get_size PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_get_gp_size PARAMS ((bfd *abfd));
+
+void
+bfd_set_gp_size PARAMS ((bfd *abfd, unsigned int i));
+
+bfd_vma
+bfd_scan_vma PARAMS ((const char *string, const char **end, int base));
+
+boolean
+bfd_copy_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_copy_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_copy_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_merge_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_merge_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_merge_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_set_private_flags PARAMS ((bfd *abfd, flagword flags));
+
+#define bfd_set_private_flags(abfd, flags) \
+     BFD_SEND (abfd, _bfd_set_private_flags, \
+               (abfd, flags))
+#define bfd_sizeof_headers(abfd, reloc) \
+     BFD_SEND (abfd, _bfd_sizeof_headers, (abfd, reloc))
+
+#define bfd_find_nearest_line(abfd, sec, syms, off, file, func, line) \
+     BFD_SEND (abfd, _bfd_find_nearest_line,  (abfd, sec, syms, off, file, func, line))
+
+       /* Do these three do anything useful at all, for any back end?  */
+#define bfd_debug_info_start(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_start, (abfd))
+
+#define bfd_debug_info_end(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_end, (abfd))
+
+#define bfd_debug_info_accumulate(abfd, section) \
+        BFD_SEND (abfd, _bfd_debug_info_accumulate, (abfd, section))
+
+
+#define bfd_stat_arch_elt(abfd, stat) \
+        BFD_SEND (abfd, _bfd_stat_arch_elt,(abfd, stat))
+
+#define bfd_update_armap_timestamp(abfd) \
+        BFD_SEND (abfd, _bfd_update_armap_timestamp, (abfd))
+
+#define bfd_set_arch_mach(abfd, arch, mach)\
+        BFD_SEND ( abfd, _bfd_set_arch_mach, (abfd, arch, mach))
+
+#define bfd_relax_section(abfd, section, link_info, again) \
+       BFD_SEND (abfd, _bfd_relax_section, (abfd, section, link_info, again))
+
+#define bfd_gc_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_gc_sections, (abfd, link_info))
+
+#define bfd_merge_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_merge_sections, (abfd, link_info))
+
+#define bfd_link_hash_table_create(abfd) \
+       BFD_SEND (abfd, _bfd_link_hash_table_create, (abfd))
+
+#define bfd_link_add_symbols(abfd, info) \
+       BFD_SEND (abfd, _bfd_link_add_symbols, (abfd, info))
+
+#define bfd_final_link(abfd, info) \
+       BFD_SEND (abfd, _bfd_final_link, (abfd, info))
+
+#define bfd_free_cached_info(abfd) \
+       BFD_SEND (abfd, _bfd_free_cached_info, (abfd))
+
+#define bfd_get_dynamic_symtab_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_symtab_upper_bound, (abfd))
+
+#define bfd_print_private_bfd_data(abfd, file)\
+       BFD_SEND (abfd, _bfd_print_private_bfd_data, (abfd, file))
+
+#define bfd_canonicalize_dynamic_symtab(abfd, asymbols) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_symtab, (abfd, asymbols))
+
+#define bfd_get_dynamic_reloc_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_reloc_upper_bound, (abfd))
+
+#define bfd_canonicalize_dynamic_reloc(abfd, arels, asyms) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_reloc, (abfd, arels, asyms))
+
+extern bfd_byte *bfd_get_relocated_section_contents
+       PARAMS ((bfd *, struct bfd_link_info *,
+                 struct bfd_link_order *, bfd_byte *,
+                 boolean, asymbol **));
+
+boolean
+bfd_alt_mach_code PARAMS ((bfd *abfd, int index));
+
+symindex
+bfd_get_next_mapent PARAMS ((bfd *abfd, symindex previous, carsym **sym));
+
+boolean
+bfd_set_archive_head PARAMS ((bfd *output, bfd *new_head));
+
+bfd *
+bfd_openr_next_archived_file PARAMS ((bfd *archive, bfd *previous));
+
+const char *
+bfd_core_file_failing_command PARAMS ((bfd *abfd));
+
+int
+bfd_core_file_failing_signal PARAMS ((bfd *abfd));
+
+boolean
+core_file_matches_executable_p PARAMS ((bfd *core_bfd, bfd *exec_bfd));
+
+#define BFD_SEND(bfd, message, arglist) \
+               ((*((bfd)->xvec->message)) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND
+#define BFD_SEND(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+    ((*((bfd)->xvec->message)) arglist) : \
+    (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+#define BFD_SEND_FMT(bfd, message, arglist) \
+            (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND_FMT
+#define BFD_SEND_FMT(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+   (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist) : \
+   (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+enum bfd_flavour {
+  bfd_target_unknown_flavour,
+  bfd_target_aout_flavour,
+  bfd_target_coff_flavour,
+  bfd_target_ecoff_flavour,
+  bfd_target_xcoff_flavour,
+  bfd_target_elf_flavour,
+  bfd_target_ieee_flavour,
+  bfd_target_nlm_flavour,
+  bfd_target_oasys_flavour,
+  bfd_target_tekhex_flavour,
+  bfd_target_srec_flavour,
+  bfd_target_ihex_flavour,
+  bfd_target_som_flavour,
+  bfd_target_os9k_flavour,
+  bfd_target_versados_flavour,
+  bfd_target_msdos_flavour,
+  bfd_target_ovax_flavour,
+  bfd_target_evax_flavour
+};
+
+enum bfd_endian { BFD_ENDIAN_BIG, BFD_ENDIAN_LITTLE, BFD_ENDIAN_UNKNOWN };
+
+/* Forward declaration.  */
+typedef struct bfd_link_info _bfd_link_info;
+
+typedef struct bfd_target
+{
+  char *name;
+  enum bfd_flavour flavour;
+  enum bfd_endian byteorder;
+  enum bfd_endian header_byteorder;
+  flagword object_flags;
+  flagword section_flags;
+  char symbol_leading_char;
+  char ar_pad_char;
+  unsigned short ar_max_namelen;
+  bfd_vma        (*bfd_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  const struct bfd_target *(*_bfd_check_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_set_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_write_contents[bfd_type_end]) PARAMS ((bfd *));
+
+  /* Generic entry points.  */
+#define BFD_JUMP_TABLE_GENERIC(NAME) \
+CONCAT2 (NAME,_close_and_cleanup), \
+CONCAT2 (NAME,_bfd_free_cached_info), \
+CONCAT2 (NAME,_new_section_hook), \
+CONCAT2 (NAME,_get_section_contents), \
+CONCAT2 (NAME,_get_section_contents_in_window)
+
+  /* Called when the BFD is being closed to do any necessary cleanup.  */
+  boolean  (*_close_and_cleanup) PARAMS ((bfd *));
+  /* Ask the BFD to free all cached information.  */
+  boolean  (*_bfd_free_cached_info) PARAMS ((bfd *));
+  /* Called when a new section is created.  */
+  boolean  (*_new_section_hook) PARAMS ((bfd *, sec_ptr));
+  /* Read the contents of a section.  */
+  boolean  (*_bfd_get_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+  boolean  (*_bfd_get_section_contents_in_window)
+    PARAMS ((bfd *, sec_ptr, bfd_window *, file_ptr, bfd_size_type));
+
+  /* Entry points to copy private data.  */
+#define BFD_JUMP_TABLE_COPY(NAME) \
+CONCAT2 (NAME,_bfd_copy_private_bfd_data), \
+CONCAT2 (NAME,_bfd_merge_private_bfd_data), \
+CONCAT2 (NAME,_bfd_copy_private_section_data), \
+CONCAT2 (NAME,_bfd_copy_private_symbol_data), \
+CONCAT2 (NAME,_bfd_set_private_flags), \
+CONCAT2 (NAME,_bfd_print_private_bfd_data) \
+  /* Called to copy BFD general private data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to merge BFD general private data from one object file
+     to a common output file when linking.  */
+  boolean  (*_bfd_merge_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to copy BFD private section data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_section_data) PARAMS ((bfd *, sec_ptr,
+                                                      bfd *, sec_ptr));
+  /* Called to copy BFD private symbol data from one symbol
+     to another.  */
+  boolean  (*_bfd_copy_private_symbol_data) PARAMS ((bfd *, asymbol *,
+                                                     bfd *, asymbol *));
+  /* Called to set private backend flags */
+  boolean  (*_bfd_set_private_flags) PARAMS ((bfd *, flagword));
+
+  /* Called to print private BFD data */
+  boolean  (*_bfd_print_private_bfd_data) PARAMS ((bfd *, PTR));
+
+  /* Core file entry points.  */
+#define BFD_JUMP_TABLE_CORE(NAME) \
+CONCAT2 (NAME,_core_file_failing_command), \
+CONCAT2 (NAME,_core_file_failing_signal), \
+CONCAT2 (NAME,_core_file_matches_executable_p)
+  char *   (*_core_file_failing_command) PARAMS ((bfd *));
+  int      (*_core_file_failing_signal) PARAMS ((bfd *));
+  boolean  (*_core_file_matches_executable_p) PARAMS ((bfd *, bfd *));
+
+  /* Archive entry points.  */
+#define BFD_JUMP_TABLE_ARCHIVE(NAME) \
+CONCAT2 (NAME,_slurp_armap), \
+CONCAT2 (NAME,_slurp_extended_name_table), \
+CONCAT2 (NAME,_construct_extended_name_table), \
+CONCAT2 (NAME,_truncate_arname), \
+CONCAT2 (NAME,_write_armap), \
+CONCAT2 (NAME,_read_ar_hdr), \
+CONCAT2 (NAME,_openr_next_archived_file), \
+CONCAT2 (NAME,_get_elt_at_index), \
+CONCAT2 (NAME,_generic_stat_arch_elt), \
+CONCAT2 (NAME,_update_armap_timestamp)
+  boolean  (*_bfd_slurp_armap) PARAMS ((bfd *));
+  boolean  (*_bfd_slurp_extended_name_table) PARAMS ((bfd *));
+  boolean  (*_bfd_construct_extended_name_table)
+    PARAMS ((bfd *, char **, bfd_size_type *, const char **));
+  void     (*_bfd_truncate_arname) PARAMS ((bfd *, const char *, char *));
+  boolean  (*write_armap)
+    PARAMS ((bfd *, unsigned int, struct orl *, unsigned int, int));
+  PTR      (*_bfd_read_ar_hdr_fn) PARAMS ((bfd *));
+  bfd *    (*openr_next_archived_file) PARAMS ((bfd *, bfd *));
+#define bfd_get_elt_at_index(b,i) BFD_SEND(b, _bfd_get_elt_at_index, (b,i))
+  bfd *    (*_bfd_get_elt_at_index) PARAMS ((bfd *, symindex));
+  int      (*_bfd_stat_arch_elt) PARAMS ((bfd *, struct stat *));
+  boolean  (*_bfd_update_armap_timestamp) PARAMS ((bfd *));
+
+  /* Entry points used for symbols.  */
+#define BFD_JUMP_TABLE_SYMBOLS(NAME) \
+CONCAT2 (NAME,_get_symtab_upper_bound), \
+CONCAT2 (NAME,_get_symtab), \
+CONCAT2 (NAME,_make_empty_symbol), \
+CONCAT2 (NAME,_print_symbol), \
+CONCAT2 (NAME,_get_symbol_info), \
+CONCAT2 (NAME,_bfd_is_local_label_name), \
+CONCAT2 (NAME,_get_lineno), \
+CONCAT2 (NAME,_find_nearest_line), \
+CONCAT2 (NAME,_bfd_make_debug_symbol), \
+CONCAT2 (NAME,_read_minisymbols), \
+CONCAT2 (NAME,_minisymbol_to_symbol)
+  long     (*_bfd_get_symtab_upper_bound) PARAMS ((bfd *));
+  long     (*_bfd_canonicalize_symtab) PARAMS ((bfd *,
+                                                struct symbol_cache_entry **));
+  struct symbol_cache_entry *
+           (*_bfd_make_empty_symbol) PARAMS ((bfd *));
+  void     (*_bfd_print_symbol) PARAMS ((bfd *, PTR,
+                                         struct symbol_cache_entry *,
+                                         bfd_print_symbol_type));
+#define bfd_print_symbol(b,p,s,e) BFD_SEND(b, _bfd_print_symbol, (b,p,s,e))
+  void     (*_bfd_get_symbol_info) PARAMS ((bfd *,
+                                            struct symbol_cache_entry *,
+                                            symbol_info *));
+#define bfd_get_symbol_info(b,p,e) BFD_SEND(b, _bfd_get_symbol_info, (b,p,e))
+  boolean  (*_bfd_is_local_label_name) PARAMS ((bfd *, const char *));
+
+  alent *  (*_get_lineno) PARAMS ((bfd *, struct symbol_cache_entry *));
+  boolean  (*_bfd_find_nearest_line)
+    PARAMS ((bfd *, struct sec *, struct symbol_cache_entry **, bfd_vma,
+             const char **, const char **, unsigned int *));
+ /* Back-door to allow format-aware applications to create debug symbols
+    while using BFD for everything else.  Currently used by the assembler
+    when creating COFF files.  */
+  asymbol *(*_bfd_make_debug_symbol) PARAMS ((bfd *, void *,
+                                              unsigned long size));
+#define bfd_read_minisymbols(b, d, m, s) \
+  BFD_SEND (b, _read_minisymbols, (b, d, m, s))
+  long     (*_read_minisymbols) PARAMS ((bfd *, boolean, PTR *,
+                                         unsigned int *));
+#define bfd_minisymbol_to_symbol(b, d, m, f) \
+  BFD_SEND (b, _minisymbol_to_symbol, (b, d, m, f))
+  asymbol *(*_minisymbol_to_symbol) PARAMS ((bfd *, boolean, const PTR,
+                                             asymbol *));
+
+  /* Routines for relocs.  */
+#define BFD_JUMP_TABLE_RELOCS(NAME) \
+CONCAT2 (NAME,_get_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_reloc), \
+CONCAT2 (NAME,_bfd_reloc_type_lookup)
+  long     (*_get_reloc_upper_bound) PARAMS ((bfd *, sec_ptr));
+  long     (*_bfd_canonicalize_reloc) PARAMS ((bfd *, sec_ptr, arelent **,
+                                               struct symbol_cache_entry **));
+  /* See documentation on reloc types.  */
+  reloc_howto_type *
+           (*reloc_type_lookup) PARAMS ((bfd *, bfd_reloc_code_real_type));
+
+  /* Routines used when writing an object file.  */
+#define BFD_JUMP_TABLE_WRITE(NAME) \
+CONCAT2 (NAME,_set_arch_mach), \
+CONCAT2 (NAME,_set_section_contents)
+  boolean  (*_bfd_set_arch_mach) PARAMS ((bfd *, enum bfd_architecture,
+                                          unsigned long));
+  boolean  (*_bfd_set_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+
+  /* Routines used by the linker.  */
+#define BFD_JUMP_TABLE_LINK(NAME) \
+CONCAT2 (NAME,_sizeof_headers), \
+CONCAT2 (NAME,_bfd_get_relocated_section_contents), \
+CONCAT2 (NAME,_bfd_relax_section), \
+CONCAT2 (NAME,_bfd_link_hash_table_create), \
+CONCAT2 (NAME,_bfd_link_add_symbols), \
+CONCAT2 (NAME,_bfd_final_link), \
+CONCAT2 (NAME,_bfd_link_split_section), \
+CONCAT2 (NAME,_bfd_gc_sections), \
+CONCAT2 (NAME,_bfd_merge_sections)
+  int      (*_bfd_sizeof_headers) PARAMS ((bfd *, boolean));
+  bfd_byte *(*_bfd_get_relocated_section_contents)
+    PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_order *,
+             bfd_byte *, boolean, struct symbol_cache_entry **));
+
+  boolean  (*_bfd_relax_section)
+    PARAMS ((bfd *, struct sec *, struct bfd_link_info *, boolean *));
+
+  /* Create a hash table for the linker.  Different backends store
+     different information in this table.  */
+  struct bfd_link_hash_table *(*_bfd_link_hash_table_create) PARAMS ((bfd *));
+
+  /* Add symbols from this object file into the hash table.  */
+  boolean  (*_bfd_link_add_symbols) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Do a link based on the link_order structures attached to each
+     section of the BFD.  */
+  boolean  (*_bfd_final_link) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Should this section be split up into smaller pieces during linking.  */
+  boolean  (*_bfd_link_split_section) PARAMS ((bfd *, struct sec *));
+
+  /* Remove sections that are not referenced from the output.  */
+  boolean  (*_bfd_gc_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Attempt to merge SEC_MERGE sections.  */
+  boolean  (*_bfd_merge_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Routines to handle dynamic symbols and relocs.  */
+#define BFD_JUMP_TABLE_DYNAMIC(NAME) \
+CONCAT2 (NAME,_get_dynamic_symtab_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_symtab), \
+CONCAT2 (NAME,_get_dynamic_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_reloc)
+  /* Get the amount of memory required to hold the dynamic symbols. */
+  long     (*_bfd_get_dynamic_symtab_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic symbols.  */
+  long     (*_bfd_canonicalize_dynamic_symtab)
+    PARAMS ((bfd *, struct symbol_cache_entry **));
+  /* Get the amount of memory required to hold the dynamic relocs.  */
+  long     (*_bfd_get_dynamic_reloc_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic relocs.  */
+  long     (*_bfd_canonicalize_dynamic_reloc)
+    PARAMS ((bfd *, arelent **, struct symbol_cache_entry **));
+
+ /* Opposite endian version of this target.  */
+ const struct bfd_target * alternative_target;
+
+ PTR backend_data;
+
+} bfd_target;
+boolean
+bfd_set_default_target PARAMS ((const char *name));
+
+const bfd_target *
+bfd_find_target PARAMS ((const char *target_name, bfd *abfd));
+
+const char **
+bfd_target_list PARAMS ((void));
+
+const bfd_target *
+bfd_search_for_target PARAMS ((int (* search_func) (const bfd_target *, void *), void *));
+
+boolean
+bfd_check_format PARAMS ((bfd *abfd, bfd_format format));
+
+boolean
+bfd_check_format_matches PARAMS ((bfd *abfd, bfd_format format, char ***matching));
+
+boolean
+bfd_set_format PARAMS ((bfd *abfd, bfd_format format));
+
+const char *
+bfd_format_string PARAMS ((bfd_format format));
+
+#ifdef __cplusplus
+}
+#endif
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/genkallsyms.pl linuxppc64_2_4/arch/ppc64/kdb/genkallsyms.pl
--- linux-2.4.19/arch/ppc64/kdb/genkallsyms.pl	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/genkallsyms.pl	Mon Jun  4 10:24:01 2001
@@ -0,0 +1,226 @@
+#!/usr/bin/perl -w
+# File genkallsyms.pl created by Todd Inglett at 07:56:04 on Mon Apr 30 2001. 
+# use Math::BigInt;
+
+my $me = "genkallsyms";
+my $objdump = "objdump";
+my $outfile;
+my $objfile;
+my $verbose = 0;
+my $dummy = 0;
+
+my %sections;		# indexed by sect name.  Value is [Name, Size, Addr, Name_off, Sect_idx]
+my $sectlist;		# same section references in a list.
+my $nextsect = 0;	# index of next free section in sectlist.
+
+my @symbols;		# Value is [Name, sec_ref, Addr, Name_off]
+my $nextsym = 0;	# index of next free symbol
+my $strtablen = 0;	# string table length.
+
+# my $shift32multiplier = Math::BigInt->new(2**30) * 4;  # 2**32 without going floating point
+
+#
+# Parse the cmdline options
+#
+my $arg;
+while (defined($arg = shift)) {
+    if ($arg eq '-objdump') {
+	$objdump = shift;
+    } elsif ($arg eq '-o') {
+	$outfile = shift;
+    } elsif ($arg eq '-h') {
+	usage();
+    } elsif ($arg eq '-v') {
+	$verbose = 1;
+    } elsif ($arg eq '-dummy') {
+	$dummy = 1;
+    } elsif ($arg !~ m/^-/) {
+	unshift @ARGV, $arg;
+	last;
+    } else {
+	die "$me: unknown option $arg\n";
+    }
+}
+
+usage() unless ($#ARGV == 0 || $dummy);
+$objfile = $ARGV[0];	# typically vmlinux
+
+if ($verbose) {
+    print "outfile=$outfile\n";
+    print "objdump=$objdump\n";
+    print "objfile=$objfile\n";
+}
+( $dummy || -r $objfile ) or die "$me: cannot read $objfile: $!\n";
+
+
+if (!$dummy) {
+    #
+    # Read the section table from objdump -h into hash
+    # %sections.  Each entry is [Name, Size, Addr, Name_Off, Sect_idx] and
+    # the hash is indexed by Name.
+    #
+    open(S, "$objdump -h $objfile 2>&1 |") or die "$me: cannot $objdump -h $objfile: $!\n";
+    my $sline;
+    while (defined($sline = <S>)) {
+	chomp($sline);
+	$sline =~ s/^\s*//;
+	next unless $sline =~ m/^[0-9]/;
+	my ($idx, $name, $size, $vma, $rest) = split(/\s+/, $sline, 5);
+	print "name=$name, size=$size, vma=$vma\n" if $verbose;
+	$sectlist[$nextsect] = [$name, $size, $vma, 0, 0];
+	$sections{$name} = $sectlist[$nextsect++];
+    }
+    close(S);
+    print "$me: processed $nextsect sections\n" if $verbose;
+
+    # Sort by start addr for readability.  This is not required.
+    @sectlist = sort { $a->[2] cmp $b->[2] } @sectlist;
+    for (my $i = 0; $i < $nextsect; $i++) {
+	$sectlist[$i]->[4] = $i;	# Sect_Idx
+    }
+
+    #
+    # Now read the symbol table from objdump -t
+    #
+    open(S, "$objdump -t $objfile 2>&1 |") or die "$me: cannot $objdump -t $objfile: $!\n";
+    while (defined($sline = <S>)) {
+	last if $sline =~ m/^SYMBOL/;
+    }
+    while (defined($sline = <S>)) {
+	chomp($sline);
+	my ($addr, $foo, $type, $sect, $val, $name) = split(/\s+/, $sline);
+	# Now type might be empty.
+	if (!defined($name)) {
+	    $name = $val;
+	    $val = $sect;
+	    $sect = $type;
+	    $type = " ";
+	}
+	# Weed out symbols which are not useful.
+	next if !$name;
+	next if $sect eq ".rodata"; # TOC entries, I think.
+	next if $sect eq "*ABS*";
+	next if $name =~ m/^LC?\.\./;
+	# print "$addr,$type,$sect,$val,$name\n" if $verbose;
+	my $sec_ref = $sections{$sect};
+	$symbols[$nextsym++] = [$name, $sec_ref, $addr, 0];
+    }
+    close(S);
+    print "$me: processed $nextsym symbols\n" if $verbose;
+    # Sort by addr for readability.  This is not required.
+    @symbols = sort { $a->[2] cmp $b->[2] } @symbols;
+
+    #
+    # Now that everything is sorted, we need to assign space for
+    # each string in the string table.
+    #
+    for (my $i = 0; $i < $nextsect; $i++) {
+	my $len = length($sectlist[$i]->[0]);
+	$sectlist[$i]->[3] = $strtablen;
+	$strtablen += $len+1;
+    }
+    for (my $i = 0; $i < $nextsym; $i++) {
+	my $len = length($symbols[$i]->[0]);
+	$symbols[$i]->[3] = $strtablen;
+	$strtablen += $len+1;
+    }
+}
+
+if ($outfile) {
+    open(OUT, ">$outfile") or die "$me:  cannot open $outfile for write: $!\n";
+} else {
+    open(OUT, ">&STDOUT");
+}
+
+if ($dummy) {
+    # Produce dummy output and exit.
+    print OUT "long __start___kallsyms = 0;\n";
+    print OUT "long __stop___kallsyms = 0;\n";
+    exit 0;
+}
+
+print OUT "#include <stddef.h>\n";
+print OUT "#include <linux/kallsyms.h>\n\n";
+print OUT "#define NUM_SECTIONS\t\t", int(keys %sections), "\n";
+print OUT "#define NUM_SYMBOLS\t\t", $nextsym, "\n";
+print OUT "#define NUM_STRING_CHARS\t", $strtablen, "\n";
+print OUT "#define FIRST_SECTION_ADDR\t0x", $sectlist[0]->[2], "UL /* section ", $sectlist[0]->[0], " */\n";
+print OUT "#define LAST_SECTION_ADDR\t0x", $sectlist[$nextsect-1]->[2], "UL + 0x", $sectlist[$nextsect-1]->[1], "UL /* section ", $sectlist[$nextsect-1]->[0], " */\n";
+print OUT <<EOF;
+
+#define SOFF(secidx) (sizeof(struct kallsyms_section)*secidx)
+typedef struct kdata {
+	struct kallsyms_header hdr;
+	struct kallsyms_section sections[NUM_SECTIONS];
+	struct kallsyms_symbol symbols[NUM_SYMBOLS];
+	const char strings[NUM_STRING_CHARS];
+} kdata_t;
+
+kdata_t __start___kallsyms = {
+  {	/* kallsyms_header */
+	sizeof(struct kallsyms_header),	/* Size of this header */
+	sizeof(kdata_t),		/* Total size of kallsyms data */
+	NUM_SECTIONS,			/* Number of section entries */
+	offsetof(kdata_t, sections),	/* Offset to first section entry */
+	sizeof(struct kallsyms_section),/* Size of one section entry */
+	NUM_SYMBOLS,			/* Number of symbol entries */
+	offsetof(kdata_t, symbols),	/* Offset to first symbol entry */
+	sizeof(struct kallsyms_symbol),	/* Size of one symbol entry */
+	offsetof(kdata_t, strings),	/* Offset to first string */
+	FIRST_SECTION_ADDR,		/* Start address of first section */
+	LAST_SECTION_ADDR,		/* End address of last section */
+  },
+  {	/* kallsyms_section table */
+	/* start_addr, size, name_off, flags */
+EOF
+
+for (my $i = 0; $i < $nextsect; $i++) {
+    my $sectref = $sectlist[$i];
+    print OUT "\t{0x", $sectref->[2], "UL, 0x", $sectref->[1], ", ", $sectref->[3], "},\t /* [$i] ", $sectref->[0], " */\n";
+}
+
+print OUT <<EOF;
+  },
+  {	/* kallsyms_symbol table */
+	/* section_off, addr, name_off */
+EOF
+
+for (my $i = 0; $i < $nextsym; $i++) {
+    my $symref = $symbols[$i];
+    my $secref = $symref->[1];
+    print OUT "\t{SOFF(", $secref->[4], "), 0x", $symref->[2], "UL, ", $symref->[3], "},  /* ", $symref->[0], " */\n";
+}
+
+print OUT <<EOF;
+  },
+  /* string table */ ""
+EOF
+
+for (my $i = 0; $i < $nextsect; $i++) {
+    my $sectref = $sectlist[$i];
+    print OUT "\t\"", $sectref->[0], "\\0\"\t/* ", $sectref->[3], " */\n";
+}
+for (my $i = 0; $i < $nextsym; $i++) {
+    my $symref = $symbols[$i];
+    print OUT "\t\"", $symref->[0], "\\0\"\t/* ", $symref->[3], " */\n";
+}
+
+
+print OUT <<EOF;
+};
+
+long __stop___kallsyms = 0;
+EOF
+
+sub usage {
+    die "Usage:  genkallsyms [-h] -objdump /path/to/objdump -o _ksyms.c vmlinux\n";
+}
+
+# Return a BigInt given a 64bit hex number (without a leading 0x)..
+# The number is assumed to be 16 chars long.
+#sub bighex {
+#    my ($hexstr) = @_;
+#    my $bighi = Math::BigInt->new(hex(substr($hexstr, 0, 8)));
+#    my $biglow = Math::BigInt->new(hex(substr($hexstr, 8, 8)));
+#    return $bighi * $shift32multiplier + $biglow;
+#}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdba_bp.c linuxppc64_2_4/arch/ppc64/kdb/kdba_bp.c
--- linux-2.4.19/arch/ppc64/kdb/kdba_bp.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdba_bp.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,733 @@
+/*
+ * Kernel Debugger Architecture Dependent Breakpoint Handling
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/ptrace.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include "privinst.h"
+
+static char *kdba_rwtypes[] = { "Instruction(Register)", "Data Write",
+			"I/O", "Data Access"};
+
+/*
+ * Table describing processor architecture hardware
+ * breakpoint registers.
+ */
+
+kdbhard_bp_t	kdb_hardbreaks[KDB_MAXHARDBPT];
+
+/*
+ * kdba_db_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor debugger fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_DB_BPT	Standard instruction or data breakpoint encountered
+ *	KDB_DB_SS	Single Step fault ('ss' command or end of 'ssb' command)
+ *	KDB_DB_SSB	Single Step fault, caller should continue ('ssb' command)
+ *	KDB_DB_SSBPT	Single step over breakpoint
+ *	KDB_DB_NOBPT	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Yup, there be goto's here.
+ *
+ *	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor
+ *	which is waiting has already encountered.  If this is the case,
+ *	the debug registers will no longer match any entry in the
+ *	breakpoint table, and we'll return the value KDB_DB_NOBPT.
+ *	This can cause a panic in die_if_kernel().  It is safer to
+ *	disable the breakpoint (bd), go until all processors are past
+ *	the breakpoint then clear the breakpoint (bc).  This code
+ *	recognises a breakpoint even when disabled but not when it has
+ *	been cleared.
+ *
+ *	WARNING: This routine clears the debug state.  It should be called
+ *		 once per debug and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_db_trap(kdb_eframe_t ef, int error_unused)
+{
+	kdb_machreg_t  msr,trap;
+	int rw, reg;
+	int i;
+	kdb_dbtrap_t rv = KDB_DB_BPT;
+	kdb_bp_t *bp;
+
+	msr = get_msr();
+	trap = ef->trap;
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdb: msr 0x%lx trap 0x%lx\n", msr,trap);
+	if (msr & MSR_SE || ((trap & 0x700) || (trap & 0xd00))) 
+	{
+		if (KDB_STATE(SSBPT)) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("ssbpt\n");
+			KDB_STATE_CLEAR(SSBPT);
+			for(i=0,bp=kdb_breakpoints;
+			    i < KDB_MAXBPT;
+			    i++, bp++) {
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp 0x%p enabled %d delayed %d global %d cpu %d\n",
+						   bp, bp->bp_enabled, bp->bp_delayed, bp->bp_global, bp->bp_cpu);
+				if (!bp->bp_enabled)
+					continue;
+				if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+					continue;
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp for this cpu\n");
+				if (bp->bp_delayed) {
+					bp->bp_delayed = 0;
+					if (KDB_DEBUG(BP))
+						kdb_printf("kdba_installbp\n");
+					kdba_installbp(ef, bp);
+					if (!KDB_STATE(DOING_SS)) {
+						set_msr(get_msr() & ~MSR_SE);
+						return(KDB_DB_SSBPT);
+					}
+					break;
+				}
+			}
+			if (i == KDB_MAXBPT) {
+				kdb_printf("kdb: Unable to find delayed breakpoint\n");
+			}
+			if (!KDB_STATE(DOING_SS)) {
+				set_msr(get_msr() & ~MSR_SE);
+				return(KDB_DB_NOBPT);
+			}
+			/* FALLTHROUGH */
+		}
+
+		/*
+		 * KDB_STATE_DOING_SS is set when the kernel debugger is using
+		 * the processor trap flag to single-step a processor.  If a
+		 * single step trap occurs and this flag is clear, the SS trap
+		 * will be ignored by KDB and the kernel will be allowed to deal
+		 * with it as necessary (e.g. for ptrace).
+		 */
+		if (!KDB_STATE(DOING_SS))
+			goto unknown;
+
+		/* single step */
+		rv = KDB_DB_SS;		/* Indicate single step */
+		if (KDB_STATE(DOING_SSB)) {
+
+			kdb_id1(ef->nip);
+			rv = KDB_DB_SSB; /* Indicate ssb - dismiss immediately */
+		} else {
+			/*
+			 * Print current insn
+			 */
+			kdb_printf("SS trap at ");
+			kdb_symbol_print(ef->nip, NULL, KDB_SP_DEFAULT|KDB_SP_NEWLINE);
+			kdb_id1(ef->nip);
+			KDB_STATE_CLEAR(DOING_SS);
+		}
+
+		if (rv != KDB_DB_SSB)
+			set_msr(get_msr() & ~MSR_SE);
+	}
+	if (rv > 0)
+		goto handled;
+	
+	goto handle;
+
+
+handle:
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (!(bp->bp_free)
+		 && (bp->bp_global || bp->bp_cpu == smp_processor_id())
+		 && (bp->bp_hard)
+		 && (bp->bp_hard->bph_reg == reg)) {
+			/*
+			 * Hit this breakpoint.
+			 */
+//			kdb_printf("%s breakpoint #%d at " kdb_bfd_vma_fmt "\n",
+			kdb_printf("%s breakpoint #%d at 0x%ld\n",
+				  kdba_rwtypes[rw],
+				  i, (long )bp->bp_addr);
+
+			/*
+			 * For an instruction breakpoint, disassemble
+			 * the current instruction.
+			 */
+			if (rw == 0) {
+				kdb_id1(ef->nip);
+			}
+
+			goto handled;
+		}
+	}
+
+unknown:
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+handled:
+
+
+	return rv;
+}
+
+/*
+ * kdba_bp_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor breakpoint instruction fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	0	Standard instruction or data breakpoint encountered
+ *	1	Single Step fault ('ss' command)
+ *	2	Single Step fault, caller should continue ('ssb' command)
+ *	3	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ * 	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor which
+ * 	is waiting has already encountered.   If this is the case, the
+ *	debug registers will no longer match any entry in the breakpoint
+ *	table, and we'll return the value '3'.  This can cause a panic
+ *	in die_if_kernel().  It is safer to disable the breakpoint (bd),
+ *	'go' until all processors are past the breakpoint then clear the
+ *	breakpoint (bc).  This code recognises a breakpoint even when
+ *	disabled but not when it has been cleared.
+ *
+ *	WARNING: This routine resets the eip.  It should be called
+ *		 once per breakpoint and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_bp_trap(kdb_eframe_t ef, int error_unused)
+{
+	int i;
+	kdb_dbtrap_t rv;
+	kdb_bp_t *bp;
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdba_bp_trap: eip=0x%lx (not adjusted) "
+			   "msr=0x%lx trap=0x%lx ef=0x%p esp=0x%lx\n",
+			   ef->nip, ef->msr, ef->trap, ef, ef->gpr[1]);
+
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (bp->bp_free)
+			continue;
+		if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+			continue;
+		 if (bp->bp_addr == (ef->nip - bp->bp_adjust)) {
+			/* Hit this breakpoint.  */
+			ef->nip -= bp->bp_adjust;
+			kdb_printf("Instruction(i) breakpoint #%d at 0x%lx (adjusted)\n",
+				  i, ef->nip);
+			kdb_id1(ef->nip);
+			rv = KDB_DB_BPT;
+			bp->bp_delay = 1;
+			break;
+		}
+	}
+
+	return rv;
+}
+
+/*
+ * kdba_handle_bp
+ *
+ *	Handle an instruction-breakpoint trap.  Called when re-installing
+ *	an enabled breakpoint which has has the bp_delay bit set.
+ *
+ * Parameters:
+ * Returns:
+ * Locking:
+ * Remarks:
+ *
+ * Ok, we really need to:
+ *	1) Restore the original instruction byte
+ *	2) Single Step
+ *	3) Restore breakpoint instruction
+ *	4) Continue.
+ *
+ *
+ */
+
+static void
+kdba_handle_bp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+	if (!ef) {
+		kdb_printf("kdba_handle_bp: ef == NULL\n");
+		return;
+	}
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("ef->eip = 0x%lx\n", ef->nip);
+
+	/*
+	 * Setup single step
+	 */
+	kdba_setsinglestep(ef);
+
+	/* KDB_STATE_SSBPT is set when the kernel debugger must single step
+	 * a task in order to re-establish an instruction breakpoint which
+	 * uses the instruction replacement mechanism. 
+	 */
+	KDB_STATE_SET(SSBPT);
+
+	/*
+	 * Reset delay attribute
+	 */
+	bp->bp_delay = 0;
+	bp->bp_delayed = 1;
+}
+
+
+/*
+ * kdba_bptype
+ *
+ *	Return a string describing type of breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Character string.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+char *
+kdba_bptype(kdbhard_bp_t *bph)
+{
+	char *mode;
+
+	mode = kdba_rwtypes[bph->bph_mode];
+
+	return mode;
+}
+
+/*
+ * kdba_printbpreg
+ *
+ *	Print register name assigned to breakpoint
+ *
+ * Parameters:
+ *	bph	Pointer hardware breakpoint structure
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbpreg(kdbhard_bp_t *bph)
+{
+	kdb_printf(" in dr%ld", bph->bph_reg);
+}
+
+/*
+ * kdba_printbp
+ *
+ *	Print string describing hardware breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbp(kdb_bp_t *bp)
+{
+	kdb_printf("\n    is enabled");
+	if (bp->bp_hardtype) {
+		kdba_printbpreg(bp->bp_hard);
+		if (bp->bp_hard->bph_mode != 0) {
+			kdb_printf(" for %d bytes",
+				   bp->bp_hard->bph_length+1);
+		}
+	}
+}
+
+/*
+ * kdba_parsebp
+ *
+ *	Parse architecture dependent portion of the
+ *	breakpoint command.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *	for Ia32 architure, data access, data write and
+ *	I/O breakpoints are supported in addition to instruction
+ * 	breakpoints.
+ *
+ *	{datar|dataw|io|inst} [length]
+ */
+
+int
+kdba_parsebp(int argc, const char **argv, int *nextargp, kdb_bp_t *bp)
+{
+	int		nextarg = *nextargp;
+	int		diag;
+	kdbhard_bp_t 	*bph = &bp->bp_template;
+
+	bph->bph_mode = 0;		/* Default to instruction breakpoint */
+	bph->bph_length = 0;		/* Length must be zero for insn bp */
+	if ((argc + 1) != nextarg) {
+		if (strnicmp(argv[nextarg], "datar", sizeof("datar")) == 0) {
+			bph->bph_mode = 3;
+		} else if (strnicmp(argv[nextarg], "dataw", sizeof("dataw")) == 0) {
+			bph->bph_mode = 1;
+		} else if (strnicmp(argv[nextarg], "io", sizeof("io")) == 0) {
+			bph->bph_mode = 2;
+		} else if (strnicmp(argv[nextarg], "inst", sizeof("inst")) == 0) {
+			bph->bph_mode = 0;
+		} else {
+			return KDB_ARGCOUNT;
+		}
+
+		bph->bph_length = 3;	/* Default to 4 byte */
+
+		nextarg++;
+
+		if ((argc + 1) != nextarg) {
+			unsigned long len;
+
+			diag = kdbgetularg((char *)argv[nextarg],
+					   &len);
+			if (diag)
+				return diag;
+
+
+			if ((len > 4) || (len == 3))
+				return KDB_BADLENGTH;
+
+			bph->bph_length = len;
+			bph->bph_length--; /* Normalize for debug register */
+			nextarg++;
+		}
+
+		if ((argc + 1) != nextarg)
+			return KDB_ARGCOUNT;
+
+		/*
+		 * Indicate to architecture independent level that
+		 * a hardware register assignment is required to enable
+		 * this breakpoint.
+		 */
+
+		bph->bph_free = 0;
+	} else {
+		if (KDB_DEBUG(BP))
+			kdb_printf("kdba_bp: no args, forcehw is %d\n", bp->bp_forcehw);
+		if (bp->bp_forcehw) {
+			/*
+			 * We are forced to use a hardware register for this
+			 * breakpoint because either the bph or bpha
+			 * commands were used to establish this breakpoint.
+			 */
+			bph->bph_free = 0;
+		} else {
+			/*
+			 * Indicate to architecture dependent level that
+			 * the instruction replacement breakpoint technique
+			 * should be used for this breakpoint.
+			 */
+			bph->bph_free = 1;
+			bp->bp_adjust = 1;	/* software, int 3 is one byte */
+		}
+	}
+
+	*nextargp = nextarg;
+	return 0;
+}
+
+/*
+ * kdba_allocbp
+ *
+ *	Associate a hardware register with a breakpoint.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	A pointer to the allocated register kdbhard_bp_t structure for
+ *	success, Null and a non-zero diagnostic for failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+kdbhard_bp_t *
+kdba_allocbp(kdbhard_bp_t *bph, int *diagp)
+{
+	int i;
+	kdbhard_bp_t *newbph;
+
+	for(i=0,newbph=kdb_hardbreaks; i < KDB_MAXHARDBPT; i++, newbph++) {
+		if (newbph->bph_free) {
+			break;
+		}
+	}
+
+	if (i == KDB_MAXHARDBPT) {
+		*diagp = KDB_TOOMANYDBREGS;
+		return NULL;
+	}
+
+	*diagp = 0;
+
+	/*
+	 * Copy data from template.  Can't just copy the entire template
+	 * here because the register number in kdb_hardbreaks must be
+	 * preserved.
+	 */
+	newbph->bph_data = bph->bph_data;
+	newbph->bph_write = bph->bph_write;
+	newbph->bph_mode = bph->bph_mode;
+	newbph->bph_length = bph->bph_length;
+
+	/*
+	 * Mark entry allocated.
+	 */
+	newbph->bph_free = 0;
+
+	return newbph;
+}
+
+/*
+ * kdba_freebp
+ *
+ *	Deallocate a hardware breakpoint
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_freebp(kdbhard_bp_t *bph)
+{
+	bph->bph_free = 1;
+}
+
+/*
+ * kdba_initbp
+ *
+ *	Initialize the breakpoint table for the hardware breakpoint
+ *	register.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	There is one entry per register.  On the ia32 architecture
+ *	all the registers are interchangeable, so no special allocation
+ *	criteria are required.
+ */
+
+void
+kdba_initbp(void)
+{
+	int i;
+	kdbhard_bp_t *bph;
+
+	/*
+	 * Clear the hardware breakpoint table
+	 */
+
+	memset(kdb_hardbreaks, '\0', sizeof(kdb_hardbreaks));
+
+	for(i=0,bph=kdb_hardbreaks; i<KDB_MAXHARDBPT; i++, bph++) {
+		bph->bph_reg = i;
+		bph->bph_free = 1;
+	}
+}
+
+/*
+ * kdba_installbp
+ *
+ *	Install a breakpoint
+ *
+ * Parameters:
+ *	ef	Exception frame
+ *	bp	Breakpoint structure for the breakpoint to be installed
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	For hardware breakpoints, a debug register is allocated
+ *	and assigned to the breakpoint.  If no debug register is
+ *	available, a warning message is printed and the breakpoint
+ *	is disabled.
+ *
+ *	For instruction replacement breakpoints, we must single-step
+ *	over the replaced instruction at this point so we can re-install
+ *	the breakpoint instruction after the single-step.
+ */
+
+int
+kdba_installbp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+	/*
+	 * Install the breakpoint, if it is not already installed.
+	 */
+
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_installbp bp_installed %d\n", bp->bp_installed);
+	}
+	if (!bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			//kdba_installdbreg(bp);
+			bp->bp_installed = 1;
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdba_installbp hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   (long unsigned int) bp->bp_hard->bph_reg, (long unsigned int) bp->bp_addr);
+			}
+		} else if (bp->bp_delay) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdba_installbp delayed bp\n");
+			kdba_handle_bp(ef, bp);
+		} else {
+			bp->bp_inst = kdb_getword((unsigned long *)bp->bp_addr, 1,sizeof(bp->bp_addr));
+#if 0
+			bp->bp_inst = kdba_getword(bp->bp_addr, 1);
+			kdba_putword(bp->bp_addr, 1, 0x7fe00008);
+#endif
+			kdb_putword(bp->bp_addr, 1, 0x7fe00008);
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdba_installbp instruction 0x%x at " kdb_bfd_vma_fmt "\n",
+					   0x7fe00008, bp->bp_addr);
+			bp->bp_installed = 1;
+		}
+	}
+return 0;
+}
+
+/*
+ * kdba_removebp
+ *
+ *	Make a breakpoint ineffective.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdba_removebp(kdb_bp_t *bp)
+{
+	/*
+	 * For hardware breakpoints, remove it from the active register,
+	 * for software breakpoints, restore the instruction stream.
+	 */
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_removebp bp_installed %d\n", bp->bp_installed);
+	}
+	if (bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdb: removing hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_hard->bph_reg, bp->bp_addr);
+			}
+//			kdba_removedbreg(bp);
+		} else
+		{
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdb: restoring instruction 0x%x at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_inst, bp->bp_addr);
+#if 0
+			kdba_putword(bp->bp_addr, 1, bp->bp_inst);
+#endif
+			kdb_putword(bp->bp_addr, 1, bp->bp_inst);
+		}
+		bp->bp_installed = 0;
+	}
+return 0;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdba_bt.c linuxppc64_2_4/arch/ppc64/kdb/kdba_bt.c
--- linux-2.4.19/arch/ppc64/kdb/kdba_bt.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdba_bt.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,280 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Stack Traceback
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/kallsyms.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/ptrace.h>	/* for STACK_FRAME_OVERHEAD */
+#include <asm/system.h>
+#include "privinst.h"
+
+void systemreset(struct pt_regs *regs)
+{
+	udbg_printf("Oh no!\n");
+	kdb_printf("Oh no!\n");
+	kdb(KDB_REASON_OOPS, 0, (kdb_eframe_t) regs);
+	for (;;);
+}
+
+/* human name vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+
+extern unsigned long kdba_getword(unsigned long addr, size_t width);
+
+/* Copy a block of memory using kdba_getword().
+ * This is not efficient.
+ */
+static void kdba_getmem(unsigned long addr, void *p, int size)
+{
+	unsigned char *dst = (unsigned char *)p;
+	while (size > 0) {
+		*dst++ = kdba_getword(addr++, 1);
+		size--;
+	}
+}
+
+
+/*
+ * kdba_bt_stack_ppc
+ *
+ *	kdba_bt_stack with ppc specific parameters.
+ *	Specification as kdba_bt_stack plus :-
+ *
+ * Inputs:
+ *	As kba_bt_stack plus
+ *	regs_esp If 1 get esp from the registers (exception frame), if 0
+ *		 get esp from kdba_getregcontents.
+ */
+
+static int
+kdba_bt_stack_ppc(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+		   struct task_struct *p, int regs_esp)
+{
+
+	kdb_machreg_t	esp,eip,ebp,old_esp;
+	kdb_symtab_t	symtab, *sym;
+	kdbtbtable_t	tbtab;
+	/* declare these as raw ptrs so we don't get func descriptors */
+	extern void *ret_from_except, *ret_from_syscall_1, *do_bottom_half_ret;
+
+if (!regs && !addr)
+{
+    kdb_printf(" invalid regs pointer \n");
+    return 0;
+}
+
+	/*
+	 * The caller may have supplied an address at which the
+	 * stack traceback operation should begin.  This address
+	 * is assumed by this code to point to a return-address
+	 * on the stack to be traced back.
+	 *
+	 * The end result of this will make it appear as if a function
+	 * entitled '<unknown>' was called from the function which
+	 * contains return-address.
+	 */
+	if (addr) {
+		eip = 0;
+		esp = *addr;
+		ebp=0;
+	} else {
+		ebp=regs->link;
+		eip = regs->nip;
+		if (regs_esp)
+			esp = regs->gpr[1];
+		else
+			kdba_getregcontents("esp", regs, &esp);
+	}
+
+	kdb_printf("          SP(esp)            PC(eip)      Function(args)\n");
+
+	/* (Ref: 64-bit PowerPC ELF ABI Spplement; Ian Lance Taylor, Zembu Labs).
+	 A PPC stack frame looks like this:
+
+	 High Address
+	 Back Chain
+	 FP reg save area
+	 GP reg save area
+	 Local var space
+	 Parameter save area		(SP+48)
+	 TOC save area		(SP+40)
+	 link editor doubleword	(SP+32)
+	 compiler doubleword		(SP+24)
+	 LR save			(SP+16)
+	 CR save			(SP+8)
+	 Back Chain			(SP+0)
+
+	 Note that the LR (ret addr) may not be saved in the *current* frame if
+	 no functions have been called from the current function.
+	 */
+
+	/*
+	 * Run through the activation records and print them.
+	 */
+	while (1) {
+		kdb_printf("0x%016lx  0x%016lx  ", esp, eip);
+		kdbnearsym(eip, &symtab);
+		kdba_find_tb_table(eip, &tbtab);
+		sym = symtab.sym_name ? &symtab : &tbtab.symtab; /* use fake symtab if necessary */
+		if (esp >= PAGE_OFFSET) { 
+		    if ((sym) && sym->sym_name) {
+			{
+
+/* if this fails, eip is outside of kernel space, dont trust it. */
+			    if (eip > PAGE_OFFSET) { 
+				    kdb_printf("%s", sym->sym_name);
+			    } else {
+				    kdb_printf("NO_SYMBOL");
+			    }
+			}
+/* if this fails, eip is outside of kernel space, dont trust data. */
+			if (eip > PAGE_OFFSET) { 
+			    if (eip - sym->sym_start > 0) {
+				kdb_printf(" +0x%lx", eip - sym->sym_start);
+			    }
+			}
+		    } else
+			kdb_printf("NO_SYMBOL");
+		}
+		else  /* userspace... */ {
+		    kdb_printf("UserSpace function");
+		    /* more code here to look up userspace function names..*/
+		}
+
+		kdb_printf("\n");
+		/* ret_from_except=0xa5e0 ret_from_syscall_1=a378 do_bottom_half_ret=a5e0 */
+		if (esp < PAGE_OFFSET) { /* below kernelspace..   */
+                            kdb_printf("<Stack contents outside of kernel space.  %.16lx>\n", esp );
+			    break;
+		} else {
+		    if (eip == (kdb_machreg_t)ret_from_except ||
+			eip == (kdb_machreg_t)ret_from_syscall_1 ||
+			eip == (kdb_machreg_t)do_bottom_half_ret) {
+			/* pull exception regs from the stack */
+			struct pt_regs eregs;
+			kdba_getmem(esp+STACK_FRAME_OVERHEAD, &eregs, sizeof(eregs));
+			kdb_printf("  [exception: %lx:%s regs 0x%lx] nip:[0x%x] gpr[1]:[0x%x]\n", eregs.trap,getvecname(eregs.trap), esp+STACK_FRAME_OVERHEAD,(unsigned int)eregs.nip,(unsigned int)eregs.gpr[1]);
+			old_esp = esp;
+			esp = kdba_getword(esp, 8);
+			if (!esp)
+			    break;
+			eip = kdba_getword(esp+16, 8);	/* saved lr */
+			if (esp < PAGE_OFFSET) {  /* userspace... */
+			    if (old_esp > PAGE_OFFSET) {
+				kdb_printf("<Stack drops into userspace here %.16lx>\n",esp);
+				break;
+			    }
+			}
+/* we want to follow exception registers, not into user stack.  ...   */
+			esp = eregs.gpr[1];
+			eip = eregs.nip;
+		    } else {
+			esp = kdba_getword(esp, 8);
+			if (!esp)
+			    break;
+			eip = kdba_getword(esp+16, 8);	/* saved lr */
+
+#if 0
+			if (esp < p) {
+			    kdb_printf("<Stack drops into userspace %.16lx  %.16lx >\n", esp,p );
+			    break;
+			}
+#endif
+		    }
+		}
+	}
+	return 0;
+}
+
+
+/*
+ * kdba_bt_stack
+ *
+ *	This function implements the 'bt' command.  Print a stack
+ *	traceback.
+ *
+ *	bt [<address-expression>]   (addr-exp is for alternate stacks)
+ *	btp <pid>		     (Kernel stack for <pid>)
+ *
+ * 	address expression refers to a return address on the stack.  It
+ *	may be preceeded by a frame pointer.
+ *
+ * Inputs:
+ *	regs	registers at time kdb was entered.
+ *	addr	Pointer to Address provided to 'bt' command, if any.
+ *	argcount
+ *	p	Pointer to task for 'btp' command.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	mds comes in handy when examining the stack to do a manual
+ *	traceback.
+ */
+
+int
+kdba_bt_stack(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+	      struct task_struct *p)
+{
+	return(kdba_bt_stack_ppc(regs, addr, argcount, p, 0));
+}
+
+int
+kdba_bt_process(struct task_struct *p, int argcount)
+{
+	return(kdba_bt_stack_ppc(p->thread.regs, (kdb_machreg_t *) p->thread.ksp, argcount, p, 0));
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdba_id.c linuxppc64_2_4/arch/ppc64/kdb/kdba_id.c
--- linux-2.4.19/arch/ppc64/kdb/kdba_id.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdba_id.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,278 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Instruction Disassembly
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <stdarg.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+/*
+ * kdba_dis_getsym
+ *
+ *	Get a symbol for the disassembler.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *	Not used for kdb.
+ */
+
+/* ARGSUSED */
+static int
+kdba_dis_getsym(bfd_vma addr, disassemble_info *dip)
+{
+
+	return 0;
+}
+
+/*
+ * kdba_printaddress
+ *
+ *	Print (symbolically) an address.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ *	flag	True if a ":<tab>" sequence should follow the address
+ * Returns:
+ *	number of chars printed
+ * Locking:
+ * Remarks:
+ *
+ */
+
+/* ARGSUSED */
+void
+kdba_printaddress(kdb_machreg_t addr, disassemble_info *dip, int flag)
+{
+	kdb_symtab_t symtab;
+
+	/*
+	 * Print a symbol name or address as necessary.
+	 */
+	kdbnearsym(addr, &symtab);
+	if (symtab.sym_name) {
+		/* Do not use kdb_symbol_print here, it always does
+		 * kdb_printf but we want dip->fprintf_func.
+		 */
+		dip->fprintf_func(dip->stream,
+			"0x%0*lx %s",
+			2*sizeof(addr), addr, symtab.sym_name);
+		/* Add offset if needed.  Pad output with blanks to get
+		 * consistent size symbols for disassembly listings.
+		 */
+		if (addr == symtab.sym_start) {
+			if (!flag)
+				dip->fprintf_func(dip->stream, "         ");
+		} else {
+			int len, i;
+			char buf[20];
+			sprintf(buf, "%lx", addr - symtab.sym_start);
+			dip->fprintf_func(dip->stream, "+0x%s", buf);
+			if (!flag) {
+				len = strlen(buf);
+				for (i = len; i < 6; i++)
+					dip->fprintf_func(dip->stream, " ");
+			}
+		}
+
+	} else {
+		dip->fprintf_func(dip->stream, "0x%0*lx", 2*sizeof(addr), addr);
+	}
+
+	if (flag)
+		dip->fprintf_func(dip->stream, ":   ");
+}
+
+/*
+ * kdba_dis_printaddr
+ *
+ *	Print (symbolically) an address.  Called by GNU disassembly
+ *	code via disassemble_info structure.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	number of chars printed.
+ * Locking:
+ * Remarks:
+ *	This function will never append ":<tab>" to the printed
+ *	symbolic address.
+ */
+
+static void
+kdba_dis_printaddr(bfd_vma addr, disassemble_info *dip)
+{
+	return kdba_printaddress(addr, dip, 0);
+}
+
+/*
+ * kdba_dis_getmem
+ *
+ *	Fetch 'length' bytes from 'addr' into 'buf'.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	buf	Address of buffer to fill with bytes from 'addr'
+ *	length	Number of bytes to fetch
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *
+ */
+extern int kdba_getword(unsigned long addr, size_t width);
+
+
+/* ARGSUSED */
+static int
+kdba_dis_getmem(bfd_vma addr, bfd_byte *buf, unsigned int length, disassemble_info *dip)
+{
+	bfd_byte	*bp = buf;
+	int		i;
+
+	/*
+	 * Fill the provided buffer with bytes from
+	 * memory, starting at address 'addr' for 'length bytes.
+	 *
+	 */
+
+	for(i=0; i<length; i++ ){
+		*bp++ = (bfd_byte)kdba_getword(addr++, sizeof(bfd_byte));
+	}
+
+	return 0;
+}
+
+/*
+ * kdba_id_parsemode
+ *
+ * 	Parse IDMODE environment variable string and
+ *	set appropriate value into "disassemble_info" structure.
+ *
+ * Parameters:
+ *	mode	Mode string
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ * Locking:
+ * Remarks:
+ *	We handle the values 'x86' and '8086' to enable either
+ *	32-bit instruction set or 16-bit legacy instruction set.
+ */
+
+int
+kdba_id_parsemode(const char *mode, disassemble_info *dip)
+{
+
+
+	return 0;
+}
+
+/*
+ * kdba_check_pc
+ *
+ * 	Check that the pc is satisfactory.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ * Returns:
+ *	None
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Can change pc.
+ */
+
+void
+kdba_check_pc(kdb_machreg_t *pc)
+{
+	/* No action */
+}
+
+/*
+ * kdba_id_printinsn
+ *
+ * 	Format and print a single instruction at 'pc'. Return the
+ *	length of the instruction.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ *	Length of instruction, -1 for error.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Depends on 'IDMODE' environment variable.
+ */
+
+int
+kdba_id_printinsn(kdb_machreg_t pc, disassemble_info *dip)
+{
+	kdba_dis_printaddr(pc, dip);
+	return print_insn_big_powerpc(pc, dip);
+}
+
+/*
+ * kdba_id_init
+ *
+ * 	Initialize the architecture dependent elements of
+ *	the disassembly information structure
+ *	for the GNU disassembler.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void __init
+kdba_id_init(disassemble_info *dip)
+{
+	dip->read_memory_func       = kdba_dis_getmem;
+	dip->print_address_func     = kdba_dis_printaddr;
+	dip->symbol_at_address_func = kdba_dis_getsym;
+
+	dip->flavour                = bfd_target_elf_flavour;
+	dip->arch		    = bfd_arch_powerpc;
+	dip->mach		    = bfd_mach_ppc_750;
+	dip->endian	    	    = BFD_ENDIAN_BIG;
+
+	dip->display_endian         = BFD_ENDIAN_BIG;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdba_io.c linuxppc64_2_4/arch/ppc64/kdb/kdba_io.c
--- linux-2.4.19/arch/ppc64/kdb/kdba_io.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdba_io.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,167 @@
+/*
+ * Kernel Debugger Console I/O handler
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *	Chuck Fleckenstein		1999/07/20
+ *		Move kdb_info struct declaration to this file
+ *		for cases where serial support is not compiled into
+ *		the kernel.
+ *
+ *	Masahiro Adegawa		1999/07/20
+ *		Handle some peculiarities of japanese 86/106
+ *		keyboards.
+ *
+ *	marc@mucom.co.il		1999/07/20
+ *		Catch buffer overflow for serial input.
+ *
+ *      Scott Foehner
+ *              Port to ia64
+ *
+ *	Scott Lurndal			2000/01/03
+ *		Restructure for v1.0
+ *
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ *	Andi Kleen			2000/03/19
+ *		Support simultaneous input from serial line and keyboard.
+ */
+
+#include <linux/kernel.h>
+#include <asm/io.h>
+#include <linux/wait.h>
+#include <linux/delay.h>
+#include <linux/pc_keyb.h>
+#include <linux/console.h>
+#include <linux/ctype.h>
+#include <linux/keyboard.h>
+#include <linux/serial_reg.h>
+
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#undef FILE
+
+int kdb_port;
+int inchar(void);
+
+/*
+ * This module contains code to read characters from the keyboard or a serial
+ * port.
+ *
+ * It is used by the kernel debugger, and is polled, not interrupt driven.
+ *
+ */
+void
+kdb_resetkeyboard(void)
+{
+#if 0
+	kdb_kbdsend(KBD_CMD_ENABLE);
+#endif
+}
+
+
+#if 0
+/* code that may be resurrected later.. */
+#if defined(CONFIG_VT)
+/*
+ * Check if the keyboard controller has a keypress for us.
+ * Some parts (Enter Release, LED change) are still blocking polled here,
+ * but hopefully they are all short.
+ */
+static int get_kbd_char(void)
+{
+	int keychar;
+/*  	keychar = inchar(); */
+	keychar = udbg_getc_poll();
+	if (keychar == '\n')
+	{
+	    kdb_printf("\n");
+	}
+	/*
+	 * echo the character.
+	 */
+	kdb_printf("%c", keychar);
+
+	return keychar ;
+}
+#endif /* CONFIG_VT */
+#endif
+
+
+
+
+typedef int (*get_char_func)(void);
+
+static get_char_func poll_funcs[] = {
+
+    udbg_getc_poll,
+#if 0
+    kbd_read_data, /* maybe.. not tried yet */
+#endif
+#if 0
+#if defined(CONFIG_VT)
+	get_kbd_char,
+#endif
+#endif
+	NULL
+};
+
+
+void flush_input(void);
+
+
+char *
+kdba_read(char *buffer, size_t bufsize)
+{
+	char	*cp = buffer;
+	char	*bufend = buffer+bufsize-2;	/* Reserve space for newline and null byte */
+	flush_input();
+	for (;;) {
+		int key;
+		get_char_func *f;
+		for (f = &poll_funcs[0]; ; ++f) {
+			if (*f == NULL)
+				f = &poll_funcs[0];
+			key = (*f)();
+			if (key != -1)
+				break;
+		}
+		/* Echo is done in the low level functions */
+		switch (key) {
+		case '\b': /* backspace */
+			if (cp > buffer) {
+				udbg_puts("\b \b");
+				--cp;
+			}
+			break;
+		case '\n': /* enter */
+		case '\r': /* - the other enter... */
+			udbg_putc('\n');
+			*cp++ = '\n';
+			*cp++ = '\0';
+			return buffer;
+		default:
+			if (cp < bufend)
+			udbg_putc(key);
+				*cp++ = key;
+			break;
+		}
+	}
+}
+static char *lineptr;
+void flush_input(void)
+{
+	lineptr = NULL;
+}
+
+
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdba_utils.c linuxppc64_2_4/arch/ppc64/kdb/kdba_utils.c
--- linux-2.4.19/arch/ppc64/kdb/kdba_utils.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdba_utils.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,353 @@
+/* utilities migrated from Xmon or other kernel debug tools. */
+
+/*
+
+Notes for migrating functions from xmon...
+Add functions to this file.  parmlist for functions must match
+   (int argc, const char **argv, const char **envp, struct pt_regs *fp)
+add function prototype to kdbasupport.c
+add function hook to kdba_init() within kdbasupport.c
+
+
+Common bits...
+mread() function calls need to be changed to kdba_readarea_size calls.  straightforward change.
+This:
+	nr = mread(codeaddr, &namlen, 2); 
+becomes this:
+	nr = kdba_readarea_size(codeaddr,&namlen,2);
+
+*/
+
+
+#include <linux/string.h>
+#include <linux/stddef.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ptrace.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#include "privinst.h"
+
+#define EOF	(-1)
+
+
+/* prototypes */
+int scanhex(unsigned long *);
+int hexdigit(int c);
+int kdba_readarea_size(unsigned long from_xxx,void *to,  size_t size);
+
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+struct tbtable {
+	unsigned long	flags;		/* flags: */
+#define TBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define TBTAB_FLAGSISEPROL	(1L<<46)
+#define TBTAB_FLAGSHASTBOFF	(1L<<45)
+#define TBTAB_FLAGSINTPROC	(1L<<44)
+#define TBTAB_FLAGSHASCTL	(1L<<43)
+#define TBTAB_FLAGSTOCLESS	(1L<<42)
+#define TBTAB_FLAGSFPPRESENT	(1L<<41)
+#define TBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define TBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define TBTAB_FLAGSSAVESCR	(1L<<33)
+#define TBTAB_FLAGSSAVESLR	(1L<<32)
+#define TBTAB_FLAGSSTORESBC	(1L<<31)
+#define TBTAB_FLAGSFIXUP	(1L<<30)
+#define TBTAB_FLAGSPARMSONSTK	(1L<<0)
+	unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+	unsigned char	gpr_saved;	/* num gpr's saved */
+	unsigned char	fixedparms;	/* num fixed point parms */
+	unsigned char	floatparms;	/* num float parms */
+	unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define TBTAB_PARMFIXED 1
+#define TBTAB_PARMSFLOAT 2
+#define TBTAB_PARMDFLOAT 3
+	unsigned int	tb_offset;	/* offset from start of func */
+	unsigned long	funcstart;	/* addr of start of function */
+	char		name[64];	/* name of function (null terminated)*/
+};
+
+
+static int find_tb_table(unsigned long codeaddr, struct tbtable *tab);
+
+
+/* Very cheap human name for vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+int kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    extern void machine_halt();
+    kdb_printf("halting machine. ");
+    machine_halt();
+return 0;
+}
+
+
+int kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+	struct task_struct *c;
+	struct tbtable tab;
+
+#ifdef CONFIG_SMP
+	kdb_printf("cpu %d: ", smp_processor_id());
+#endif /* CONFIG_SMP */
+
+	kdb_printf("Vector: %lx %s at  [%p]\n", fp->trap, getvecname(fp->trap), fp);
+	kdb_printf("    pc: %lx", fp->nip);
+	if (find_tb_table(fp->nip, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->nip - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    lr: %lx", fp->link);
+	if (find_tb_table(fp->link, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->link - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    sp: %lx\n", fp->gpr[1]);
+	kdb_printf("   msr: %lx\n", fp->msr);
+
+	if (fp->trap == 0x300 || fp->trap == 0x600) {
+		kdb_printf("   dar: %lx\n", fp->dar);
+		kdb_printf(" dsisr: %lx\n", fp->dsisr);
+	}
+
+	/* XXX: need to copy current or we die.  Why? */
+	c = current;
+	kdb_printf("  current = 0x%p\n", c);
+	kdb_printf("  paca    = 0x%p\n", get_paca());
+	if (c) {
+		kdb_printf("  current = %p, pid = %ld, comm = %s\n",
+		       c, (unsigned long)c->pid, (char *)c->comm);
+	}
+return 0;
+}
+
+
+
+
+/* Starting at codeaddr scan forward for a tbtable and fill in the
+ given table.  Return non-zero if successful at doing something.
+ */
+static int
+find_tb_table(unsigned long codeaddr, struct tbtable *tab)
+{
+	unsigned long codeaddr_max;
+	unsigned long tbtab_start;
+	int nr;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+	    nr=kdba_readarea_size(codeaddr,&instr,4);
+		if (nr != 4)
+			return 0;	/* Bad read.  Give up promptly. */
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			nr = kdba_readarea_size(codeaddr,&flags,8);
+			if (nr != 8)
+				return 0;	/* Bad read or no tb table. */
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				nr = kdba_readarea_size(codeaddr,&parminfo,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = TBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = TBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = TBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & TBTAB_FLAGSHASTBOFF) {
+			    nr = kdba_readarea_size(codeaddr,&tab->tb_offset,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & TBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & TBTAB_FLAGSNAMEPRESENT) {
+				short namlen;
+				nr = kdba_readarea_size(codeaddr,&namlen,2);
+				if (nr != 2)
+					return 1;	/* incomplete */
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				nr = kdba_readarea_size(codeaddr,tab->name,namlen);
+				tab->name[namlen] = '\0';
+				codeaddr += namlen;
+			}
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+
+int kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+   long int msr;
+   msr = get_msr();
+
+   kdb_printf("msr dissection: %lx\n",msr);
+   if (msr & MSR_SF)   kdb_printf(" 64 bit mode enabled \n");
+   if (msr & MSR_ISF)  kdb_printf(" Interrupt 64b mode valid on 630 \n");
+   if (msr & MSR_HV)   kdb_printf(" Hypervisor State \n");
+   if (msr & MSR_VEC)  kdb_printf(" Enable Altivec \n");
+   if (msr & MSR_POW)  kdb_printf(" Enable Power Management  \n");
+   if (msr & MSR_WE)   kdb_printf(" Wait State Enable   \n");
+   if (msr & MSR_TGPR) kdb_printf(" TLB Update registers in use   \n");
+   if (msr & MSR_CE)   kdb_printf(" Critical Interrupt Enable   \n");
+   if (msr & MSR_ILE)  kdb_printf(" Interrupt Little Endian   \n");
+   if (msr & MSR_EE)   kdb_printf(" External Interrupt Enable   \n");
+   if (msr & MSR_PR)   kdb_printf(" Problem State / Privilege Level  \n"); 
+   if (msr & MSR_FP)   kdb_printf(" Floating Point enable   \n");
+   if (msr & MSR_ME)   kdb_printf(" Machine Check Enable   \n");
+   if (msr & MSR_FE0)  kdb_printf(" Floating Exception mode 0  \n"); 
+   if (msr & MSR_SE)   kdb_printf(" Single Step   \n");
+   if (msr & MSR_BE)   kdb_printf(" Branch Trace   \n");
+   if (msr & MSR_DE)   kdb_printf(" Debug Exception Enable   \n");
+   if (msr & MSR_FE1)  kdb_printf(" Floating Exception mode 1   \n");
+   if (msr & MSR_IP)   kdb_printf(" Exception prefix 0x000/0xFFF   \n");
+   if (msr & MSR_IR)   kdb_printf(" Instruction Relocate   \n");
+   if (msr & MSR_DR)   kdb_printf(" Data Relocate   \n");
+   if (msr & MSR_PE)   kdb_printf(" Protection Enable   \n");
+   if (msr & MSR_PX)   kdb_printf(" Protection Exclusive Mode   \n");
+   if (msr & MSR_RI)   kdb_printf(" Recoverable Exception   \n");
+   if (msr & MSR_LE)   kdb_printf(" Little Endian   \n");
+   kdb_printf(".\n");
+return 0;
+}
+
+
+
+
+
+int kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+	int i;
+/*	int cmd; */
+/*	unsigned long val; */
+	struct paca_struct*  ptrPaca = NULL;
+	struct ItLpPaca*  ptrLpPaca = NULL;
+	struct ItLpRegSave*  ptrLpRegSave = NULL;
+
+/*	cmd = skipbl(); */
+/*	if (cmd == '\n') { */
+	{
+	        unsigned long sp, toc;
+		kdb_printf("sr::");
+		asm("mr %0,1" : "=r" (sp) :);
+		asm("mr %0,2" : "=r" (toc) :);
+
+		kdb_printf("msr  = %.16lx  sprg0= %.16lx\n", get_msr(), get_sprg0());
+		kdb_printf("pvr  = %.16lx  sprg1= %.16lx\n", get_pvr(), get_sprg1()); 
+		kdb_printf("dec  = %.16lx  sprg2= %.16lx\n", get_dec(), get_sprg2());
+		kdb_printf("sp   = %.16lx  sprg3= %.16lx\n", sp, get_sprg3());
+		kdb_printf("toc  = %.16lx  dar  = %.16lx\n", toc, get_dar());
+		kdb_printf("srr0 = %.16lx  srr1 = %.16lx\n", get_srr0(), get_srr1());
+		kdb_printf("asr  = %.16lx\n", mfasr());
+		for (i = 0; i < 8; ++i)
+			kdb_printf("sr%.2ld = %.16lx  sr%.2ld = %.16lx\n", (long int)i, (unsigned long)get_sr(i), (long int)(i+8), (long unsigned int) get_sr(i+8));
+
+		// Dump out relevant Paca data areas.
+		kdb_printf("Paca: \n");
+		ptrPaca = (struct Paca*)get_sprg3();
+    
+		kdb_printf("  Local Processor Control Area (LpPaca): \n");
+		ptrLpPaca = ptrPaca->xLpPacaPtr;
+		kdb_printf("    Saved Srr0=%.16lx  Saved Srr1=%.16lx \n", ptrLpPaca->xSavedSrr0, ptrLpPaca->xSavedSrr1);
+		kdb_printf("    Saved Gpr3=%.16lx  Saved Gpr4=%.16lx \n", ptrLpPaca->xSavedGpr3, ptrLpPaca->xSavedGpr4);
+		kdb_printf("    Saved Gpr5=%.16lx \n", ptrLpPaca->xSavedGpr5);
+    
+		kdb_printf("  Local Processor Register Save Area (LpRegSave): \n");
+		ptrLpRegSave = ptrPaca->xLpRegSavePtr;
+		kdb_printf("    Saved Sprg0=%.16lx  Saved Sprg1=%.16lx \n", ptrLpRegSave->xSPRG0, ptrLpRegSave->xSPRG0);
+		kdb_printf("    Saved Sprg2=%.16lx  Saved Sprg3=%.16lx \n", ptrLpRegSave->xSPRG2, ptrLpRegSave->xSPRG3);
+		kdb_printf("    Saved Msr  =%.16lx  Saved Nia  =%.16lx \n", ptrLpRegSave->xMSR, ptrLpRegSave->xNIA);
+    
+		return 0;
+	} 
+
+
+}
+
+
+
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/kdbasupport.c linuxppc64_2_4/arch/ppc64/kdb/kdbasupport.c
--- linux-2.4.19/arch/ppc64/kdb/kdbasupport.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/kdbasupport.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,1233 @@
+/*
+ * Kernel Debugger Architecture Independent Support Functions
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/stddef.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ptrace.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#include <asm/processor.h>
+#include "privinst.h"
+#include <asm/uaccess.h>
+
+extern const char *kdb_diemsg;
+
+
+/* prototypes */
+int valid_ppc64_kernel_address(unsigned long addr, unsigned long size);
+extern int kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+extern int kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+extern int kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+extern int kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+
+
+/*
+ * kdba_init
+ * 	Architecture specific initialization.
+ */
+/*
+kdb_register("commandname",              # name of command user will use to invoke function  
+             function_name,              # name of function within the code 
+             "function example usage",   # sample usage 
+             "function description",     # brief description. 
+             0                           # if i hit enter again, will command repeat itself ?
+Note: functions must take parameters as such:
+functionname(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+*/
+
+void __init
+kdba_init(void)
+{
+	kdba_enable_lbr();
+	kdb_register("excp", kdba_excprint, "excp", "print exception info", 0);
+	kdb_register("superreg", kdba_super_regs, "superreg", "display super_regs", 0);
+	kdb_register("msr", kdba_dissect_msr, "msr", "dissect msr", 0);
+	kdb_register("halt", kdba_halt, "halt", "halt machine", 0);
+
+#if defined(CONFIG_SMP)
+
+#endif
+	return;
+}
+
+
+
+
+/*
+ * kdba_prologue
+ *
+ *	This function analyzes a gcc-generated function prototype
+ *	with or without frame pointers to determine the amount of
+ *	automatic storage and register save storage is used on the
+ *	stack of the target function.  It only counts instructions
+ *	that have been executed up to but excluding the current eip.
+ * Inputs:
+ *	code	Start address of function code to analyze
+ *	pc	Current program counter within function
+ *	sp	Current stack pointer for function
+ *	fp	Current frame pointer for function, may not be valid
+ *	ss	Start of stack for current process.
+ *	caller	1 if looking for data on the caller frame, 0 for callee.
+ * Outputs:
+ *	ar	Activation record, all fields may be set.  fp and oldfp
+ *		are 0 if they cannot be extracted.  return is 0 if the
+ *		code cannot find a valid return address.  args and arg0
+ *		are 0 if the number of arguments cannot be safely
+ *		calculated.
+ * Returns:
+ *	1 if prologue is valid, 0 otherwise.  If pc is 0 treat it as a
+ *	valid prologue to allow bt on wild branches.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_prologue(const kdb_symtab_t *symtab, kdb_machreg_t pc, kdb_machreg_t sp,
+	      kdb_machreg_t fp, kdb_machreg_t ss, int caller, kdb_ar_t *ar)
+{
+	/* We don't currently use kdb's generic activation record scanning
+	 * code to handle backtrace.
+	 */
+	return 0;
+}
+
+
+
+/*
+ * kdba_getregcontents
+ *
+ *	Return the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	The following pseudo register names are supported:
+ *	   &regs	 - Prints address of exception frame
+ *	   kesp		 - Prints kernel stack pointer at time of fault
+ *	   cesp		 - Prints current kernel stack pointer, inside kdb
+ *	   ceflags	 - Prints current flags, inside kdb
+ *	   %<regname>	 - Uses the value of the registers at the
+ *			   last time the user process entered kernel
+ *			   mode, instead of the registers at the time
+ *			   kdb was entered.
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ * Outputs:
+ *	*contents	Pointer to unsigned long to recieve register contents
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ * 	If kdb was entered via an interrupt from the kernel itself then
+ *	ss and esp are *not* on the stack.
+ */
+
+static struct kdbregs {
+	char   *reg_name;
+	size_t	reg_offset;
+} kdbreglist[] = {
+	{ "gpr0",	offsetof(struct pt_regs, gpr[0]) },
+	{ "gpr1",	offsetof(struct pt_regs, gpr[1]) },
+	{ "gpr2",	offsetof(struct pt_regs, gpr[2]) },
+	{ "gpr3",	offsetof(struct pt_regs, gpr[3]) },
+	{ "gpr4",	offsetof(struct pt_regs, gpr[4]) },
+	{ "gpr5",	offsetof(struct pt_regs, gpr[5]) },
+	{ "gpr6",	offsetof(struct pt_regs, gpr[6]) },
+	{ "gpr7",	offsetof(struct pt_regs, gpr[7]) },
+	{ "gpr8",	offsetof(struct pt_regs, gpr[8]) },
+	{ "gpr9",	offsetof(struct pt_regs, gpr[9]) },
+	{ "gpr10",	offsetof(struct pt_regs, gpr[10]) },
+	{ "gpr11",	offsetof(struct pt_regs, gpr[11]) },
+	{ "gpr12",	offsetof(struct pt_regs, gpr[12]) },
+	{ "gpr13",	offsetof(struct pt_regs, gpr[13]) },
+	{ "gpr14",	offsetof(struct pt_regs, gpr[14]) },
+	{ "gpr15",	offsetof(struct pt_regs, gpr[15]) },
+	{ "gpr16",	offsetof(struct pt_regs, gpr[16]) },
+	{ "gpr17",	offsetof(struct pt_regs, gpr[17]) },
+	{ "gpr18",	offsetof(struct pt_regs, gpr[18]) },
+	{ "gpr19",	offsetof(struct pt_regs, gpr[19]) },
+	{ "gpr20",	offsetof(struct pt_regs, gpr[20]) },
+	{ "gpr21",	offsetof(struct pt_regs, gpr[21]) },
+	{ "gpr22",	offsetof(struct pt_regs, gpr[22]) },
+	{ "gpr23",	offsetof(struct pt_regs, gpr[23]) },
+	{ "gpr24",	offsetof(struct pt_regs, gpr[24]) },
+	{ "gpr25",	offsetof(struct pt_regs, gpr[25]) },
+	{ "gpr26",	offsetof(struct pt_regs, gpr[26]) },
+	{ "gpr27",	offsetof(struct pt_regs, gpr[27]) },
+	{ "gpr28",	offsetof(struct pt_regs, gpr[28]) },
+	{ "gpr29",	offsetof(struct pt_regs, gpr[29]) },
+	{ "gpr30",	offsetof(struct pt_regs, gpr[30]) },
+	{ "gpr31",	offsetof(struct pt_regs, gpr[31]) },
+	{ "eip",	offsetof(struct pt_regs, nip) },
+	{ "msr",	offsetof(struct pt_regs, msr) },
+	{ "esp",	offsetof(struct pt_regs, gpr[1]) },
+  	{ "orig_gpr3",  offsetof(struct pt_regs, orig_gpr3) },
+	{ "ctr", 	offsetof(struct pt_regs, ctr) },
+	{ "link",	offsetof(struct pt_regs, link) },
+	{ "xer", 	offsetof(struct pt_regs, xer) },
+	{ "ccr",	offsetof(struct pt_regs, ccr) },
+	{ "mq",		offsetof(struct pt_regs, softe) /* mq */ },
+	{ "trap",	offsetof(struct pt_regs, trap) },
+	{ "dar",	offsetof(struct pt_regs, dar)  },
+	{ "dsisr",	offsetof(struct pt_regs, dsisr) },
+	{ "result",	offsetof(struct pt_regs, result) },
+};
+
+static const int nkdbreglist = sizeof(kdbreglist) / sizeof(struct kdbregs);
+
+unsigned long
+getsp(void)
+{
+	unsigned long x;
+	asm("mr %0,1" : "=r" (x):);
+	return x;
+}
+int
+kdba_getregcontents(const char *regname,
+		    struct pt_regs *regs,
+		    kdb_machreg_t *contents)
+{
+	int i;
+
+	if (strcmp(regname, "&regs") == 0) {
+		*contents = (unsigned long)regs;
+		return 0;
+	}
+
+	if (strcmp(regname, "kesp") == 0) {
+		*contents = (unsigned long) current->thread.ksp;
+		return 0;
+	}
+
+	if (strcmp(regname, "cesp") == 0) {
+		*contents = getsp();
+		return 0;
+	}
+
+	if (strcmp(regname, "ceflags") == 0) {
+		int flags;
+		save_flags(flags);
+		*contents = flags;
+		return 0;
+	}
+
+	if (regname[0] == '%') {
+		/* User registers:  %%e[a-c]x, etc */
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*contents = *(unsigned long *)((unsigned long)regs +
+				kdbreglist[i].reg_offset);
+		return(0);
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_setregcontents
+ *
+ *	Set the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	Supports modification of user-mode registers via
+ *	%<register-name>
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ *	contents	Unsigned long containing new register contents
+ * Outputs:
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ */
+
+int
+kdba_setregcontents(const char *regname,
+		  struct pt_regs *regs,
+		  unsigned long contents)
+{
+	int i;
+
+	if (regname[0] == '%') {
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*(unsigned long *)((unsigned long)regs
+				   + kdbreglist[i].reg_offset) = contents;
+		return 0;
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_dumpregs
+ *
+ *	Dump the specified register set to the display.
+ *
+ * Parameters:
+ *	regs		Pointer to structure containing registers.
+ *	type		Character string identifying register set to dump
+ *	extra		string further identifying register (optional)
+ * Outputs:
+ * Returns:
+ *	0		Success
+ * Locking:
+ * 	None.
+ * Remarks:
+ *	This function will dump the general register set if the type
+ *	argument is NULL (struct pt_regs).   The alternate register
+ *	set types supported by this function:
+ *
+ *	d 		Debug registers
+ *	c		Control registers
+ *	u		User registers at most recent entry to kernel
+ * Following not yet implemented:
+ *	m		Model Specific Registers (extra defines register #)
+ *	r		Memory Type Range Registers (extra defines register)
+ */
+
+int
+kdba_dumpregs(struct pt_regs *regs,
+	    const char *type,
+	    const char *extra)
+{
+	int i;
+	int count = 0;
+
+	if (type
+	 && (type[0] == 'u')) {
+		type = NULL;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	if (type == NULL) {
+		struct kdbregs *rlp;
+		kdb_machreg_t contents;
+
+		for (i=0, rlp=kdbreglist; i<nkdbreglist; i++,rlp++) {
+			kdba_getregcontents(rlp->reg_name, regs, &contents);
+			kdb_printf("%-5s = 0x%p%c", rlp->reg_name, (void *)contents, (++count % 2) ? ' ' : '\n');
+		}
+
+		kdb_printf("&regs = 0x%p\n", regs);
+
+		return 0;
+	}
+
+	switch (type[0]) {
+	case 'm':
+		break;
+	case 'r':
+		break;
+	default:
+		return KDB_BADREG;
+	}
+
+	/* NOTREACHED */
+	return 0;
+}
+
+kdb_machreg_t
+kdba_getpc(kdb_eframe_t ef)
+{
+	return ef->nip;
+}
+
+int
+kdba_setpc(kdb_eframe_t ef, kdb_machreg_t newpc)
+{
+	ef->nip = newpc;
+	KDB_STATE_SET(IP_ADJUSTED);
+	return 0;
+}
+
+/*
+ * kdba_main_loop
+ *
+ *	Do any architecture specific set up before entering the main kdb loop.
+ *	The primary function of this routine is to make all processes look the
+ *	same to kdb, kdb must be able to list a process without worrying if the
+ *	process is running or blocked, so make all process look as though they
+ *	are blocked.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	error2		kdb's current reason code.  Initially error but can change
+ *			acording to kdb state.
+ *	db_result	Result from break or debug point.
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ * Outputs:
+ *	Sets eip and esp in current->thread.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	none.
+ */
+
+int
+kdba_main_loop(kdb_reason_t reason, kdb_reason_t reason2, int error,
+	       kdb_dbtrap_t db_result, kdb_eframe_t ef)
+{
+	int rv;
+	unsigned int msr;
+	if (current->thread.regs == NULL)
+	{
+		struct pt_regs regs;
+		asm volatile ("std	0,0(%0)\n\
+                               std	1,8(%0)\n\
+                               std	2,16(%0)\n\
+                               std	3,24(%0)\n\
+                               std	4,32(%0)\n\
+                               std	5,40(%0)\n\
+                               std	6,48(%0)\n\
+                               std	7,56(%0)\n\
+                               std	8,64(%0)\n\
+                               std	9,72(%0)\n\
+                               std	10,80(%0)\n\
+                               std	11,88(%0)\n\
+                               std	12,96(%0)\n\
+                               std	13,104(%0)\n\
+                               std	14,112(%0)\n\
+                               std	15,120(%0)\n\
+                               std	16,128(%0)\n\
+                               std	17,136(%0)\n\
+                               std	18,144(%0)\n\
+                               std	19,152(%0)\n\
+                               std	20,160(%0)\n\
+                               std	21,168(%0)\n\
+                               std	22,176(%0)\n\
+                               std	23,184(%0)\n\
+                               std	24,192(%0)\n\
+                               std	25,200(%0)\n\
+                               std	26,208(%0)\n\
+                               std	27,216(%0)\n\
+                               std	28,224(%0)\n\
+                               std	29,232(%0)\n\
+                               std	30,240(%0)\n\
+                               std	31,248(%0)" : : "b" (&regs));
+		/* Fetch the link reg for this stack frame.
+		 NOTE: the prev kdb_printf fills in the lr. */
+		regs.nip = regs.link = ((unsigned long *)regs.gpr[1])[2];
+		regs.msr = get_msr();
+		regs.ctr = get_ctr();
+		regs.xer = get_xer();
+		regs.ccr = get_cr();
+		regs.trap = 0;
+		current->thread.regs = &regs;
+	}
+	if (ef) {
+		kdba_getregcontents("eip", ef, &(current->thread.regs->nip));
+		kdba_getregcontents("esp", ef, &(current->thread.regs->gpr[1]));
+		
+	}
+	msr = get_msr();
+	set_msr( msr & ~0x8000);
+	rv = kdb_main_loop(reason, reason2, error, db_result, ef);
+	set_msr(msr);
+	return rv;
+}
+
+void
+kdba_disableint(kdb_intstate_t *state)
+{
+	int *fp = (int *)state;
+	int   flags;
+
+	save_flags(flags);
+	cli();
+
+	*fp = flags;
+}
+
+void
+kdba_restoreint(kdb_intstate_t *state)
+{
+	int flags = *(int *)state;
+	restore_flags(flags);
+}
+
+void
+kdba_setsinglestep(struct pt_regs *regs)
+{
+	regs->msr |= MSR_SE;
+}
+
+void
+kdba_clearsinglestep(struct pt_regs *regs)
+{
+	
+	regs->msr &= ~MSR_SE;
+}
+
+int
+kdba_getcurrentframe(struct pt_regs *regs)
+{
+	regs->gpr[1] = getsp();
+	return 0;
+}
+
+#ifdef KDB_HAVE_LONGJMP
+int kdba_setjmp(kdb_jmp_buf *buf)
+{
+    asm volatile (
+	"mflr 0; std 0,0(%0)\n\
+	 std	1,8(%0)\n\
+	 std	2,16(%0)\n\
+	 mfcr 0; std 0,24(%0)\n\
+	 std	13,32(%0)\n\
+	 std	14,40(%0)\n\
+	 std	15,48(%0)\n\
+	 std	16,56(%0)\n\
+	 std	17,64(%0)\n\
+	 std	18,72(%0)\n\
+	 std	19,80(%0)\n\
+	 std	20,88(%0)\n\
+	 std	21,96(%0)\n\
+	 std	22,104(%0)\n\
+	 std	23,112(%0)\n\
+	 std	24,120(%0)\n\
+	 std	25,128(%0)\n\
+	 std	26,136(%0)\n\
+	 std	27,144(%0)\n\
+	 std	28,152(%0)\n\
+	 std	29,160(%0)\n\
+	 std	30,168(%0)\n\
+	 std	31,176(%0)\n\
+	 " : : "r" (buf));
+    KDB_STATE_SET(LONGJMP);
+    return 0;
+}
+void kdba_longjmp(kdb_jmp_buf *buf, int val)
+{
+    if (val == 0)
+	val = 1;
+    asm volatile (
+	"ld	13,32(%0)\n\
+	 ld	14,40(%0)\n\
+	 ld	15,48(%0)\n\
+	 ld	16,56(%0)\n\
+	 ld	17,64(%0)\n\
+	 ld	18,72(%0)\n\
+	 ld	19,80(%0)\n\
+	 ld	20,88(%0)\n\
+	 ld	21,96(%0)\n\
+	 ld	22,104(%0)\n\
+	 ld	23,112(%0)\n\
+	 ld	24,120(%0)\n\
+	 ld	25,128(%0)\n\
+	 ld	26,136(%0)\n\
+	 ld	27,144(%0)\n\
+	 ld	28,152(%0)\n\
+	 ld	29,160(%0)\n\
+	 ld	30,168(%0)\n\
+	 ld	31,176(%0)\n\
+	 ld	0,24(%0)\n\
+	 mtcrf	0x38,0\n\
+	 ld	0,0(%0)\n\
+	 ld	1,8(%0)\n\
+	 ld	2,16(%0)\n\
+	 mtlr	0\n\
+	 mr	3,%1\n\
+	 " : : "r" (buf), "r" (val));
+}
+#endif
+
+/*
+ * kdba_enable_mce
+ *
+ *	This function is called once on each CPU to enable machine
+ *	check exception handling.
+ *
+ * Inputs:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+void
+kdba_enable_mce(void)
+{
+}
+
+/*
+ * kdba_enable_lbr
+ *
+ *	Enable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_enable_lbr(void)
+{
+}
+
+/*
+ * kdba_disable_lbr
+ *
+ *	disable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_disable_lbr(void)
+{
+}
+
+/*
+ * kdba_print_lbr
+ *
+ *	Print last branch and last exception addresses
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_print_lbr(void)
+{
+}
+
+/*
+ * kdba_getword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+/* 	if (access_ok(VERIFY_READ,__gu_addr,size))			\ */
+ 
+extern inline void sync(void)
+{
+	asm volatile("sync; isync");
+}
+
+extern void (*debugger_fault_handler)(struct pt_regs *);
+extern void longjmp(u_int *, int);
+#if 0
+static void handle_fault(struct pt_regs *);
+
+static int fault_type;
+#endif
+
+unsigned long
+kdba_getword(unsigned long addr, size_t width)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+    if (!valid_ppc64_kernel_address(addr, width)) {
+		        /*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+#if 1
+				kdb_printf("    kdb: Possibly Bad kernel address 0x%lx \n",addr);
+#else
+				kdb_printf("kdb: ! \n");
+#endif
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+	}
+
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (width) {
+	case 8:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		return *lp;
+	}
+	case 4:
+	{	unsigned int *ip;
+
+		ip = (unsigned int *)(addr);
+		return *ip;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		return *sp;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		return *cp;
+	}
+	}
+
+	kdb_printf("kdbgetword: Bad width\n");
+	return 0L;
+}
+
+
+
+/*
+ * kdba_putword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+unsigned long
+kdba_putword(unsigned long addr, size_t size, unsigned long contents)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+	if (addr < PAGE_OFFSET) {
+		/*
+		 * Usermode address.
+		 */
+		unsigned long diag;
+
+		switch (size) {
+		case 4:
+		{	unsigned long *lp;
+
+			lp = (unsigned long *) addr;
+			diag = put_user(contents, lp);
+			break;
+		}
+		case 2:
+		{	unsigned short *sp;
+
+			sp = (unsigned short *) addr;
+			diag = put_user(contents, sp);
+			break;
+		}
+		case 1:
+		{	unsigned char *cp;
+
+			cp = (unsigned char *) addr;
+			diag = put_user(contents, cp);
+			break;
+		}
+		default:
+			kdb_printf("kdba_putword: Bad width\n");
+			return 0;
+		}
+
+		if (diag) {
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: Bad user address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0;
+		}
+		KDB_STATE_CLEAR(SUPPRESS);
+		return 0;
+	}
+
+#if 0
+	if (addr > (unsigned long)high_memory) {
+		if (!kdb_vmlist_check(addr, addr+size)) {
+			/*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: xx Bad kernel address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+		}
+	}
+#endif
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (size) {
+	case 4:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		*lp = contents;
+		return 0;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		*sp = (unsigned short) contents;
+		return 0;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		*cp = (unsigned char) contents;
+		return 0;
+	}
+	}
+
+	kdb_printf("kdba_putword: Bad width\n");
+	return 0;
+}
+
+/*
+ * kdba_callback_die
+ *
+ *	Callback function for kernel 'die' function.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Pointer to die message
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_callback_die(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	/*
+	 * Save a pointer to the message provided to 'die()'.
+	 */
+	kdb_diemsg = (char *)vp;
+
+	return kdb(KDB_REASON_OOPS, error_code, (kdb_eframe_t) regs);
+}
+
+/*
+ * kdba_callback_bp
+ *
+ *	Callback function for kernel breakpoint trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not Used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_bp(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	int diag;
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p\n", error_code,
+			   trapno, regs);
+
+	diag = kdb(KDB_REASON_BREAK, error_code, (kdb_eframe_t) regs);
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p diag = %d\n", error_code,
+			   trapno, regs, diag);
+	return diag;
+}
+
+/*
+ * kdba_callback_debug
+ *
+ *	Callback function for kernel debug register trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_debug(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	return kdb(KDB_REASON_DEBUG, error_code, (kdb_eframe_t) regs);
+}
+
+
+
+
+/*
+ * kdba_adjust_ip
+ *
+ * 	Architecture specific adjustment of instruction pointer before leaving
+ *	kdb.
+ *
+ * Parameters:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	noop on ix86.
+ */
+
+void
+kdba_adjust_ip(kdb_reason_t reason, int error, kdb_eframe_t ef)
+{
+	return;
+}
+
+
+
+/*
+ * kdba_find_tb_table
+ *
+ * 	Find the traceback table (defined by the ELF64 ABI) located at
+ *	the end of the function containing pc.
+ *
+ * Parameters:
+ *	eip	starting instruction addr.  does not need to be at the start of the func.
+ *	tab	table to populate if successful
+ * Returns:
+ *	non-zero if successful.  unsuccessful means that a valid tb table was not found
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+int kdba_find_tb_table(kdb_machreg_t eip, kdbtbtable_t *tab)
+{
+	kdb_machreg_t codeaddr = eip;
+	kdb_machreg_t codeaddr_max;
+	kdb_machreg_t tbtab_start;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	if (eip < PAGE_OFFSET) {  /* this is gonna fail for userspace, at least for now.. */
+	    return 0;
+	}
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+		instr = kdba_getword(codeaddr, 4);
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			flags = kdba_getword(codeaddr, 8);
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				parminfo = kdba_getword(codeaddr, 4);
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = KDBTBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = KDBTBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = KDBTBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & KDBTBTAB_FLAGSHASTBOFF) {
+				tab->tb_offset = kdba_getword(codeaddr, 4);
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & KDBTBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & KDBTBTAB_FLAGSNAMEPRESENT) {
+				int i;
+				short namlen = kdba_getword(codeaddr, 2);
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				for (i = 0; i < namlen; i++) {
+					tab->name[i] = kdba_getword(codeaddr++, 1);
+				}
+				tab->name[namlen] = '\0';
+			}
+			/* Fake up a symtab entry in case the caller finds it useful */
+			tab->symtab.value = tab->symtab.sym_start = tab->funcstart;
+			tab->symtab.sym_name = tab->name;
+			tab->symtab.sym_end = tbtab_start;
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+
+int
+kdba_putarea_size(unsigned long to_xxx, void *from, size_t size)
+{
+    char c;
+    kdb_printf("   ** this function calls copy_to_user...  \n");
+    kdb_printf("   kdba_putarea_size [0x%ul]\n",(unsigned int) to_xxx);
+    c = *((volatile char *)from);
+    c=*((volatile char *)from+size-1);
+    return __copy_to_user((void *)to_xxx,from,size);
+}
+
+
+
+
+
+/*
+ * valid_ppc64_kernel_address() returns '1' if the address passed in is
+ * within a valid range.  Function returns 0 if address is outside valid ranges.
+ */
+
+/*
+
+    KERNELBASE    c000000000000000
+        (good range)
+    high_memory   c0000000 20000000
+
+    VMALLOC_START d000000000000000
+        (good range)
+    VMALLOC_END   VMALLOC_START + VALID_EA_BITS  
+
+    IMALLOC_START e000000000000000
+        (good range)
+    IMALLOC_END   IMALLOC_START + VALID_EA_BITS
+
+*/
+
+int valid_ppc64_kernel_address(unsigned long addr, unsigned long size)
+{
+	unsigned long i;
+	unsigned long end = (addr + size - 1);
+
+
+	for (i = addr; i <= end; i = i ++ ) {
+	    if (((unsigned long)i < (unsigned long long)KERNELBASE     )  || 
+		(((unsigned long)i > (unsigned long long)high_memory) &&
+		 ((unsigned long)i < (unsigned long long)VMALLOC_START) )  ||
+		(((unsigned long)i > (unsigned long long)VMALLOC_END) &&
+		 ((unsigned long)i < (unsigned long long)IMALLOC_START) )  ||
+		( (unsigned long)i > (unsigned long long)IMALLOC_END    )       ) {
+		return 0;
+	    }
+	}
+	return 1;
+}
+
+
+int
+kdba_getarea_size(void *to, unsigned long from_xxx, size_t size)
+{
+	int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+	int diag = 0;
+
+	*((volatile char *)to) = '\0';
+	*((volatile char *)to + size - 1) = '\0';
+
+
+	if (is_valid_kern_addr) {
+		memcpy(to, (void *)from_xxx, size);
+	} else {
+            /*  user space address, just return.  */
+	    diag = -1;
+	}
+
+	return diag;
+}
+
+
+
+/*
+ *  kdba_readarea_size, reads size-lump of memory into to* passed in, returns size.
+ * Making it feel a bit more like mread.. when i'm clearer on kdba end, probally will
+ * remove one of these.
+ */
+int
+kdba_readarea_size(unsigned long from_xxx,void *to, size_t size)
+{
+    int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+
+    *((volatile char *)to) = '\0';
+    *((volatile char *)to + size - 1) = '\0';
+
+    if (is_valid_kern_addr) {
+	memcpy(to, (void *)from_xxx, size);
+	return size;
+    } else {
+	/*  user-space, just return...    */
+	return 0;
+    }
+    /* wont get here */
+    return 0;
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/nonstdio.h linuxppc64_2_4/arch/ppc64/kdb/nonstdio.h
--- linux-2.4.19/arch/ppc64/kdb/nonstdio.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/nonstdio.h	Fri May 24 15:08:51 2002
@@ -0,0 +1,32 @@
+#ifndef NONSTDIO_H
+#define NONSTDIO_H
+
+#define EOF	(-1)
+#if 0 
+typedef int	FILE;
+#endif
+
+/* candidate for removal.. */
+extern FILE *xmon_stdin, *xmon_stdout;
+#if 0
+#define stdin	xmon_stdin
+#define stdout	xmon_stdout
+#endif
+#define printf	xmon_printf
+#define fprintf	xmon_fprintf
+#define fputs	xmon_fputs
+#define fgets	xmon_fgets
+#define putchar	xmon_putchar
+#define getchar	xmon_getchar
+#define putc	xmon_putc
+#define getc	xmon_getc
+#define fopen(n, m)	NULL
+#define fflush(f)	do {} while (0)
+#define fclose(f)	do {} while (0)
+extern char *fgets(char *, int, void *);
+extern void xmon_printf(const char *, ...);
+extern void xmon_fprintf(void *, const char *, ...);
+extern void xmon_sprintf(char *, const char *, ...);
+#define perror(s)	printf("%s: no files!\n", (s))
+
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/opintl.h linuxppc64_2_4/arch/ppc64/kdb/opintl.h
--- linux-2.4.19/arch/ppc64/kdb/opintl.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/opintl.h	Mon Jun  4 10:24:01 2001
@@ -0,0 +1,42 @@
+/* opintl.h - opcodes specific header for gettext code.
+   Copyright (C) 1998, 1999 Free Software Foundation, Inc.
+
+   Written by Tom Tromey <tromey@cygnus.com>
+
+   This file is part of the opcodes library used by GAS and the GNU binutils.
+
+   You should have received a copy of the GNU General Public License
+   along with GAS; see the file COPYING.  If not, write to the Free
+   Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+   02111-1307, USA. */
+
+#ifdef ENABLE_NLS
+# include <libintl.h>
+/* Note the use of dgetext() and PACKAGE here, rather than gettext().
+   
+   This is because the code in this directory is used to build a library which
+   will be linked with code in other directories to form programs.  We want to
+   maintain a seperate translation file for this directory however, rather
+   than being forced to merge it with that of any program linked to
+   libopcodes.  This is a library, so it cannot depend on the catalog
+   currently loaded.
+
+   In order to do this, we have to make sure that when we extract messages we
+   use the OPCODES domain rather than the domain of the program that included
+   the opcodes library, (eg OBJDUMP).  Hence we use dgettext (PACKAGE, String)
+   and define PACKAGE to be 'opcodes'.  (See the code in configure).  */
+# define _(String) dgettext (PACKAGE, String)
+# ifdef gettext_noop
+#  define N_(String) gettext_noop (String)
+# else
+#  define N_(String) (String)
+# endif
+#else
+# define gettext(Msgid) (Msgid)
+# define dgettext(Domainname, Msgid) (Msgid)
+# define dcgettext(Domainname, Msgid, Category) (Msgid)
+# define textdomain(Domainname) while (0) /* nothing */
+# define bindtextdomain(Domainname, Dirname) while (0) /* nothing */
+# define _(String) (String)
+# define N_(String) (String)
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/ppc-dis.c linuxppc64_2_4/arch/ppc64/kdb/ppc-dis.c
--- linux-2.4.19/arch/ppc64/kdb/ppc-dis.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/ppc-dis.c	Fri May 24 15:08:51 2002
@@ -0,0 +1,281 @@
+/* ppc-dis.c -- Disassemble PowerPC instructions
+   Copyright 1994 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+#ifdef __KERNEL__
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+
+#if 0
+#include <setjmp.h>
+#endif
+
+#else
+#include <stdio.h>
+#include "sysdep.h"
+#include "dis-asm.h"
+#include "opcode/ppc.h"
+#endif
+bfd_vma
+bfd_getb32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0] << 24;
+  v |= (unsigned long) addr[1] << 16;
+  v |= (unsigned long) addr[2] << 8;
+  v |= (unsigned long) addr[3];
+  return (bfd_vma) v;
+}
+
+bfd_vma
+bfd_getl32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0];
+  v |= (unsigned long) addr[1] << 8;
+  v |= (unsigned long) addr[2] << 16;
+  v |= (unsigned long) addr[3] << 24;
+  return (bfd_vma) v;
+}
+/* This file provides several disassembler functions, all of which use
+   the disassembler interface defined in dis-asm.h.  Several functions
+   are provided because this file handles disassembly for the PowerPC
+   in both big and little endian mode and also for the POWER (RS/6000)
+   chip.  */
+
+static int print_insn_powerpc PARAMS ((bfd_vma, struct disassemble_info *,
+				       int bigendian, int dialect));
+
+/* Print a big endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_big_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a little endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_little_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 0,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a POWER (RS/6000) instruction.  */
+
+int
+print_insn_rs6000 (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1, PPC_OPCODE_POWER);
+}
+
+/* Print a PowerPC or POWER instruction.  */
+
+static int
+print_insn_powerpc (memaddr, info, bigendian, dialect)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+     int bigendian;
+     int dialect;
+{
+  bfd_byte buffer[4];
+  int status;
+  unsigned long insn;
+  const struct powerpc_opcode *opcode;
+  const struct powerpc_opcode *opcode_end;
+  unsigned long op;
+
+  (*info->fprintf_func) (info->stream, "  ");
+
+  status = (*info->read_memory_func) (memaddr, buffer, 4, info);
+  if (status != 0)
+    {
+      (*info->memory_error_func) (status, memaddr, info);
+      return -1;
+    }
+
+  if (bigendian)
+    insn = bfd_getb32 (buffer);
+  else
+    insn = bfd_getl32 (buffer);
+
+  /* Get the major opcode of the instruction.  */
+  op = PPC_OP (insn);
+
+  /* Find the first match in the opcode table.  We could speed this up
+     a bit by doing a binary search on the major opcode.  */
+  opcode_end = powerpc_opcodes + powerpc_num_opcodes;
+  for (opcode = powerpc_opcodes; opcode < opcode_end; opcode++)
+    {
+      unsigned long table_op;
+      const unsigned char *opindex;
+      const struct powerpc_operand *operand;
+      int invalid;
+      int need_comma;
+      int need_paren;
+
+      table_op = PPC_OP (opcode->opcode);
+      if (op < table_op)
+	break;
+      if (op > table_op)
+	continue;
+
+      if ((insn & opcode->mask) != opcode->opcode
+	  || (opcode->flags & dialect) == 0)
+	continue;
+
+      /* Make two passes over the operands.  First see if any of them
+	 have extraction functions, and, if they do, make sure the
+	 instruction is valid.  */
+      invalid = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  operand = powerpc_operands + *opindex;
+	  if (operand->extract)
+	    (*operand->extract) (insn, &invalid);
+	}
+      if (invalid)
+	continue;
+
+      /* The instruction is valid.  */
+      (*info->fprintf_func) (info->stream, "%s", opcode->name);
+      if (opcode->operands[0] != 0)
+	(*info->fprintf_func) (info->stream, "\t");
+
+      /* Now extract and print the operands.  */
+      need_comma = 0;
+      need_paren = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  long value;
+
+	  operand = powerpc_operands + *opindex;
+
+	  /* Operands that are marked FAKE are simply ignored.  We
+	     already made sure that the extract function considered
+	     the instruction to be valid.  */
+	  if ((operand->flags & PPC_OPERAND_FAKE) != 0)
+	    continue;
+
+	  /* Extract the value from the instruction.  */
+	  if (operand->extract)
+	    value = (*operand->extract) (insn, (int *) NULL);
+	  else
+	    {
+	      value = (insn >> operand->shift) & ((1 << operand->bits) - 1);
+	      if ((operand->flags & PPC_OPERAND_SIGNED) != 0
+		  && (value & (1 << (operand->bits - 1))) != 0)
+		value -= 1 << operand->bits;
+	    }
+
+	  /* If the operand is optional, and the value is zero, don't
+	     print anything.  */
+	  if ((operand->flags & PPC_OPERAND_OPTIONAL) != 0
+	      && (operand->flags & PPC_OPERAND_NEXT) == 0
+	      && value == 0)
+	    continue;
+
+	  if (need_comma)
+	    {
+	      (*info->fprintf_func) (info->stream, ",");
+	      need_comma = 0;
+	    }
+
+	  /* Print the operand as directed by the flags.  */
+	  if ((operand->flags & PPC_OPERAND_GPR) != 0)
+	    (*info->fprintf_func) (info->stream, "r%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_FPR) != 0)
+	    (*info->fprintf_func) (info->stream, "f%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_VR) != 0)
+	    (*info->fprintf_func) (info->stream, "v%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_RELATIVE) != 0)
+	    (*info->print_address_func) (memaddr + value, info);
+	  else if ((operand->flags & PPC_OPERAND_ABSOLUTE) != 0)
+	    (*info->print_address_func) ((bfd_vma) value & 0xffffffff, info);
+	  else if ((operand->flags & PPC_OPERAND_CR) == 0
+		   || (dialect & PPC_OPCODE_PPC) == 0)
+	    (*info->fprintf_func) (info->stream, "%ld", value);
+	  else
+	    {
+	      if (operand->bits == 3)
+		(*info->fprintf_func) (info->stream, "cr%d", value);
+	      else
+		{
+		  static const char *cbnames[4] = { "lt", "gt", "eq", "so" };
+		  int cr;
+		  int cc;
+
+		  cr = value >> 2;
+		  if (cr != 0)
+		    (*info->fprintf_func) (info->stream, "4*cr%d", cr);
+		  cc = value & 3;
+		  if (cc != 0)
+		    {
+		      if (cr != 0)
+			(*info->fprintf_func) (info->stream, "+");
+		      (*info->fprintf_func) (info->stream, "%s", cbnames[cc]);
+		    }
+		}
+	    }
+
+	  if (need_paren)
+	    {
+	      (*info->fprintf_func) (info->stream, ")");
+	      need_paren = 0;
+	    }
+
+	  if ((operand->flags & PPC_OPERAND_PARENS) == 0)
+	    need_comma = 1;
+	  else
+	    {
+	      (*info->fprintf_func) (info->stream, "(");
+	      need_paren = 1;
+	    }
+	}
+
+      /* We have found and printed an instruction; return.  */
+      return 4;
+    }
+
+  /* We could not find a match.  */
+  (*info->fprintf_func) (info->stream, ".long 0x%lx", insn);
+
+  return 4;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/ppc-opc.c linuxppc64_2_4/arch/ppc64/kdb/ppc-opc.c
--- linux-2.4.19/arch/ppc64/kdb/ppc-opc.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/ppc-opc.c	Mon Jun  4 10:24:01 2001
@@ -0,0 +1,3491 @@
+/* ppc-opc.c -- PowerPC opcode list
+   Copyright (c) 1994, 95, 96, 97, 98, 99, 2000 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+#ifndef __KERNEL__
+#include <stdio.h>
+#include "sysdep.h"
+#include "opcode/ppc.h"
+#include "opintl.h"
+#else
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+#include "opintl.h"
+#endif
+/* This file holds the PowerPC opcode table.  The opcode table
+   includes almost all of the extended instruction mnemonics.  This
+   permits the disassembler to use them, and simplifies the assembler
+   logic, at the cost of increasing the table size.  The table is
+   strictly constant data, so the compiler should be able to put it in
+   the .text section.
+
+   This file also holds the operand table.  All knowledge about
+   inserting operands into instructions and vice-versa is kept in this
+   file.  */
+
+/* Local insertion and extraction functions.  */
+
+static unsigned long insert_bat PARAMS ((unsigned long, long, const char **));
+static long extract_bat PARAMS ((unsigned long, int *));
+static unsigned long insert_bba PARAMS ((unsigned long, long, const char **));
+static long extract_bba PARAMS ((unsigned long, int *));
+static unsigned long insert_bd PARAMS ((unsigned long, long, const char **));
+static long extract_bd PARAMS ((unsigned long, int *));
+static unsigned long insert_bdm PARAMS ((unsigned long, long, const char **));
+static long extract_bdm PARAMS ((unsigned long, int *));
+static unsigned long insert_bdp PARAMS ((unsigned long, long, const char **));
+static long extract_bdp PARAMS ((unsigned long, int *));
+static int valid_bo PARAMS ((long));
+static unsigned long insert_bo PARAMS ((unsigned long, long, const char **));
+static long extract_bo PARAMS ((unsigned long, int *));
+static unsigned long insert_boe PARAMS ((unsigned long, long, const char **));
+static long extract_boe PARAMS ((unsigned long, int *));
+static unsigned long insert_ds PARAMS ((unsigned long, long, const char **));
+static long extract_ds PARAMS ((unsigned long, int *));
+static unsigned long insert_li PARAMS ((unsigned long, long, const char **));
+static long extract_li PARAMS ((unsigned long, int *));
+static unsigned long insert_mbe PARAMS ((unsigned long, long, const char **));
+static long extract_mbe PARAMS ((unsigned long, int *));
+static unsigned long insert_mb6 PARAMS ((unsigned long, long, const char **));
+static long extract_mb6 PARAMS ((unsigned long, int *));
+static unsigned long insert_nb PARAMS ((unsigned long, long, const char **));
+static long extract_nb PARAMS ((unsigned long, int *));
+static unsigned long insert_nsi PARAMS ((unsigned long, long, const char **));
+static long extract_nsi PARAMS ((unsigned long, int *));
+static unsigned long insert_ral PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ram PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ras PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_rbs PARAMS ((unsigned long, long, const char **));
+static long extract_rbs PARAMS ((unsigned long, int *));
+static unsigned long insert_sh6 PARAMS ((unsigned long, long, const char **));
+static long extract_sh6 PARAMS ((unsigned long, int *));
+static unsigned long insert_spr PARAMS ((unsigned long, long, const char **));
+static long extract_spr PARAMS ((unsigned long, int *));
+static unsigned long insert_tbr PARAMS ((unsigned long, long, const char **));
+static long extract_tbr PARAMS ((unsigned long, int *));
+
+/* The operands table.
+
+   The fields are bits, shift, insert, extract, flags.
+
+   We used to put parens around the various additions, like the one
+   for BA just below.  However, that caused trouble with feeble
+   compilers with a limit on depth of a parenthesized expression, like
+   (reportedly) the compiler in Microsoft Developer Studio 5.  So we
+   omit the parens, since the macros are never used in a context where
+   the addition will be ambiguous.  */
+
+const struct powerpc_operand powerpc_operands[] =
+{
+  /* The zero index is used to indicate the end of the list of
+     operands.  */
+#define UNUSED 0
+  { 0, 0, 0, 0, 0 },
+
+  /* The BA field in an XL form instruction.  */
+#define BA UNUSED + 1
+#define BA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BA field in an XL form instruction when it must be the same
+     as the BT field in the same instruction.  */
+#define BAT BA + 1
+  { 5, 16, insert_bat, extract_bat, PPC_OPERAND_FAKE },
+
+  /* The BB field in an XL form instruction.  */
+#define BB BAT + 1
+#define BB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_CR },
+
+  /* The BB field in an XL form instruction when it must be the same
+     as the BA field in the same instruction.  */
+#define BBA BB + 1
+  { 5, 11, insert_bba, extract_bba, PPC_OPERAND_FAKE },
+
+  /* The BD field in a B form instruction.  The lower two bits are
+     forced to zero.  */
+#define BD BBA + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when absolute addressing is
+     used.  */
+#define BDA BD + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDM BDA + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used
+     and absolute address is used.  */
+#define BDMA BDM + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDP BDMA + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used
+     and absolute addressing is used.  */
+#define BDPA BDP + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BF field in an X or XL form instruction.  */
+#define BF BDPA + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR },
+
+  /* An optional BF field.  This is used for comparison instructions,
+     in which an omitted BF field is taken as zero.  */
+#define OBF BF + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The BFA field in an X or XL form instruction.  */
+#define BFA OBF + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR },
+
+  /* The BI field in a B form or XL form instruction.  */
+#define BI BFA + 1
+#define BI_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BO field in a B form instruction.  Certain values are
+     illegal.  */
+#define BO BI + 1
+#define BO_MASK (0x1f << 21)
+  { 5, 21, insert_bo, extract_bo, 0 },
+
+  /* The BO field in a B form instruction when the + or - modifier is
+     used.  This is like the BO field, but it must be even.  */
+#define BOE BO + 1
+  { 5, 21, insert_boe, extract_boe, 0 },
+
+  /* The BT field in an X or XL form instruction.  */
+#define BT BOE + 1
+  { 5, 21, 0, 0, PPC_OPERAND_CR },
+
+  /* The condition register number portion of the BI field in a B form
+     or XL form instruction.  This is used for the extended
+     conditional branch mnemonics, which set the lower two bits of the
+     BI field.  This field is optional.  */
+#define CR BT + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The D field in a D form instruction.  This is a displacement off
+     a register, and implies that the next operand is a register in
+     parentheses.  */
+#define D CR + 1
+  { 16, 0, 0, 0, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The DS field in a DS form instruction.  This is like D, but the
+     lower two bits are forced to zero.  */
+#define DS D + 1
+  { 16, 0, insert_ds, extract_ds, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The E field in a wrteei instruction.  */
+#define E DS + 1
+  { 1, 15, 0, 0, 0 },
+
+  /* The FL1 field in a POWER SC form instruction.  */
+#define FL1 E + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The FL2 field in a POWER SC form instruction.  */
+#define FL2 FL1 + 1
+  { 3, 2, 0, 0, 0 },
+
+  /* The FLM field in an XFL form instruction.  */
+#define FLM FL2 + 1
+  { 8, 17, 0, 0, 0 },
+
+  /* The FRA field in an X or A form instruction.  */
+#define FRA FLM + 1
+#define FRA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRB field in an X or A form instruction.  */
+#define FRB FRA + 1
+#define FRB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRC field in an A form instruction.  */
+#define FRC FRB + 1
+#define FRC_MASK (0x1f << 6)
+  { 5, 6, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRS field in an X form instruction or the FRT field in a D, X
+     or A form instruction.  */
+#define FRS FRC + 1
+#define FRT FRS
+  { 5, 21, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FXM field in an XFX instruction.  */
+#define FXM FRS + 1
+#define FXM_MASK (0xff << 12)
+  { 8, 12, 0, 0, 0 },
+
+  /* The L field in a D or X form instruction.  */
+#define L FXM + 1
+  { 1, 21, 0, 0, PPC_OPERAND_OPTIONAL },
+
+  /* The LEV field in a POWER SC form instruction.  */
+#define LEV L + 1
+  { 7, 5, 0, 0, 0 },
+
+  /* The LI field in an I form instruction.  The lower two bits are
+     forced to zero.  */
+#define LI LEV + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The LI field in an I form instruction when used as an absolute
+     address.  */
+#define LIA LI + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The MB field in an M form instruction.  */
+#define MB LIA + 1
+#define MB_MASK (0x1f << 6)
+  { 5, 6, 0, 0, 0 },
+
+  /* The ME field in an M form instruction.  */
+#define ME MB + 1
+#define ME_MASK (0x1f << 1)
+  { 5, 1, 0, 0, 0 },
+
+  /* The MB and ME fields in an M form instruction expressed a single
+     operand which is a bitmask indicating which bits to select.  This
+     is a two operand form using PPC_OPERAND_NEXT.  See the
+     description in opcode/ppc.h for what this means.  */
+#define MBE ME + 1
+  { 5, 6, 0, 0, PPC_OPERAND_OPTIONAL | PPC_OPERAND_NEXT },
+  { 32, 0, insert_mbe, extract_mbe, 0 },
+
+  /* The MB or ME field in an MD or MDS form instruction.  The high
+     bit is wrapped to the low end.  */
+#define MB6 MBE + 2
+#define ME6 MB6
+#define MB6_MASK (0x3f << 5)
+  { 6, 5, insert_mb6, extract_mb6, 0 },
+
+  /* The NB field in an X form instruction.  The value 32 is stored as
+     0.  */
+#define NB MB6 + 1
+  { 6, 11, insert_nb, extract_nb, 0 },
+
+  /* The NSI field in a D form instruction.  This is the same as the
+     SI field, only negated.  */
+#define NSI NB + 1
+  { 16, 0, insert_nsi, extract_nsi,
+      PPC_OPERAND_NEGATIVE | PPC_OPERAND_SIGNED },
+
+  /* The RA field in an D, DS, X, XO, M, or MDS form instruction.  */
+#define RA NSI + 1
+#define RA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     load, which means that the RA field may not be zero and may not
+     equal the RT field.  */
+#define RAL RA + 1
+  { 5, 16, insert_ral, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in an lmw instruction, which has special value
+     restrictions.  */
+#define RAM RAL + 1
+  { 5, 16, insert_ram, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     store or an updating floating point load, which means that the RA
+     field may not be zero.  */
+#define RAS RAM + 1
+  { 5, 16, insert_ras, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X, XO, M, or MDS form instruction.  */
+#define RB RAS + 1
+#define RB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X form instruction when it must be the same as
+     the RS field in the instruction.  This is used for extended
+     mnemonics like mr.  */
+#define RBS RB + 1
+  { 5, 1, insert_rbs, extract_rbs, PPC_OPERAND_FAKE },
+
+  /* The RS field in a D, DS, X, XFX, XS, M, MD or MDS form
+     instruction or the RT field in a D, DS, X, XFX or XO form
+     instruction.  */
+#define RS RBS + 1
+#define RT RS
+#define RT_MASK (0x1f << 21)
+  { 5, 21, 0, 0, PPC_OPERAND_GPR },
+
+  /* The SH field in an X or M form instruction.  */
+#define SH RS + 1
+#define SH_MASK (0x1f << 11)
+  { 5, 11, 0, 0, 0 },
+
+  /* The SH field in an MD form instruction.  This is split.  */
+#define SH6 SH + 1
+#define SH6_MASK ((0x1f << 11) | (1 << 1))
+  { 6, 1, insert_sh6, extract_sh6, 0 },
+
+  /* The SI field in a D form instruction.  */
+#define SI SH6 + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED },
+
+  /* The SI field in a D form instruction when we accept a wide range
+     of positive values.  */
+#define SISIGNOPT SI + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED | PPC_OPERAND_SIGNOPT },
+
+  /* The SPR field in an XFX form instruction.  This is flipped--the
+     lower 5 bits are stored in the upper 5 and vice- versa.  */
+#define SPR SISIGNOPT + 1
+#define SPR_MASK (0x3ff << 11)
+  { 10, 11, insert_spr, extract_spr, 0 },
+
+  /* The BAT index number in an XFX form m[ft]ibat[lu] instruction.  */
+#define SPRBAT SPR + 1
+#define SPRBAT_MASK (0x3 << 17)
+  { 2, 17, 0, 0, 0 },
+
+  /* The SPRG register number in an XFX form m[ft]sprg instruction.  */
+#define SPRG SPRBAT + 1
+#define SPRG_MASK (0x3 << 16)
+  { 2, 16, 0, 0, 0 },
+
+  /* The SR field in an X form instruction.  */
+#define SR SPRG + 1
+  { 4, 16, 0, 0, 0 },
+
+  /* The SV field in a POWER SC form instruction.  */
+#define SV SR + 1
+  { 14, 2, 0, 0, 0 },
+
+  /* The TBR field in an XFX form instruction.  This is like the SPR
+     field, but it is optional.  */
+#define TBR SV + 1
+  { 10, 11, insert_tbr, extract_tbr, PPC_OPERAND_OPTIONAL },
+
+  /* The TO field in a D or X form instruction.  */
+#define TO TBR + 1
+#define TO_MASK (0x1f << 21)
+  { 5, 21, 0, 0, 0 },
+
+  /* The U field in an X form instruction.  */
+#define U TO + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The UI field in a D form instruction.  */
+#define UI U + 1
+  { 16, 0, 0, 0, 0 },
+
+  /* The VA field in a VA, VX or VXR form instruction. */
+#define VA UI + 1
+#define VA_MASK	(0x1f << 16)
+  {5, 16, 0, 0, PPC_OPERAND_VR},
+
+  /* The VB field in a VA, VX or VXR form instruction. */
+#define VB VA + 1
+#define VB_MASK (0x1f << 11)
+  {5, 11, 0, 0, PPC_OPERAND_VR}, 
+
+  /* The VC field in a VA form instruction. */
+#define VC VB + 1
+#define VC_MASK (0x1f << 6)
+  {5, 6, 0, 0, PPC_OPERAND_VR},
+
+  /* The VD or VS field in a VA, VX, VXR or X form instruction. */
+#define VD VC + 1
+#define VS VD
+#define VD_MASK (0x1f << 21)
+  {5, 21, 0, 0, PPC_OPERAND_VR},
+
+  /* The SIMM field in a VX form instruction. */
+#define SIMM VD + 1
+  { 5, 16, 0, 0, PPC_OPERAND_SIGNED},
+
+  /* The UIMM field in a VX form instruction. */
+#define UIMM SIMM + 1
+  { 5, 16, 0, 0, 0 },
+
+  /* The SHB field in a VA form instruction. */
+#define SHB UIMM + 1
+  { 4, 6, 0, 0, 0 },
+};
+
+/* The functions used to insert and extract complicated operands.  */
+
+/* The BA field in an XL form instruction when it must be the same as
+   the BT field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BT field into the BA field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bat (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 16);
+}
+
+static long
+extract_bat (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 16) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BB field in an XL form instruction when it must be the same as
+   the BA field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BA field into the BB field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bba (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 16) & 0x1f) << 11);
+}
+
+static long
+extract_bba (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 16) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BD field in a B form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bd (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_bd (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the - modifier is used.
+   This modifier means that the branch is not expected to be taken.
+   We must set the y bit of the BO field to 1 if the offset is
+   negative.  When extracting, we require that the y bit be 1 and that
+   the offset be positive, since if the y bit is 0 we just want to
+   print the normal form of the instruction.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdm (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) != 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdm (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) == 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the + modifier is used.
+   This is like BDM, above, except that the branch is expected to be
+   taken.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdp (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) == 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdp (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) != 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* Check for legal values of a BO field.  */
+
+static int
+valid_bo (value)
+     long value;
+{
+  /* Certain encodings have bits that are required to be zero.  These
+     are (z must be zero, y may be anything):
+         001zy
+	 011zy
+	 1z00y
+	 1z01y
+	 1z1zz
+     */
+  switch (value & 0x14)
+    {
+    default:
+    case 0:
+      return 1;
+    case 0x4:
+      return (value & 0x2) == 0;
+    case 0x10:
+      return (value & 0x8) == 0;
+    case 0x14:
+      return value == 0x14;
+    }
+}
+
+/* The BO field in a B form instruction.  Warn about attempts to set
+   the field to an illegal value.  */
+
+static unsigned long
+insert_bo (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL
+      && ! valid_bo (value))
+    *errmsg = _("invalid conditional option");
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_bo (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value;
+}
+
+/* The BO field in a B form instruction when the + or - modifier is
+   used.  This is like the BO field, but it must be even.  When
+   extracting it, we force it to be even.  */
+
+static unsigned long
+insert_boe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL)
+    {
+      if (! valid_bo (value))
+	*errmsg = _("invalid conditional option");
+      else if ((value & 1) != 0)
+	*errmsg = _("attempt to set y bit when using + or - modifier");
+    }
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_boe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value & 0x1e;
+}
+
+/* The DS field in a DS form instruction.  This is like D, but the
+   lower two bits are forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_ds (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_ds (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The LI field in an I form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_li (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((value & 3) != 0 && errmsg != (const char **) NULL)
+    *errmsg = _("ignoring least significant bits in branch offset");
+  return insn | (value & 0x3fffffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_li (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x2000000) != 0)
+    return (insn & 0x3fffffc) - 0x4000000;
+  else
+    return insn & 0x3fffffc;
+}
+
+/* The MB and ME fields in an M form instruction expressed as a single
+   operand which is itself a bitmask.  The extraction function always
+   marks it as invalid, since we never want to recognize an
+   instruction which uses a field of this type.  */
+
+static unsigned long
+insert_mbe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  unsigned long uval, mask;
+  int mb, me, mx, count, last;
+
+  uval = value;
+
+  if (uval == 0)
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+      return insn;
+    }
+
+  mb = 0;
+  me = 32;
+  if ((uval & 1) != 0)
+    last = 1;
+  else
+    last = 0;
+  count = 0;
+
+  /* mb: location of last 0->1 transition */
+  /* me: location of last 1->0 transition */
+  /* count: # transitions */
+
+  for (mx = 0, mask = 1 << 31; mx < 32; ++mx, mask >>= 1)
+    {
+      if ((uval & mask) && !last)
+	{
+	  ++count;
+	  mb = mx;
+	  last = 1;
+	}
+      else if (!(uval & mask) && last)
+	{
+	  ++count;
+	  me = mx;
+	  last = 0;
+	}
+    }
+  if (me == 0)
+    me = 32;
+
+  if (count != 2 && (count != 0 || ! last))
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+    }
+
+  return insn | (mb << 6) | ((me - 1) << 1);
+}
+
+static long
+extract_mbe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long ret;
+  int mb, me;
+  int i;
+
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+
+  mb = (insn >> 6) & 0x1f;
+  me = (insn >> 1) & 0x1f;
+  if (mb < me + 1)
+    {
+      ret = 0;
+      for (i = mb; i <= me; i++)
+	ret |= (long) 1 << (31 - i);
+    }
+  else if (mb == me + 1)
+    ret = ~0;
+  else /* (mb > me + 1) */
+    {
+      ret = ~ (long) 0;
+      for (i = me + 1; i < mb; i++)
+	ret &= ~ ((long) 1 << (31 - i));
+    }
+  return ret;
+}
+
+/* The MB or ME field in an MD or MDS form instruction.  The high bit
+   is wrapped to the low end.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_mb6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 6) | (value & 0x20);
+}
+
+/*ARGSUSED*/
+static long
+extract_mb6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 6) & 0x1f) | (insn & 0x20);
+}
+
+/* The NB field in an X form instruction.  The value 32 is stored as
+   0.  */
+
+static unsigned long
+insert_nb (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value < 0 || value > 32)
+    *errmsg = _("value out of range");
+  if (value == 32)
+    value = 0;
+  return insn | ((value & 0x1f) << 11);
+}
+
+/*ARGSUSED*/
+static long
+extract_nb (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = (insn >> 11) & 0x1f;
+  if (ret == 0)
+    ret = 32;
+  return ret;
+}
+
+/* The NSI field in a D form instruction.  This is the same as the SI
+   field, only negated.  The extraction function always marks it as
+   invalid, since we never want to recognize an instruction which uses
+   a field of this type.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_nsi (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((- value) & 0xffff);
+}
+
+static long
+extract_nsi (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return - ((long)(insn & 0xffff) - 0x10000);
+  else
+    return - (long)(insn & 0xffff);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   load, which means that the RA field may not be zero and may not
+   equal the RT field.  */
+
+static unsigned long
+insert_ral (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0
+      || (unsigned long) value == ((insn >> 21) & 0x1f))
+    *errmsg = "invalid register operand when updating";
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in an lmw instruction, which has special value
+   restrictions.  */
+
+static unsigned long
+insert_ram (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((unsigned long) value >= ((insn >> 21) & 0x1f))
+    *errmsg = _("index register in load range");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   store or an updating floating point load, which means that the RA
+   field may not be zero.  */
+
+static unsigned long
+insert_ras (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0)
+    *errmsg = _("invalid register operand when updating");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RB field in an X form instruction when it must be the same as
+   the RS field in the instruction.  This is used for extended
+   mnemonics like mr.  This operand is marked FAKE.  The insertion
+   function just copies the BT field into the BA field, and the
+   extraction function just checks that the fields are the same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_rbs (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 11);
+}
+
+static long
+extract_rbs (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The SH field in an MD form instruction.  This is split.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_sh6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 11) | ((value & 0x20) >> 4);
+}
+
+/*ARGSUSED*/
+static long
+extract_sh6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 11) & 0x1f) | ((insn << 4) & 0x20);
+}
+
+/* The SPR field in an XFX form instruction.  This is flipped--the
+   lower 5 bits are stored in the upper 5 and vice- versa.  */
+
+static unsigned long
+insert_spr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_spr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+}
+
+/* The TBR field in an XFX instruction.  This is just like SPR, but it
+   is optional.  When TBR is omitted, it must be inserted as 268 (the
+   magic number of the TB register).  These functions treat 0
+   (indicating an omitted optional operand) as 268.  This means that
+   ``mftb 4,0'' is not handled correctly.  This does not matter very
+   much, since the architecture manual does not define mftb as
+   accepting any values other than 268 or 269.  */
+
+#define TB (268)
+
+static unsigned long
+insert_tbr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if (value == 0)
+    value = TB;
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_tbr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+  if (ret == TB)
+    ret = 0;
+  return ret;
+}
+
+/* Macros used to form opcodes.  */
+
+/* The main opcode.  */
+#define OP(x) ((((unsigned long)(x)) & 0x3f) << 26)
+#define OP_MASK OP (0x3f)
+
+/* The main opcode combined with a trap code in the TO field of a D
+   form instruction.  Used for extended mnemonics for the trap
+   instructions.  */
+#define OPTO(x,to) (OP (x) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define OPTO_MASK (OP_MASK | TO_MASK)
+
+/* The main opcode combined with a comparison size bit in the L field
+   of a D form or X form instruction.  Used for extended mnemonics for
+   the comparison instructions.  */
+#define OPL(x,l) (OP (x) | ((((unsigned long)(l)) & 1) << 21))
+#define OPL_MASK OPL (0x3f,1)
+
+/* An A form instruction.  */
+#define A(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1f) << 1) | (((unsigned long)(rc)) & 1))
+#define A_MASK A (0x3f, 0x1f, 1)
+
+/* An A_MASK with the FRB field fixed.  */
+#define AFRB_MASK (A_MASK | FRB_MASK)
+
+/* An A_MASK with the FRC field fixed.  */
+#define AFRC_MASK (A_MASK | FRC_MASK)
+
+/* An A_MASK with the FRA and FRC fields fixed.  */
+#define AFRAFRC_MASK (A_MASK | FRA_MASK | FRC_MASK)
+
+/* A B form instruction.  */
+#define B(op, aa, lk) (OP (op) | ((((unsigned long)(aa)) & 1) << 1) | ((lk) & 1))
+#define B_MASK B (0x3f, 1, 1)
+
+/* A B form instruction setting the BO field.  */
+#define BBO(op, bo, aa, lk) (B ((op), (aa), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define BBO_MASK BBO (0x3f, 0x1f, 1, 1)
+
+/* A BBO_MASK with the y bit of the BO field removed.  This permits
+   matching a conditional branch regardless of the setting of the y
+   bit.  */
+#define Y_MASK (((unsigned long)1) << 21)
+#define BBOY_MASK (BBO_MASK &~ Y_MASK)
+
+/* A B form instruction setting the BO field and the condition bits of
+   the BI field.  */
+#define BBOCB(op, bo, cb, aa, lk) \
+  (BBO ((op), (bo), (aa), (lk)) | ((((unsigned long)(cb)) & 0x3) << 16))
+#define BBOCB_MASK BBOCB (0x3f, 0x1f, 0x3, 1, 1)
+
+/* A BBOCB_MASK with the y bit of the BO field removed.  */
+#define BBOYCB_MASK (BBOCB_MASK &~ Y_MASK)
+
+/* A BBOYCB_MASK in which the BI field is fixed.  */
+#define BBOYBI_MASK (BBOYCB_MASK | BI_MASK)
+
+/* The main opcode mask with the RA field clear.  */
+#define DRA_MASK (OP_MASK | RA_MASK)
+
+/* A DS form instruction.  */
+#define DSO(op, xop) (OP (op) | ((xop) & 0x3))
+#define DS_MASK DSO (0x3f, 3)
+
+/* An M form instruction.  */
+#define M(op, rc) (OP (op) | ((rc) & 1))
+#define M_MASK M (0x3f, 1)
+
+/* An M form instruction with the ME field specified.  */
+#define MME(op, me, rc) (M ((op), (rc)) | ((((unsigned long)(me)) & 0x1f) << 1))
+
+/* An M_MASK with the MB and ME fields fixed.  */
+#define MMBME_MASK (M_MASK | MB_MASK | ME_MASK)
+
+/* An M_MASK with the SH and ME fields fixed.  */
+#define MSHME_MASK (M_MASK | SH_MASK | ME_MASK)
+
+/* An MD form instruction.  */
+#define MD(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x7) << 2) | ((rc) & 1))
+#define MD_MASK MD (0x3f, 0x7, 1)
+
+/* An MD_MASK with the MB field fixed.  */
+#define MDMB_MASK (MD_MASK | MB6_MASK)
+
+/* An MD_MASK with the SH field fixed.  */
+#define MDSH_MASK (MD_MASK | SH6_MASK)
+
+/* An MDS form instruction.  */
+#define MDS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0xf) << 1) | ((rc) & 1))
+#define MDS_MASK MDS (0x3f, 0xf, 1)
+
+/* An MDS_MASK with the MB field fixed.  */
+#define MDSMB_MASK (MDS_MASK | MB6_MASK)
+
+/* An SC form instruction.  */
+#define SC(op, sa, lk) (OP (op) | ((((unsigned long)(sa)) & 1) << 1) | ((lk) & 1))
+#define SC_MASK (OP_MASK | (((unsigned long)0x3ff) << 16) | (((unsigned long)1) << 1) | 1)
+
+/* An VX form instruction. */
+#define VX(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x7ff))
+
+/* The mask for an VX form instruction. */
+#define VX_MASK	VX(0x3f, 0x7ff)
+
+/* An VA form instruction. */
+#define VXA(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x07f))
+
+/* The mask for an VA form instruction. */
+#define VXA_MASK VXA(0x3f, 0x7f)
+
+/* An VXR form instruction. */
+#define VXR(op, xop, rc) (OP (op) | (((rc) & 1) << 10) | (((unsigned long)(xop)) & 0x3ff))
+
+/* The mask for a VXR form instruction. */
+#define VXR_MASK VXR(0x3f, 0x3ff, 1)
+
+/* An X form instruction.  */
+#define X(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An X form instruction with the RC bit specified.  */
+#define XRC(op, xop, rc) (X ((op), (xop)) | ((rc) & 1))
+
+/* The mask for an X form instruction.  */
+#define X_MASK XRC (0x3f, 0x3ff, 1)
+
+/* An X_MASK with the RA field fixed.  */
+#define XRA_MASK (X_MASK | RA_MASK)
+
+/* An X_MASK with the RB field fixed.  */
+#define XRB_MASK (X_MASK | RB_MASK)
+
+/* An X_MASK with the RT field fixed.  */
+#define XRT_MASK (X_MASK | RT_MASK)
+
+/* An X_MASK with the RA and RB fields fixed.  */
+#define XRARB_MASK (X_MASK | RA_MASK | RB_MASK)
+
+/* An X_MASK with the RT and RA fields fixed.  */
+#define XRTRA_MASK (X_MASK | RT_MASK | RA_MASK)
+
+/* An X form comparison instruction.  */
+#define XCMPL(op, xop, l) (X ((op), (xop)) | ((((unsigned long)(l)) & 1) << 21))
+
+/* The mask for an X form comparison instruction.  */
+#define XCMP_MASK (X_MASK | (((unsigned long)1) << 22))
+
+/* The mask for an X form comparison instruction with the L field
+   fixed.  */
+#define XCMPL_MASK (XCMP_MASK | (((unsigned long)1) << 21))
+
+/* An X form trap instruction with the TO field specified.  */
+#define XTO(op, xop, to) (X ((op), (xop)) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define XTO_MASK (X_MASK | TO_MASK)
+
+/* An X form tlb instruction with the SH field specified.  */
+#define XTLB(op, xop, sh) (X ((op), (xop)) | ((((unsigned long)(sh)) & 0x1f) << 11))
+#define XTLB_MASK (X_MASK | SH_MASK)
+
+/* An XFL form instruction.  */
+#define XFL(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1) | (((unsigned long)(rc)) & 1))
+#define XFL_MASK (XFL (0x3f, 0x3ff, 1) | (((unsigned long)1) << 25) | (((unsigned long)1) << 16))
+
+/* An XL form instruction with the LK field set to 0.  */
+#define XL(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An XL form instruction which uses the LK field.  */
+#define XLLK(op, xop, lk) (XL ((op), (xop)) | ((lk) & 1))
+
+/* The mask for an XL form instruction.  */
+#define XL_MASK XLLK (0x3f, 0x3ff, 1)
+
+/* An XL form instruction which explicitly sets the BO field.  */
+#define XLO(op, bo, xop, lk) \
+  (XLLK ((op), (xop), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define XLO_MASK (XL_MASK | BO_MASK)
+
+/* An XL form instruction which explicitly sets the y bit of the BO
+   field.  */
+#define XLYLK(op, xop, y, lk) (XLLK ((op), (xop), (lk)) | ((((unsigned long)(y)) & 1) << 21))
+#define XLYLK_MASK (XL_MASK | Y_MASK)
+
+/* An XL form instruction which sets the BO field and the condition
+   bits of the BI field.  */
+#define XLOCB(op, bo, cb, xop, lk) \
+  (XLO ((op), (bo), (xop), (lk)) | ((((unsigned long)(cb)) & 3) << 16))
+#define XLOCB_MASK XLOCB (0x3f, 0x1f, 0x3, 0x3ff, 1)
+
+/* An XL_MASK or XLYLK_MASK or XLOCB_MASK with the BB field fixed.  */
+#define XLBB_MASK (XL_MASK | BB_MASK)
+#define XLYBB_MASK (XLYLK_MASK | BB_MASK)
+#define XLBOCBBB_MASK (XLOCB_MASK | BB_MASK)
+
+/* An XL_MASK with the BO and BB fields fixed.  */
+#define XLBOBB_MASK (XL_MASK | BO_MASK | BB_MASK)
+
+/* An XL_MASK with the BO, BI and BB fields fixed.  */
+#define XLBOBIBB_MASK (XL_MASK | BO_MASK | BI_MASK | BB_MASK)
+
+/* An XO form instruction.  */
+#define XO(op, xop, oe, rc) \
+  (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 1) | ((((unsigned long)(oe)) & 1) << 10) | (((unsigned long)(rc)) & 1))
+#define XO_MASK XO (0x3f, 0x1ff, 1, 1)
+
+/* An XO_MASK with the RB field fixed.  */
+#define XORB_MASK (XO_MASK | RB_MASK)
+
+/* An XS form instruction.  */
+#define XS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 2) | (((unsigned long)(rc)) & 1))
+#define XS_MASK XS (0x3f, 0x1ff, 1)
+
+/* A mask for the FXM version of an XFX form instruction.  */
+#define XFXFXM_MASK (X_MASK | (((unsigned long)1) << 20) | (((unsigned long)1) << 11))
+
+/* An XFX form instruction with the FXM field filled in.  */
+#define XFXM(op, xop, fxm) \
+  (X ((op), (xop)) | ((((unsigned long)(fxm)) & 0xff) << 12))
+
+/* An XFX form instruction with the SPR field filled in.  */
+#define XSPR(op, xop, spr) \
+  (X ((op), (xop)) | ((((unsigned long)(spr)) & 0x1f) << 16) | ((((unsigned long)(spr)) & 0x3e0) << 6))
+#define XSPR_MASK (X_MASK | SPR_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRBAT field.  */
+#define XSPRBAT_MASK (XSPR_MASK &~ SPRBAT_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRG field.  */
+#define XSPRG_MASK (XSPR_MASK &~ SPRG_MASK)
+
+/* An X form instruction with everything filled in except the E field.  */
+#define XE_MASK (0xffff7fff)
+
+/* The BO encodings used in extended conditional branch mnemonics.  */
+#define BODNZF	(0x0)
+#define BODNZFP	(0x1)
+#define BODZF	(0x2)
+#define BODZFP	(0x3)
+#define BOF	(0x4)
+#define BOFP	(0x5)
+#define BODNZT	(0x8)
+#define BODNZTP	(0x9)
+#define BODZT	(0xa)
+#define BODZTP	(0xb)
+#define BOT	(0xc)
+#define BOTP	(0xd)
+#define BODNZ	(0x10)
+#define BODNZP	(0x11)
+#define BODZ	(0x12)
+#define BODZP	(0x13)
+#define BOU	(0x14)
+
+/* The BI condition bit encodings used in extended conditional branch
+   mnemonics.  */
+#define CBLT	(0)
+#define CBGT	(1)
+#define CBEQ	(2)
+#define CBSO	(3)
+
+/* The TO encodings used in extended trap mnemonics.  */
+#define TOLGT	(0x1)
+#define TOLLT	(0x2)
+#define TOEQ	(0x4)
+#define TOLGE	(0x5)
+#define TOLNL	(0x5)
+#define TOLLE	(0x6)
+#define TOLNG	(0x6)
+#define TOGT	(0x8)
+#define TOGE	(0xc)
+#define TONL	(0xc)
+#define TOLT	(0x10)
+#define TOLE	(0x14)
+#define TONG	(0x14)
+#define TONE	(0x18)
+#define TOU	(0x1f)
+
+/* Smaller names for the flags so each entry in the opcodes table will
+   fit on a single line.  */
+#undef	PPC
+#define PPC     PPC_OPCODE_PPC | PPC_OPCODE_ANY
+#define PPCCOM	PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define PPC32   PPC_OPCODE_PPC | PPC_OPCODE_32 | PPC_OPCODE_ANY
+#undef PPC64
+#define PPC64   PPC_OPCODE_PPC | PPC_OPCODE_64 | PPC_OPCODE_ANY
+#define PPCONLY	PPC_OPCODE_PPC
+#define PPC403	PPC
+#define PPC405	PPC403
+#define PPC750	PPC
+#define PPC860	PPC
+#define PPCVEC	PPC_OPCODE_ALTIVEC | PPC_OPCODE_ANY
+#define	POWER   PPC_OPCODE_POWER | PPC_OPCODE_ANY
+#define	POWER2	PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define PPCPWR2	PPC_OPCODE_PPC | PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define	POWER32	PPC_OPCODE_POWER | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	COM     PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	COM32   PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	M601    PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_ANY
+#define PWRCOM	PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	MFDEC1	PPC_OPCODE_POWER
+#define	MFDEC2	PPC_OPCODE_PPC | PPC_OPCODE_601
+
+/* The opcode table.
+
+   The format of the opcode table is:
+
+   NAME	     OPCODE	MASK		FLAGS		{ OPERANDS }
+
+   NAME is the name of the instruction.
+   OPCODE is the instruction opcode.
+   MASK is the opcode mask; this is used to tell the disassembler
+     which bits in the actual opcode must match OPCODE.
+   FLAGS are flags indicated what processors support the instruction.
+   OPERANDS is the list of operands.
+
+   The disassembler reads the table in order and prints the first
+   instruction which matches, so this table is sorted to put more
+   specific instructions before more general instructions.  It is also
+   sorted by major opcode.  */
+
+const struct powerpc_opcode powerpc_opcodes[] = {
+{ "tdlgti",  OPTO(2,TOLGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllti",  OPTO(2,TOLLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdeqi",   OPTO(2,TOEQ), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlgei",  OPTO(2,TOLGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlnli",  OPTO(2,TOLNL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllei",  OPTO(2,TOLLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlngi",  OPTO(2,TOLNG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgti",   OPTO(2,TOGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgei",   OPTO(2,TOGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnli",   OPTO(2,TONL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlti",   OPTO(2,TOLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlei",   OPTO(2,TOLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdngi",   OPTO(2,TONG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnei",   OPTO(2,TONE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdi",     OP(2),	OP_MASK,	PPC64,		{ TO, RA, SI } },
+
+{ "twlgti",  OPTO(3,TOLGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgti",   OPTO(3,TOLGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllti",  OPTO(3,TOLLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllti",   OPTO(3,TOLLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "tweqi",   OPTO(3,TOEQ), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "teqi",    OPTO(3,TOEQ), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlgei",  OPTO(3,TOLGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgei",   OPTO(3,TOLGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlnli",  OPTO(3,TOLNL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlnli",   OPTO(3,TOLNL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllei",  OPTO(3,TOLLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllei",   OPTO(3,TOLLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlngi",  OPTO(3,TOLNG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlngi",   OPTO(3,TOLNG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgti",   OPTO(3,TOGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgti",    OPTO(3,TOGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgei",   OPTO(3,TOGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgei",    OPTO(3,TOGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnli",   OPTO(3,TONL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnli",    OPTO(3,TONL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlti",   OPTO(3,TOLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlti",    OPTO(3,TOLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlei",   OPTO(3,TOLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlei",    OPTO(3,TOLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twngi",   OPTO(3,TONG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tngi",    OPTO(3,TONG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnei",   OPTO(3,TONE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnei",    OPTO(3,TONE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twi",     OP(3),	OP_MASK,	PPCCOM,		{ TO, RA, SI } },
+{ "ti",      OP(3),	OP_MASK,	PWRCOM,		{ TO, RA, SI } },
+
+{ "macchw",	XO(4,172,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchw.",	XO(4,172,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo",	XO(4,172,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo.",	XO(4,172,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws",	XO(4,236,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws.",	XO(4,236,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso",	XO(4,236,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso.",	XO(4,236,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu",	XO(4,204,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu.",	XO(4,204,0,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo",	XO(4,204,1,0), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo.",	XO(4,204,1,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwu",	XO(4,140,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwu.",	XO(4,140,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo",	XO(4,140,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo.",	XO(4,140,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw",	XO(4,44,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw.",	XO(4,44,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo",	XO(4,44,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo.",	XO(4,44,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws",	XO(4,108,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws.",	XO(4,108,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso",	XO(4,108,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso.",	XO(4,108,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu",	XO(4,76,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu.",	XO(4,76,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo",	XO(4,76,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo.",	XO(4,76,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu",	XO(4,12,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu.",	XO(4,12,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo",	XO(4,12,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo.",	XO(4,12,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw",	XO(4,428,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw.",	XO(4,428,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo",	XO(4,428,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo.",	XO(4,428,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws",	XO(4,492,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws.",	XO(4,492,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso",	XO(4,492,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso.",	XO(4,492,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu",	XO(4,460,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu.",	XO(4,460,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo",	XO(4,460,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo.",	XO(4,460,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu",	XO(4,396,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu.",	XO(4,396,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo",	XO(4,396,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo.",	XO(4,396,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw",	XRC(4,168,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw.",	XRC(4,168,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu",	XRC(4,136,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu.",	XRC(4,136,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw",	XRC(4,40,0),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw.",	XRC(4,40,1),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu",	XRC(4,8,0),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu.",	XRC(4,8,1),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw",	XRC(4,424,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw.",	XRC(4,424,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu",	XRC(4,392,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu.",	XRC(4,392,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw",	XO(4,174,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw.",	XO(4,174,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo",	XO(4,174,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo.",	XO(4,174,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws",	XO(4,238,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws.",	XO(4,238,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso",	XO(4,238,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso.",	XO(4,238,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw",	XO(4,46,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw.",	XO(4,46,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo",	XO(4,46,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo.",	XO(4,46,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws",	XO(4,110,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws.",	XO(4,110,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso",	XO(4,110,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso.",	XO(4,110,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw",	XO(4,430,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw.",	XO(4,430,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo",	XO(4,430,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo.",	XO(4,430,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws",	XO(4,494,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws.",	XO(4,494,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso",	XO(4,494,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso.",	XO(4,494,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mfvscr",  VX(4, 1540), VX_MASK,	PPCVEC,		{ VD } },
+{ "mtvscr",  VX(4, 1604), VX_MASK,	PPCVEC,		{ VD } },
+{ "vaddcuw", VX(4,  384), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddfp",  VX(4,   10), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsbs", VX(4,  768), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddshs", VX(4,  832), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsws", VX(4,  896), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubm", VX(4,    0), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubs", VX(4,  512), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhm", VX(4,   64), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhs", VX(4,  576), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduwm", VX(4,  128), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduws", VX(4,  640), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vand",    VX(4, 1028), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vandc",   VX(4, 1092), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsb",  VX(4, 1282), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsh",  VX(4, 1346), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsw",  VX(4, 1410), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgub",  VX(4, 1026), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguh",  VX(4, 1090), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguw",  VX(4, 1154), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vcfsx",   VX(4,  842), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcfux",   VX(4,  778), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcmpbfp",   VXR(4, 966, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpbfp.",  VXR(4, 966, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp",  VXR(4, 198, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp.", VXR(4, 198, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb",  VXR(4,   6, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb.", VXR(4,   6, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh",  VXR(4,  70, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh.", VXR(4,  70, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw",  VXR(4, 134, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw.", VXR(4, 134, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp",  VXR(4, 454, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp.", VXR(4, 454, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp",  VXR(4, 710, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp.", VXR(4, 710, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb",  VXR(4, 774, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb.", VXR(4, 774, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh",  VXR(4, 838, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh.", VXR(4, 838, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw",  VXR(4, 902, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw.", VXR(4, 902, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub",  VXR(4, 518, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub.", VXR(4, 518, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh",  VXR(4, 582, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh.", VXR(4, 582, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw",  VXR(4, 646, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw.", VXR(4, 646, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vctsxs",    VX(4,  970), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vctuxs",    VX(4,  906), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vexptefp",  VX(4,  394), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vlogefp",   VX(4,  458), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vmaddfp",   VXA(4,  46), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmaxfp",    VX(4, 1034), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsb",    VX(4,  258), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsh",    VX(4,  322), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsw",    VX(4,  386), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxub",    VX(4,    2), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuh",    VX(4,   66), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuw",    VX(4,  130), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmhaddshs", VXA(4,  32), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmhraddshs", VXA(4, 33), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vminfp",    VX(4, 1098), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsb",    VX(4,  770), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsh",    VX(4,  834), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsw",    VX(4,  898), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminub",    VX(4,  514), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuh",    VX(4,  578), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuw",    VX(4,  642), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmladduhm", VXA(4,  34), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmrghb",    VX(4,   12), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrghh",    VX(4,   76), VX_MASK,    PPCVEC,		{ VD, VA, VB } },
+{ "vmrghw",    VX(4,  140), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglb",    VX(4,  268), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglh",    VX(4,  332), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglw",    VX(4,  396), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmsummbm",  VXA(4,  37), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshm",  VXA(4,  40), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshs",  VXA(4,  41), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumubm",  VXA(4,  36), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhm",  VXA(4,  38), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhs",  VXA(4,  39), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmulesb",   VX(4,  776), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulesh",   VX(4,  840), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleub",   VX(4,  520), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleuh",   VX(4,  584), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosb",   VX(4,  264), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosh",   VX(4,  328), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuloub",   VX(4,    8), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulouh",   VX(4,   72), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vnmsubfp",  VXA(4,  47), VXA_MASK,	PPCVEC,		{ VD, VA, VC, VB } },
+{ "vnor",      VX(4, 1284), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vor",       VX(4, 1156), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vperm",     VXA(4,  43), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vpkpx",     VX(4,  782), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshss",   VX(4,  398), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshus",   VX(4,  270), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswss",   VX(4,  462), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswus",   VX(4,  334), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhum",   VX(4,   14), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhus",   VX(4,  142), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwum",   VX(4,   78), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwus",   VX(4,  206), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrefp",     VX(4,  266), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfim",     VX(4,  714), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfin",     VX(4,  522), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfip",     VX(4,  650), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfiz",     VX(4,  586), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrlb",      VX(4,    4), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlh",      VX(4,   68), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlw",      VX(4,  132), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrsqrtefp", VX(4,  330), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vsel",      VXA(4,  42), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vsl",       VX(4,  452), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslb",      VX(4,  260), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsldoi",    VXA(4,  44), VXA_MASK,	PPCVEC,		{ VD, VA, VB, SHB } },
+{ "vslh",      VX(4,  324), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslo",      VX(4, 1036), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslw",      VX(4,  388), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vspltb",    VX(4,  524), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsplth",    VX(4,  588), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vspltisb",  VX(4,  780), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltish",  VX(4,  844), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltisw",  VX(4,  908), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltw",    VX(4,  652), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsr",       VX(4,  708), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrab",     VX(4,  772), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrah",     VX(4,  836), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsraw",     VX(4,  900), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrb",      VX(4,  516), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrh",      VX(4,  580), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsro",      VX(4, 1100), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrw",      VX(4,  644), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubcuw",   VX(4, 1408), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubfp",    VX(4,   74), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsbs",   VX(4, 1792), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubshs",   VX(4, 1856), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsws",   VX(4, 1920), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububm",   VX(4, 1024), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububs",   VX(4, 1536), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhm",   VX(4, 1088), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhs",   VX(4, 1600), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuwm",   VX(4, 1152), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuws",   VX(4, 1664), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsumsws",   VX(4, 1928), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum2sws",  VX(4, 1672), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4sbs",  VX(4, 1800), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4shs",  VX(4, 1608), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4ubs",  VX(4, 1544), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vupkhpx",   VX(4,  846), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsb",   VX(4,  526), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsh",   VX(4,  590), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklpx",   VX(4,  974), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsb",   VX(4,  654), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsh",   VX(4,  718), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vxor",      VX(4, 1220), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+
+{ "mulli",   OP(7),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "muli",    OP(7),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "subfic",  OP(8),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "sfi",     OP(8),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "dozi",    OP(9),	OP_MASK,	M601,		{ RT, RA, SI } },
+
+{ "cmplwi",  OPL(10,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, UI } },
+{ "cmpldi",  OPL(10,1), OPL_MASK,	PPC64,		{ OBF, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PPCONLY,	{ BF, L, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PWRCOM,		{ BF, RA, UI } },
+
+{ "cmpwi",   OPL(11,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, SI } },
+{ "cmpdi",   OPL(11,1),	OPL_MASK,	PPC64,		{ OBF, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PPCONLY,	{ BF, L, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PWRCOM,		{ BF, RA, SI } },
+
+{ "addic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai",	     OP(12),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "addic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai.",     OP(13),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "li",	     OP(14),	DRA_MASK,	PPCCOM,		{ RT, SI } },
+{ "lil",     OP(14),	DRA_MASK,	PWRCOM,		{ RT, SI } },
+{ "addi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "cal",     OP(14),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+{ "subi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+{ "la",	     OP(14),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+
+{ "lis",     OP(15),	DRA_MASK,	PPCCOM,		{ RT, SISIGNOPT } },
+{ "liu",     OP(15),	DRA_MASK,	PWRCOM,		{ RT, SISIGNOPT } },
+{ "addis",   OP(15),	OP_MASK,	PPCCOM,		{ RT,RA,SISIGNOPT } },
+{ "cau",     OP(15),	OP_MASK,	PWRCOM,		{ RT,RA,SISIGNOPT } },
+{ "subis",   OP(15),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "bdnz-",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnz+",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnz",    BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdn",     BBO(16,BODNZ,0,0), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnzl-",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnzl+",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnzl",   BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdnl",    BBO(16,BODNZ,0,1), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnza-",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnza+",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnza",   BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdna",    BBO(16,BODNZ,1,0), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdnzla-", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnzla+", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnzla",  BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdnla",   BBO(16,BODNZ,1,1), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdz-",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdz+",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdz",     BBO(16,BODZ,0,0),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdzl-",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdzl+",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdzl",    BBO(16,BODZ,0,1),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdza-",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdza+",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdza",    BBO(16,BODZ,1,0),  BBOYBI_MASK, COM,	{ BDA } },
+{ "bdzla-",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdzla+",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdzla",   BBO(16,BODZ,1,1),  BBOYBI_MASK, COM,	{ BDA } },
+{ "blt-",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blt+",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blt",     BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bltl-",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bltl+",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bltl",    BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blta-",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blta+",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blta",    BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bltla-",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bltla+",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bltla",   BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgt-",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgt+",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgt",     BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgtl-",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgtl+",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgtl",    BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgta-",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgta+",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgta",    BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgtla-",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgtla+",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgtla",   BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beq-",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beq+",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beq",     BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beql-",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beql+",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beql",    BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beqa-",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqa+",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqa",    BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beqla-",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqla+",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqla",   BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bso-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bso+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bso",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsol-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bsol+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bsol",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsoa-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsoa+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsoa",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bsola-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsola+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsola",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bun-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bun+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bun",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bunl-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bunl+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bunl",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "buna-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "buna+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "buna",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bunla-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bunla+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bunla",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bge-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bge+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bge",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgel-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgel+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgel",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgea-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgea+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgea",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgela-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgela+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgela",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnl-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnl+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnl",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnll-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnll+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnll",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnla-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnla+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnla",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnlla-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnlla+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnlla",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "ble-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "ble+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "ble",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blel-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blel+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blel",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blea-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blea+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blea",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "blela-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blela+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blela",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bng-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bng+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bng",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bngl-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bngl+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bngl",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnga-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnga+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnga",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bngla-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bngla+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bngla",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bne-",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bne+",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bne",     BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnel-",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnel+",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnel",    BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnea-",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnea+",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnea",    BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnela-",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnela+",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnela",   BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bns-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bns+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bns",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsl-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnsl+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnsl",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsa-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsa+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsa",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnsla-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsla+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsla",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnu-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnu+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnu",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnul-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnul+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnul",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnua-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnua+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnua",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bnula-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnula+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnula",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bdnzt-",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzt+",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzt",   BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnztl-", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnztl+", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnztl",  BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzta-", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzta+", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzta",  BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnztla-",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnztla+",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnztla", BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzf-",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzf+",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzf",   BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfl-", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzfl+", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzfl",  BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfa-", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfa+", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfa",  BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzfla-",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfla+",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfla", BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bt-",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bt+",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bt",	     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbt",     BBO(16,BOT,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "btl-",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "btl+",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "btl",     BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbtl",    BBO(16,BOT,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bta-",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bta+",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bta",     BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbta",    BBO(16,BOT,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "btla-",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "btla+",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "btla",    BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbtla",   BBO(16,BOT,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bf-",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bf+",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bf",	     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbf",     BBO(16,BOF,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfl-",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bfl+",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bfl",     BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbfl",    BBO(16,BOF,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfa-",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfa+",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfa",     BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfa",    BBO(16,BOF,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bfla-",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfla+",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfla",    BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfla",   BBO(16,BOF,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bdzt-",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzt+",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzt",    BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdztl-",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdztl+",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdztl",   BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzta-",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzta+",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzta",   BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdztla-", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdztla+", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdztla",  BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzf-",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzf+",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzf",    BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfl-",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzfl+",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzfl",   BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfa-",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfa+",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfa",   BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzfla-", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfla+", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfla",  BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bc-",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bc+",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bc",	     B(16,0,0),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bcl-",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bcl+",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bcl",     B(16,0,1),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bca-",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bca+",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bca",     B(16,1,0),	B_MASK,		COM,		{ BO, BI, BDA } },
+{ "bcla-",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bcla+",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bcla",    B(16,1,1),	B_MASK,		COM,		{ BO, BI, BDA } },
+
+{ "sc",      SC(17,1,0), 0xffffffff,	PPC,		{ 0 } },
+{ "svc",     SC(17,0,0), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svcl",    SC(17,0,1), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svca",    SC(17,1,0), SC_MASK,	PWRCOM,		{ SV } },
+{ "svcla",   SC(17,1,1), SC_MASK,	POWER,		{ SV } },
+
+{ "b",	     B(18,0,0),	B_MASK,		COM,	{ LI } },
+{ "bl",      B(18,0,1),	B_MASK,		COM,	{ LI } },
+{ "ba",      B(18,1,0),	B_MASK,		COM,	{ LIA } },
+{ "bla",     B(18,1,1),	B_MASK,		COM,	{ LIA } },
+
+{ "mcrf",    XL(19,0),	XLBB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "blr",     XLO(19,BOU,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "br",      XLO(19,BOU,16,0), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "blrl",    XLO(19,BOU,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "brl",     XLO(19,BOU,16,1), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "bdnzlr",  XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr-", XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr+", XLO(19,BODNZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl", XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl-",XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl+",XLO(19,BODNZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr",   XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr-",  XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr+",  XLO(19,BODZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl",  XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl-", XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl+", XLO(19,BODZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bltlr",   XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr-",  XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr+",  XLOCB(19,BOTP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltr",    XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bltlrl",  XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl-", XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl+", XLOCB(19,BOTP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltrl",   XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlr",   XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr-",  XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr+",  XLOCB(19,BOTP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtr",    XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlrl",  XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl-", XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl+", XLOCB(19,BOTP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtrl",   XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlr",   XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr-",  XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr+",  XLOCB(19,BOTP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqr",    XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlrl",  XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl-", XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl+", XLOCB(19,BOTP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqrl",   XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsor",    XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsorl",   XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bunlr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bger",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgelrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgerl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlr",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlrl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bler",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blerl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngr",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngrl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelr",   XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr-",  XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr+",  XLOCB(19,BOFP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bner",    XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelrl",  XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl-", XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl+", XLOCB(19,BOFP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnerl",   XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsr",    XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsrl",   XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnulr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "btlr",    XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr-",   XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr+",   XLO(19,BOTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtr",    XLO(19,BOT,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "btlrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl-",  XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl+",  XLO(19,BOTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflr",    XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr-",   XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr+",   XLO(19,BOFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfr",    XLO(19,BOF,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl-",  XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl+",  XLO(19,BOFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bdnztlr", XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr-",XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr+",XLO(19,BODNZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl-",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl+",XLO(19,BODNZTP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdnzflr", XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr-",XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr+",XLO(19,BODNZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl-",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl+",XLO(19,BODNZFP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdztlr",  XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr-", XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr+", XLO(19,BODZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl", XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl-",XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl+",XLO(19,BODZTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr",  XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr-", XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr+", XLO(19,BODZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl", XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl-",XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl+",XLO(19,BODZFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bclr",    XLLK(19,16,0), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclrl",   XLLK(19,16,1), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclr+",   XLYLK(19,16,1,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl+",  XLYLK(19,16,1,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclr-",   XLYLK(19,16,0,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl-",  XLYLK(19,16,0,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bcr",     XLLK(19,16,0), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+{ "bcrl",    XLLK(19,16,1), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+
+{ "rfid",    XL(19,18),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "crnot",   XL(19,33), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "crnor",   XL(19,33),	XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "rfi",     XL(19,50),	0xffffffff,	COM,		{ 0 } },
+{ "rfci",    XL(19,51),	0xffffffff,	PPC403,		{ 0 } },
+
+{ "rfsvc",   XL(19,82),	0xffffffff,	POWER,		{ 0 } },
+
+{ "crandc",  XL(19,129), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "isync",   XL(19,150), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "ics",     XL(19,150), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "crclr",   XL(19,193), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "crxor",   XL(19,193), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crnand",  XL(19,225), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crand",   XL(19,257), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crset",   XL(19,289), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "creqv",   XL(19,289), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crorc",   XL(19,417), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crmove",  XL(19,449), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "cror",    XL(19,449), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "bctr",    XLO(19,BOU,528,0), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bctrl",   XLO(19,BOU,528,1), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bltctr",  XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr-", XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr+", XLOCB(19,BOTP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl", XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl-",XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl+",XLOCB(19,BOTP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr",  XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr-", XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr+", XLOCB(19,BOTP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl", XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl-",XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl+",XLOCB(19,BOTP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr",  XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr-", XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr+", XLOCB(19,BOTP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl", XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl-",XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl+",XLOCB(19,BOTP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr",  XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr-", XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr+", XLOCB(19,BOFP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl", XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl-",XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl+",XLOCB(19,BOFP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "btctr",   XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr-",  XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr+",  XLO(19,BOTP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl",  XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl-", XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl+", XLO(19,BOTP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr",   XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr-",  XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr+",  XLO(19,BOFP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl",  XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl-", XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl+", XLO(19,BOFP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bcctr",   XLLK(19,528,0),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctr-",  XLYLK(19,528,0,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctr+",  XLYLK(19,528,1,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl",  XLLK(19,528,1),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctrl-", XLYLK(19,528,0,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl+", XLYLK(19,528,1,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcc",     XLLK(19,528,0),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+{ "bccl",    XLLK(19,528,1),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+
+{ "rlwimi",  M(20,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi",   M(20,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlwimi.", M(20,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi.",  M(20,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rotlwi",  MME(21,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "clrlwi",  MME(21,31,0), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm",  M(21,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm",   M(21,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rotlwi.", MME(21,31,1), MMBME_MASK,	PPCCOM,		{ RA,RS,SH } },
+{ "clrlwi.", MME(21,31,1), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm.", M(21,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm.",  M(21,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlmi",    M(22,0),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+{ "rlmi.",   M(22,1),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+
+{ "rotlw",   MME(23,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm",   M(23,0),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm",    M(23,0),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rotlw.",  MME(23,31,1), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm.",  M(23,1),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm.",   M(23,1),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+
+{ "nop",     OP(24),	0xffffffff,	PPCCOM,		{ 0 } },
+{ "ori",     OP(24),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oril",    OP(24),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "oris",    OP(25),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oriu",    OP(25),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xori",    OP(26),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoril",   OP(26),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xoris",   OP(27),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoriu",   OP(27),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andi.",   OP(28),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andil.",  OP(28),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andis.",  OP(29),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andiu.",  OP(29),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "rotldi",  MD(30,0,0), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi",  MD(30,0,0), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl",  MD(30,0,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rotldi.", MD(30,0,1), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi.", MD(30,0,1), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl.", MD(30,0,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldicr",  MD(30,1,0), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+{ "rldicr.", MD(30,1,1), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+
+{ "rldic",   MD(30,2,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldic.",  MD(30,2,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldimi",  MD(30,3,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldimi.", MD(30,3,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rotld",   MDS(30,8,0), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl",   MDS(30,8,0), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+{ "rotld.",  MDS(30,8,1), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl.",  MDS(30,8,1), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+
+{ "rldcr",   MDS(30,9,0), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+{ "rldcr.",  MDS(30,9,1), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+
+{ "cmpw",    XCMPL(31,0,0), XCMPL_MASK, PPCCOM,		{ OBF, RA, RB } },
+{ "cmpd",    XCMPL(31,0,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmp",     X(31,0),	XCMP_MASK,	PPCONLY,	{ BF, L, RA, RB } },
+{ "cmp",     X(31,0),	XCMPL_MASK,	PWRCOM,		{ BF, RA, RB } },
+
+{ "twlgt",   XTO(31,4,TOLGT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlgt",    XTO(31,4,TOLGT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twllt",   XTO(31,4,TOLLT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tllt",    XTO(31,4,TOLLT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "tweq",    XTO(31,4,TOEQ), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "teq",     XTO(31,4,TOEQ), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlge",   XTO(31,4,TOLGE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlge",    XTO(31,4,TOLGE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlnl",   XTO(31,4,TOLNL), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlnl",    XTO(31,4,TOLNL), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlle",   XTO(31,4,TOLLE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlle",    XTO(31,4,TOLLE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlng",   XTO(31,4,TOLNG), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlng",    XTO(31,4,TOLNG), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twgt",    XTO(31,4,TOGT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tgt",     XTO(31,4,TOGT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twge",    XTO(31,4,TOGE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tge",     XTO(31,4,TOGE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twnl",    XTO(31,4,TONL), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tnl",     XTO(31,4,TONL), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlt",    XTO(31,4,TOLT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tlt",     XTO(31,4,TOLT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twle",    XTO(31,4,TOLE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tle",     XTO(31,4,TOLE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twng",    XTO(31,4,TONG), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tng",     XTO(31,4,TONG), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twne",    XTO(31,4,TONE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tne",     XTO(31,4,TONE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "trap",    XTO(31,4,TOU), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "tw",      X(31,4),	X_MASK,		PPCCOM,		{ TO, RA, RB } },
+{ "t",       X(31,4),	X_MASK,		PWRCOM,		{ TO, RA, RB } },
+
+{ "subfc",   XO(31,8,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf",      XO(31,8,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc",    XO(31,8,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfc.",  XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf.",     XO(31,8,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc.",   XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RB, RA } },
+{ "subfco",  XO(31,8,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo",     XO(31,8,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco",   XO(31,8,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfco.", XO(31,8,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo.",    XO(31,8,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco.",  XO(31,8,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "mulhdu",  XO(31,9,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulhdu.", XO(31,9,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addc",    XO(31,10,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a",       XO(31,10,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addc.",   XO(31,10,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a.",      XO(31,10,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco",   XO(31,10,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao",      XO(31,10,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco.",  XO(31,10,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao.",     XO(31,10,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mulhwu",  XO(31,11,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhwu.", XO(31,11,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mfcr",    X(31,19),	XRARB_MASK,	COM,		{ RT } },
+
+{ "lwarx",   X(31,20),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "ldx",     X(31,21),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lwzx",    X(31,23),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lx",      X(31,23),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "slw",     XRC(31,24,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl",      XRC(31,24,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "slw.",    XRC(31,24,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl.",     XRC(31,24,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "cntlzw",  XRC(31,26,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz",   XRC(31,26,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "cntlzw.", XRC(31,26,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz.",  XRC(31,26,1), XRB_MASK, 	PWRCOM,		{ RA, RS } },
+
+{ "sld",     XRC(31,27,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "sld.",    XRC(31,27,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "and",     XRC(31,28,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "and.",    XRC(31,28,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "maskg",   XRC(31,29,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskg.",  XRC(31,29,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "cmplw",   XCMPL(31,32,0), XCMPL_MASK, PPCCOM,	{ OBF, RA, RB } },
+{ "cmpld",   XCMPL(31,32,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmpl",    X(31,32),	XCMP_MASK,	 PPCONLY,	{ BF, L, RA, RB } },
+{ "cmpl",    X(31,32),	XCMPL_MASK,	 PWRCOM,	{ BF, RA, RB } },
+
+{ "subf",    XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub",     XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subf.",   XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub.",    XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo",   XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo",    XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo.",  XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo.",   XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "ldux",    X(31,53),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "dcbst",   X(31,54),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lwzux",   X(31,55),	X_MASK,		PPCCOM,		{ RT, RAL, RB } },
+{ "lux",     X(31,55),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "cntlzd",  XRC(31,58,0), XRB_MASK,	PPC64,		{ RA, RS } },
+{ "cntlzd.", XRC(31,58,1), XRB_MASK,	PPC64,		{ RA, RS } },
+
+{ "andc",    XRC(31,60,0), X_MASK,	COM,	{ RA, RS, RB } },
+{ "andc.",   XRC(31,60,1), X_MASK,	COM,	{ RA, RS, RB } },
+
+{ "tdlgt",   XTO(31,68,TOLGT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdllt",   XTO(31,68,TOLLT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdeq",    XTO(31,68,TOEQ), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlge",   XTO(31,68,TOLGE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlnl",   XTO(31,68,TOLNL), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlle",   XTO(31,68,TOLLE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlng",   XTO(31,68,TOLNG), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdgt",    XTO(31,68,TOGT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdge",    XTO(31,68,TOGE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdnl",    XTO(31,68,TONL), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlt",    XTO(31,68,TOLT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdle",    XTO(31,68,TOLE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdng",    XTO(31,68,TONG), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdne",    XTO(31,68,TONE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "td",	     X(31,68),	X_MASK,		 PPC64,		{ TO, RA, RB } },
+
+{ "mulhd",   XO(31,73,0,0), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+{ "mulhd.",  XO(31,73,0,1), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+
+{ "mulhw",   XO(31,75,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhw.",  XO(31,75,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtsrd",   X(31,82),	XRB_MASK|(1<<20), PPC64,	{ SR, RS } },
+
+{ "mfmsr",   X(31,83),	XRARB_MASK,	COM,		{ RT } },
+
+{ "ldarx",   X(31,84),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "dcbf",    X(31,86),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lbzx",    X(31,87),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "neg",     XO(31,104,0,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "neg.",    XO(31,104,0,1), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego",    XO(31,104,1,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego.",   XO(31,104,1,1), XORB_MASK,	COM,		{ RT, RA } },
+
+{ "mul",     XO(31,107,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mul.",    XO(31,107,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo",    XO(31,107,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo.",   XO(31,107,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mtsrdin", X(31,114),	XRA_MASK,	PPC64,		{ RS, RB } },
+
+{ "clf",     X(31,118), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "lbzux",   X(31,119),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "not",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "not.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "wrtee",   X(31,131),	XRARB_MASK,	PPC403,		{ RS } },
+
+{ "subfe",   XO(31,136,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe",     XO(31,136,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfe.",  XO(31,136,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe.",    XO(31,136,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo",  XO(31,136,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo",    XO(31,136,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo.", XO(31,136,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo.",   XO(31,136,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "adde",    XO(31,138,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae",      XO(31,138,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "adde.",   XO(31,138,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae.",     XO(31,138,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo",   XO(31,138,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo",     XO(31,138,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo.",  XO(31,138,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo.",    XO(31,138,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtcr",    XFXM(31,144,0xff), XFXFXM_MASK|FXM_MASK, COM,	{ RS }},
+{ "mtcrf",   X(31,144),	XFXFXM_MASK,	COM,		{ FXM, RS } },
+
+{ "mtmsr",   X(31,146),	XRARB_MASK,	COM,		{ RS } },
+
+{ "stdx",    X(31,149), X_MASK,		PPC64,		{ RS, RA, RB } },
+
+{ "stwcx.",  XRC(31,150,1), X_MASK,	PPC,		{ RS, RA, RB } },
+
+{ "stwx",    X(31,151), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stx",     X(31,151), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "slq",     XRC(31,152,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "slq.",    XRC(31,152,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sle",     XRC(31,153,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sle.",    XRC(31,153,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "wrteei",  X(31,163),	XE_MASK,	PPC403,		{ E } },
+
+{ "mtmsrd",  X(31,178),	XRARB_MASK,	PPC64,		{ RS } },
+
+{ "stdux",   X(31,181),	X_MASK,		PPC64,		{ RS, RAS, RB } },
+
+{ "stwux",   X(31,183),	X_MASK,		PPCCOM,		{ RS, RAS, RB } },
+{ "stux",    X(31,183),	X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "sliq",    XRC(31,184,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sliq.",   XRC(31,184,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "subfze",  XO(31,200,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze",    XO(31,200,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfze.", XO(31,200,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze.",   XO(31,200,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo", XO(31,200,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo",   XO(31,200,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo.",XO(31,200,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo.",  XO(31,200,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "addze",   XO(31,202,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze",     XO(31,202,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addze.",  XO(31,202,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze.",    XO(31,202,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo",  XO(31,202,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo",    XO(31,202,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo.", XO(31,202,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo.",   XO(31,202,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mtsr",    X(31,210),	XRB_MASK|(1<<20), COM32,	{ SR, RS } },
+
+{ "stdcx.",  XRC(31,214,1), X_MASK,	PPC64,		{ RS, RA, RB } },
+
+{ "stbx",    X(31,215),	X_MASK,		COM,	{ RS, RA, RB } },
+
+{ "sllq",    XRC(31,216,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sllq.",   XRC(31,216,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sleq",    XRC(31,217,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sleq.",   XRC(31,217,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "subfme",  XO(31,232,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme",    XO(31,232,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfme.", XO(31,232,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme.",   XO(31,232,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo", XO(31,232,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo",   XO(31,232,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo.",XO(31,232,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo.",  XO(31,232,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mulld",   XO(31,233,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulld.",  XO(31,233,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo",  XO(31,233,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo.", XO(31,233,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addme",   XO(31,234,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame",     XO(31,234,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addme.",  XO(31,234,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame.",    XO(31,234,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo",  XO(31,234,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo",    XO(31,234,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo.", XO(31,234,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo.",   XO(31,234,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mullw",   XO(31,235,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls",    XO(31,235,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullw.",  XO(31,235,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls.",   XO(31,235,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo",  XO(31,235,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso",   XO(31,235,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo.", XO(31,235,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso.",  XO(31,235,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtsrin",  X(31,242),	XRA_MASK,	PPC32,		{ RS, RB } },
+{ "mtsri",   X(31,242),	XRA_MASK,	POWER32,	{ RS, RB } },
+
+{ "dcbtst",  X(31,246),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stbux",   X(31,247),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "slliq",   XRC(31,248,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "slliq.",  XRC(31,248,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "doz",     XO(31,264,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "doz.",    XO(31,264,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo",    XO(31,264,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo.",   XO(31,264,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "add",     XO(31,266,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax",     XO(31,266,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "add.",    XO(31,266,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax.",    XO(31,266,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo",    XO(31,266,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo",    XO(31,266,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo.",   XO(31,266,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo.",   XO(31,266,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "lscbx",   XRC(31,277,0), X_MASK,	M601,		{ RT, RA, RB } },
+{ "lscbx.",  XRC(31,277,1), X_MASK,	M601,		{ RT, RA, RB } },
+
+{ "dcbt",    X(31,278),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lhzx",    X(31,279),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "icbt",    X(31,262),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "eqv",     XRC(31,284,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "eqv.",    XRC(31,284,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "tlbie",   X(31,306),	XRTRA_MASK,	PPC,		{ RB } },
+{ "tlbi",    X(31,306),	XRT_MASK,	POWER,		{ RA, RB } },
+
+{ "eciwx",   X(31,310), X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "lhzux",   X(31,311),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "xor",     XRC(31,316,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "xor.",    XRC(31,316,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mfexisr", XSPR(31,323,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mfexier", XSPR(31,323,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr0",   XSPR(31,323,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr1",   XSPR(31,323,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr2",   XSPR(31,323,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr3",   XSPR(31,323,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr4",   XSPR(31,323,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr5",   XSPR(31,323,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr6",   XSPR(31,323,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr7",   XSPR(31,323,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbear",  XSPR(31,323,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbesr",  XSPR(31,323,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiocr",  XSPR(31,323,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr0", XSPR(31,323,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact0", XSPR(31,323,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada0", XSPR(31,323,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa0", XSPR(31,323,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc0", XSPR(31,323,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr1", XSPR(31,323,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact1", XSPR(31,323,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada1", XSPR(31,323,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa1", XSPR(31,323,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc1", XSPR(31,323,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr2", XSPR(31,323,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact2", XSPR(31,323,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada2", XSPR(31,323,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa2", XSPR(31,323,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc2", XSPR(31,323,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr3", XSPR(31,323,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact3", XSPR(31,323,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada3", XSPR(31,323,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa3", XSPR(31,323,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc3", XSPR(31,323,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasr", XSPR(31,323,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdcr",   X(31,323),	X_MASK,		PPC403,		{ RT, SPR } },
+
+{ "div",     XO(31,331,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "div.",    XO(31,331,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo",    XO(31,331,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo.",   XO(31,331,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mfmq",     XSPR(31,339,0),   XSPR_MASK, M601,	{ RT } },
+{ "mfxer",    XSPR(31,339,1),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcu",   XSPR(31,339,4),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcl",   XSPR(31,339,5),   XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,6),   XSPR_MASK, MFDEC1,	{ RT } },
+{ "mflr",     XSPR(31,339,8),   XSPR_MASK, COM,		{ RT } },
+{ "mfctr",    XSPR(31,339,9),   XSPR_MASK, COM,		{ RT } },
+{ "mftid",    XSPR(31,339,17),  XSPR_MASK, POWER,	{ RT } },
+{ "mfdsisr",  XSPR(31,339,18),  XSPR_MASK, COM,		{ RT } },
+{ "mfdar",    XSPR(31,339,19),  XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,22),  XSPR_MASK, MFDEC2,	{ RT } },
+{ "mfsdr0",   XSPR(31,339,24),  XSPR_MASK, POWER,	{ RT } },
+{ "mfsdr1",   XSPR(31,339,25),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr0",   XSPR(31,339,26),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr1",   XSPR(31,339,27),  XSPR_MASK, COM,		{ RT } },
+{ "mfcmpa",   XSPR(31,339,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpb",   XSPR(31,339,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpc",   XSPR(31,339,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpd",   XSPR(31,339,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mficr",    XSPR(31,339,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mfder",    XSPR(31,339,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcounta", XSPR(31,339,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcountb", XSPR(31,339,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpe",   XSPR(31,339,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpf",   XSPR(31,339,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpg",   XSPR(31,339,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmph",   XSPR(31,339,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl1", XSPR(31,339,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl2", XSPR(31,339,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mfictrl",  XSPR(31,339,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mfbar",    XSPR(31,339,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mfsprg4",  XSPR(31,339,260), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg5",  XSPR(31,339,261), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg6",  XSPR(31,339,262), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg7",  XSPR(31,339,263), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg",   XSPR(31,339,272), XSPRG_MASK, PPC,	{ RT, SPRG } },
+{ "mfsprg0",  XSPR(31,339,272), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg1",  XSPR(31,339,273), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg2",  XSPR(31,339,274), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg3",  XSPR(31,339,275), XSPR_MASK, PPC,		{ RT } },
+{ "mfasr",    XSPR(31,339,280), XSPR_MASK, PPC64,	{ RT } },
+{ "mfear",    XSPR(31,339,282), XSPR_MASK, PPC,		{ RT } },
+{ "mfpvr",    XSPR(31,339,287), XSPR_MASK, PPC,		{ RT } },
+{ "mfibatu",  XSPR(31,339,528), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfibatl",  XSPR(31,339,529), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatu",  XSPR(31,339,536), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatl",  XSPR(31,339,537), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfic_cst", XSPR(31,339,560), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_adr", XSPR(31,339,561), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_dat", XSPR(31,339,562), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_cst", XSPR(31,339,568), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_adr", XSPR(31,339,569), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_dat", XSPR(31,339,570), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpdr",   XSPR(31,339,630), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpir",   XSPR(31,339,631), XSPR_MASK, PPC860,	{ RT } },
+{ "mfimmr",   XSPR(31,339,638), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ctr", XSPR(31,339,784), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ap",  XSPR(31,339,786), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_epn", XSPR(31,339,787), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_twc", XSPR(31,339,789), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_rpn", XSPR(31,339,790), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ctr", XSPR(31,339,792), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_casid",XSPR(31,339,793), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ap",  XSPR(31,339,794), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_epn", XSPR(31,339,795), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twb", XSPR(31,339,796), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twc", XSPR(31,339,797), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_rpn", XSPR(31,339,798), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_tw",   XSPR(31,339,799), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbcam",XSPR(31,339,816), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram0",XSPR(31,339,817), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram1",XSPR(31,339,818), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbcam", XSPR(31,339,824), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram0",XSPR(31,339,825), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram1",XSPR(31,339,826), XSPR_MASK, PPC860,	{ RT } },
+{ "mfzpr",   	XSPR(31,339,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpid",   	XSPR(31,339,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mfccr0",  	XSPR(31,339,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mficdbdr",	XSPR(31,339,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mfummcr0",	XSPR(31,339,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc1",	XSPR(31,339,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc2",	XSPR(31,339,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfusia",	XSPR(31,339,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfummcr1",	XSPR(31,339,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc3",	XSPR(31,339,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc4",	XSPR(31,339,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfiac3",     XSPR(31,339,948),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac4",     XSPR(31,339,949),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc1",     XSPR(31,339,950),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc2",     XSPR(31,339,951),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr0",	XSPR(31,339,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfpmc1",	XSPR(31,339,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsgr",	XSPR(31,339,953),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfpmc2",	XSPR(31,339,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdcwr", 	XSPR(31,339,954),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfsia",	XSPR(31,339,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsler",	XSPR(31,339,955),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr1",	XSPR(31,339,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsu0r",	XSPR(31,339,956),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc3",	XSPR(31,339,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdbcr1", 	XSPR(31,339,957),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc4",	XSPR(31,339,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfesr",   XSPR(31,339,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdear",  XSPR(31,339,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mfevpr",  XSPR(31,339,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mfcdbcr", XSPR(31,339,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mftsr",   XSPR(31,339,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mftcr",   XSPR(31,339,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpit",   XSPR(31,339,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mftbhi",  XSPR(31,339,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mftblo",  XSPR(31,339,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr2",  XSPR(31,339,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr3",  XSPR(31,339,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbsr",  XSPR(31,339,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbcr0", XSPR(31,339,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac1",  XSPR(31,339,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiac2",  XSPR(31,339,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac1",  XSPR(31,339,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac2",  XSPR(31,339,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdccr",  XSPR(31,339,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mficcr",  XSPR(31,339,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl1",  XSPR(31,339,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu1",  XSPR(31,339,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl2",  XSPR(31,339,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu2",  XSPR(31,339,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mfl2cr",	XSPR(31,339,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mfictc",	XSPR(31,339,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm1",	XSPR(31,339,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm2",	XSPR(31,339,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm3",	XSPR(31,339,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mfspr",   X(31,339),	X_MASK,		COM,		{ RT, SPR } },
+
+{ "lwax",    X(31,341),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lhax",    X(31,343),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "dccci",   X(31,454),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "abs",     XO(31,360,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abs.",    XO(31,360,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "abso",    XO(31,360,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abso.",   XO(31,360,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divs",    XO(31,363,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divs.",   XO(31,363,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso",   XO(31,363,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso.",  XO(31,363,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "tlbia",   X(31,370),	0xffffffff,	PPC,		{ 0 } },
+
+{ "mftbl",   XSPR(31,371,268), XSPR_MASK, PPC,		{ RT } },
+{ "mftbu",   XSPR(31,371,269), XSPR_MASK, PPC,		{ RT } },
+{ "mftb",    X(31,371),	X_MASK,		PPC,		{ RT, TBR } },
+
+{ "lwaux",   X(31,373),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "lhaux",   X(31,375),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "sthx",    X(31,407),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "lfqx",    X(31,791),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "lfqux",   X(31,823),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "stfqx",   X(31,919),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "stfqux",  X(31,951),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "orc",     XRC(31,412,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "orc.",    XRC(31,412,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "sradi",   XS(31,413,0), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "sradi.",  XS(31,413,1), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+
+{ "slbie",   X(31,434),	XRTRA_MASK,	PPC64,		{ RB } },
+
+{ "ecowx",   X(31,438),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "sthux",   X(31,439),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "mr",	     XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or",      XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "mr.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mtexisr", XSPR(31,451,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mtexier", XSPR(31,451,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr0",   XSPR(31,451,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr1",   XSPR(31,451,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr2",   XSPR(31,451,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr3",   XSPR(31,451,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr4",   XSPR(31,451,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr5",   XSPR(31,451,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr6",   XSPR(31,451,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr7",   XSPR(31,451,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbear",  XSPR(31,451,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbesr",  XSPR(31,451,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiocr",  XSPR(31,451,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr0", XSPR(31,451,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact0", XSPR(31,451,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada0", XSPR(31,451,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa0", XSPR(31,451,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc0", XSPR(31,451,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr1", XSPR(31,451,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact1", XSPR(31,451,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada1", XSPR(31,451,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa1", XSPR(31,451,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc1", XSPR(31,451,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr2", XSPR(31,451,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact2", XSPR(31,451,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada2", XSPR(31,451,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa2", XSPR(31,451,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc2", XSPR(31,451,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr3", XSPR(31,451,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact3", XSPR(31,451,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada3", XSPR(31,451,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa3", XSPR(31,451,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc3", XSPR(31,451,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasr", XSPR(31,451,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mtummcr0",	XSPR(31,451,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc1",	XSPR(31,451,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc2",	XSPR(31,451,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtusia",	XSPR(31,451,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtummcr1",	XSPR(31,451,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc3",	XSPR(31,451,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc4",	XSPR(31,451,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr0",	XSPR(31,451,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc1",	XSPR(31,451,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc2",	XSPR(31,451,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtsia",	XSPR(31,451,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr1",	XSPR(31,451,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc3",	XSPR(31,451,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc4",	XSPR(31,451,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtl2cr",	XSPR(31,451,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mtictc",	XSPR(31,451,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm1",	XSPR(31,451,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm2",	XSPR(31,451,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm3",	XSPR(31,451,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mtdcr",   X(31,451),	X_MASK,		PPC403,		{ SPR, RS } },
+
+{ "divdu",   XO(31,457,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdu.",  XO(31,457,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo",  XO(31,457,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo.", XO(31,457,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divwu",   XO(31,459,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwu.",  XO(31,459,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo",  XO(31,459,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo.", XO(31,459,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtmq",    XSPR(31,467,0),   XSPR_MASK,    M601,	{ RS } },
+{ "mtxer",   XSPR(31,467,1),   XSPR_MASK,    COM,	{ RS } },
+{ "mtlr",    XSPR(31,467,8),   XSPR_MASK,    COM,	{ RS } },
+{ "mtctr",   XSPR(31,467,9),   XSPR_MASK,    COM,	{ RS } },
+{ "mttid",   XSPR(31,467,17),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtdsisr", XSPR(31,467,18),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdar",   XSPR(31,467,19),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcu",  XSPR(31,467,20),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcl",  XSPR(31,467,21),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdec",   XSPR(31,467,22),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsdr0",  XSPR(31,467,24),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtsdr1",  XSPR(31,467,25),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr0",  XSPR(31,467,26),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr1",  XSPR(31,467,27),  XSPR_MASK,    COM,	{ RS } },
+{ "mtcmpa",   XSPR(31,467,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpb",   XSPR(31,467,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpc",   XSPR(31,467,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpd",   XSPR(31,467,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mticr",    XSPR(31,467,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mtder",    XSPR(31,467,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcounta", XSPR(31,467,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcountb", XSPR(31,467,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpe",   XSPR(31,467,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpf",   XSPR(31,467,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpg",   XSPR(31,467,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmph",   XSPR(31,467,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl1", XSPR(31,467,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl2", XSPR(31,467,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mtictrl",  XSPR(31,467,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mtbar",    XSPR(31,467,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mtsprg",  XSPR(31,467,272), XSPRG_MASK,   PPC,	{ SPRG, RS } },
+{ "mtsprg0", XSPR(31,467,272), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg1", XSPR(31,467,273), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg2", XSPR(31,467,274), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg3", XSPR(31,467,275), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg4", XSPR(31,467,276), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg5", XSPR(31,467,277), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg6", XSPR(31,467,278), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg7", XSPR(31,467,279), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtasr",   XSPR(31,467,280), XSPR_MASK,    PPC64,	{ RS } },
+{ "mtear",   XSPR(31,467,282), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbl",   XSPR(31,467,284), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbu",   XSPR(31,467,285), XSPR_MASK,    PPC,	{ RS } },
+{ "mtibatu", XSPR(31,467,528), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtibatl", XSPR(31,467,529), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatu", XSPR(31,467,536), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatl", XSPR(31,467,537), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtzpr",   XSPR(31,467,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpid",   XSPR(31,467,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mtccr0",  XSPR(31,467,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac3",  XSPR(31,467,948), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac4",  XSPR(31,467,949), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc1",  XSPR(31,467,950), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc2",  XSPR(31,467,951), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsgr",   XSPR(31,467,953), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdcwr",  XSPR(31,467,954), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsler",  XSPR(31,467,955), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsu0r",  XSPR(31,467,956), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdbcr1", XSPR(31,467,957), XSPR_MASK, PPC405,	{ RT } },
+{ "mticdbdr",XSPR(31,467,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mtesr",   XSPR(31,467,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdear",  XSPR(31,467,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mtevpr",  XSPR(31,467,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mtcdbcr", XSPR(31,467,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mttsr",   XSPR(31,467,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mttcr",   XSPR(31,467,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpit",   XSPR(31,467,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mttbhi",  XSPR(31,467,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mttblo",  XSPR(31,467,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr2",  XSPR(31,467,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr3",  XSPR(31,467,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbsr",  XSPR(31,467,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbcr0", XSPR(31,467,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac1",  XSPR(31,467,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiac2",  XSPR(31,467,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac1",  XSPR(31,467,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac2",  XSPR(31,467,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdccr",  XSPR(31,467,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mticcr",  XSPR(31,467,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl1",  XSPR(31,467,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu1",  XSPR(31,467,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl2",  XSPR(31,467,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu2",  XSPR(31,467,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mtspr",   X(31,467),	       X_MASK,	     COM,	{ SPR, RS } },
+
+{ "dcbi",    X(31,470),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "nand",    XRC(31,476,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "nand.",   XRC(31,476,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "dcread",  X(31,486),	X_MASK,		PPC403,		{ RT, RA, RB }},
+
+{ "nabs",    XO(31,488,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabs.",   XO(31,488,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso",   XO(31,488,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso.",  XO(31,488,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divd",    XO(31,489,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divd.",   XO(31,489,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo",   XO(31,489,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo.",  XO(31,489,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divw",    XO(31,491,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divw.",   XO(31,491,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo",   XO(31,491,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo.",  XO(31,491,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "slbia",   X(31,498),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "cli",     X(31,502), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "mcrxr",   X(31,512),	XRARB_MASK|(3<<21), COM,	{ BF } },
+
+{ "clcs",    X(31,531), XRB_MASK,	M601,		{ RT, RA } },
+
+{ "lswx",    X(31,533),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lsx",     X(31,533),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lwbrx",   X(31,534),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lbrx",    X(31,534),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lfsx",    X(31,535),	X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "srw",     XRC(31,536,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr",      XRC(31,536,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "srw.",    XRC(31,536,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr.",     XRC(31,536,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "rrib",    XRC(31,537,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "rrib.",   XRC(31,537,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srd",     XRC(31,539,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srd.",    XRC(31,539,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "maskir",  XRC(31,541,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskir.", XRC(31,541,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "tlbsync", X(31,566),	0xffffffff,	PPC,		{ 0 } },
+
+{ "lfsux",   X(31,567),	X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsr",    X(31,595),	XRB_MASK|(1<<20), COM32,	{ RT, SR } },
+
+{ "lswi",    X(31,597),	X_MASK,		PPCCOM,		{ RT, RA, NB } },
+{ "lsi",     X(31,597),	X_MASK,		PWRCOM,		{ RT, RA, NB } },
+
+{ "sync",    X(31,598), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "dcs",     X(31,598), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "lfdx",    X(31,599), X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "mfsri",   X(31,627), X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "dclst",   X(31,630), XRB_MASK,	PWRCOM,		{ RS, RA } },
+
+{ "lfdux",   X(31,631), X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsrin",  X(31,659), XRA_MASK,	PPC32,		{ RT, RB } },
+
+{ "stswx",   X(31,661), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stsx",    X(31,661), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stwbrx",  X(31,662), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stbrx",   X(31,662), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stfsx",   X(31,663), X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srq",     XRC(31,664,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srq.",    XRC(31,664,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sre",     XRC(31,665,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sre.",    XRC(31,665,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "stfsux",  X(31,695),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "sriq",    XRC(31,696,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sriq.",   XRC(31,696,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "stswi",   X(31,725),	X_MASK,		PPCCOM,		{ RS, RA, NB } },
+{ "stsi",    X(31,725),	X_MASK,		PWRCOM,		{ RS, RA, NB } },
+
+{ "stfdx",   X(31,727),	X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srlq",    XRC(31,728,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srlq.",   XRC(31,728,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sreq",    XRC(31,729,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sreq.",   XRC(31,729,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "dcba",    X(31,758),	XRT_MASK,	PPC405,		{ RA, RB } },
+
+{ "stfdux",  X(31,759),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "srliq",   XRC(31,760,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "srliq.",  XRC(31,760,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "lhbrx",   X(31,790),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "sraw",    XRC(31,792,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra",     XRC(31,792,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "sraw.",   XRC(31,792,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra.",    XRC(31,792,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "srad",    XRC(31,794,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srad.",   XRC(31,794,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "rac",     X(31,818),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "srawi",   XRC(31,824,0), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai",    XRC(31,824,0), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+{ "srawi.",  XRC(31,824,1), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai.",   XRC(31,824,1), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+
+{ "eieio",   X(31,854),	0xffffffff,	PPC,		{ 0 } },
+
+{ "tlbsx",   XRC(31,914,0), X_MASK, PPC403,	{ RT, RA, RB } },
+{ "tlbsx.",  XRC(31,914,1), X_MASK, PPC403,	{ RT, RA, RB } },
+
+{ "sthbrx",  X(31,918),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "sraq",    XRC(31,920,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sraq.",   XRC(31,920,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srea",    XRC(31,921,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srea.",   XRC(31,921,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "extsh",   XRC(31,922,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts",    XRC(31,922,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "extsh.",  XRC(31,922,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts.",   XRC(31,922,1), XRB_MASK,	PWRCOM,		{ RA, RS } },
+
+{ "tlbrehi", XTLB(31,946,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbrelo", XTLB(31,946,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbre",   X(31,946),	X_MASK,		PPC403,		{ RT, RA, SH } },
+
+{ "sraiq",   XRC(31,952,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sraiq.",  XRC(31,952,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "extsb",   XRC(31,954,0), XRB_MASK,	PPC,		{ RA, RS} },
+{ "extsb.",  XRC(31,954,1), XRB_MASK,	PPC,		{ RA, RS} },
+
+{ "iccci",   X(31,966),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbld",   X(31,978),	XRTRA_MASK,	PPC,		{ RB } },
+
+{ "tlbwehi", XTLB(31,978,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwelo", XTLB(31,978,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwe",   X(31,978),	X_MASK,		PPC403,		{ RS, RA, SH } },
+
+{ "icbi",    X(31,982),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stfiwx",  X(31,983),	X_MASK,		PPC,		{ FRS, RA, RB } },
+
+{ "extsw",   XRC(31,986,0), XRB_MASK,	PPC,		{ RA, RS } },
+{ "extsw.",  XRC(31,986,1), XRB_MASK,	PPC,		{ RA, RS } },
+
+{ "icread",  X(31,998),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbli",   X(31,1010), XRTRA_MASK,	PPC,		{ RB } },
+
+{ "dcbz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+{ "dclz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lvebx",   X(31,   7), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvehx",   X(31,  39), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvewx",   X(31,  71), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsl",    X(31,   6), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsr",    X(31,  38), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvx",     X(31, 103), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvxl",    X(31, 359), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "stvebx",  X(31, 135), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvehx",  X(31, 167), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvewx",  X(31, 199), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvx",    X(31, 231), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvxl",   X(31, 487), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+
+{ "lwz",     OP(32),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+{ "l",	     OP(32),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lwzu",    OP(33),	OP_MASK,	PPCCOM,		{ RT, D, RAL } },
+{ "lu",      OP(33),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lbz",     OP(34),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lbzu",    OP(35),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "stw",     OP(36),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "st",      OP(36),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stwu",    OP(37),	OP_MASK,	PPCCOM,		{ RS, D, RAS } },
+{ "stu",     OP(37),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stb",     OP(38),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "stbu",    OP(39),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lhz",     OP(40),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhzu",    OP(41),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "lha",     OP(42),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhau",    OP(43),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "sth",     OP(44),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "sthu",    OP(45),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lmw",     OP(46),	OP_MASK,	PPCCOM,		{ RT, D, RAM } },
+{ "lm",      OP(46),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "stmw",    OP(47),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "stm",     OP(47),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "lfs",     OP(48),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfsu",    OP(49),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "lfd",     OP(50),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfdu",    OP(51),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "stfs",    OP(52),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfsu",   OP(53),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "stfd",    OP(54),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfdu",   OP(55),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "lfq",     OP(56),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "lfqu",    OP(57),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "ld",      DSO(58,0),	DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "ldu",     DSO(58,1), DS_MASK,	PPC64,		{ RT, DS, RAL } },
+
+{ "lwa",     DSO(58,2), DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "fdivs",   A(59,18,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fdivs.",  A(59,18,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsubs",   A(59,20,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fsubs.",  A(59,20,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fadds",   A(59,21,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fadds.",  A(59,21,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsqrts",  A(59,22,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fsqrts.", A(59,22,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fres",    A(59,24,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fres.",   A(59,24,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmuls",   A(59,25,0), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+{ "fmuls.",  A(59,25,1), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+
+{ "fmsubs",  A(59,28,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmsubs.", A(59,28,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadds",  A(59,29,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmadds.", A(59,29,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsubs", A(59,30,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsubs.",A(59,30,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadds", A(59,31,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadds.",A(59,31,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "stfq",    OP(60),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "stfqu",   OP(61),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "std",     DSO(62,0),	DS_MASK,	PPC64,		{ RS, DS, RA } },
+
+{ "stdu",    DSO(62,1),	DS_MASK,	PPC64,		{ RS, DS, RAS } },
+
+{ "fcmpu",   X(63,0),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "frsp",    XRC(63,12,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "frsp.",   XRC(63,12,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fctiw",   XRC(63,14,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir",    XRC(63,14,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiw.",  XRC(63,14,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir.",   XRC(63,14,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fctiwz",  XRC(63,15,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz",   XRC(63,15,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiwz.", XRC(63,15,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz.",  XRC(63,15,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fdiv",    A(63,18,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd",      A(63,18,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fdiv.",   A(63,18,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd.",     A(63,18,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsub",    A(63,20,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs",      A(63,20,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fsub.",   A(63,20,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs.",     A(63,20,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fadd",    A(63,21,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa",      A(63,21,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fadd.",   A(63,21,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa.",     A(63,21,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsqrt",   A(63,22,0), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+{ "fsqrt.",  A(63,22,1), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+
+{ "fsel",    A(63,23,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fsel.",   A(63,23,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmul",    A(63,25,0), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm",      A(63,25,0), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+{ "fmul.",   A(63,25,1), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm.",     A(63,25,1), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+
+{ "frsqrte", A(63,26,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "frsqrte.",A(63,26,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmsub",   A(63,28,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms",     A(63,28,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmsub.",  A(63,28,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms.",    A(63,28,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadd",   A(63,29,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma",     A(63,29,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmadd.",  A(63,29,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma.",    A(63,29,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsub",  A(63,30,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms",    A(63,30,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsub.", A(63,30,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms.",   A(63,30,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadd",  A(63,31,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma",    A(63,31,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadd.", A(63,31,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma.",   A(63,31,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fcmpo",   X(63,32),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "mtfsb1",  XRC(63,38,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb1.", XRC(63,38,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fneg",    XRC(63,40,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fneg.",   XRC(63,40,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mcrfs",   X(63,64),	XRB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "mtfsb0",  XRC(63,70,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb0.", XRC(63,70,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fmr",     XRC(63,72,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fmr.",    XRC(63,72,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mtfsfi",  XRC(63,134,0), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+{ "mtfsfi.", XRC(63,134,1), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+
+{ "fnabs",   XRC(63,136,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fnabs.",  XRC(63,136,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fabs",    XRC(63,264,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fabs.",   XRC(63,264,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mffs",    XRC(63,583,0), XRARB_MASK,	COM,		{ FRT } },
+{ "mffs.",   XRC(63,583,1), XRARB_MASK,	COM,		{ FRT } },
+
+{ "mtfsf",   XFL(63,711,0), XFL_MASK,	COM,		{ FLM, FRB } },
+{ "mtfsf.",  XFL(63,711,1), XFL_MASK,	COM,		{ FLM, FRB } },
+
+{ "fctid",   XRC(63,814,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctid.",  XRC(63,814,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fctidz",  XRC(63,815,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctidz.", XRC(63,815,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fcfid",   XRC(63,846,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fcfid.",  XRC(63,846,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+};
+
+const int powerpc_num_opcodes =
+  sizeof (powerpc_opcodes) / sizeof (powerpc_opcodes[0]);
+
+/* The macro table.  This is only used by the assembler.  */
+
+/* The expressions of the form (-x ! 31) & (x | 31) have the value 0
+   when x=0; 32-x when x is between 1 and 31; are negative if x is
+   negative; and are 32 or more otherwise.  This is what you want
+   when, for instance, you are emulating a right shift by a
+   rotate-left-and-mask, because the underlying instructions support
+   shifts of size 0 but not shifts of size 32.  By comparison, when
+   extracting x bits from some word you want to use just 32-x, because
+   the underlying instructions don't support extracting 0 bits but do
+   support extracting the whole word (32 bits in this case).  */
+
+const struct powerpc_macro powerpc_macros[] = {
+{ "extldi",  4,   PPC64,	"rldicr %0,%1,%3,(%2)-1" },
+{ "extldi.", 4,   PPC64,	"rldicr. %0,%1,%3,(%2)-1" },
+{ "extrdi",  4,   PPC64,	"rldicl %0,%1,(%2)+(%3),64-(%2)" },
+{ "extrdi.", 4,   PPC64,	"rldicl. %0,%1,(%2)+(%3),64-(%2)" },
+{ "insrdi",  4,   PPC64,	"rldimi %0,%1,64-((%2)+(%3)),%3" },
+{ "insrdi.", 4,   PPC64,	"rldimi. %0,%1,64-((%2)+(%3)),%3" },
+{ "rotrdi",  3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "rotrdi.", 3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "sldi",    3,   PPC64,	"rldicr %0,%1,%2,63-(%2)" },
+{ "sldi.",   3,   PPC64,	"rldicr. %0,%1,%2,63-(%2)" },
+{ "srdi",    3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "srdi.",   3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "clrrdi",  3,   PPC64,	"rldicr %0,%1,0,63-(%2)" },
+{ "clrrdi.", 3,   PPC64,	"rldicr. %0,%1,0,63-(%2)" },
+{ "clrlsldi",4,   PPC64,	"rldic %0,%1,%3,(%2)-(%3)" },
+{ "clrlsldi.",4,  PPC64,	"rldic. %0,%1,%3,(%2)-(%3)" },
+
+{ "extlwi",  4,   PPCCOM,	"rlwinm %0,%1,%3,0,(%2)-1" },
+{ "extlwi.", 4,   PPCCOM,	"rlwinm. %0,%1,%3,0,(%2)-1" },
+{ "extrwi",  4,   PPCCOM,	"rlwinm %0,%1,(%2)+(%3),32-(%2),31" },
+{ "extrwi.", 4,   PPCCOM,	"rlwinm. %0,%1,(%2)+(%3),32-(%2),31" },
+{ "inslwi",  4,   PPCCOM,	"rlwimi %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1" },
+{ "inslwi.", 4,   PPCCOM,	"rlwimi. %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1"},
+{ "insrwi",  4,   PPCCOM,	"rlwimi %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1" },
+{ "insrwi.", 4,   PPCCOM,	"rlwimi. %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1"},
+{ "rotrwi",  3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "rotrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "slwi",    3,   PPCCOM,	"rlwinm %0,%1,%2,0,31-(%2)" },
+{ "sli",     3,   PWRCOM,	"rlinm %0,%1,%2,0,31-(%2)" },
+{ "slwi.",   3,   PPCCOM,	"rlwinm. %0,%1,%2,0,31-(%2)" },
+{ "sli.",    3,   PWRCOM,	"rlinm. %0,%1,%2,0,31-(%2)" },
+{ "srwi",    3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri",     3,   PWRCOM,	"rlinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "srwi.",   3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri.",    3,   PWRCOM,	"rlinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "clrrwi",  3,   PPCCOM,	"rlwinm %0,%1,0,0,31-(%2)" },
+{ "clrrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,0,0,31-(%2)" },
+{ "clrlslwi",4,   PPCCOM,	"rlwinm %0,%1,%3,(%2)-(%3),31-(%3)" },
+{ "clrlslwi.",4,  PPCCOM,	"rlwinm. %0,%1,%3,(%2)-(%3),31-(%3)" },
+
+};
+
+const int powerpc_num_macros =
+  sizeof (powerpc_macros) / sizeof (powerpc_macros[0]);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/ppc.h linuxppc64_2_4/arch/ppc64/kdb/ppc.h
--- linux-2.4.19/arch/ppc64/kdb/ppc.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/ppc.h	Mon Jun  4 10:24:01 2001
@@ -0,0 +1,259 @@
+/* ppc.h -- Header file for PowerPC opcode table
+   Copyright 1994, 1995 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+1, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef PPC_H
+#define PPC_H
+
+/* The opcode table is an array of struct powerpc_opcode.  */
+
+struct powerpc_opcode
+{
+  /* The opcode name.  */
+  const char *name;
+
+  /* The opcode itself.  Those bits which will be filled in with
+     operands are zeroes.  */
+  unsigned long opcode;
+
+  /* The opcode mask.  This is used by the disassembler.  This is a
+     mask containing ones indicating those bits which must match the
+     opcode field, and zeroes indicating those bits which need not
+     match (and are presumably filled in by operands).  */
+  unsigned long mask;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The defined values
+     are listed below.  */
+  unsigned long flags;
+
+  /* An array of operand codes.  Each code is an index into the
+     operand table.  They appear in the order which the operands must
+     appear in assembly code, and are terminated by a zero.  */
+  unsigned char operands[8];
+};
+
+/* The table itself is sorted by major opcode number, and is otherwise
+   in the order in which the disassembler should consider
+   instructions.  */
+extern const struct powerpc_opcode powerpc_opcodes[];
+extern const int powerpc_num_opcodes;
+
+/* Values defined for the flags field of a struct powerpc_opcode.  */
+
+/* Opcode is defined for the PowerPC architecture.  */
+#define PPC_OPCODE_PPC (01)
+
+/* Opcode is defined for the POWER (RS/6000) architecture.  */
+#define PPC_OPCODE_POWER (02)
+
+/* Opcode is defined for the POWER2 (Rios 2) architecture.  */
+#define PPC_OPCODE_POWER2 (04)
+
+/* Opcode is only defined on 32 bit architectures.  */
+#define PPC_OPCODE_32 (010)
+
+/* Opcode is only defined on 64 bit architectures.  */
+#define PPC_OPCODE_64 (020)
+
+/* Opcode is supported by the Motorola PowerPC 601 processor.  The 601
+   is assumed to support all PowerPC (PPC_OPCODE_PPC) instructions,
+   but it also supports many additional POWER instructions.  */
+#define PPC_OPCODE_601 (040)
+
+/* Opcode is supported in both the Power and PowerPC architectures
+   (ie, compiler's -mcpu=common or assembler's -mcom).  */
+#define PPC_OPCODE_COMMON (0100)
+
+/* Opcode is supported for any Power or PowerPC platform (this is
+   for the assembler's -many option, and it eliminates duplicates).  */
+#define PPC_OPCODE_ANY (0200)
+
+/* Opcode is supported as part of the 64-bit bridge.  */
+#define PPC_OPCODE_64_BRIDGE (0400)
+
+/* Opcode is supported by Altivec Vector Unit */
+#define PPC_OPCODE_ALTIVEC   (01000)
+
+/* A macro to extract the major opcode from an instruction.  */
+#define PPC_OP(i) (((i) >> 26) & 0x3f)
+
+/* The operands table is an array of struct powerpc_operand.  */
+
+struct powerpc_operand
+{
+  /* The number of bits in the operand.  */
+  int bits;
+
+  /* How far the operand is left shifted in the instruction.  */
+  int shift;
+
+  /* Insertion function.  This is used by the assembler.  To insert an
+     operand value into an instruction, check this field.
+
+     If it is NULL, execute
+         i |= (op & ((1 << o->bits) - 1)) << o->shift;
+     (i is the instruction which we are filling in, o is a pointer to
+     this structure, and op is the opcode value; this assumes twos
+     complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction and the operand value.  It will return the new value
+     of the instruction.  If the ERRMSG argument is not NULL, then if
+     the operand value is illegal, *ERRMSG will be set to a warning
+     string (the operand will be inserted in any case).  If the
+     operand value is legal, *ERRMSG will be unchanged (most operands
+     can accept any value).  */
+  unsigned long (*insert) PARAMS ((unsigned long instruction, long op,
+				   const char **errmsg));
+
+  /* Extraction function.  This is used by the disassembler.  To
+     extract this operand type from an instruction, check this field.
+
+     If it is NULL, compute
+         op = ((i) >> o->shift) & ((1 << o->bits) - 1);
+	 if ((o->flags & PPC_OPERAND_SIGNED) != 0
+	     && (op & (1 << (o->bits - 1))) != 0)
+	   op -= 1 << o->bits;
+     (i is the instruction, o is a pointer to this structure, and op
+     is the result; this assumes twos complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction value.  It will return the value of the operand.  If
+     the INVALID argument is not NULL, *INVALID will be set to
+     non-zero if this operand type can not actually be extracted from
+     this operand (i.e., the instruction does not match).  If the
+     operand is valid, *INVALID will not be changed.  */
+  long (*extract) PARAMS ((unsigned long instruction, int *invalid));
+
+  /* One bit syntax flags.  */
+  unsigned long flags;
+};
+
+/* Elements in the table are retrieved by indexing with values from
+   the operands field of the powerpc_opcodes table.  */
+
+extern const struct powerpc_operand powerpc_operands[];
+
+/* Values defined for the flags field of a struct powerpc_operand.  */
+
+/* This operand takes signed values.  */
+#define PPC_OPERAND_SIGNED (01)
+
+/* This operand takes signed values, but also accepts a full positive
+   range of values when running in 32 bit mode.  That is, if bits is
+   16, it takes any value from -0x8000 to 0xffff.  In 64 bit mode,
+   this flag is ignored.  */
+#define PPC_OPERAND_SIGNOPT (02)
+
+/* This operand does not actually exist in the assembler input.  This
+   is used to support extended mnemonics such as mr, for which two
+   operands fields are identical.  The assembler should call the
+   insert function with any op value.  The disassembler should call
+   the extract function, ignore the return value, and check the value
+   placed in the valid argument.  */
+#define PPC_OPERAND_FAKE (04)
+
+/* The next operand should be wrapped in parentheses rather than
+   separated from this one by a comma.  This is used for the load and
+   store instructions which want their operands to look like
+       reg,displacement(reg)
+   */
+#define PPC_OPERAND_PARENS (010)
+
+/* This operand may use the symbolic names for the CR fields, which
+   are
+       lt  0	gt  1	eq  2	so  3	un  3
+       cr0 0	cr1 1	cr2 2	cr3 3
+       cr4 4	cr5 5	cr6 6	cr7 7
+   These may be combined arithmetically, as in cr2*4+gt.  These are
+   only supported on the PowerPC, not the POWER.  */
+#define PPC_OPERAND_CR (020)
+
+/* This operand names a register.  The disassembler uses this to print
+   register names with a leading 'r'.  */
+#define PPC_OPERAND_GPR (040)
+
+/* This operand names a floating point register.  The disassembler
+   prints these with a leading 'f'.  */
+#define PPC_OPERAND_FPR (0100)
+
+/* This operand is a relative branch displacement.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_RELATIVE (0200)
+
+/* This operand is an absolute branch address.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_ABSOLUTE (0400)
+
+/* This operand is optional, and is zero if omitted.  This is used for
+   the optional BF and L fields in the comparison instructions.  The
+   assembler must count the number of operands remaining on the line,
+   and the number of operands remaining for the opcode, and decide
+   whether this operand is present or not.  The disassembler should
+   print this operand out only if it is not zero.  */
+#define PPC_OPERAND_OPTIONAL (01000)
+
+/* This flag is only used with PPC_OPERAND_OPTIONAL.  If this operand
+   is omitted, then for the next operand use this operand value plus
+   1, ignoring the next operand field for the opcode.  This wretched
+   hack is needed because the Power rotate instructions can take
+   either 4 or 5 operands.  The disassembler should print this operand
+   out regardless of the PPC_OPERAND_OPTIONAL field.  */
+#define PPC_OPERAND_NEXT (02000)
+
+/* This operand should be regarded as a negative number for the
+   purposes of overflow checking (i.e., the normal most negative
+   number is disallowed and one more than the normal most positive
+   number is allowed).  This flag will only be set for a signed
+   operand.  */
+#define PPC_OPERAND_NEGATIVE (04000)
+
+/* This operand names a vector unit register.  The disassembler
+   prints these with a leading 'v'.  */
+#define PPC_OPERAND_VR (010000)
+
+
+/* The POWER and PowerPC assemblers use a few macros.  We keep them
+   with the operands table for simplicity.  The macro table is an
+   array of struct powerpc_macro.  */
+
+struct powerpc_macro
+{
+  /* The macro name.  */
+  const char *name;
+
+  /* The number of operands the macro takes.  */
+  unsigned int operands;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The values are the
+     same as those for the struct powerpc_opcode flags field.  */
+  unsigned long flags;
+
+  /* A format string to turn the macro into a normal instruction.
+     Each %N in the string is replaced with operand number N (zero
+     based).  */
+  const char *format;
+};
+
+extern const struct powerpc_macro powerpc_macros[];
+extern const int powerpc_num_macros;
+
+#endif /* PPC_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/privinst.h linuxppc64_2_4/arch/ppc64/kdb/privinst.h
--- linux-2.4.19/arch/ppc64/kdb/privinst.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/privinst.h	Fri May 24 15:08:51 2002
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 1996 Paul Mackerras.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#include <linux/config.h>
+
+#define GETREG(reg)		\
+    static inline unsigned long get_ ## reg (void)	\
+	{ unsigned long ret; asm volatile ("mf" #reg " %0" : "=r" (ret) :); return ret; }
+
+#define SETREG(reg)		\
+    static inline void set_ ## reg (unsigned long val)	\
+	{ asm volatile ("mt" #reg " %0" : : "r" (val)); }
+
+GETREG(msr)
+SETREG(msr)
+SETREG(msrd)
+GETREG(cr)
+
+#define GSETSPR(n, name)	\
+    static inline long get_ ## name (void) \
+	{ long ret; asm volatile ("mfspr %0," #n : "=r" (ret) : ); return ret; } \
+    static inline void set_ ## name (long val) \
+	{ asm volatile ("mtspr " #n ",%0" : : "r" (val)); }
+
+GSETSPR(0, mq)
+GSETSPR(1, xer)
+GSETSPR(4, rtcu)
+GSETSPR(5, rtcl)
+GSETSPR(8, lr)
+GSETSPR(9, ctr)
+GSETSPR(18, dsisr)
+GSETSPR(19, dar)
+GSETSPR(22, dec)
+GSETSPR(25, sdr1)
+GSETSPR(26, srr0)
+GSETSPR(27, srr1)
+GSETSPR(272, sprg0)
+GSETSPR(273, sprg1)
+GSETSPR(274, sprg2)
+GSETSPR(275, sprg3)
+GSETSPR(282, ear)
+GSETSPR(287, pvr)
+GSETSPR(528, bat0u)
+GSETSPR(529, bat0l)
+GSETSPR(530, bat1u)
+GSETSPR(531, bat1l)
+GSETSPR(532, bat2u)
+GSETSPR(533, bat2l)
+GSETSPR(534, bat3u)
+GSETSPR(535, bat3l)
+GSETSPR(1008, hid0)
+GSETSPR(1009, hid1)
+GSETSPR(1010, iabr)
+GSETSPR(1013, dabr)
+GSETSPR(1023, pir)
+
+static inline int get_sr(int n)
+{
+    int ret;
+
+#if 0
+// DRENG does not assemble 
+    asm (" mfsrin %0,%1" : "=r" (ret) : "r" (n << 28));
+#endif
+    return ret;
+}
+
+static inline void set_sr(int n, int val)
+{
+#if 0
+// DRENG does not assemble 
+    asm ("mtsrin %0,%1" : : "r" (val), "r" (n << 28));
+#endif
+}
+
+static inline void store_inst(void *p)
+{
+    asm volatile ("dcbst 0,%0; sync; icbi 0,%0; isync" : : "r" (p));
+}
+
+static inline void cflush(void *p)
+{
+    asm volatile ("dcbf 0,%0; icbi 0,%0" : : "r" (p));
+}
+
+static inline void cinval(void *p)
+{
+    asm volatile ("dcbi 0,%0; icbi 0,%0" : : "r" (p));
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/start.c linuxppc64_2_4/arch/ppc64/kdb/start.c
--- linux-2.4.19/arch/ppc64/kdb/start.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/start.c	Mon Aug 19 09:15:52 2002
@@ -0,0 +1,56 @@
+/*
+ * Copyright (C) 1996 Paul Mackerras.
+ */
+#include <linux/config.h>
+#include <linux/string.h>
+#include <linux/fs.h>
+#include <asm/machdep.h>
+#include <asm/io.h>
+#include <asm/page.h>
+#include <linux/adb.h>
+#include <linux/pmu.h>
+#include <linux/cuda.h>
+#include <linux/kernel.h>
+#include <asm/prom.h>
+
+#include <linux/sysrq.h>
+#include <linux/kdb.h>
+#include <asm/kdb.h>
+
+#include <asm/processor.h>
+#include <asm/delay.h>
+#ifdef CONFIG_SMP
+#include <asm/bitops.h>
+#endif
+
+/* kdb will use UDBG */
+#define USE_UDBG
+
+#ifdef USE_UDBG
+#include <asm/udbg.h>
+#endif
+
+
+
+static void sysrq_handle_kdb(int key, struct pt_regs *pt_regs, struct kbd_struct *kbd, struct tty_struct *tty) 
+{
+  kdb(KDB_REASON_KEYBOARD,0,pt_regs);
+}
+
+static struct sysrq_key_op sysrq_kdb_op = 
+{
+	handler:	sysrq_handle_kdb,
+	help_msg:	"kdb",
+	action_msg:	"Entering kdb\n",
+};
+
+
+
+void
+kdb_map_scc(void)
+{
+	/* register sysrq 'x' */
+	__sysrq_put_key_op('x', &sysrq_kdb_op);
+}
+
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/subr_prf.c linuxppc64_2_4/arch/ppc64/kdb/subr_prf.c
--- linux-2.4.19/arch/ppc64/kdb/subr_prf.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/subr_prf.c	Mon Jun  4 14:13:55 2001
@@ -0,0 +1,50 @@
+/*
+ * Written by Cort Dougan to replace the version originally used
+ * by Paul Mackerras, which came from NetBSD and thus had copyright
+ * conflicts with Linux.
+ *
+ * This file makes liberal use of the standard linux utility
+ * routines to reduce the size of the binary.  We assume we can
+ * trust some parts of Linux inside the debugger.
+ *   -- Cort (cort@cs.nmt.edu)
+ *
+ * Copyright (C) 1999 Cort Dougan.
+ */
+
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <stdarg.h>
+#include "nonstdio.h"
+
+extern int xmon_write(void *, void *, int);
+
+void
+xmon_vfprintf(void *f, const char *fmt, va_list ap)
+{
+	static char xmon_buf[2048];
+	int n;
+
+	n = vsprintf(xmon_buf, fmt, ap);
+	xmon_write(f, xmon_buf, n);
+}
+
+void
+xmon_printf(const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	xmon_vfprintf(stdout, fmt, ap);
+	va_end(ap);
+}
+
+void
+xmon_fprintf(void *f, const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	xmon_vfprintf(f, fmt, ap);
+	va_end(ap);
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kdb/symcat.h linuxppc64_2_4/arch/ppc64/kdb/symcat.h
--- linux-2.4.19/arch/ppc64/kdb/symcat.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kdb/symcat.h	Fri May 24 15:08:51 2002
@@ -0,0 +1,49 @@
+/* Symbol concatenation utilities.
+
+   Copyright (C) 1998, 2000 Free Software Foundation, Inc.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+ 
+   You should have received a copy of the GNU General Public License along
+   with this program; if not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef SYM_CAT_H
+#define SYM_CAT_H
+
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#define CONCAT2(a,b)	 a##b
+#define CONCAT3(a,b,c)	 a##b##c
+#define CONCAT4(a,b,c,d) a##b##c##d
+#define STRINGX(s) #s
+#else
+/* Note one should never pass extra whitespace to the CONCATn macros,
+   e.g. CONCAT2(foo, bar) because traditonal C will keep the space between
+   the two labels instead of concatenating them.  Instead, make sure to
+   write CONCAT2(foo,bar).  */
+#define CONCAT2(a,b)	 a/**/b
+#define CONCAT3(a,b,c)	 a/**/b/**/c
+#define CONCAT4(a,b,c,d) a/**/b/**/c/**/d
+#define STRINGX(s) "s"
+#endif
+
+#define XCONCAT2(a,b)     CONCAT2(a,b)
+#define XCONCAT3(a,b,c)   CONCAT3(a,b,c)
+#define XCONCAT4(a,b,c,d) CONCAT4(a,b,c,d)
+
+/* Note the layer of indirection here is typically used to allow
+   stringification of the expansion of macros.  I.e. "#define foo
+   bar", "XSTRING(foo)", to yield "bar".  Be aware that this only
+   works for __STDC__, not for traditional C which will still resolve
+   to "foo".  */
+#define XSTRING(s) STRINGX(s) 
+
+#endif /* SYM_CAT_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/LparData.c linuxppc64_2_4/arch/ppc64/kernel/LparData.c
--- linux-2.4.19/arch/ppc64/kernel/LparData.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/LparData.c	Tue Jul 23 02:50:05 2002
@@ -6,14 +6,12 @@
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
-#define __KERNEL__ 1
 #include <asm/types.h>
 #include <asm/page.h>
 #include <stddef.h>
 #include <linux/threads.h>
 #include <asm/processor.h>
 #include <asm/ptrace.h>
-#include <asm/init.h>
 #include <asm/naca.h>
 #include <asm/abs_addr.h>
 #include <asm/bitops.h>
@@ -25,6 +23,7 @@
 #include <asm/iSeries/LparMap.h>
 #include <asm/iSeries/ItVpdAreas.h>
 #include <asm/iSeries/ItIplParmsReal.h>
+#include <asm/iSeries/ItExtVpdPanel.h>
 #include <asm/iSeries/ItLpQueue.h>
 #include <asm/iSeries/IoHriProcessorVpd.h>
 #include <asm/iSeries/ItSpCommArea.h>
@@ -144,6 +143,8 @@
 
 struct ItIplParmsReal xItIplParmsReal = {};
 
+struct ItExtVpdPanel xItExtVpdPanel = {};
+
 #define maxPhysicalProcessors 32
 
 struct IoHriProcessorVpd xIoHriProcessorVpd[maxPhysicalProcessors] = {
@@ -185,7 +186,8 @@
 	{0},		/* DMA lengths */
 	{0},		/* DMA tokens */
 	{		/* VPD lengths */
-		0,0,0,0,		/*  0 -  3 */
+	        0,0,0,		        /*  0 - 2 */
+		sizeof(xItExtVpdPanel), /*       3 Extended VPD   */
 		sizeof(struct paca_struct),	/*       4 length of Paca  */
 		0,			/*       5 */
 		sizeof(struct ItIplParmsReal),/* 6 length of IPL parms */
@@ -202,7 +204,8 @@
 		0,0			/* 24 - 25 */
 		},
 	{			/* VPD addresses */
-		0,0,0,0,		/*	 0 -  3 */
+		0,0,0,  		/*	 0 -  2 */
+		&xItExtVpdPanel,        /*       3 Extended VPD */
 		&paca[0],		/*       4 first Paca */
 		0,			/*       5 */
 		&xItIplParmsReal,	/*	 6 IPL parms */
@@ -244,7 +247,3 @@
 
 	return mem;
 }
-
-
-
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/Makefile linuxppc64_2_4/arch/ppc64/kernel/Makefile
--- linux-2.4.19/arch/ppc64/kernel/Makefile	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/Makefile	Tue Aug  6 10:06:42 2002
@@ -27,18 +27,20 @@
 			pmc.o mf_proc.o proc_pmc.o proc_pcifr.o iSeries_setup.o \
 			ItLpQueue.o hvCall.o mf.o HvLpEvent.o ras.o \
 			iSeries_proc.o HvCall.o flight_recorder.o HvLpConfig.o \
-			rtc.o
+			rtc.o perfmon.o
 
-obj-$(CONFIG_PCI) +=  pci.o pci_dn.o pci_dma.o
+obj-$(CONFIG_PCI) +=  pci.o pci_dn.o pci_dma.o pSeries_lpar.o pSeries_hvCall.o
 
 ifeq ($(CONFIG_PPC_ISERIES),y)
 obj-$(CONFIG_PCI) += iSeries_pci.o iSeries_pci_reset.o iSeries_IoMmTable.o iSeries_irq.o iSeries_VpdInfo.o XmPciLpEvent.o 
 endif
 ifeq ($(CONFIG_PPC_PSERIES),y)
-obj-$(CONFIG_PCI) += pSeries_pci.o pSeries_lpar.o pSeries_hvCall.o eeh.o
+obj-$(CONFIG_PCI) += pSeries_pci.o eeh.o
 
 obj-y += rtasd.o nvram.o
 endif
+
+obj-$(CONFIG_RTAS_FLASH) += rtas_flash.o
 
 obj-$(CONFIG_KGDB) += ppc-stub.o
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/XmPciLpEvent.c linuxppc64_2_4/arch/ppc64/kernel/XmPciLpEvent.c
--- linux-2.4.19/arch/ppc64/kernel/XmPciLpEvent.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/XmPciLpEvent.c	Tue Aug 20 08:31:30 2002
@@ -14,22 +14,32 @@
 #include <linux/bootmem.h>
 #include <linux/blk.h>
 #include <linux/ide.h>
+#include <linux/rtc.h>
+#include <linux/time.h>
 
 #include <asm/iSeries/HvTypes.h>
 #include <asm/iSeries/HvLpEvent.h>
 #include <asm/iSeries/HvCallPci.h>
 #include <asm/iSeries/XmPciLpEvent.h>
 #include <asm/ppcdebug.h>
+#include <asm/time.h>
+#include <asm/flight_recorder.h>
+
+extern struct flightRecorder* PciFr;
+
 
 long Pci_Interrupt_Count = 0;
 long Pci_Event_Count     = 0;
 
 enum XmPciLpEvent_Subtype {
 	XmPciLpEvent_BusCreated	   = 0,		// PHB has been created
-	XmPciLpEvent_BusFailed	   = 1,		// PHB has failed
-	XmPciLpEvent_BusRecovered  = 12,	// PHB has been recovered
+	XmPciLpEvent_BusError	   = 1,		// PHB has failed
+	XmPciLpEvent_BusFailed	   = 2,		// Msg to Seconday, Primary failed bus
 	XmPciLpEvent_NodeFailed	   = 4,		// Multi-adapter bridge has failed
 	XmPciLpEvent_NodeRecovered = 5,		// Multi-adapter bridge has recovered
+	XmPciLpEvent_BusRecovered  = 12,	// PHB has been recovered
+	XmPciLpEvent_UnQuiesceBus  = 18,	// Secondary bus unqiescing
+	XmPciLpEvent_BridgeError   = 21,	// Bridge Error
 	XmPciLpEvent_SlotInterrupt = 22		// Slot interrupt
 };
 
@@ -69,6 +79,7 @@
 };
 
 static void intReceived(struct XmPciLpEvent* eventParm, struct pt_regs* regsParm);
+static void logXmEvent( char* ErrorText, int busNumber);
 
 static void XmPciLpEvent_handler( struct HvLpEvent* eventParm, struct pt_regs* regsParm)
 {
@@ -81,10 +92,10 @@
 			intReceived( (struct XmPciLpEvent*)eventParm, regsParm );
 			break;
 		case HvLpEvent_Function_Ack:
-			printk(KERN_ERR "XmPciLpEvent.c: unexpected ack received\n");
+			printk(KERN_ERR "XmPciLpEvent.c: Unexpected ack received\n");
 			break;
 		default:
-			printk(KERN_ERR "XmPciLpEvent.c: unexpected event function %d\n",(int)eventParm->xFlags.xFunction);
+			printk(KERN_ERR "XmPciLpEvent.c: Unexpected event function %d\n",(int)eventParm->xFlags.xFunction);
 			break;
 		}
 	}
@@ -114,23 +125,25 @@
 		break;
 		/* Ignore error recovery events for now */
 	case XmPciLpEvent_BusCreated:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d created\n", eventParm->eventData.busCreated.busNumber);
+	        logXmEvent("System bus created.",eventParm->eventData.busCreated.busNumber);
 		break;
+	case XmPciLpEvent_BusError:
 	case XmPciLpEvent_BusFailed:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d failed\n", eventParm->eventData.busFailed.busNumber);
+		logXmEvent("System bus failed.", eventParm->eventData.busFailed.busNumber);
 		break;
 	case XmPciLpEvent_BusRecovered:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d recovered\n", eventParm->eventData.busRecovered.busNumber);
+	case XmPciLpEvent_UnQuiesceBus:
+		logXmEvent("System bus recovered.",eventParm->eventData.busRecovered.busNumber);
 		break;
 	case XmPciLpEvent_NodeFailed:
-		printk(KERN_INFO "XmPciLpEvent.c: multi-adapter bridge %d/%d/%d failed\n", eventParm->eventData.nodeFailed.busNumber, eventParm->eventData.nodeFailed.subBusNumber, eventParm->eventData.nodeFailed.deviceId);
-		break;
+	case XmPciLpEvent_BridgeError:
+	        logXmEvent("Multi-adapter bridge failed.",eventParm->eventData.nodeFailed.busNumber);
+	        break;
 	case XmPciLpEvent_NodeRecovered:
-		printk(KERN_INFO "XmPciLpEvent.c: multi-adapter bridge %d/%d/%d recovered\n", eventParm->eventData.nodeRecovered.busNumber, eventParm->eventData.nodeRecovered.subBusNumber, eventParm->eventData.nodeRecovered.deviceId);
+		logXmEvent("Multi-adapter bridge recovered",eventParm->eventData.nodeRecovered.busNumber);
 		break;
 	default:
-		printk(KERN_ERR "XmPciLpEvent.c: unrecognized event subtype 0x%x\n",
-		       eventParm->hvLpEvent.xSubtype);
+	        logXmEvent("Unrecognized event subtype.",eventParm->hvLpEvent.xSubtype);
 		break;
 	};
 }
@@ -153,5 +166,26 @@
 		printk(KERN_ERR "XmPciLpEvent.c: register handler failed with rc 0x%x\n", xRc);
     	}
     return xRc;
+}
+/***********************************************************************/
+/* printk                                                              */
+/*   XmPciLpEvent: System bus failed, bus 0x4A. Time:128-16.10.44      */
+/* pcifr                                                               */
+/*  0045. XmPciLpEvent: System bus failed, bus 0x4A. Time:128-16.10.44 */
+/***********************************************************************/
+static void logXmEvent( char* ErrorText, int busNumber) {
+	struct  timeval  TimeClock;
+	struct  rtc_time CurTime;
+
+	do_gettimeofday(&TimeClock);
+	to_tm(TimeClock.tv_sec, &CurTime);
+	char    EventLog[128];
+
+	sprintf(EventLog,"XmPciLpEvent: %s, Bus:0x%03X.  Time:%02d.%02d.%02d",
+		ErrorText,busNumber,
+		CurTime.tm_hour,CurTime.tm_min,CurTime.tm_sec);
+
+	fr_Log_Entry(PciFr,EventLog);
+	printk(KERN_INFO "%s\n",EventLog);
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/align.c linuxppc64_2_4/arch/ppc64/kernel/align.c
--- linux-2.4.19/arch/ppc64/kernel/align.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/align.c	Tue Aug 20 14:46:58 2002
@@ -238,7 +238,7 @@
 	dsisr = regs->dsisr;
 
 	/* Power4 doesn't set DSISR for an alignment interrupt */
-	if (__is_processor(PV_POWER4))
+	if (__is_processor(PV_POWER4) || __is_processor(PV_POWER4p))
 		dsisr = make_dsisr( *((unsigned *)regs->nip) );
 
 	/* extract the operation and registers from the dsisr */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/chrp_setup.c linuxppc64_2_4/arch/ppc64/kernel/chrp_setup.c
--- linux-2.4.19/arch/ppc64/kernel/chrp_setup.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/chrp_setup.c	Mon Aug 19 08:27:12 2002
@@ -102,6 +102,8 @@
 extern int probingmem;
 extern unsigned long loops_per_jiffy;
 
+extern unsigned long ppc_proc_freq;
+extern unsigned long ppc_tb_freq;
 #ifdef CONFIG_BLK_DEV_RAM
 extern int rd_doload;		/* 1 = load ramdisk, 0 = don't load */
 extern int rd_prompt;		/* 1 = prompt for ramdisk, 0 = don't prompt */
@@ -114,6 +116,8 @@
 	struct device_node *root;
 	const char *model = "";
 
+	seq_printf(m, "timebase\t: %lu\n", ppc_tb_freq);
+
 	root = find_path_device("/");
 	if (root)
 		model = get_property(root, "model", NULL);
@@ -174,7 +178,6 @@
 		for (openpic = 0; n > 0; --n)
 			openpic = (openpic << 32) + *opprop++;
 		printk(KERN_DEBUG "OpenPIC addr: %lx\n", openpic);
-		udbg_printf("OpenPIC addr: %lx\n", openpic);
 		OpenPIC_Addr = __ioremap(openpic, 0x40000, _PAGE_NO_CACHE);
 	}
 
@@ -191,7 +194,9 @@
 	 * -- tibit
 	 */
 	chrp_request_regions();
-	ppc_md.progress(UTS_RELEASE, 0x7777);
+	/* Manually leave the kernel version on the panel. */
+	ppc_md.progress("Linux ppc64\n", 0);
+	ppc_md.progress(UTS_RELEASE, 0);
 }
 
 /* Initialize firmware assisted non-maskable interrupts if
@@ -259,11 +264,9 @@
 	if(naca->interrupt_controller == IC_OPEN_PIC) {
 		ppc_md.init_IRQ       = openpic_init_IRQ; 
 		ppc_md.get_irq        = openpic_get_irq;
-		ppc_md.post_irq	      = NULL;
 	} else {
 		ppc_md.init_IRQ       = xics_init_IRQ;
 		ppc_md.get_irq        = xics_get_irq;
-		ppc_md.post_irq	      = NULL;
 	}
 	ppc_md.init_ras_IRQ = init_ras_IRQ;
 
@@ -301,8 +304,6 @@
 	SYSRQ_KEY = 0x63;	/* Print Screen */
 #endif
 #endif
-	
-	ppc_md.progress("Linux ppc64\n", 0x0);
 }
 
 void __chrp
@@ -314,10 +315,7 @@
 	static int display_character, set_indicator;
 	static int max_width;
 
-	if (hex)
-		udbg_printf("<chrp_progress> %s\n", s);
-
-	if (!rtas.base || (naca->platform != PLATFORM_PSERIES))
+	if (!rtas.base)
 		return;
 
 	if (max_width == 0) {
@@ -363,9 +361,6 @@
 
 extern void setup_default_decr(void);
 
-extern unsigned long ppc_proc_freq;
-extern unsigned long ppc_tb_freq;
-
 void __init pSeries_calibrate_decr(void)
 {
 	struct device_node *cpu;
@@ -401,7 +396,6 @@
 	tb_ticks_per_jiffy = freq / HZ;
 	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
 	tb_ticks_per_usec = freq / 1000000;
-	tb_to_us = mulhwu_scale_factor(freq, 1000000);
 	div128_by_32( 1024*1024, 0, tb_ticks_per_sec, &divres );
 	tb_to_xs = divres.result_low;
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/eeh.c linuxppc64_2_4/arch/ppc64/kernel/eeh.c
--- linux-2.4.19/arch/ppc64/kernel/eeh.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/eeh.c	Tue Jul 23 02:40:20 2002
@@ -59,15 +59,14 @@
 	return ((IO_UNMAPPED_REGION_ID << 60) | (phb << 48UL) | ((bus & 0xff) << 40UL) | (devfn << 32UL) | (offset & 0xffffffff));
 }
 
-
-
-int eeh_get_state(unsigned long ea) {
+int eeh_get_state(unsigned long ea)
+{
 	return 0;
 }
 
-
 /* Check for an eeh failure at the given token address.
- * The given value has been read and it should be 1's (0xff, 0xffff or 0xffffffff).
+ * The given value has been read and it should be 1's (0xff, 0xffff or
+ * 0xffffffff).
  *
  * Probe to determine if an error actually occurred.  If not return val.
  * Otherwise panic.
@@ -113,7 +112,8 @@
 	return val;	/* good case */
 }
 
-void eeh_init(void) {
+void eeh_init(void)
+{
 	extern char cmd_line[];	/* Very early cmd line parse.  Cheap, but works. */
 	char *eeh_force_off = strstr(cmd_line, "eeh-force-off");
 	char *eeh_force_on = strstr(cmd_line, "eeh-force-on");
@@ -334,6 +334,7 @@
 {
 	return eeh_parm(str, 0);
 }
+
 static int __init eehon_parm(char *str)
 {
 	return eeh_parm(str, 1);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/entry.S linuxppc64_2_4/arch/ppc64/kernel/entry.S
--- linux-2.4.19/arch/ppc64/kernel/entry.S	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/entry.S	Tue Jul 23 02:50:05 2002
@@ -305,9 +305,7 @@
 	cmpdi	0,r5,0
 	beq	4f
 irq_recheck:
-	/*
-	 * Check for pending interrupts (iSeries)
-	 */
+	/* Check for pending interrupts (iSeries) */
 	CHECKANYINT(r3,r4)
 	beq+	4f	/* skip do_IRQ if no interrupts */
 
@@ -346,6 +344,7 @@
 	mtlr	r0
 	ld	r3,_XER(r1)
 	mtspr	XER,r3
+
 	REST_8GPRS(5, r1)
 	REST_10GPRS(14, r1)
 	REST_8GPRS(24, r1)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/head.S linuxppc64_2_4/arch/ppc64/kernel/head.S
--- linux-2.4.19/arch/ppc64/kernel/head.S	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/head.S	Mon Aug 12 05:52:45 2002
@@ -31,7 +31,7 @@
 #include <asm/page.h>
 #include <linux/config.h>
 #include <asm/mmu.h>
-// #include <asm/paca.h>
+#include <asm/perfmon.h>
 
 #ifdef CONFIG_PPC_ISERIES
 #define DO_SOFT_DISABLE
@@ -154,12 +154,13 @@
  */
 
 /*
- * We make as much of the exception code common between native Pseries
- * and Iseries LPAR implementations as possible.
+ * We make as much of the exception code common between native
+ * exception handlers (including pSeries LPAR) and iSeries LPAR
+ * implementations as possible.
  */
 
 /*
- * This is the start of the interrupt handlers for Pseries
+ * This is the start of the interrupt handlers for pSeries
  * This code runs with relocation off.
  */
 #define EX_SRR0		0
@@ -200,13 +201,13 @@
 	rfid
 
 /*
- * This is the start of the interrupt handlers for i_series
+ * This is the start of the interrupt handlers for iSeries
  * This code runs with relocation on.
  */
 #define EXCEPTION_PROLOG_ISERIES(n)	                                      \
 	mtspr	SPRG2,r20;		    /* use SPRG2 as scratch reg    */ \
 	mtspr   SPRG1,r21;                  /* save r21                    */ \
-	mfspr	r20,SPRG3;		    /* get Paca                    */ \
+	mfspr	r20,SPRG3;		    /* get paca                    */ \
 	ld      r21,PACAEXCSP(r20);         /* get exception stack ptr     */ \
 	addi    r21,r21,EXC_FRAME_SIZE;     /* make exception frame        */ \
 	std	r22,EX_R22(r21);	    /* save r22 on exception frame */ \
@@ -302,9 +303,9 @@
 	.globl label##_Iseries;			\
 label##_Iseries:				\
 	EXCEPTION_PROLOG_ISERIES( n );		\
-	lbz	r22,PACAPROFENABLED(r20);	\
-	cmpi	0,r22,0;			\
-	bne-	label##_Iseries_profile;	\
+	lbz	r22,PACAPROFMODE(r20);		\
+	cmpi	0,r22,PMC_STATE_DECR_PROFILE;	\
+	beq-	label##_Iseries_profile;	\
 label##_Iseries_prof_ret:			\
 	lbz	r22,PACAPROCENABLED(r20);	\
 	cmpi	0,r22,0;			\
@@ -359,7 +360,7 @@
 
 	/* Space for the naca.  Architected to be located at real address
 	 * 0x4000.  Various tools rely on this location being fixed.
-	 * The first dword of the Naca is required by iSeries LPAR to
+	 * The first dword of the naca is required by iSeries LPAR to
 	 * point to itVpdAreas.  On pSeries native, this value is not used.
 	 */
 	. = 0x4000
@@ -430,7 +431,7 @@
 	STD_EXCEPTION_ISERIES( 0xc00, SystemCall )
 	STD_EXCEPTION_ISERIES( 0xd00, SingleStep )
 	STD_EXCEPTION_ISERIES( 0xe00, Trap_0e )
-	STD_EXCEPTION_ISERIES( 0xf00, PerformanceMonitor )
+	MASKABLE_EXCEPTION_ISERIES( 0xf00, PerformanceMonitor )
 
 	.globl SystemReset_Iseries
 SystemReset_Iseries:
@@ -484,6 +485,12 @@
 HardwareInterrupt_Iseries_masked:
 	b	maskable_exception_exit
 
+	.globl PerformanceMonitor_Iseries_masked
+PerformanceMonitor_Iseries_masked:
+	li	r22,1
+	stb	r22,PACALPPACA+LPPACAPDCINT(r20)
+	b	maskable_exception_exit
+
 	.globl Decrementer_Iseries_masked
 Decrementer_Iseries_masked:
 	li	r22,1
@@ -529,7 +536,6 @@
 	STD_EXCEPTION_COMMON( 0xb00, Trap_0b, .UnknownException )
 	STD_EXCEPTION_COMMON( 0xd00, SingleStep, .SingleStepException )
 	STD_EXCEPTION_COMMON( 0xe00, Trap_0e, .UnknownException )
-	STD_EXCEPTION_COMMON( 0xf00, PerformanceMonitor, .PerformanceMonitorException )
 	STD_EXCEPTION_COMMON(0x1300, InstructionBreakpoint, .InstructionBreakpointException )
 
 /*
@@ -583,7 +589,8 @@
 	bl	.do_stab_SI
 	b	1f
 
-2:	bl	.do_hash_page_DSI 	/* Try to handle as hpte fault */
+2:	li	r5,0x300
+	bl	.do_hash_page_DSI 	/* Try to handle as hpte fault */
 1:
 	ld      r4,_DAR(r1)
 	ld      r5,_DSISR(r1)
@@ -668,7 +675,7 @@
 #else
 	rldicl	r20,r23,49,63   	/* copy EE bit from saved MSR */
 #endif
-	li	r6,0x380
+	li	r6,0x480
 	bl      .save_remaining_regs
 	bl      .do_page_fault
 	b       .ret_from_except
@@ -787,6 +794,150 @@
 	bl      .DoSyscall
 	b       .ret_from_except
 
+	.globl PerformanceMonitor_common
+PerformanceMonitor_common:	
+	EXCEPTION_PROLOG_COMMON
+	bl	.PerformanceMonitorException
+	b	fast_exception_return
+	
+_GLOBAL(PerformanceMonitorException)
+	mfspr	r7,SPRG3
+	lbz	r8,PACAPROFMODE(r7)
+	cmpi	0,r8,PMC_STATE_PROFILE_KERN
+	beq 	5f
+	cmpi	0,r8,PMC_STATE_TRACE_KERN
+	beq 	6f
+	cmpi	0,r8,PMC_STATE_TRACE_USER
+	beq 	9f
+	blr
+
+	/* PMC Profile Kernel */
+5:	mfspr   r9,SIAR	
+	srdi    r8,r9,60
+	cmpi    0,r8,0xc
+	beq     3f
+	li	r9,0xc
+	sldi	r9,r9,60
+3:      ld	r8,PACAPROFSTEXT(r7)	/* _stext */
+	subf	r9,r8,r9		/* offset into kernel */
+	lwz	r8,PACAPROFSHIFT(r7)
+	srd	r9,r9,r8
+	lwz	r8,PACAPROFLEN(r7)	/* length of profile table (-1) */
+	srdi	r8,r8,2
+	cmpd	r9,r8		/* off end? */
+	ble	1f
+	mr	r9,r8			/* force into last entry */
+	srdi	r9,r9,2
+1:	sldi	r9,r9,2		/* convert to offset into buffer */
+	ld	r8,PACAPROFBUFFER(r7)	/* profile buffer */
+	add	r8,r8,r9
+2:	lwarx	r9,0,r8		/* atomically increment */
+	addi	r9,r9,1
+	stwcx.	r9,0,r8
+	bne-	2b
+	addi	r10,r7,PACAPMC1
+	addi	r7,r7,PACAPMCC1
+	b	7f
+
+	/* PMC Trace Kernel */
+6:	LOADADDR(r11, perfmon_base)
+  	addi	r8,r11,32
+	ld	r12,24(r11)
+	subi	r12,r12,1
+8:	ldarx	r10,0,r8
+	addi	r9,r10,16
+	and	r9,r9,r12
+	stdcx.	r9,0,r8
+	bne-	8b
+	ld	r9,16(r11)	/* profile buffer */
+	add	r8,r9,r10
+  	mfspr   r9,SIAR	
+	std	r9,0(r8)
+	mfspr   r9,SDAR	
+	std	r9,8(r8)
+	addi	r10,r7,PACAPMC1
+	addi	r7,r7,PACAPMCC1
+	b	7f
+
+	/* PMC Trace User */
+9:	LOADADDR(r11, perfmon_base)
+#if 0
+  	addi	r8,r11,32
+	ld	r12,24(r11)
+	subi	r12,r12,1
+8:	ldarx	r10,0,r8
+	addi	r9,r10,16
+	and	r9,r9,r12
+	stdcx.	r9,0,r8
+	bne-	8b
+	ld	r9,16(r11)	/* profile buffer */
+	add	r8,r9,r10
+  	mfspr   r9,SIAR	
+	std	r9,0(r8)
+	mfspr   r9,SDAR	
+	std	r9,8(r8)
+	addi	r10,r13,THREAD+THREAD_PMC1
+	addi	r7,r13,THREAD+THREAD_PMCC1
+#endif
+	/* Accumulate counter values for kernel traces */
+7:	ld	r9,0(r7)
+	mfspr   r8,PMC1	
+	add	r9,r9,r8
+	std	r9,0(r7)
+	ld	r9,8(r7)
+	mfspr	r8,PMC2
+	add	r9,r9,r8
+	std	r9,8(r7)
+	ld	r9,16(r7)
+	mfspr	r8,PMC3
+	add	r9,r9,r8
+	std	r9,16(r7)
+	ld	r9,24(r7)
+	mfspr	r8,PMC4
+	add	r9,r9,r8
+	std	r9,24(r7)
+	ld	r9,32(r7)
+	mfspr	r8,PMC5
+	add	r9,r9,r8
+	std	r9,32(r7)
+	ld	r9,40(r7)
+	mfspr	r8,PMC6
+	add	r9,r9,r8
+	std	r9,40(r7)
+	ld	r9,48(r7)
+	mfspr	r8,PMC7
+	add	r9,r9,r8
+	std	r9,48(r7)
+	ld	r9,56(r7)
+	mfspr	r8,PMC8
+	add	r9,r9,r8
+	std	r9,56(r7)
+
+	/* Reset all counters for kernel traces */
+	lwz	r9,0(r10)
+	mtspr	PMC1,r9
+	lwz	r9,4(r10)
+	mtspr	PMC2,r9
+	lwz	r9,8(r10)
+	mtspr	PMC3,r9
+	lwz	r9,12(r10)
+	mtspr	PMC4,r9
+	lwz	r9,16(r10)
+	mtspr	PMC5,r9
+	lwz	r9,20(r10)
+	mtspr	PMC6,r9
+	lwz	r9,24(r10)
+	mtspr	PMC7,r9
+	lwz	r9,28(r10)
+	mtspr	PMC8,r9
+	lwz	r9,32(r10)
+	mtspr	MMCR0,r9
+	lwz	r9,36(r10)
+	mtspr	MMCR1,r9
+	lwz	r9,40(r10)
+	mtspr	MMCRA,r9
+	blr
+
 _GLOBAL(do_hash_page_ISI)
 	li	r4,0
 _GLOBAL(do_hash_page_DSI)
@@ -812,6 +963,7 @@
 	/*
 	 * r3 contains the faulting address
 	 * r4 contains the required access permissions
+	 * r5 contains the trap number
 	 *
 	 * at return r3 = 0 for success
 	 */
@@ -1029,6 +1181,13 @@
 	ori	r20,r20,256    /* map kernel region with large ptes */
 #endif
 	
+	/* Invalidate the old entry */
+	slbmfee	r21,r22
+	lis	r23,-2049
+	ori	r23,r23,65535
+	and	r21,r21,r23
+	slbie	r21
+
 	/* Put together the esid portion of the entry. */
 	mfspr	r21,DAR        /* Get the new esid                     */
 	rldicl  r21,r21,36,28  /* Permits a full 36b of ESID           */
@@ -1104,7 +1263,7 @@
 
 	/*
 	 * Indicate that r1 contains the kernel stack and
-	 * get the Kernel TOC and CURRENT pointers from the Paca
+	 * get the Kernel TOC and CURRENT pointers from the paca
 	 */
 	mfspr	r23,SPRG3		/* Get PACA */
 	std	r22,PACAKSAVE(r23)	/* r1 is now kernel sp */
@@ -1129,7 +1288,9 @@
 	mtmsrd  r22
 	blr
 
-
+/*
+ * Kernel profiling with soft disable on iSeries
+ */
 do_profile:
 	ld	r22,8(r21)		/* Get SRR1 */
 	andi.	r22,r22,MSR_PR		/* Test if in kernel */
@@ -1163,7 +1324,7 @@
 	bl	.enable_64b_mode
 	isync
 
-	/* Set up a Paca value for this processor. */
+	/* Set up a paca value for this processor. */
 	LOADADDR(r24, paca) 		 /* Get base vaddr of Paca array  */
 	mulli	r25,r3,PACA_SIZE	 /* Calculate vaddr of right Paca */
 	add	r25,r25,r24              /* for this processor.           */
@@ -1208,7 +1369,7 @@
 	std	r4,0(r9)		/* set the naca pointer */
 
 	/* Get the pointer to the segment table */
-	ld	r6,PACA(r4)             /* Get the base Paca pointer       */
+	ld	r6,PACA(r4)             /* Get the base paca pointer       */
 	ld	r4,PACASTABVIRT(r6)
 
 	bl      .iSeries_fixup_klimit
@@ -1316,17 +1477,10 @@
 					/* executed here.                 */
 
         LOADADDR(r0, 4f)                /* Jump to the copy of this code  */
-	mtctr	r0			/* that we just made              */
+	mtctr	r0			/* that we just made/relocated    */
 	bctr
 
-4:	LOADADDR(r9,rtas)
-	sub	r9,r9,r26
-	ld	r5,RTASBASE(r9)		/* get the value of rtas->base */
-	ld	r9,RTASSIZE(r9)		/* get the value of rtas->size */
-	bl	.copy_and_flush		/* copy upto rtas->base */
-	add     r6,r6,r9		/* then skip over rtas->size bytes */
-
-	LOADADDR(r5,klimit)
+4:	LOADADDR(r5,klimit)
 	sub	r5,r5,r26
 	ld	r5,0(r5)		/* get the value of klimit */
 	sub	r5,r5,r27
@@ -1470,15 +1624,15 @@
 /*
  * This function is called after the master CPU has released the
  * secondary processors.  The execution environment is relocation off.
- * The Paca for this processor has the following fields initialized at
+ * The paca for this processor has the following fields initialized at
  * this point:
  *   1. Processor number
  *   2. Segment table pointer (virtual address)
  * On entry the following are set:
  *   r1    = stack pointer.  vaddr for iSeries, raddr (temp stack) for pSeries
  *   r24   = cpu# (in Linux terms)
- *   r25   = Paca virtual address
- *   SPRG3 = Paca virtual address
+ *   r25   = paca virtual address
+ *   SPRG3 = paca virtual address
  */
 _GLOBAL(__secondary_start)
 
@@ -1651,9 +1805,6 @@
 	addi    r2,r2,0x4000
 	sub	r2,r2,r26
 
-	/* Init naca->debug_switch so it can be used in stab & htab init.  */
-	bl	.ppcdbg_initialize
-
 	/* Get the pointer to the segment table which is used by           */
 	/* stab_initialize                                                 */
 	li	r27,0x4000
@@ -1681,11 +1832,15 @@
 	bl	.stab_initialize
 	bl	.htab_initialize
 
-	LOADADDR(r6,_SDR1)
+	addi  r3,0,0x4000     /* r3 = ptr to naca */
+	lhz   r3,PLATFORM(r3) /* r3 = platform flags */
+	cmpldi r3,PLATFORM_PSERIES
+	bne    98f
+	LOADADDR(r6,_SDR1)		/* Only if NOT LPAR */
 	sub	r6,r6,r26
 	ld	r6,0(r6)		/* get the value of _SDR1 */
 	mtspr	SDR1,r6			/* set the htab location  */
-
+98: 
 	LOADADDR(r3,.start_here_common)
 	SET_REG_TO_CONST(r4, MSR_KERNEL)
 	mtspr	SRR0,r3
@@ -1735,9 +1890,9 @@
 	addi	r8,r8,0x4000
 	std	r8,0(r9)		/* set the value of the naca ptr  */
 
-	LOADADDR(r4,naca)               /* Get Naca ptr address           */
+	LOADADDR(r4,naca)               /* Get naca ptr address           */
 	ld	r4,0(r4)                /* Get the location of the naca   */
-	ld	r4,PACA(r4)             /* Get the base Paca pointer      */
+	ld	r4,PACA(r4)             /* Get the base paca pointer      */
 	mtspr	SPRG3,r4
 
 	/* ptr to current */
@@ -1887,11 +2042,3 @@
 	.globl	stab_array
 stab_array:
         .space	4096 * (48 - 1)
-	
-/*
- * This space gets a copy of optional info passed to us by the bootstrap
- * Used to pass parameters into the kernel like root=/dev/sda1, etc.
- */
-	.globl	cmd_line
-cmd_line:
-	.space	512
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/htab.c linuxppc64_2_4/arch/ppc64/kernel/htab.c
--- linux-2.4.19/arch/ppc64/kernel/htab.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/htab.c	Mon Aug 19 09:16:30 2002
@@ -1,10 +1,11 @@
 /*
- * 
- *
  * PowerPC64 port by Mike Corrigan and Dave Engebretsen
  *   {mikejc|engebret}@us.ibm.com
  *
  *    Copyright (c) 2000 Mike Corrigan <mikejc@us.ibm.com>
+ *
+ * SMP scalability work:
+ *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
  * 
  *    Module name: htab.c
  *
@@ -18,17 +19,7 @@
  */
 
 #include <linux/config.h>
-#include <asm/processor.h>
-#include <asm/pgtable.h>
-#include <asm/mmu.h>
-#include <asm/mmu_context.h>
-#include <asm/page.h>
-#include <asm/types.h>
-#include <asm/init.h>
-#include <asm/system.h>
-#include <asm/iSeries/LparData.h>
 #include <linux/spinlock.h>
-#include <asm/ppcdebug.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
 #include <linux/proc_fs.h>
@@ -36,62 +27,74 @@
 #include <linux/sysctl.h>
 #include <linux/ctype.h>
 #include <linux/cache.h>
+
+#include <asm/ppcdebug.h>
+#include <asm/processor.h>
+#include <asm/pgtable.h>
+#include <asm/mmu.h>
+#include <asm/mmu_context.h>
+#include <asm/page.h>
+#include <asm/types.h>
 #include <asm/uaccess.h>
 #include <asm/naca.h>
-#include <asm/system.h>
 #include <asm/pmc.h>
 #include <asm/machdep.h>
 #include <asm/lmb.h>
+#include <asm/abs_addr.h>
+#include <asm/io.h>
 #include <asm/eeh.h>
-
-/* For iSeries */
+#include <asm/hvcall.h>
+#include <asm/iSeries/LparData.h>
 #include <asm/iSeries/HvCallHpt.h>
 
-/* Note:  pte   --> Linux PTE
+/*
+ * Note:  pte   --> Linux PTE
  *        HPTE  --> PowerPC Hashed Page Table Entry
+ *
+ * Execution context:
+ *   htab_initialize is called with the MMU off (of course), but
+ *   the kernel has been copied down to zero so it can directly
+ *   reference global data.  At this point it is very difficult
+ *   to print debug info.
+ *
  */
 
 HTAB htab_data = {NULL, 0, 0, 0, 0};
 
-int proc_dol2crvec(ctl_table *table, int write, struct file *filp,
-		   void *buffer, size_t *lenp);
-
-void htab_initialize(void);
-void make_pte_LPAR(HPTE *htab,
-	      unsigned long va, unsigned long pa, int mode,
-	      unsigned long hash_mask, int large);
-
-extern unsigned long reloc_offset(void);
-extern unsigned long get_kernel_vsid( unsigned long ea );
-extern void cacheable_memzero( void *, unsigned int );
-
 extern unsigned long _SDR1;
 extern unsigned long klimit;
 
-extern unsigned long _ASR;
-extern inline void make_ste(unsigned long stab,
-			    unsigned long esid, unsigned long vsid);
-
-extern char _stext[], _etext[], __start_naca[], __end_stab[];
-
-static spinlock_t hash_table_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
-
-#define PTRRELOC(x)	((typeof(x))((unsigned long)(x) - offset))
-#define PTRUNRELOC(x)	((typeof(x))((unsigned long)(x) + offset))
-#define RELOC(x)	(*PTRRELOC(&(x)))
-
-extern unsigned long htab_size( unsigned long );
-unsigned long hpte_getword0_iSeries( unsigned long slot );
+void make_pte(HPTE *htab, unsigned long va, unsigned long pa,
+	      int mode, unsigned long hash_mask, int large);
+long plpar_pte_enter(unsigned long flags,
+		     unsigned long ptex,
+		     unsigned long new_pteh, unsigned long new_ptel,
+		     unsigned long *old_pteh_ret, unsigned long *old_ptel_ret);
+static long hpte_remove(unsigned long hpte_group);
+static long rpa_lpar_hpte_remove(unsigned long hpte_group);
+static long iSeries_hpte_remove(unsigned long hpte_group);
+
+static spinlock_t pSeries_tlbie_lock = SPIN_LOCK_UNLOCKED;
+static spinlock_t pSeries_lpar_tlbie_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t hash_table_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
 
 #define KB (1024)
 #define MB (1024*KB)
+
+static inline void
+loop_forever(void)
+{
+	volatile unsigned long x = 1;
+	for(;x;x|=1)
+		;
+}
+
 static inline void
 create_pte_mapping(unsigned long start, unsigned long end,
 		   unsigned long mode, unsigned long mask, int large)
 {
-	unsigned long addr, offset = reloc_offset();
-	HTAB *_htab_data = PTRRELOC(&htab_data);
-	HPTE  *htab  = (HPTE *)__v2a(_htab_data->htab);
+	unsigned long addr;
+	HPTE *htab = (HPTE *)__v2a(htab_data.htab);
 	unsigned int step;
 
 	if (large)
@@ -102,8 +105,8 @@
 	for (addr = start; addr < end; addr += step) {
 		unsigned long vsid = get_kernel_vsid(addr);
 		unsigned long va = (vsid << 28) | (addr & 0xfffffff);
-		make_pte(htab, va, (unsigned long)__v2a(addr), mode, mask,
-				large);
+		make_pte(htab, va, (unsigned long)__v2a(addr), 
+			 mode, mask, large);
 	}
 }
 
@@ -112,16 +115,14 @@
 {
 	unsigned long table, htab_size_bytes;
 	unsigned long pteg_count;
-	unsigned long mode_ro, mode_rw, mask;
-	unsigned long offset = reloc_offset();
-	struct naca_struct *_naca = RELOC(naca);
-	HTAB *_htab_data = PTRRELOC(&htab_data);
+	unsigned long mode_rw, mask;
 
+	ppc64_boot_msg(0x05, "htab init");
 	/*
 	 * Calculate the required size of the htab.  We want the number of
 	 * PTEGs to equal one half the number of real pages.
 	 */ 
-	htab_size_bytes = 1UL << _naca->pftSize;
+	htab_size_bytes = 1UL << naca->pftSize;
 	pteg_count = htab_size_bytes >> 7;
 
 	/* For debug, make the HTAB 1/8 as big as it normally would be. */
@@ -130,335 +131,526 @@
 		htab_size_bytes = pteg_count << 7;
 	}
 
-	_htab_data->htab_num_ptegs = pteg_count;
-	_htab_data->htab_hash_mask = pteg_count - 1;
+	htab_data.htab_num_ptegs = pteg_count;
+	htab_data.htab_hash_mask = pteg_count - 1;
 
-	if(_naca->platform == PLATFORM_PSERIES) {
+	if(naca->platform == PLATFORM_PSERIES) {
 		/* Find storage for the HPT.  Must be contiguous in
 		 * the absolute address space.
 		 */
 		table = lmb_alloc(htab_size_bytes, htab_size_bytes);
-		if ( !table )
-			panic("ERROR, cannot find space for HPTE\n");
-		_htab_data->htab = (HPTE *)__a2v(table);
+		if ( !table ) {
+			ppc64_terminate_msg(0x20, "hpt space");
+			loop_forever();
+		}
+		htab_data.htab = (HPTE *)__a2v(table);
 
 		/* htab absolute addr + encoded htabsize */
-		RELOC(_SDR1) = table + __ilog2(pteg_count) - 11;
+		_SDR1 = table + __ilog2(pteg_count) - 11;
 
 		/* Initialize the HPT with no entries */
-		cacheable_memzero((void *)table, htab_size_bytes);
+		memset((void *)table, 0, htab_size_bytes);
 	} else {
-		_htab_data->htab = NULL;
-		RELOC(_SDR1) = 0; 
+		/* Using a hypervisor which owns the htab */
+		htab_data.htab = NULL;
+		_SDR1 = 0; 
 	}
 
-	mode_ro = _PAGE_ACCESSED | _PAGE_COHERENT | PP_RXRX;
 	mode_rw = _PAGE_ACCESSED | _PAGE_COHERENT | PP_RWXX;
 	mask = pteg_count-1;
 
-	/* Create PTE's for the kernel text and data sections plus
-	 * the HPT and HPTX arrays.  Make the assumption that
-	 * (addr & KERNELBASE) == 0 (ie they are disjoint).
-	 * We also assume that the va is <= 64 bits.
-	 */
-#if 0
-	create_pte_mapping((unsigned long)_stext,       (unsigned long)__start_naca,                 mode_ro, mask);
-	create_pte_mapping((unsigned long)__start_naca, (unsigned long)__end_stab,                   mode_rw, mask);
-	create_pte_mapping((unsigned long)__end_stab,   (unsigned long)_etext,                       mode_ro, mask);
-	create_pte_mapping((unsigned long)_etext,       RELOC(klimit),                               mode_rw, mask);
-	create_pte_mapping((unsigned long)__a2v(table), (unsigned long)__a2v(table+htab_size_bytes), mode_rw, mask);
-#else
-#ifndef CONFIG_PPC_ISERIES
-	if (__is_processor(PV_POWER4) && _naca->physicalMemorySize > 256*MB) {
+	/* XXX we currently map kernel text rw, should fix this */
+	if ((naca->platform & PLATFORM_PSERIES) &&
+	   cpu_has_largepage() && (naca->physicalMemorySize > 256*MB)) {
 		create_pte_mapping((unsigned long)KERNELBASE, 
 				   KERNELBASE + 256*MB, mode_rw, mask, 0);
 		create_pte_mapping((unsigned long)KERNELBASE + 256*MB, 
-				   KERNELBASE + (_naca->physicalMemorySize), 
+				   KERNELBASE + (naca->physicalMemorySize), 
 				   mode_rw, mask, 1);
-		return;
+	} else {
+		create_pte_mapping((unsigned long)KERNELBASE, 
+				   KERNELBASE+(naca->physicalMemorySize), 
+				   mode_rw, mask, 0);
 	}
-#endif
-	create_pte_mapping((unsigned long)KERNELBASE, 
-			   KERNELBASE+(_naca->physicalMemorySize), 
-			   mode_rw, mask, 0);
-#endif
+	ppc64_boot_msg(0x06, "htab done");
 }
 #undef KB
 #undef MB
 
 /*
- * Create a pte.  Used during initialization only.
+ * Create a pte. Used during initialization only.
  * We assume the PTE will fit in the primary PTEG.
  */
-void make_pte(HPTE *htab,
-	      unsigned long va, unsigned long pa, int mode,
-	      unsigned long hash_mask, int large)
+void make_pte(HPTE *htab, unsigned long va, unsigned long pa,
+	      int mode, unsigned long hash_mask, int large)
 {
-	HPTE  *hptep;
-	unsigned long hash, i;
-	volatile unsigned long x = 1;
-	unsigned long vpn;
-
-#ifdef CONFIG_PPC_PSERIES
-	if(naca->platform == PLATFORM_PSERIES_LPAR) {
-		make_pte_LPAR(htab, va, pa, mode, hash_mask, large); 
-		return;
-	}
-#endif
+	HPTE *hptep, local_hpte, rhpte;
+	unsigned long hash, vpn, flags, lpar_rc;
+	unsigned long i, dummy1, dummy2;
+	long slot;
 
 	if (large)
-		vpn = va >> 24;
+		vpn = va >> LARGE_PAGE_SHIFT;
 	else
-		vpn = va >> 12;
+		vpn = va >> PAGE_SHIFT;
 
 	hash = hpt_hash(vpn, large);
 
-	hptep  = htab +  ((hash & hash_mask)*HPTES_PER_GROUP);
-
-	for (i = 0; i < 8; ++i, ++hptep) {
-		if ( hptep->dw0.dw0.v == 0 ) {		/* !valid */
-			hptep->dw1.dword1 = pa | mode;
-			hptep->dw0.dword0 = 0;
-			hptep->dw0.dw0.avpn = va >> 23;
-			hptep->dw0.dw0.bolted = 1;	/* bolted */
-			hptep->dw0.dw0.v = 1;		/* make valid */
-			return;
+	local_hpte.dw1.dword1 = pa | mode;
+	local_hpte.dw0.dword0 = 0;
+	local_hpte.dw0.dw0.avpn = va >> 23;
+	local_hpte.dw0.dw0.bolted = 1;		/* bolted */
+	if (large) {
+		local_hpte.dw0.dw0.l = 1;	/* large page */
+		local_hpte.dw0.dw0.avpn &= ~0x1UL;
+	}
+	local_hpte.dw0.dw0.v = 1;
+
+	if (naca->platform == PLATFORM_PSERIES) {
+		hptep  = htab + ((hash & hash_mask)*HPTES_PER_GROUP);
+
+		for (i = 0; i < 8; ++i, ++hptep) {
+			if (hptep->dw0.dw0.v == 0) {		/* !valid */
+				*hptep = local_hpte;
+				return;
+			}
 		}
+	} else if (naca->platform == PLATFORM_PSERIES_LPAR) {
+		slot = ((hash & hash_mask)*HPTES_PER_GROUP);
+		
+		/* Set CEC cookie to 0                   */
+		/* Zero page = 0                         */
+		/* I-cache Invalidate = 0                */
+		/* I-cache synchronize = 0               */
+		/* Exact = 0 - modify any entry in group */
+		flags = 0;
+		
+		lpar_rc =  plpar_pte_enter(flags, slot, local_hpte.dw0.dword0,
+					   local_hpte.dw1.dword1, 
+					   &dummy1, &dummy2);
+		if (lpar_rc != H_Success) {
+			ppc64_terminate_msg(0x21, "hpte enter");
+			loop_forever();
+		}
+		return;
+	} else if (naca->platform == PLATFORM_ISERIES_LPAR) {
+		slot = HvCallHpt_findValid(&rhpte, vpn);
+		if (slot < 0) {
+			/* Must find space in primary group */
+			panic("hash_page: hpte already exists\n");
+		}
+		HvCallHpt_addValidate(slot, 0, (HPTE *)&local_hpte );
+		return;
 	}
 
 	/* We should _never_ get here and too early to call xmon. */
-	for(;x;x|=1);
+	ppc64_terminate_msg(0x22, "hpte platform");
+	loop_forever();
+}
+
+/*
+ * find_linux_pte returns the address of a linux pte for a given 
+ * effective address and directory.  If not found, it returns zero.
+ */
+pte_t *find_linux_pte(pgd_t *pgdir, unsigned long ea)
+{
+	pgd_t *pg;
+	pmd_t *pm;
+	pte_t *pt = NULL;
+	pte_t pte;
+
+	pg = pgdir + pgd_index(ea);
+	if (!pgd_none(*pg)) {
+		pm = pmd_offset(pg, ea);
+		if (!pmd_none(*pm)) { 
+			pt = pte_offset(pm, ea);
+			pte = *pt;
+			if (!pte_present(pte))
+				pt = NULL;
+		}
+	}
+
+	return pt;
 }
 
-/* Functions to invalidate a HPTE */
-static void hpte_invalidate_iSeries( unsigned long slot )
+static inline unsigned long computeHptePP(unsigned long pte)
 {
-	HvCallHpt_invalidateSetSwBitsGet( slot, 0, 0 );
+	return (pte & _PAGE_USER) |
+		(((pte & _PAGE_USER) >> 1) &
+		 ((~((pte >> 2) &	/* _PAGE_RW */
+		     (pte >> 7))) &	/* _PAGE_DIRTY */
+		  1));
 }
 
-static void hpte_invalidate_pSeries( unsigned long slot )
+/*
+ * Handle a fault by adding an HPTE. If the address can't be determined
+ * to be valid via Linux page tables, return 1. If handled return 0
+ */
+int __hash_page(unsigned long ea, unsigned long access, 
+		unsigned long vsid, pte_t *ptep)
 {
-	/* Local copy of the first doubleword of the HPTE */
-	union {
-		unsigned long d;
-		Hpte_dword0   h;
-	} hpte_dw0;
+	unsigned long va, vpn;
+	unsigned long newpp, prpn;
+	unsigned long hpteflags;
+	long slot;
+	pte_t old_pte, new_pte;
 
-	/* Locate the HPTE */
-	HPTE  * hptep  = htab_data.htab  + slot;
+	/* Search the Linux page table for a match with va */
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	vpn = va >> PAGE_SHIFT;
 
-	/* Get the first doubleword of the HPTE */
-	hpte_dw0.d = hptep->dw0.dword0;
+	/* Acquire the hash table lock to guarantee that the linux
+	 * pte we fetch will not change
+	 */
+	spin_lock( &hash_table_lock );
+	
+	/* 
+	 * Check the user's access rights to the page.  If access should be
+	 * prevented then send the problem up to do_page_fault.
+	 */
+	access |= _PAGE_PRESENT;
+	if (unlikely(access & ~(pte_val(*ptep)))) {
+		spin_unlock( &hash_table_lock );
+		return 1;
+	}
 
-	/* Invalidate the hpte */
-	hptep->dw0.dword0 = 0;
+	/* 
+	 * We have found a pte (which was present).
+	 * The spinlocks prevent this status from changing
+	 * The hash_table_lock prevents the _PAGE_HASHPTE status
+	 * from changing (RPN, DIRTY and ACCESSED too)
+	 * The page_table_lock prevents the pte from being 
+	 * invalidated or modified
+	 */
 
-	/* Invalidate the tlb   */
-	{
-		unsigned long vsid, group, pi, pi_high;
-
-		vsid = hpte_dw0.h.avpn >> 5;
-		group = slot >> 3;
-		if(hpte_dw0.h.h) {
-			group = ~group;
-		} 
-		pi = (vsid ^ group) & 0x7ff;
-		pi_high = (hpte_dw0.h.avpn & 0x1f) << 11;
-		pi |= pi_high;
-		_tlbie(pi << 12);
-	}
-}
+	/*
+	 * At this point, we have a pte (old_pte) which can be used to build
+	 * or update an HPTE. There are 2 cases:
+	 *
+	 * 1. There is a valid (present) pte with no associated HPTE (this is 
+	 *	the most common case)
+	 * 2. There is a valid (present) pte with an associated HPTE. The
+	 *	current values of the pp bits in the HPTE prevent access
+	 *	because we are doing software DIRTY bit management and the
+	 *	page is currently not DIRTY. 
+	 */
 
+	old_pte = *ptep;
+	new_pte = old_pte;
 
-/* Select an available HPT slot for a new HPTE
- *   return slot index (if in primary group)
- *   return -slot index (if in secondary group) 
- */
-static long hpte_selectslot_iSeries( unsigned long vpn )
-{
-	HPTE hpte;
-	long ret_slot, orig_slot;
-	unsigned long primary_hash;
-	unsigned long hpteg_slot;
-	unsigned long slot;
-	unsigned i, k;
-	union {
-		unsigned long	d;
-		Hpte_dword0	h;
-	} hpte_dw0;
+	/* If the attempted access was a store */
+	if (access & _PAGE_RW)
+		pte_val(new_pte) |= _PAGE_ACCESSED | _PAGE_DIRTY;
+	else
+		pte_val(new_pte) |= _PAGE_ACCESSED;
+
+	newpp = computeHptePP(pte_val(new_pte));
+	
+	/* Check if pte already has an hpte (case 2) */
+	if (unlikely(pte_val(old_pte) & _PAGE_HASHPTE)) {
+		/* There MIGHT be an HPTE for this pte */
+		unsigned long hash, slot, secondary;
+
+		/* XXX fix large pte flag */
+		hash = hpt_hash(vpn, 0);
+		secondary = (pte_val(old_pte) & _PAGE_SECONDARY) >> 15;
+		if (secondary)
+			hash = ~hash;
+		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
+		slot += (pte_val(old_pte) & _PAGE_GROUP_IX) >> 12;
 
-	ret_slot = orig_slot = HvCallHpt_findValid( &hpte, vpn );
-	if ( hpte.dw0.dw0.v ) {		/* If valid ...what do we do now? */
-		udbg_printf( "hpte_selectslot_iSeries: vpn 0x%016lx already valid at slot 0x%016lx\n", vpn, ret_slot );
-		udbg_printf( "hpte_selectslot_iSeries: returned hpte 0x%016lx 0x%016lx\n", hpte.dw0.dword0, hpte.dw1.dword1 );
-
-		return (0x8000000000000000); 
-		/*			panic("select_hpte_slot found entry already valid\n"); */
-	}
-	if ( ret_slot == -1 ) {		/* -1 indicates no available slots */
-
-		/* No available entry found in secondary group */
-
-		PMC_SW_SYSTEM(htab_capacity_castouts);
-
-		primary_hash = hpt_hash(vpn, 0);
-		hpteg_slot = ( primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-		k = htab_data.next_round_robin++ & 0x7;
-
-		for ( i=0; i<HPTES_PER_GROUP; ++i ) {
-			if ( k == HPTES_PER_GROUP )
-				k = 0;
-			slot = hpteg_slot + k;
-			hpte_dw0.d = hpte_getword0_iSeries( slot );
-			if ( !hpte_dw0.h.bolted ) {
-				hpte_invalidate_iSeries( slot );
-				ret_slot = slot;
+		/* XXX fix large pte flag */
+		if (ppc_md.hpte_updatepp(slot, secondary, 
+					 newpp, va, 0) == -1) {
+			pte_val(old_pte) &= ~_PAGE_HPTEFLAGS;
+		} else {
+			if (!pte_same(old_pte, new_pte)) {
+				*ptep = new_pte;
 			}
-			++k;
-		}
-	} else {
-		if ( ret_slot < 0 ) {
-			PMC_SW_SYSTEM(htab_primary_overflows);
-			ret_slot &= 0x7fffffffffffffff;
-			ret_slot = -ret_slot;
 		}
 	}
-	if ( ret_slot == -1 ) {
-		/* No non-bolted entry found in primary group - time to panic */
-        	udbg_printf("hpte_selectslot_pSeries - No non-bolted HPTE in group 0x%lx! \n", hpteg_slot/HPTES_PER_GROUP);
-        	panic("No non-bolted HPTE in group %lx", (unsigned long)hpteg_slot/HPTES_PER_GROUP);
+
+	if (likely(!(pte_val(old_pte) & _PAGE_HASHPTE))) {
+		/* Update the linux pte with the HPTE slot */
+		pte_val(new_pte) &= ~_PAGE_HPTEFLAGS;
+		pte_val(new_pte) |= _PAGE_HASHPTE;
+		prpn = pte_val(old_pte) >> PTE_SHIFT;
+
+		/* copy appropriate flags from linux pte */
+		hpteflags = (pte_val(new_pte) & 0x1f8) | newpp;
+
+		slot = ppc_md.hpte_insert(vpn, prpn, hpteflags, 0, 0);
+
+		pte_val(new_pte) |= ((slot<<12) & 
+				     (_PAGE_GROUP_IX | _PAGE_SECONDARY));
+
+		*ptep = new_pte;
 	}
-	PPCDBG(PPCDBG_MM, "hpte_selectslot_iSeries: vpn=0x%016lx, orig_slot=0x%016lx, ret_slot=0x%016lx \n",
-	       vpn, orig_slot, ret_slot );	
-	return ret_slot;
+
+	spin_unlock(&hash_table_lock);
+
+	return 0;
 }
 
-static long hpte_selectslot_pSeries(unsigned long vpn)
+/*
+ * Handle a fault by adding an HPTE. If the address can't be determined
+ * to be valid via Linux page tables, return 1. If handled return 0
+ */
+int hash_page(unsigned long ea, unsigned long access)
 {
-	HPTE * hptep;
-	unsigned long primary_hash;
-	unsigned long hpteg_slot;
-	unsigned i, k;
+	void *pgdir;
+	unsigned long vsid;
+	struct mm_struct *mm;
+	pte_t *ptep;
+	int ret;
 
-	/* Search the primary group for an available slot */
+	/* Check for invalid addresses. */
+	if (!IS_VALID_EA(ea)) return 1;
 
-	primary_hash = hpt_hash(vpn, 0);
-	hpteg_slot = ( primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-	hptep = htab_data.htab + hpteg_slot;
-	
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		if ( hptep->dw0.dw0.v == 0 ) {
-			/* If an available slot found, return it */
-			return hpteg_slot + i;
-		}
-		hptep++;
+ 	switch (REGION_ID(ea)) {
+	case USER_REGION_ID:
+		mm = current->mm;
+		if (mm == NULL) return 1;
+		vsid = get_vsid(mm->context, ea);
+		break;
+	case IO_REGION_ID:
+		mm = &ioremap_mm;
+		vsid = get_kernel_vsid(ea);
+		break;
+	case VMALLOC_REGION_ID:
+		mm = &init_mm;
+		vsid = get_kernel_vsid(ea);
+		break;
+	case IO_UNMAPPED_REGION_ID:
+		udbg_printf("EEH Error ea = 0x%lx\n", ea);
+		PPCDBG_ENTER_DEBUGGER();
+		panic("EEH Error ea = 0x%lx\n", ea);
+		break;
+	case KERNEL_REGION_ID:
+		/*
+		 * As htab_initialize is now, we shouldn't ever get here since
+		 * we're bolting the entire 0xC0... region.
+		 */
+		udbg_printf("Little faulted on kernel address 0x%lx\n", ea);
+		PPCDBG_ENTER_DEBUGGER();
+		panic("Little faulted on kernel address 0x%lx\n", ea);
+		break;
+	default:
+		/* Not a valid range, send the problem up to do_page_fault */
+		return 1;
+		break;
 	}
 
-	/* No available entry found in primary group */
-
-	PMC_SW_SYSTEM(htab_primary_overflows);
-
-	/* Search the secondary group */
+	pgdir = mm->pgd;
+	if (pgdir == NULL) return 1;
 
-	hpteg_slot = ( ~primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-	hptep = htab_data.htab + hpteg_slot;
+	/*
+	 * Lock the Linux page table to prevent mmap and kswapd
+	 * from modifying entries while we search and update
+	 */
+	spin_lock(&mm->page_table_lock);
 
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		if ( hptep->dw0.dw0.v == 0 ) {
-			/* If an available slot found, return it */
-			return -(hpteg_slot + i);
-		}
-		hptep++;
+	ptep = find_linux_pte(pgdir, ea);
+	/*
+	 * If no pte found or not present, send the problem up to
+	 * do_page_fault
+	 */
+	if (ptep && pte_present(*ptep)) {
+		ret = __hash_page(ea, access, vsid, ptep);
+	} else {	
+		/* If no pte, send the problem up to do_page_fault */
+		ret = 1;
 	}
 
-	/* No available entry found in secondary group */
+	spin_unlock(&mm->page_table_lock);
+
+	return ret;
+}
 
-	PMC_SW_SYSTEM(htab_capacity_castouts);
+void flush_hash_page(unsigned long context, unsigned long ea, pte_t *ptep)
+{
+	unsigned long vsid, vpn, va, hash, secondary, slot, flags;
+	unsigned long large = 0, local = 0;
+	pte_t pte;
 
-	/* Select an entry in the primary group to replace */
+	if ((ea >= USER_START) && (ea <= USER_END))
+		vsid = get_vsid(context, ea);
+	else
+		vsid = get_kernel_vsid(ea);
 
-	hpteg_slot = ( primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-	hptep = htab_data.htab + hpteg_slot;
-	k = htab_data.next_round_robin++ & 0x7;
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
+	hash = hpt_hash(vpn, large);
 
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		if (k == HPTES_PER_GROUP)
-			k = 0;
+	spin_lock_irqsave( &hash_table_lock, flags);
 
-		if (!hptep[k].dw0.dw0.bolted) {
-			hpteg_slot += k;
-			/* Invalidate the current entry */
-			ppc_md.hpte_invalidate(hpteg_slot); 
-			return hpteg_slot;
-		}
-		++k;
+	pte = __pte(pte_update(ptep, _PAGE_HPTEFLAGS, 0));
+	secondary = (pte_val(pte) & _PAGE_SECONDARY) >> 15;
+	if (secondary) hash = ~hash;
+	slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
+	slot += (pte_val(pte) & _PAGE_GROUP_IX) >> 12;
+	
+	if (pte_val(pte) & _PAGE_HASHPTE) {
+		ppc_md.hpte_invalidate(slot, secondary, va, large, local);
 	}
 
-	/* No non-bolted entry found in primary group - time to panic */
-        udbg_printf("hpte_selectslot_pSeries - No non-bolted HPTE in group 0x%lx! \n", hpteg_slot/HPTES_PER_GROUP);
-	/*      xmon(0); */
-        panic("No non-bolted HPTE in group %lx", (unsigned long)hpteg_slot/HPTES_PER_GROUP);
+	spin_unlock_irqrestore( &hash_table_lock, flags );
+}
 
-	/* keep the compiler happy */
-	return 0;
+long plpar_pte_enter(unsigned long flags,
+		     unsigned long ptex,
+		     unsigned long new_pteh, unsigned long new_ptel,
+		     unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
+{
+	unsigned long dummy, ret;
+	ret = plpar_hcall(H_ENTER, flags, ptex, new_pteh, new_ptel,
+			   old_pteh_ret, old_ptel_ret, &dummy);
+	return(ret);
+}
+
+long plpar_pte_remove(unsigned long flags,
+		      unsigned long ptex,
+		      unsigned long avpn,
+		      unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
+{
+	unsigned long dummy;
+	return plpar_hcall(H_REMOVE, flags, ptex, avpn, 0,
+			   old_pteh_ret, old_ptel_ret, &dummy);
+}
+
+long plpar_pte_read(unsigned long flags,
+		    unsigned long ptex,
+		    unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
+{
+	unsigned long dummy;
+	return plpar_hcall(H_READ, flags, ptex, 0, 0,
+			   old_pteh_ret, old_ptel_ret, &dummy);
+}
+
+long plpar_pte_protect(unsigned long flags,
+		       unsigned long ptex,
+		       unsigned long avpn)
+{
+	return plpar_hcall_norets(H_PROTECT, flags, ptex, avpn);
+}
+
+static __inline__ void set_pp_bit(unsigned long pp, HPTE *addr)
+{
+	unsigned long old;
+	unsigned long *p = &addr->dw1.dword1;
+
+	__asm__ __volatile__(
+        "1:	ldarx	%0,0,%3\n\
+                rldimi  %0,%2,0,62\n\
+                stdcx.	%0,0,%3\n\
+            	bne	1b"
+        : "=&r" (old), "=m" (*p)
+        : "r" (pp), "r" (p), "m" (*p)
+        : "cc");
 }
 
-unsigned long hpte_getword0_iSeries( unsigned long slot )
+/*
+ * Functions used to retrieve word 0 of a given page table entry.
+ *
+ * Input : slot : PTE index within the page table of the entry to retrieve 
+ * Output: Contents of word 0 of the specified entry
+ */
+static unsigned long rpa_lpar_hpte_getword0(unsigned long slot)
 {
 	unsigned long dword0;
+	unsigned long lpar_rc;
+	unsigned long dummy_word1;
+	unsigned long flags;
+
+	/* Read 1 pte at a time                        */
+	/* Do not need RPN to logical page translation */
+	/* No cross CEC PFT access                     */
+	flags = 0;
+	
+	lpar_rc = plpar_pte_read(flags, slot, &dword0, &dummy_word1);
 
-	HPTE hpte;
-	HvCallHpt_get( &hpte, slot );
-	dword0 = hpte.dw0.dword0;
+	if (lpar_rc != H_Success)
+		panic("Error on pte read in get_hpte0 rc = %lx\n", lpar_rc);
 
 	return dword0;
 }
 
-unsigned long hpte_getword0_pSeries( unsigned long slot )
+unsigned long iSeries_hpte_getword0(unsigned long slot)
 {
 	unsigned long dword0;
-	HPTE * hptep = htab_data.htab + slot;
 
-	dword0 = hptep->dw0.dword0;
+	HPTE hpte;
+	HvCallHpt_get(&hpte, slot);
+	dword0 = hpte.dw0.dword0;
+
 	return dword0;
 }
 
-static long hpte_find_iSeries(unsigned long vpn)
+/*
+ * Functions used to find the PTE for a particular virtual address. 
+ * Only used during boot when bolting pages.
+ *
+ * Input : vpn      : virtual page number
+ * Output: PTE index within the page table of the entry
+ *         -1 on failure
+ */
+static long hpte_find(unsigned long vpn)
 {
-	HPTE hpte;
+	HPTE *hptep;
+	unsigned long hash;
+	unsigned long i, j;
 	long slot;
+	Hpte_dword0 dw0;
 
-	slot = HvCallHpt_findValid( &hpte, vpn );
-	if ( hpte.dw0.dw0.v ) {
-		if ( slot < 0 ) {
-			slot &= 0x7fffffffffffffff;
-			slot = -slot;
+	hash = hpt_hash(vpn, 0);
+
+	for (j = 0; j < 2; j++) {
+		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
+		for (i = 0; i < HPTES_PER_GROUP; i++) {
+			hptep = htab_data.htab + slot;
+			dw0 = hptep->dw0.dw0;
+
+			if ((dw0.avpn == (vpn >> 11)) && dw0.v &&
+			    (dw0.h == j)) {
+				/* HPTE matches */
+				if (j)
+					slot = -slot;
+				return slot;
+			}
+			++slot;
 		}
-	} else
-		slot = -1;
-	return slot;
+		hash = ~hash;
+	}
+
+	return -1;
 }
 
-static long hpte_find_pSeries(unsigned long vpn)
+static long rpa_lpar_hpte_find(unsigned long vpn)
 {
+	unsigned long hash;
+	unsigned long i, j;
+	long slot;
 	union {
-		unsigned long d;
-		Hpte_dword0   h;
+		unsigned long dword0;
+		Hpte_dword0 dw0;
 	} hpte_dw0;
-	long slot;
-	unsigned long hash;
-	unsigned long i,j;
+	Hpte_dword0 dw0;
 
 	hash = hpt_hash(vpn, 0);
-	for ( j=0; j<2; ++j ) {
+
+	for (j = 0; j < 2; j++) {
 		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-		for ( i=0; i<HPTES_PER_GROUP; ++i ) {
-			hpte_dw0.d = hpte_getword0_pSeries( slot );
-			if ( ( hpte_dw0.h.avpn == ( vpn >> 11 ) ) &&
-			     ( hpte_dw0.h.v ) &&
-			     ( hpte_dw0.h.h == j ) ) {
+		for (i = 0; i < HPTES_PER_GROUP; i++) {
+			hpte_dw0.dword0 = rpa_lpar_hpte_getword0(slot);
+			dw0 = hpte_dw0.dw0;
+
+			if ((dw0.avpn == (vpn >> 11)) && dw0.v &&
+			    (dw0.h == j)) {
 				/* HPTE matches */
-				if ( j )
+				if (j)
 					slot = -slot;
 				return slot;
 			}
@@ -466,247 +658,286 @@
 		}
 		hash = ~hash;
 	}
+
 	return -1;
 } 
 
-/* This function is called by iSeries setup when initializing the hpt */
-void build_valid_hpte( unsigned long vsid, unsigned long ea, unsigned long pa,
-		       pte_t * ptep, unsigned hpteflags, unsigned bolted )
-{
-	unsigned long vpn, flags;
-	long hpte_slot;
-	unsigned hash;
-	pte_t pte;
-
-	vpn = ((vsid << 28) | ( ea & 0xffff000 )) >> 12;
-
-	spin_lock_irqsave( &hash_table_lock, flags );
+static long iSeries_hpte_find(unsigned long vpn)
+{
+	HPTE hpte;
+	long slot;
 
-	hpte_slot = ppc_md.hpte_selectslot( vpn );
-	hash = 0;
-	if ( hpte_slot < 0 ) {
-		if ( hpte_slot == 0x8000000000000000 ) {
-			udbg_printf("hash_page: ptep    = 0x%016lx\n", 
-				    (unsigned long)ptep );
-			udbg_printf("hash_page: ea      = 0x%016lx\n", ea );
-			udbg_printf("hash_page: vpn     = 0x%016lx\n", vpn );
-               
-			panic("hash_page: hpte already exists\n");
+	/*
+	 * The HvCallHpt_findValid interface is as follows:
+	 * 0xffffffffffffffff : No entry found.
+	 * 0x00000000xxxxxxxx : Entry found in primary group, slot x
+	 * 0x80000000xxxxxxxx : Entry found in secondary group, slot x
+	 */
+	slot = HvCallHpt_findValid(&hpte, vpn); 
+	if (hpte.dw0.dw0.v) {
+		if (slot < 0) {
+			slot &= 0x7fffffffffffffff;
+			slot = -slot;
 		}
-		hash = 1;
-		hpte_slot = -hpte_slot;
+	} else {
+		slot = -1;
 	}
-	ppc_md.hpte_create_valid( hpte_slot, vpn, pa >> 12, hash, ptep,
-				  hpteflags, bolted );
 
-	if ( ptep ) {
-		/* Get existing pte flags */
-		pte = *ptep;
-		pte_val(pte) &= ~_PAGE_HPTEFLAGS;
-
-		/* Add in the has hpte flag */
-		pte_val(pte) |= _PAGE_HASHPTE;
-
-		/* Add in the _PAGE_SECONDARY flag */
-		pte_val(pte) |= hash << 15;
-
-		/* Add in the hpte slot */
-		pte_val(pte) |= (hpte_slot << 12) & _PAGE_GROUP_IX;
-               
-		/* Save the new pte.  */
-		*ptep = pte;
-               
-	}
-	spin_unlock_irqrestore( &hash_table_lock, flags );
+	return slot;
 }
 
-
-/* Create an HPTE and validate it
- *   It is assumed that the HPT slot currently is invalid.
- *   The HPTE is set with the vpn, rpn (converted to absolute)
- *   and flags
+/*
+ * Functions used to invalidate a page table entry from the page table
+ * and tlb.
+ *
+ * Input : slot  : PTE index within the page table of the entry to invalidated
+ *         va    : Virtual address of the entry being invalidated
+ *         large : 1 = large page (16M)
+ *         local : 1 = Use tlbiel to only invalidate the local tlb 
  */
-static void hpte_create_valid_iSeries(unsigned long slot, unsigned long vpn,
-				      unsigned long prpn, unsigned hash, 
-				      void * ptep, unsigned hpteflags, 
-				      unsigned bolted )
-{
-	/* Local copy of HPTE */
-	struct {
-		/* Local copy of first doubleword of HPTE */
-		union {
-			unsigned long d;
-			Hpte_dword0   h;
-		} dw0;
-		/* Local copy of second doubleword of HPTE */
-		union {
-			unsigned long     d;
-			Hpte_dword1       h;
-			Hpte_dword1_flags f;
-		} dw1;
-	} lhpte;
-	
-	unsigned long avpn = vpn >> 11;
-	unsigned long arpn = physRpn_to_absRpn( prpn );
-
-	/* Fill in the local HPTE with absolute rpn, avpn and flags */
-	lhpte.dw1.d        = 0;
-	lhpte.dw1.h.rpn    = arpn;
-	lhpte.dw1.f.flags  = hpteflags;
-
-	lhpte.dw0.d        = 0;
-	lhpte.dw0.h.avpn   = avpn;
-	lhpte.dw0.h.h      = hash;
-	lhpte.dw0.h.bolted = bolted;
-	lhpte.dw0.h.v      = 1;
+static void hpte_invalidate(unsigned long slot, 
+			    unsigned long secondary,
+			    unsigned long va,
+			    int large, int local)
+{
+	HPTE *hptep = htab_data.htab + slot;
+	Hpte_dword0 dw0;
+	unsigned long vpn, avpn;
+	unsigned long flags;
 
-	/* Now fill in the actual HPTE */
-	HvCallHpt_addValidate( slot, hash, (HPTE *)&lhpte );
-}
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
 
-static void hpte_create_valid_pSeries(unsigned long slot, unsigned long vpn,
-				      unsigned long prpn, unsigned hash, 
-				      void * ptep, unsigned hpteflags, 
-				      unsigned bolted)
-{
-	/* Local copy of HPTE */
-	struct {
-		/* Local copy of first doubleword of HPTE */
-		union {
-			unsigned long d;
-			Hpte_dword0   h;
-		} dw0;
-		/* Local copy of second doubleword of HPTE */
-		union {
-			unsigned long     d;
-			Hpte_dword1       h;
-			Hpte_dword1_flags f;
-		} dw1;
-	} lhpte;
-	
-	unsigned long avpn = vpn >> 11;
-	unsigned long arpn = physRpn_to_absRpn( prpn );
+	avpn = vpn >> 11;
 
-	HPTE *hptep;
+	dw0 = hptep->dw0.dw0;
 
-	/* Fill in the local HPTE with absolute rpn, avpn and flags */
-	lhpte.dw1.d        = 0;
-	lhpte.dw1.h.rpn    = arpn;
-	lhpte.dw1.f.flags  = hpteflags;
-
-	lhpte.dw0.d        = 0;
-	lhpte.dw0.h.avpn   = avpn;
-	lhpte.dw0.h.h      = hash;
-	lhpte.dw0.h.bolted = bolted;
-	lhpte.dw0.h.v      = 1;
+	/*
+	 * Do not remove bolted entries.  Alternatively, we could check
+	 * the AVPN, hash group, and valid bits.  By doing it this way,
+	 * it is common with the pSeries LPAR optimal path.
+	 */
+	if (dw0.bolted) return;
 
-	/* Now fill in the actual HPTE */
-	hptep  = htab_data.htab  + slot;
+	/* Invalidate the hpte. */
+	hptep->dw0.dword0 = 0;
 
-	/* Set the second dword first so that the valid bit
-	 * is the last thing set
+	/* Invalidate the tlb */
+	spin_lock_irqsave(&pSeries_tlbie_lock, flags);
+	_tlbie(va, large);
+	spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
+}
+
+static void rpa_lpar_hpte_invalidate(unsigned long slot, 
+				     unsigned long secondary,
+				     unsigned long va,
+				     int large, int local)
+{
+	unsigned long lpar_rc;
+	unsigned long dummy1, dummy2;
+
+	/* 
+	 * Don't remove a bolted entry.  This case can occur when we bolt
+	 * pages dynamically after initial boot.
 	 */
-	
-	hptep->dw1.dword1 = lhpte.dw1.d;
+	lpar_rc = plpar_pte_remove(H_ANDCOND, slot, (0x1UL << 4), 
+				   &dummy1, &dummy2);
 
-	/* Guarantee the second dword is visible before
-	 * the valid bit
-	 */
-	
-	__asm__ __volatile__ ("eieio" : : : "memory");
+	if (lpar_rc != H_Success)
+		panic("Bad return code from invalidate rc = %lx\n", lpar_rc);
+}
 
-	/* Now set the first dword including the valid bit */
-	hptep->dw0.dword0 = lhpte.dw0.d;
+static void iSeries_hpte_invalidate(unsigned long slot, 
+				    unsigned long secondary,
+				    unsigned long va,
+				    int large, int local)
+{
+	HPTE lhpte;
+	unsigned long vpn, avpn;
 
-	__asm__ __volatile__ ("ptesync" : : : "memory");
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
+
+	avpn = vpn >> 11;
+
+	lhpte.dw0.dword0 = iSeries_hpte_getword0(slot);
+	
+	if ((lhpte.dw0.dw0.avpn == avpn) && 
+	    (lhpte.dw0.dw0.v) &&
+	    (lhpte.dw0.dw0.h == secondary)) {
+		HvCallHpt_invalidateSetSwBitsGet(slot, 0, 0);
+	}
 }
 
-/* find_linux_pte returns the address of a linux pte for a given 
- * effective address and directory.  If not found, it returns zero.
+/*
+ * Functions used to update page protection bits.
+ *
+ * Input : slot  : PTE index within the page table of the entry to update
+ *         newpp : new page protection bits
+ *         va    : Virtual address of the entry being updated
+ *         large : 1 = large page (16M)
+ * Output: 0 on success, -1 on failure
  */
+static long hpte_updatepp(unsigned long slot, 
+			  unsigned long secondary,
+			  unsigned long newpp,
+			  unsigned long va, int large)
+{
+	HPTE *hptep = htab_data.htab + slot;
+	Hpte_dword0 dw0;
+	Hpte_dword1 dw1;
+	unsigned long vpn, avpn;
+	unsigned long flags;
 
-pte_t  * find_linux_pte( pgd_t * pgdir, unsigned long ea )
-{
-	pgd_t *pg;
-	pmd_t *pm;
-	pte_t *pt = NULL;
-	pte_t pte;
-	pg = pgdir + pgd_index( ea );
-	if ( ! pgd_none( *pg ) ) {
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
 
-		pm = pmd_offset( pg, ea );
-		if ( ! pmd_none( *pm ) ) { 
-			pt = pte_offset( pm, ea );
-			pte = *pt;
-			if ( ! pte_present( pte ) )
-				pt = NULL;
-		}
-	}
+	avpn = vpn >> 11;
 
-	return pt;
+	dw0 = hptep->dw0.dw0;
+	if ((dw0.avpn == avpn) && 
+	    (dw0.v) && (dw0.h == secondary)) {
+		/* Turn off valid bit in HPTE */
+		dw0.v = 0;
+		hptep->dw0.dw0 = dw0;
+		
+		/* Ensure it is out of the tlb too */
+		spin_lock_irqsave(&pSeries_tlbie_lock, flags);
+		_tlbie(va, large);
+		spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
+		
+		/* Insert the new pp bits into the HPTE */
+		dw1 = hptep->dw1.dw1;
+		dw1.pp = newpp;
+		hptep->dw1.dw1 = dw1;
+		
+		/* Ensure it is visible before validating */
+		__asm__ __volatile__ ("eieio" : : : "memory");
+		
+		/* Turn the valid bit back on in HPTE */
+		dw0.v = 1;
+		hptep->dw0.dw0 = dw0;
+		
+		__asm__ __volatile__ ("ptesync" : : : "memory");
+		
+		return 0;
+	}
 
+	return -1;
 }
 
-static inline unsigned long computeHptePP( unsigned long pte )
-{
-	return (     pte & _PAGE_USER )           |
-		( ( ( pte & _PAGE_USER )    >> 1 ) &
-		  ( ( ~( ( pte >> 2 ) &		/* _PAGE_RW */
-		         ( pte >> 7 ) ) ) &     /* _PAGE_DIRTY */
-			 1 ) );
+static long rpa_lpar_hpte_updatepp(unsigned long slot, 
+				   unsigned long secondary,
+				   unsigned long newpp,
+				   unsigned long va, int large)
+{
+	unsigned long lpar_rc;
+	unsigned long flags = (newpp & 7);
+	unsigned long avpn = va >> 23;
+	HPTE hpte;
+
+	lpar_rc = plpar_pte_read(0, slot, &hpte.dw0.dword0, &hpte.dw1.dword1);
+
+	if ((hpte.dw0.dw0.avpn == avpn) &&
+	    (hpte.dw0.dw0.v) && 
+	    (hpte.dw0.dw0.h == secondary)) {
+		lpar_rc = plpar_pte_protect(flags, slot, 0);
+		if (lpar_rc != H_Success)
+			panic("bad return code from pte protect rc = %lx\n", 
+			      lpar_rc);
+		return 0;
+	}
+
+	return -1;
 }
 
-static void hpte_updatepp_iSeries(long slot, unsigned long newpp, unsigned long va)
+static long iSeries_hpte_updatepp(unsigned long slot, 
+				  unsigned long secondary,
+				  unsigned long newpp, 
+				  unsigned long va, int large)
 {
-	HvCallHpt_setPp( slot, newpp );
+	unsigned long vpn, avpn;
+	HPTE hpte;
+
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
+
+	avpn = vpn >> 11;
+
+	HvCallHpt_get(&hpte, slot);
+	if ((hpte.dw0.dw0.avpn == avpn) && 
+	    (hpte.dw0.dw0.v) &&
+	    (hpte.dw0.dw0.h == secondary)) {
+		HvCallHpt_setPp(slot, newpp);
+		return 0;
+	}
+	return -1;
 }
 
-static void hpte_updatepp_pSeries(long slot, unsigned long newpp, unsigned long va)
+/*
+ * Functions used to update the page protection bits. Intended to be used 
+ * to create guard pages for kernel data structures on pages which are bolted
+ * in the HPT. Assumes pages being operated on will not be stolen.
+ * Does not work on large pages. No need to lock here because we are the 
+ * only user.
+ * 
+ * Input : newpp : page protection flags
+ *         ea    : effective kernel address to bolt.
+ */
+static void hpte_updateboltedpp(unsigned long newpp, unsigned long ea)
 {
-	/* Local copy of first doubleword of HPTE */
-	union {
-		unsigned long d;
-		Hpte_dword0   h;
-	} hpte_dw0;
-	
-	/* Local copy of second doubleword of HPTE */
-	union {
-		unsigned long     d;
-		Hpte_dword1       h;
-		Hpte_dword1_flags f;
-	} hpte_dw1;	
-
-	HPTE *  hptep  = htab_data.htab  + slot;
-
-	/* Turn off valid bit in HPTE */
-	hpte_dw0.d = hptep->dw0.dword0;
-	hpte_dw0.h.v = 0;
-	hptep->dw0.dword0 = hpte_dw0.d;
+	unsigned long vsid, va, vpn, flags;
+	long slot;
+	HPTE *hptep;
+
+	vsid = get_kernel_vsid(ea);
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	vpn = va >> PAGE_SHIFT;
+
+	slot = hpte_find(vpn);
+	if (slot == -1)
+		panic("could not find page to bolt\n");
+	hptep = htab_data.htab + slot;
+
+	set_pp_bit(newpp, hptep);
 
 	/* Ensure it is out of the tlb too */
-	_tlbie( va );
+	spin_lock_irqsave(&pSeries_tlbie_lock, flags);
+	_tlbie(va, 0);
+	spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
+}
 
-	/* Insert the new pp bits into the HPTE */
-	hpte_dw1.d = hptep->dw1.dword1;
-	hpte_dw1.h.pp = newpp;
-	hptep->dw1.dword1 = hpte_dw1.d;
+static void rpa_lpar_hpte_updateboltedpp(unsigned long newpp, unsigned long ea)
+{
+	unsigned long lpar_rc;
+	unsigned long vsid, va, vpn, flags;
+	long slot;
 
-	/* Ensure it is visible before validating */
-	__asm__ __volatile__ ("eieio" : : : "memory");
+	vsid = get_kernel_vsid(ea);
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	vpn = va >> PAGE_SHIFT;
 
-	/* Turn the valid bit back on in HPTE */
-	hpte_dw0.h.v = 1;
-	hptep->dw0.dword0 = hpte_dw0.d;
+	slot = rpa_lpar_hpte_find(vpn);
+	if (slot == -1)
+		panic("updateboltedpp: Could not find page to bolt\n");
 
-	__asm__ __volatile__ ("ptesync" : : : "memory");
+	flags = newpp & 3;
+	lpar_rc = plpar_pte_protect(flags, slot, 0);
+
+	if (lpar_rc != H_Success)
+		panic("Bad return code from pte bolted protect rc = %lx\n",
+		      lpar_rc); 
 }
 
-/*
- * Update the page protection bits.  Intended to be used to create
- * guard pages for kernel data structures on pages which are bolted
- * in the HPT.  Assumes pages being operated on will not be stolen. 
- */
-void hpte_updateboltedpp_iSeries(unsigned long newpp, unsigned long ea )
+void iSeries_hpte_updateboltedpp(unsigned long newpp, unsigned long ea)
 {
 	unsigned long vsid,va,vpn;
 	long slot;
@@ -715,518 +946,408 @@
 	va = ( vsid << 28 ) | ( ea & 0x0fffffff );
 	vpn = va >> PAGE_SHIFT;
 
-	slot = ppc_md.hpte_find( vpn );
-	HvCallHpt_setPp( slot, newpp );
-}
+	slot = iSeries_hpte_find(vpn); 
+	if (slot == -1)
+		panic("updateboltedpp: Could not find page to bolt\n");
 
-
-static __inline__ void set_pp_bit(unsigned long  pp, HPTE *addr)
-{
-	unsigned long old;
-	unsigned long *p = (unsigned long *)(&(addr->dw1));
-
-	__asm__ __volatile__(
-        "1:	ldarx	%0,0,%3\n\
-                rldimi  %0,%2,0,62\n\
-                stdcx.	%0,0,%3\n\
-            	bne	1b"
-        : "=&r" (old), "=m" (*p)
-        : "r" (pp), "r" (p), "m" (*p)
-        : "cc");
+	HvCallHpt_setPp(slot, newpp);
 }
 
 /*
- * Update the page protection bits.  Intended to be used to create
- * guard pages for kernel data structures on pages which are bolted
- * in the HPT.  Assumes pages being operated on will not be stolen. 
+ * Functions used to insert new hardware page table entries.
+ * Will castout non-bolted entries as necessary using a random
+ * algorithm.
+ *
+ * Input : vpn      : virtual page number
+ *         prpn     : real page number in absolute space
+ *         hpteflags: page protection flags
+ *         bolted   : 1 = bolt the page
+ *         large    : 1 = large page (16M)
+ * Output: hsss, where h = hash group, sss = slot within that group
  */
-void hpte_updateboltedpp_pSeries(unsigned long newpp, unsigned long ea)
+static long hpte_insert(unsigned long vpn, unsigned long prpn,
+			unsigned long hpteflags, int bolted, int large)
 {
-	unsigned long vsid,va,vpn,flags;
-	long slot;
 	HPTE *hptep;
+	Hpte_dword0 dw0;
+	HPTE lhpte;
+	int i, secondary;
+	unsigned long hash = hpt_hash(vpn, 0);
+	unsigned long avpn = vpn >> 11;
+	unsigned long arpn = physRpn_to_absRpn(prpn);
+	unsigned long hpte_group;
 
-	vsid = get_kernel_vsid( ea );
-	va = ( vsid << 28 ) | ( ea & 0x0fffffff );
-	vpn = va >> PAGE_SHIFT;
+repeat:
+	secondary = 0;
+	hpte_group = ((hash & htab_data.htab_hash_mask) *
+		      HPTES_PER_GROUP) & ~0x7UL;
+	hptep = htab_data.htab + hpte_group;
+
+	for (i = 0; i < HPTES_PER_GROUP; i++) {
+		dw0 = hptep->dw0.dw0;
+		if (!dw0.v) {
+			/* retry with lock held */
+			dw0 = hptep->dw0.dw0;
+			if (!dw0.v)
+				break;
+		}
+		hptep++;
+	}
 
-	slot = ppc_md.hpte_find( vpn );
-	hptep = htab_data.htab  + slot;
+	if (i == HPTES_PER_GROUP) {
+		secondary = 1;
+		hpte_group = ((~hash & htab_data.htab_hash_mask) *
+			      HPTES_PER_GROUP) & ~0x7UL;
+		hptep = htab_data.htab + hpte_group;
+
+		for (i = 0; i < HPTES_PER_GROUP; i++) {
+			dw0 = hptep->dw0.dw0;
+			if (!dw0.v) {
+				/* retry with lock held */
+				dw0 = hptep->dw0.dw0;
+				if (!dw0.v)
+					break;
+			}
+			hptep++;
+		}
+		if (i == HPTES_PER_GROUP) {
+			if (mftb() & 0x1)
+				hpte_group=((hash & htab_data.htab_hash_mask)* 
+					    HPTES_PER_GROUP) & ~0x7UL;
+			
+			hpte_remove(hpte_group);
+			goto repeat;
+		}
+	}
 
-	set_pp_bit(newpp , hptep);
+	lhpte.dw1.dword1      = 0;
+	lhpte.dw1.dw1.rpn     = arpn;
+	lhpte.dw1.flags.flags = hpteflags;
 
-	/* Ensure it is out of the tlb too */
-	spin_lock_irqsave( &hash_table_lock, flags );
-	_tlbie( va );
-	spin_unlock_irqrestore( &hash_table_lock, flags );
-}
+	lhpte.dw0.dword0      = 0;
+	lhpte.dw0.dw0.avpn    = avpn;
+	lhpte.dw0.dw0.h       = secondary;
+	lhpte.dw0.dw0.bolted  = bolted;
+	lhpte.dw0.dw0.v       = 1;
 
+	if (large) lhpte.dw0.dw0.l = 1;
 
+	hptep->dw1.dword1 = lhpte.dw1.dword1;
 
-/* This is called very early. */
-void hpte_init_iSeries(void)
-{
-	ppc_md.hpte_invalidate   = hpte_invalidate_iSeries;
-	ppc_md.hpte_updatepp     = hpte_updatepp_iSeries;
-	ppc_md.hpte_updateboltedpp = hpte_updateboltedpp_iSeries;
-	ppc_md.hpte_getword0     = hpte_getword0_iSeries;
-	ppc_md.hpte_selectslot   = hpte_selectslot_iSeries;
-	ppc_md.hpte_create_valid = hpte_create_valid_iSeries;
-	ppc_md.hpte_find	 = hpte_find_iSeries;
+	/* Guarantee the second dword is visible before the valid bit */
+	__asm__ __volatile__ ("eieio" : : : "memory");
+
+	/*
+	 * Now set the first dword including the valid bit
+	 * NOTE: this also unlocks the hpte
+	 */
+	hptep->dw0.dword0 = lhpte.dw0.dword0;
+
+	__asm__ __volatile__ ("ptesync" : : : "memory");
+
+	return ((secondary << 3) | i);
 }
-void hpte_init_pSeries(void)
+
+static long rpa_lpar_hpte_insert(unsigned long vpn, unsigned long prpn,
+				 unsigned long hpteflags,
+				 int bolted, int large)
 {
-	ppc_md.hpte_invalidate   = hpte_invalidate_pSeries;
-	ppc_md.hpte_updatepp     = hpte_updatepp_pSeries;
-	ppc_md.hpte_updateboltedpp = hpte_updateboltedpp_pSeries;
-	ppc_md.hpte_getword0     = hpte_getword0_pSeries;
-	ppc_md.hpte_selectslot   = hpte_selectslot_pSeries;
-	ppc_md.hpte_create_valid = hpte_create_valid_pSeries;
-	ppc_md.hpte_find	 = hpte_find_pSeries;
+	/* XXX fix for large page */
+	unsigned long lpar_rc;
+	unsigned long flags;
+	unsigned long slot;
+	HPTE lhpte;
+	int secondary;
+	unsigned long hash = hpt_hash(vpn, 0);
+	unsigned long avpn = vpn >> 11;
+	unsigned long arpn = physRpn_to_absRpn(prpn);
+	unsigned long hpte_group;
+
+	/* Fill in the local HPTE with absolute rpn, avpn and flags */
+	lhpte.dw1.dword1      = 0;
+	lhpte.dw1.dw1.rpn     = arpn;
+	lhpte.dw1.flags.flags = hpteflags;
+
+	lhpte.dw0.dword0      = 0;
+	lhpte.dw0.dw0.avpn    = avpn;
+	lhpte.dw0.dw0.bolted  = bolted;
+	lhpte.dw0.dw0.v       = 1;
+
+	if (large) lhpte.dw0.dw0.l = 1;
+
+	/* Now fill in the actual HPTE */
+	/* Set CEC cookie to 0         */
+	/* Large page = 0              */
+	/* Zero page = 0               */
+	/* I-cache Invalidate = 0      */
+	/* I-cache synchronize = 0     */
+	/* Exact = 0                   */
+	flags = 0;
+
+	/* XXX why is this here? - Anton */
+	/*   -- Because at one point we hit a case where non cachable
+	 *      pages where marked coherent & this is rejected by the HV.
+	 *      Perhaps it is no longer an issue ... DRENG.
+	 */ 
+	if (hpteflags & (_PAGE_GUARDED|_PAGE_NO_CACHE))
+		lhpte.dw1.flags.flags &= ~_PAGE_COHERENT;
+
+repeat:
+	secondary = 0;
+	lhpte.dw0.dw0.h = secondary;
+	hpte_group = ((hash & htab_data.htab_hash_mask) *
+		      HPTES_PER_GROUP) & ~0x7UL;
+
+	__asm__ __volatile__ (
+		H_ENTER_r3
+		"mr    4, %2\n"
+                "mr    5, %3\n"
+                "mr    6, %4\n"
+                "mr    7, %5\n"
+                HSC    
+                "mr    %0, 3\n"
+                "mr    %1, 4\n"
+		: "=r" (lpar_rc), "=r" (slot)
+		: "r" (flags), "r" (hpte_group), "r" (lhpte.dw0.dword0),
+		"r" (lhpte.dw1.dword1)
+		: "r0", "r3", "r4", "r5", "r6", "r7", 
+		  "r8", "r9", "r10", "r11", "r12", "cc");
+
+	if (lpar_rc == H_PTEG_Full) {
+		secondary = 1;
+		lhpte.dw0.dw0.h = secondary;
+		hpte_group = ((~hash & htab_data.htab_hash_mask) *
+			      HPTES_PER_GROUP) & ~0x7UL;
+
+		__asm__ __volatile__ (
+			      H_ENTER_r3
+			      "mr    4, %2\n"
+			      "mr    5, %3\n"
+			      "mr    6, %4\n"
+			      "mr    7, %5\n"
+			      HSC    
+			      "mr    %0, 3\n"
+			      "mr    %1, 4\n"
+			      : "=r" (lpar_rc), "=r" (slot)
+			      : "r" (flags), "r" (hpte_group), "r" (lhpte.dw0.dword0),
+			      "r" (lhpte.dw1.dword1)
+			      : "r0", "r3", "r4", "r5", "r6", "r7",
+			        "r8", "r9", "r10", "r11", "r12", "cc");
+		if (lpar_rc == H_PTEG_Full) {
+			if (mftb() & 0x1)
+				hpte_group=((hash & htab_data.htab_hash_mask)* 
+					    HPTES_PER_GROUP) & ~0x7UL;
+			
+			rpa_lpar_hpte_remove(hpte_group);
+			goto repeat;
+		}
+	}
+
+	if (lpar_rc != H_Success)
+		panic("Bad return code from pte enter rc = %lx\n", lpar_rc);
+
+	return ((secondary << 3) | (slot & 0x7));
 }
 
-/* Handle a fault by adding an HPTE 
- * If the address can't be determined to be valid
- * via Linux page tables, return 1.  If handled
- * return 0
- */
-int hash_page( unsigned long ea, unsigned long access )
+static long iSeries_hpte_insert(unsigned long vpn, unsigned long prpn,
+				unsigned long hpteflags,
+				int bolted, int large)
 {
-	int rc = 1;
-	void * pgdir = NULL;
-	unsigned long va, vsid, vpn;
-	unsigned long newpp, hash_ind, prpn;
-	unsigned long hpteflags, regionid;
+	HPTE lhpte;
+	unsigned long hash, hpte_group;
+	unsigned long avpn = vpn >> 11;
+	unsigned long arpn = physRpn_to_absRpn( prpn );
+	int secondary = 0;
 	long slot;
-	struct mm_struct * mm;
-	pte_t old_pte, new_pte, *ptep;
 
-	/* Check for invalid addresses. */
-	if (!IS_VALID_EA(ea)) {
-		return 1;
+	hash = hpt_hash(vpn, 0);
+
+repeat:
+	slot = HvCallHpt_findValid(&lhpte, vpn);
+	if (lhpte.dw0.dw0.v) {
+		panic("select_hpte_slot found entry already valid\n");
 	}
 
-	regionid =  REGION_ID(ea);
-	switch ( regionid ) {
-	case USER_REGION_ID:
-		mm = current->mm;
-		if ( mm == NULL ) {
-			PPCDBG(PPCDBG_MM, "hash_page returning; mm = 0\n"); 
-			return 1;
+	if (slot == -1) { /* No available entry found in either group */
+		if (mftb() & 0x1) {
+			hpte_group=((hash & htab_data.htab_hash_mask)* 
+				    HPTES_PER_GROUP) & ~0x7UL;
+		} else {
+			hpte_group=((~hash & htab_data.htab_hash_mask)* 
+				    HPTES_PER_GROUP) & ~0x7UL;
 		}
-		vsid = get_vsid(mm->context, ea );
-		break;
-	case IO_REGION_ID:
-		mm = &ioremap_mm;
-		vsid = get_kernel_vsid( ea );
-		break;
-	case VMALLOC_REGION_ID:
-		mm = &init_mm;
-		vsid = get_kernel_vsid( ea );
-		break;
-	case IO_UNMAPPED_REGION_ID:
-		udbg_printf("EEH Error ea = 0x%lx\n", ea);
- 		PPCDBG_ENTER_DEBUGGER();
-		panic("EEH Error ea = 0x%lx\n", ea);
-		break;
-	case KERNEL_REGION_ID:
-		/* As htab_initialize is now, we shouldn't ever get here since
-		 * we're bolting the entire 0xC0... region.
-		 */
-		udbg_printf("Little faulted on kernel address 0x%lx\n", ea);
- 		PPCDBG_ENTER_DEBUGGER();
-		panic("Little faulted on kernel address 0x%lx\n", ea);
-		break;
-	default:
-		/* Not a valid range, send the problem up to do_page_fault */
-		return 1;
-		break;
-	}
 
-	/* Search the Linux page table for a match with va */
-        va = ( vsid << 28 ) | ( ea & 0x0fffffff );
-	vpn = va >> PAGE_SHIFT;
-	pgdir = mm->pgd;
-	PPCDBG(PPCDBG_MM, "hash_page ea = 0x%16.16lx, va = 0x%16.16lx\n          current = 0x%16.16lx, access = %lx\n", ea, va, current, access); 
-                if ( pgdir == NULL ) {
-                return 1;
-	}
-	
-	/* Lock the Linux page table to prevent mmap and kswapd
-	 * from modifying entries while we search and update
-	 */
-	
-	spin_lock( &mm->page_table_lock );
-	
-	ptep = find_linux_pte( pgdir, ea );
-	/* If no pte found, send the problem up to do_page_fault */
-	if ( ! ptep ) {
-	  spin_unlock( &mm->page_table_lock );
-	  return 1;
-	}
-	
-	/* Acquire the hash table lock to guarantee that the linux
-	 * pte we fetch will not change
-	 */
-	spin_lock( &hash_table_lock );
-	
-	old_pte = *ptep;
-	
-	/* If the pte is not "present" (valid), send the problem
-	 * up to do_page_fault.
-	 */
-	if ( ! pte_present( old_pte ) ) {
-	  spin_unlock( &hash_table_lock );
-	  spin_unlock( &mm->page_table_lock );
-	  return 1;
-	}
-	
-	/* At this point we have found a pte (which was present).
-	 * The spinlocks prevent this status from changing
-	 * The hash_table_lock prevents the _PAGE_HASHPTE status
-	 * from changing (RPN, DIRTY and ACCESSED too)
-	 * The page_table_lock prevents the pte from being 
-	 * invalidated or modified
-	 */
+		hash = hpt_hash(vpn, 0);
+		iSeries_hpte_remove(hpte_group);
+		goto repeat;
+	} else if (slot < 0) {
+		slot &= 0x7fffffffffffffff;
+		secondary = 1;
+	}
+
+	/* Create the HPTE */
+	lhpte.dw1.dword1      = 0;
+	lhpte.dw1.dw1.rpn     = arpn;
+	lhpte.dw1.flags.flags = hpteflags;
+
+	lhpte.dw0.dword0     = 0;
+	lhpte.dw0.dw0.avpn   = avpn;
+	lhpte.dw0.dw0.h      = secondary;
+	lhpte.dw0.dw0.bolted = bolted;
+	lhpte.dw0.dw0.v      = 1;
 
-/* At this point, we have a pte (old_pte) which can be used to build or update
- * an HPTE.   There are 5 cases:
- *
- * 1. There is a valid (present) pte with no associated HPTE (this is 
- *	the most common case)
- * 2. There is a valid (present) pte with an associated HPTE.  The
- *	current values of the pp bits in the HPTE prevent access because the
- *	user doesn't have appropriate access rights.
- * 3. There is a valid (present) pte with an associated HPTE.  The
- *	current values of the pp bits in the HPTE prevent access because we are
- *	doing software DIRTY bit management and the page is currently not DIRTY. 
- * 4. This is a Kernel address (0xC---) for which there is no page directory.
- *	There is an HPTE for this page, but the pp bits prevent access.
- *      Since we always set up kernel pages with R/W access for the kernel
- *	this case only comes about for users trying to access the kernel.
- *	This case is always an error and is not dealt with further here.
- * 5. This is a Kernel address (0xC---) for which there is no page directory.
- *	There is no HPTE for this page.
+	/* Now fill in the actual HPTE */
+	HvCallHpt_addValidate(slot, secondary, (HPTE *)&lhpte);
+	return ((secondary << 3) | (slot & 0x7));
+}
 
- * Check the user's access rights to the page.  If access should be prevented
- * then send the problem up to do_page_fault.
+/*
+ * Functions used to remove hardware page table entries.
+ *
+ * Input : hpte_group: PTE index of the first entry in a group
+ * Output: offset within the group of the entry removed or
+ *         -1 on failure
  */
-
-	access |= _PAGE_PRESENT;
-	if ( 0 == ( access & ~(pte_val(old_pte)) ) ) {
-		/*
-		 * Check if pte might have an hpte, but we have
-		 * no slot information
-		 */
-		if ( pte_val(old_pte) & _PAGE_HPTENOIX ) {
-			unsigned long slot;	
-			pte_val(old_pte) &= ~_PAGE_HPTEFLAGS;
-			slot = ppc_md.hpte_find( vpn );
-			if ( slot != -1 ) {
-				if ( slot < 0 ) {
-					pte_val(old_pte) |= _PAGE_SECONDARY;
-					slot = -slot;
-				}
-				pte_val(old_pte) |= ((slot << 12) & _PAGE_GROUP_IX) | _PAGE_HASHPTE;
-				
-			}
+static long hpte_remove(unsigned long hpte_group)
+{
+	HPTE *hptep;
+	Hpte_dword0 dw0;
+	int i;
+	int slot_offset;
+	unsigned long vsid, group, pi, pi_high;
+	unsigned long slot;
+	unsigned long flags;
+	int large;
+	unsigned long va;
+
+	/* pick a random slot to start at */
+	slot_offset = mftb() & 0x7;
+
+	for (i = 0; i < HPTES_PER_GROUP; i++) {
+		hptep = htab_data.htab + hpte_group + slot_offset;
+		dw0 = hptep->dw0.dw0;
+
+		if (dw0.v && !dw0.bolted) {
+			/* retry with lock held */
+			dw0 = hptep->dw0.dw0;
+			if (dw0.v && !dw0.bolted)
+				break;
 		}
 
-		/* User has appropriate access rights. */
-		new_pte = old_pte;
-		/* If the attempted access was a store */
-		if ( access & _PAGE_RW )
-			pte_val(new_pte) |= _PAGE_ACCESSED |
-				_PAGE_DIRTY;
-		else
-			pte_val(new_pte) |= _PAGE_ACCESSED;
-
-		/* Only cases 1, 3 and 5 still in play */
-
-		newpp = computeHptePP( pte_val(new_pte) );
-
-		/* Check if pte already has an hpte (case 3) */
-		if ( pte_val(old_pte) & _PAGE_HASHPTE ) {
-			/* There MIGHT be an HPTE for this pte */
-			unsigned long hash, slot, secondary;
-			/* Local copy of first doubleword of HPTE */
-			union {
-				unsigned long d;
-				Hpte_dword0   h;
-			} hpte_dw0;
-			hash = hpt_hash(vpn, 0);
-			secondary = (pte_val(old_pte) & _PAGE_SECONDARY) >> 15;
-			if ( secondary )
-				hash = ~hash;
-			slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-			slot += (pte_val(old_pte) & _PAGE_GROUP_IX) >> 12;
-			/* If there is an HPTE for this page it is indexed by slot */
-			hpte_dw0.d = ppc_md.hpte_getword0( slot );
-			if ( (hpte_dw0.h.avpn == (vpn >> 11) ) &&
-			     (hpte_dw0.h.v) && 
-			     (hpte_dw0.h.h == secondary ) ){
-				/* HPTE matches */
-				ppc_md.hpte_updatepp( slot, newpp, va );
-				if ( !pte_same( old_pte, new_pte ) )
-					*ptep = new_pte;
-			}
-			else {
-				/* HPTE is not for this pte */
-				pte_val(old_pte) &= ~_PAGE_HPTEFLAGS;
-			}
-		}
-		if ( !( pte_val(old_pte) & _PAGE_HASHPTE ) ) {
-			/* Cases 1 and 5 */
-			/* For these cases we need to create a new
-			 * HPTE and update the linux pte (for
-			 * case 1).  For case 5 there is no linux pte.
-			 *
-			 * Find an available HPTE slot
- 			 */
-			slot = ppc_md.hpte_selectslot( vpn );
-
-			/* If hpte_selectslot returns 0x8000000000000000 that means
-			 * that there was already an entry in the HPT even though
-			 * the linux PTE said there couldn't be. 
-			 */
-			/* Debug code */
-			if ( slot == 0x8000000000000000 ) {
-				unsigned long xold_pte = pte_val(old_pte);
-				unsigned long xnew_pte = pte_val(new_pte);
-				
-				udbg_printf("hash_page: ptep    = 0x%016lx\n", (unsigned long)ptep );
-				udbg_printf("hash_page: old_pte = 0x%016lx\n", xold_pte );
-				udbg_printf("hash_page: new_pte = 0x%016lx\n", xnew_pte );
-				udbg_printf("hash_page: ea      = 0x%016lx\n", ea );
-				udbg_printf("hash_page: va      = 0x%016lx\n", va );
-				udbg_printf("hash_page: access  = 0x%016lx\n", access );
-			
-				panic("hash_page: hpte already exists\n");
-			}
-			hash_ind = 0;
-			if ( slot < 0 ) {
-				slot = -slot;
-				hash_ind = 1;
-			}
+		slot_offset++;
+		slot_offset &= 0x7;
+	}
 
-			/* Set the physical address */
-			prpn = pte_val(old_pte) >> PTE_SHIFT;
-			
-			if ( ptep ) {
-				/* Update the linux pte with the HPTE slot */
-				pte_val(new_pte) &= ~_PAGE_HPTEFLAGS;
-				pte_val(new_pte) |= hash_ind << 15;
-				pte_val(new_pte) |= (slot<<12) & _PAGE_GROUP_IX;
-				pte_val(new_pte) |= _PAGE_HASHPTE;
-				/* No need to use ldarx/stdcx here because all
-				 * who might be updating the pte will hold the page_table_lock
-				 * or the hash_table_lock (we hold both)
-				 */
-				*ptep = new_pte;
-			}
+	if (i == HPTES_PER_GROUP)
+		return -1;
 
-			/* copy appropriate flags from linux pte */
-			hpteflags = (pte_val(new_pte) & 0x1f8) | newpp;
+	large = dw0.l;
 
-			/* Create the HPTE */
-			ppc_md.hpte_create_valid( slot, vpn, prpn, hash_ind, ptep, hpteflags, 0 ); 
+	/* Invalidate the hpte. NOTE: this also unlocks it */
+	hptep->dw0.dword0 = 0;
 
-		}
+	/* Invalidate the tlb */
+	vsid = dw0.avpn >> 5;
+	slot = hptep - htab_data.htab;
+	group = slot >> 3;
+	if (dw0.h)
+		group = ~group;
+	pi = (vsid ^ group) & 0x7ff;
+	pi_high = (dw0.avpn & 0x1f) << 11;
+	pi |= pi_high;
 
-		/* Indicate success */
-		rc = 0;
-	}		
-	
-	spin_unlock( &hash_table_lock );
-	if (ptep)
-		spin_unlock( &mm->page_table_lock );
+	if (large)
+		va = pi << LARGE_PAGE_SHIFT;
+	else
+		va = pi << PAGE_SHIFT;
+
+	spin_lock_irqsave(&pSeries_tlbie_lock, flags);
+	_tlbie(va, large);
+	spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
 
-	return rc;
+	return i;
 }
 
-void flush_hash_page( unsigned long context, unsigned long ea, pte_t *ptep )
+static long rpa_lpar_hpte_remove(unsigned long hpte_group)
 {
-	unsigned long vsid, vpn, va, hash, secondary, slot, flags;
-	/* Local copy of first doubleword of HPTE */
-	union {
-		unsigned long d;
-		Hpte_dword0   h;
-	} hpte_dw0;
-	pte_t pte;
+	unsigned long slot_offset;
+	unsigned long lpar_rc;
+	int i;
+	unsigned long dummy1, dummy2;
 
-	if ( (ea >= USER_START ) && ( ea <= USER_END ) )
-		vsid = get_vsid( context, ea );
-	else
-		vsid = get_kernel_vsid( ea );
-	va = (vsid << 28) | (ea & 0x0fffffff);
-	vpn = va >> PAGE_SHIFT;
-	hash = hpt_hash(vpn, 0);
+	/* pick a random slot to start at */
+	slot_offset = mftb() & 0x7;
 
-	spin_lock_irqsave( &hash_table_lock, flags);
-	pte = __pte(pte_update(ptep, _PAGE_HPTEFLAGS, 0));
-	if ( pte_val(pte) & _PAGE_HASHPTE ) {
-		secondary = (pte_val(pte) & _PAGE_SECONDARY) >> 15;
-		if ( secondary )
-			hash = ~hash;
-		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-		slot += (pte_val(pte) & _PAGE_GROUP_IX) >> 12;
-		/* If there is an HPTE for this page it is indexed by slot */
+	for (i = 0; i < HPTES_PER_GROUP; i++) {
 
-		hpte_dw0.d = ppc_md.hpte_getword0( slot );
-		if ( (hpte_dw0.h.avpn == (vpn >> 11) ) &&
-		     (hpte_dw0.h.v) && 
-		     (hpte_dw0.h.h == secondary ) ){
-			/* HPTE matches */
-			ppc_md.hpte_invalidate( slot );	
-		}
-		else {
-			unsigned k;
-			/* Temporarily lets check for the hpte in all possible slots */
-			for ( secondary = 0; secondary < 2; ++secondary ) {
-				hash = hpt_hash(vpn, 0);
-				if ( secondary )
-					hash = ~hash;
-				slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-				for ( k=0; k<8; ++k ) {
-					hpte_dw0.d = ppc_md.hpte_getword0( slot+k );
-					if ( ( hpte_dw0.h.avpn == (vpn >> 11) ) &&
-					     ( hpte_dw0.h.v ) &&
-					     ( hpte_dw0.h.h == secondary ) ) {
-						while (1) ;
-					}
-				}
-			}
-		}
+		/* Don't remove a bolted entry */
+		lpar_rc = plpar_pte_remove(H_ANDCOND, hpte_group + slot_offset,
+					   (0x1UL << 4), &dummy1, &dummy2);
+
+		if (lpar_rc == H_Success)
+			return i;
+
+		if (lpar_rc != H_Not_Found)
+			panic("Bad return code from pte remove rc = %lx\n",
+			      lpar_rc);
+
+		slot_offset++;
+		slot_offset &= 0x7;
 	}
-	spin_unlock_irqrestore( &hash_table_lock, flags );
+
+	return -1;
 }
 
-int proc_dol2crvec(ctl_table *table, int write, struct file *filp,
-		   void *buffer, size_t *lenp)
+static long iSeries_hpte_remove(unsigned long hpte_group)
 {
-	int vleft, first=1, len, left, val;
-#define TMPBUFLEN 256
-	char buf[TMPBUFLEN], *p;
-	static const char *sizestrings[4] = {
-		"2MB", "256KB", "512KB", "1MB"
-	};
-	static const char *clockstrings[8] = {
-		"clock disabled", "+1 clock", "+1.5 clock", "reserved(3)",
-		"+2 clock", "+2.5 clock", "+3 clock", "reserved(7)"
-	};
-	static const char *typestrings[4] = {
-		"flow-through burst SRAM", "reserved SRAM",
-		"pipelined burst SRAM", "pipelined late-write SRAM"
-	};
-	static const char *holdstrings[4] = {
-		"0.5", "1.0", "(reserved2)", "(reserved3)"
-	};
-
-	if ( ((_get_PVR() >> 16) != 8) && ((_get_PVR() >> 16) != 12))
-		return -EFAULT;
-	
-	if ( /*!table->maxlen ||*/ (filp->f_pos && !write)) {
-		*lenp = 0;
-		return 0;
-	}
-	
-	vleft = table->maxlen / sizeof(int);
-	left = *lenp;
-	
-	for (; left /*&& vleft--*/; first=0) {
-		if (write) {
-			while (left) {
-				char c;
-				if(get_user(c,(char *) buffer))
-					return -EFAULT;
-				if (!isspace(c))
-					break;
-				left--;
-				((char *) buffer)++;
-			}
-			if (!left)
-				break;
-			len = left;
-			if (len > TMPBUFLEN-1)
-				len = TMPBUFLEN-1;
-			if(copy_from_user(buf, buffer, len))
-				return -EFAULT;
-			buf[len] = 0;
-			p = buf;
-			if (*p < '0' || *p > '9')
-				break;
-			val = simple_strtoul(p, &p, 0);
-			len = p-buf;
-			if ((len < left) && *p && !isspace(*p))
-				break;
-			buffer += len;
-			left -= len;
-#if 0
-			/* DRENG need a def */
-			_set_L2CR(0);
-			_set_L2CR(val);
-			while ( _get_L2CR() & 0x1 )
-				/* wait for invalidate to finish */;
-#endif
-			  
-		} else {
-			p = buf;
-			if (!first)
-				*p++ = '\t';
-#if 0
-			/* DRENG need a def */
-			val = _get_L2CR();
-#endif
-			p += sprintf(p, "0x%08x: ", val);
-			p += sprintf(p, " %s", (val >> 31) & 1 ? "enabled" :
-				     "disabled");
-			p += sprintf(p, ", %sparity", (val>>30)&1 ? "" : "no ");
-			p += sprintf(p, ", %s", sizestrings[(val >> 28) & 3]);
-			p += sprintf(p, ", %s", clockstrings[(val >> 25) & 7]);
-			p += sprintf(p, ", %s", typestrings[(val >> 23) & 2]);
-			p += sprintf(p, "%s", (val>>22)&1 ? ", data only" : "");
-			p += sprintf(p, "%s", (val>>20)&1 ? ", ZZ enabled": "");
-			p += sprintf(p, ", %s", (val>>19)&1 ? "write-through" :
-				     "copy-back");
-			p += sprintf(p, "%s", (val>>18)&1 ? ", testing" : "");
-			p += sprintf(p, ", %sns hold",holdstrings[(val>>16)&3]);
-			p += sprintf(p, "%s", (val>>15)&1 ? ", DLL slow" : "");
-			p += sprintf(p, "%s", (val>>14)&1 ? ", diff clock" :"");
-			p += sprintf(p, "%s", (val>>13)&1 ? ", DLL bypass" :"");
-			
-			p += sprintf(p,"\n");
-			
-			len = strlen(buf);
-			if (len > left)
-				len = left;
-			if(copy_to_user(buffer, buf, len))
-				return -EFAULT;
-			left -= len;
-			buffer += len;
-			break;
+	unsigned long slot_offset;
+	int i;
+	HPTE lhpte;
+
+	/* Pick a random slot to start at */
+	slot_offset = mftb() & 0x7;
+
+	for (i = 0; i < HPTES_PER_GROUP; i++) {
+		lhpte.dw0.dword0 = 
+			iSeries_hpte_getword0(hpte_group + slot_offset);
+
+		if (!lhpte.dw0.dw0.bolted) {
+			HvCallHpt_invalidateSetSwBitsGet(hpte_group + 
+							 slot_offset, 0, 0);
+			return i;
 		}
-	}
 
-	if (!write && !first && left) {
-		if(put_user('\n', (char *) buffer))
-			return -EFAULT;
-		left--, buffer++;
-	}
-	if (write) {
-		p = (char *) buffer;
-		while (left) {
-			char c;
-			if(get_user(c, p++))
-				return -EFAULT;
-			if (!isspace(c))
-				break;
-			left--;
-		}
+		slot_offset++;
+		slot_offset &= 0x7;
 	}
-	if (write && first)
-		return -EINVAL;
-	*lenp -= left;
-	filp->f_pos += *lenp;
-	return 0;
+
+	return -1;
+}
+
+void hpte_init_pSeries(void)
+{
+	ppc_md.hpte_invalidate     = hpte_invalidate;
+	ppc_md.hpte_updatepp       = hpte_updatepp;
+	ppc_md.hpte_updateboltedpp = hpte_updateboltedpp;
+	ppc_md.hpte_insert	   = hpte_insert;
+	ppc_md.hpte_remove	   = hpte_remove;
+}
+
+void pSeries_lpar_mm_init(void)
+{
+	ppc_md.hpte_invalidate     = rpa_lpar_hpte_invalidate;
+	ppc_md.hpte_updatepp       = rpa_lpar_hpte_updatepp;
+	ppc_md.hpte_updateboltedpp = rpa_lpar_hpte_updateboltedpp;
+	ppc_md.hpte_insert         = rpa_lpar_hpte_insert;
+	ppc_md.hpte_remove         = rpa_lpar_hpte_remove;
+}
+
+void hpte_init_iSeries(void)
+{
+	ppc_md.hpte_invalidate     = iSeries_hpte_invalidate;
+	ppc_md.hpte_updatepp       = iSeries_hpte_updatepp;
+	ppc_md.hpte_updateboltedpp = iSeries_hpte_updateboltedpp;
+	ppc_md.hpte_insert         = iSeries_hpte_insert;
+	ppc_md.hpte_remove         = iSeries_hpte_remove;
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/iSeries_IoMmTable.c linuxppc64_2_4/arch/ppc64/kernel/iSeries_IoMmTable.c
--- linux-2.4.19/arch/ppc64/kernel/iSeries_IoMmTable.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/iSeries_IoMmTable.c	Tue Aug 20 06:05:18 2002
@@ -130,7 +130,7 @@
 /*   The HvCallPci_getBarParms is used to get the size of the BAR  */
 /*   space.  It calls iSeries_IoMmTable_AllocateEntry to allocate  */
 /*   each entry.                                                   */
-/* - Loops through The Bar resourses(0 - 5) including the the ROM  */
+/* - Loops through The Bar resourses(0 - 5) including the ROM      */
 /*   is resource(6).                                               */
 /*******************************************************************/
 void iSeries_allocateDeviceBars(struct pci_dev* PciDev)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/iSeries_pci.c linuxppc64_2_4/arch/ppc64/kernel/iSeries_pci.c
--- linux-2.4.19/arch/ppc64/kernel/iSeries_pci.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/iSeries_pci.c	Tue Aug 20 08:31:30 2002
@@ -4,7 +4,7 @@
  * Copyright (C) 2001 Allan Trautman, IBM Corporation
  *
  * iSeries specific routines for PCI.
- * 
+ * /
  * Based on code from pci.c and iSeries_pci.c 32bit
  *
  * This program is free software; you can redistribute it and/or modify
@@ -28,6 +28,8 @@
 #include <linux/init.h>
 #include <linux/ide.h>
 #include <linux/pci.h>
+#include <linux/rtc.h>
+#include <linux/time.h>
 
 #include <asm/io.h>
 #include <asm/irq.h>
@@ -37,6 +39,8 @@
 #include <asm/ppcdebug.h>
 #include <asm/naca.h>
 #include <asm/flight_recorder.h>
+#include <asm/hardirq.h>
+#include <asm/time.h>
 #include <asm/pci_dma.h>
 
 #include <asm/iSeries/HvCallPci.h>
@@ -76,9 +80,6 @@
 extern int    Pci_Error_Flag;
 extern int    Pci_Trace_Flag;
 
-extern void iSeries_MmIoTest(void);
-
-
 /*******************************************************************
  * Forward declares of prototypes. 
  *******************************************************************/
@@ -379,7 +380,10 @@
 			if(DevInfo->deviceType == HvCallPci_NodeDevice) {
 				iSeries_Scan_EADs_Bridge(Bus, SubBus, IdSel);
 			}
-			else printk("PCI: Invalid System Configuration(0x%02X.\n",DevInfo->deviceType);
+			else {
+				printk("PCI: Invalid System Configuration(0x%02X).\n",DevInfo->deviceType);
+				PCIFR(      "Invalid System Configuration(0x%02X).",  DevInfo->deviceType);
+			}
 		}
 		else pci_Log_Error("getDeviceInfo",Bus, SubBus, IdSel,HvRc);
 	}
@@ -511,7 +515,7 @@
 /* I/0 Memory copy MUST use mmio commands on iSeries                    */
 /* To do; For performance, include the hv call directly                 */
 /************************************************************************/
-void* iSeries_memset(void* dest, char c, size_t Count)
+void* iSeries_memset_io(void* dest, char c, size_t Count)
 {
 	u8    ByteValue     = c;
 	long  NumberOfBytes = Count;
@@ -580,6 +584,18 @@
 	}
 	return Node;
 }
+/******************************************************************/
+/* Set and reset Device Node Lock                                 */
+/******************************************************************/
+#define setIoLock() \
+    unsigned long IrqFlags; \
+    spin_lock_irqsave(&DevNode->IoLock, IrqFlags ); 
+
+#define resetIoLock() \
+    int RtnCode = DevNode->ReturnCode; \
+    spin_unlock_irqrestore( &DevNode->IoLock, IrqFlags ); \
+    return RtnCode;
+
 /**********************************************************************************
  *
  * Read PCI Config Space Code 
@@ -588,8 +604,8 @@
 /** BYTE  *************************************************************************/
 int iSeries_Node_read_config_byte(struct iSeries_Device_Node* DevNode, int Offset, u8* ReadValue)
 {
-	u8  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
+	u8  ReadData;
+	setIoLock();
 	++Pci_Cfg_Read_Count;
 	DevNode->ReturnCode = HvCallPci_configLoad8(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                Offset,&ReadData);
@@ -600,14 +616,14 @@
 		printk("PCI: RCB: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 		PCIFR(      "RCB: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
+	*ReadValue = ReadData;
+	resetIoLock();
 }
 /** WORD  *************************************************************************/
 int iSeries_Node_read_config_word(struct iSeries_Device_Node* DevNode, int Offset, u16* ReadValue)
 {
 	u16  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
+	setIoLock();
 	++Pci_Cfg_Read_Count;
 	DevNode->ReturnCode = HvCallPci_configLoad16(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                Offset,&ReadData);
@@ -619,14 +635,14 @@
 		PCIFR(      "RCW: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 
 	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
+	*ReadValue = ReadData;
+	resetIoLock();
 }
 /** DWORD *************************************************************************/
 int iSeries_Node_read_config_dword(struct iSeries_Device_Node* DevNode, int Offset, u32* ReadValue)
 {
  	u32  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
+	setIoLock();
 	++Pci_Cfg_Read_Count;
 	DevNode->ReturnCode = HvCallPci_configLoad32(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                Offset,&ReadData);
@@ -637,8 +653,8 @@
 		printk("PCI: RCL: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 		PCIFR(      "RCL: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
+	*ReadValue = ReadData;
+	resetIoLock();
 }
 int iSeries_pci_read_config_byte(struct pci_dev* PciDev, int Offset, u8* ReadValue) { 
 	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
@@ -662,6 +678,7 @@
 /** BYTE  *************************************************************************/
 int iSeries_Node_write_config_byte(struct iSeries_Device_Node* DevNode, int Offset, u8 WriteData)
 {
+	setIoLock();
 	++Pci_Cfg_Write_Count;
 	DevNode->ReturnCode = HvCallPci_configStore8(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                  Offset,WriteData);
@@ -672,11 +689,12 @@
 		printk("PCI: WCB: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 		PCIFR(      "WCB: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 	}
- 	return DevNode->ReturnCode;
+	resetIoLock();
 }
 /** WORD  *************************************************************************/
 int iSeries_Node_write_config_word(struct iSeries_Device_Node* DevNode, int Offset, u16 WriteData)
 {
+	setIoLock();
 	++Pci_Cfg_Write_Count;
 	DevNode->ReturnCode = HvCallPci_configStore16(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                  Offset,WriteData);
@@ -687,11 +705,12 @@
 		printk("PCI: WCW: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 		PCIFR(      "WCW: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 	}
- 	return DevNode->ReturnCode;
+	resetIoLock();
 }
 /** DWORD *************************************************************************/
 int iSeries_Node_write_config_dword(struct iSeries_Device_Node* DevNode, int Offset, u32 WriteData)
 {
+	setIoLock();
 	++Pci_Cfg_Write_Count;
 	DevNode->ReturnCode = HvCallPci_configStore32(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
 	                                                  Offset,WriteData);
@@ -702,7 +721,7 @@
 		printk("PCI: WCL: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 		PCIFR(      "WCL: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
 	}
-	return DevNode->ReturnCode;
+	resetIoLock();
 }
 int iSeries_pci_write_config_byte( struct pci_dev* PciDev,int Offset, u8 WriteValue)
 {
@@ -736,69 +755,93 @@
 };
 
 /************************************************************************
- * Check Return Code
+ * Log Pci Error and check Retry Count 
  * -> On Failure, print and log information.
  *    Increment Retry Count, if exceeds max, panic partition.
- * -> If in retry, print and log success 
- ************************************************************************
- * PCI: Device 23.90 ReadL I/O Error( 0): 0x1234
- * PCI: Device 23.90 ReadL Retry( 1)
- * PCI: Device 23.90 ReadL Retry Successful(1)
+ * -> If in retry, print and log success
  ************************************************************************/
-int  CheckReturnCode(char* TextHdr, struct iSeries_Device_Node* DevNode, u64 RtnCode)
+void logPciError(char* ErrorTxt, void* IoAddress, struct iSeries_Device_Node* DevNode, u64 RtnCode)
 {
-	if(RtnCode != 0)  {
-		++Pci_Error_Count;
-		++DevNode->IoRetry;
-		PCIFR(      "%s: Device 0x%04X:%02X  I/O Error(%2d): 0x%04X",
-			    TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry,(int)RtnCode);
-		printk("PCI: %s: Device 0x%04X:%02X  I/O Error(%2d): 0x%04X\n",
-		            TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry,(int)RtnCode);
-		/*******************************************************/
-		/* Bump the retry and check for retry count exceeded.  */
-		/* If, Exceeded, panic the system.                     */           
-		/*******************************************************/
-		if(DevNode->IoRetry > Pci_Retry_Max && Pci_Error_Flag > 0 ) {
-			mf_displaySrc(0xB6000103);
-			panic_timeout = 0; 
-			panic("PCI: Hardware I/O Error, SRC B6000103, Automatic Reboot Disabled.\n");
-		}
-		return -1;	/* Retry Try */
-	}
-	/********************************************************************
-	* If retry was in progress, log success and rest retry count        *
-	*********************************************************************/
-	else if(DevNode->IoRetry > 0) {
-		PCIFR("%s: Device 0x%04X:%02X Retry Successful(%2d).",
-		      TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry);
-		DevNode->IoRetry = 0;
-		return 0; 
+	++DevNode->IoRetry;
+	++Pci_Error_Count;
+
+	PCIFR("%s: I/O Error(%1d/%1d):0x%04X  IoAddress:0x%p  Device:0x%04X:%02X",  
+	      ErrorTxt, DevNode->IoRetry, in_interrupt(), RtnCode, IoAddress, ISERIES_BUS(DevNode),DevNode->AgentId);
+	/*******************************************************/
+	/* Filter out EADs freeze and alignment errors         */
+	/*******************************************************/
+	if(RtnCode == 0x0102) {
+		PCIFR("EADS Freeze error.......Panic........");
+		mf_displaySrc(0xB6000103);
+		panic_timeout = 0; 
+		panic("PCI: EADs Freeze error SRC B6000103\n");
+	}
+	else if(RtnCode == 0x0241) {
+		PCIFR("MMIO Alignment error: 0x%p",IoAddress);
+		mf_displaySrc(0xB6000103);
+		panic_timeout = 0; 
+		panic("PCI: MMIO Alignment error. SRC B6000103\n");
+	}
+	/*******************************************************/
+	/* Bump the retry and check for retry count exceeded.  */
+	/* If, Exceeded, panic the system.                     */           
+	/*******************************************************/
+	if(DevNode->IoRetry > Pci_Retry_Max && Pci_Error_Flag != 0 ) {
+		mf_displaySrc(0xB6000103);
+		panic_timeout = 0; 
+		panic("PCI: Hardware I/O Error, SRC B6000103, Automatic Reboot Disabled.\n");
+	}
+	/**************************************************************/
+	/* Wait x ms before retrying I/O to give I/O time to recover. */
+	/* Retry wait delay logic.                                    */
+	/* - On first retry, no delay, maybe just glitch.             */
+	/* - On successify retries, vary the delay to avoid being in a*/
+        /*   repetitive timing window.                                */ 
+	/**************************************************************/
+	if(DevNode->IoRetry > 0) {
+		udelay(DevNode->IoRetry * 50);
 	}
-	return 0; 
 }
+
+/************************************************************************
+ * Retry was successful
+ ************************************************************************/
+void  pciRetrySuccessful(struct iSeries_Device_Node* DevNode)
+{
+	struct  timeval  TimeClock;
+	struct  rtc_time CurTime;
+	do_gettimeofday(&TimeClock);
+	to_tm(TimeClock.tv_sec, &CurTime);
+
+	PCIFR("Retry Successful(%2d) on Device 0x%04X:%02X at %02d.%02d.%02d",
+	      DevNode->IoRetry,ISERIES_BUS(DevNode),DevNode->AgentId,
+	      CurTime.tm_hour,CurTime.tm_min,CurTime.tm_sec);
+
+	DevNode->IoRetry = 0;
+}
+
 /************************************************************************/
 /* Translate the I/O Address into a device node, bar, and bar offset.   */
 /* Note: Make sure the passed variable end up on the stack to avoid     */
 /* the exposure of being device global.                                 */
+/* The Device Node is Lock to block other I/O to device.                */
 /************************************************************************/
-static inline struct iSeries_Device_Node* xlateIoMmAddress(void* IoAddress,
-							    union HvDsaMap* DsaPtr,
-							   u64* BarOffsetPtr) {
-
-	unsigned long BaseIoAddr = (unsigned long)IoAddress-iSeries_Base_Io_Memory;
-	long          TableIndex = BaseIoAddr/iSeries_IoMmTable_Entry_Size;
-	struct iSeries_Device_Node* DevNode = *(iSeries_IoMmTable +TableIndex);
-	if(DevNode != NULL) {
-		DsaPtr->DsaAddr       = ISERIES_DSA(DevNode);
-		DsaPtr->Dsa.barNumber = *(iSeries_IoBarTable+TableIndex);
-		*BarOffsetPtr         = BaseIoAddr % iSeries_IoMmTable_Entry_Size;
-	}
-	else {
-		panic("PCI: Invalid PCI IoAddress detected!\n");
-	}
-	return DevNode;
-}
-
+#define setUpMmIo(IoAddress,Type) \
+unsigned long   IrqFlags; \
+struct HvCallPci_LoadReturn Return; \
+union HvDsaMap  DsaData;  \
+u64             BarOffset;\
+unsigned long BaseIoAddr = (unsigned long)IoAddress-iSeries_Base_Io_Memory; \
+long          TableIndex = (BaseIoAddr/iSeries_IoMmTable_Entry_Size);       \
+struct iSeries_Device_Node* DevNode = *(iSeries_IoMmTable+TableIndex);      \
+if(DevNode != NULL) { \
+    DsaData.DsaAddr       = ISERIES_DSA(DevNode); \
+    DsaData.Dsa.barNumber = *(iSeries_IoBarTable+TableIndex); \
+    BarOffset             = BaseIoAddr % iSeries_IoMmTable_Entry_Size; \
+    ++Pci_Io_##Type##_Count; \
+    spin_lock_irqsave(&DevNode->IoLock, IrqFlags ); \
+} \
+else panic("PCI: Invalid PCI IoAddress detected 0x%p!\n",IoAddress);
 /************************************************************************/
 /* Read MM I/O Instructions for the iSeries                             */
 /* On MM I/O error, all ones are returned and iSeries_pci_IoError is cal*/
@@ -810,47 +853,53 @@
 /************************************************************************/
 u8  iSeries_Read_Byte(void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Read);
 	do {
-		++Pci_Io_Read_Count;
 		HvCall3Ret16(HvCallPciBarLoad8, &Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDB",DevNode, Return.rc) != 0);
-
-	if(Pci_Trace_Flag == 1)	PCIFR("RDB: IoAddress 0x%p = 0x%02X",IoAddress, (u8)Return.value); 
+		if(Return.rc != 0 ) {
+			logPciError("RDB",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
+	if(Pci_Trace_Flag == 1 ) PCIFR("RDB: IoAddress 0x%p = 0x%02X",IoAddress, (u8)Return.value);
 	return (u8)Return.value;
 }
+int Retry_Test = 0;
 u16  iSeries_Read_Word(void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Read);
 	do {
-		++Pci_Io_Read_Count;
 		HvCall3Ret16(HvCallPciBarLoad16,&Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDW",DevNode, Return.rc) != 0);
 
-	if(Pci_Trace_Flag == 1) PCIFR("RDW: IoAddress 0x%p = 0x%04X",IoAddress, swab16((u16)Return.value));
+		if(Return.rc != 0 ) {
+			logPciError("RDW",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
+	if(Pci_Trace_Flag == 1 ) PCIFR("RDW: IoAddress 0x%p = 0x%04X",IoAddress, (u16)Return.value);
 	return swab16((u16)Return.value);
 }
 u32  iSeries_Read_Long(void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Read);
 	do {
-		++Pci_Io_Read_Count;
 		HvCall3Ret16(HvCallPciBarLoad32,&Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDL",DevNode, Return.rc) != 0);
 
-	if(Pci_Trace_Flag == 1) PCIFR("RDL: IoAddress 0x%p = 0x%04X",IoAddress, swab32((u32)Return.value));
+		if(Return.rc != 0 ) {
+			logPciError("RDL",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
+	if(Pci_Trace_Flag == 1 ) PCIFR("RDL: IoAddress 0x%p = 0x%08X",IoAddress, swab32((u32)Return.value));
 	return swab32((u32)Return.value);
 }
 /************************************************************************/
@@ -862,41 +911,47 @@
 /************************************************************************/
 void iSeries_Write_Byte(u8 Data, void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Write);
 	do {
-		++Pci_Io_Write_Count;
 		Return.rc = HvCall4(HvCallPciBarStore8, DsaData.DsaAddr,BarOffset, Data, 0);
-	} while (CheckReturnCode("WWB",DevNode, Return.rc) != 0);
+		if(Return.rc != 0 ) {
+			logPciError("WWB",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
 	if(Pci_Trace_Flag == 1) PCIFR("WWB: IoAddress 0x%p = 0x%02X",IoAddress,Data);
 }
 void iSeries_Write_Word(u16 Data, void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Write);
 	do {
-		++Pci_Io_Write_Count;
 		Return.rc = HvCall4(HvCallPciBarStore16,DsaData.DsaAddr,BarOffset, swab16(Data), 0);
-	} while (CheckReturnCode("WWW",DevNode, Return.rc) != 0);
+		if(Return.rc != 0 ) {
+			logPciError("WWW",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
 	if(Pci_Trace_Flag == 1) PCIFR("WWW: IoAddress 0x%p = 0x%04X",IoAddress,Data);
 }
 void iSeries_Write_Long(u32 Data, void* IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+	setUpMmIo(IoAddress,Write);
 	do {
-		++Pci_Io_Write_Count;
 		Return.rc = HvCall4(HvCallPciBarStore32,DsaData.DsaAddr,BarOffset, swab32(Data), 0);
-	} while (CheckReturnCode("WWL",DevNode, Return.rc) != 0);
+		if(Return.rc != 0 ) {
+			logPciError("WWL",IoAddress, DevNode, Return.rc);
+		}
+		else if ( DevNode->IoRetry > 0) {
+			pciRetrySuccessful(DevNode);
+		}
+	} while (Return.rc != 0);
+	spin_unlock_irqrestore(&DevNode->IoLock, IrqFlags ); 
 	if(Pci_Trace_Flag == 1) PCIFR("WWL: IoAddress 0x%p = 0x%08X",IoAddress, Data);
 }
 /*
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/iSeries_pci_reset.c linuxppc64_2_4/arch/ppc64/kernel/iSeries_pci_reset.c
--- linux-2.4.19/arch/ppc64/kernel/iSeries_pci_reset.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/iSeries_pci_reset.c	Tue Aug 20 08:31:30 2002
@@ -78,7 +78,7 @@
 		schedule_timeout(WaitDelay);
 	}
 	if (DeviceNode->ReturnCode == 0) {
-		PCIFR("Slot 0x%04X.%02 Reset\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId );
+		PCIFR("Slot 0x%04X.%02X Reset\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId );
 	} 
 	else {
 		printk("PCI: Slot 0x%04X.%02X Reset Failed, RCode: %04X\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId,DeviceNode->ReturnCode);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/iSeries_setup.c linuxppc64_2_4/arch/ppc64/kernel/iSeries_setup.c
--- linux-2.4.19/arch/ppc64/kernel/iSeries_setup.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/iSeries_setup.c	Tue Aug 20 14:46:58 2002
@@ -49,6 +49,7 @@
 #include <asm/iSeries/IoHriMainStore.h>
 #include <asm/iSeries/iSeries_proc.h>
 #include <asm/proc_pmc.h>
+#include <asm/perfmon.h>
 #include <asm/iSeries/mf.h>
 
 /* Function Prototypes */
@@ -59,13 +60,10 @@
 static void setup_iSeries_cache_sizes( void );
 static void iSeries_bolt_kernel(unsigned long saddr, unsigned long eaddr);
 #endif
-void build_valid_hpte( unsigned long vsid, unsigned long ea, unsigned long pa,
-		       pte_t * ptep, unsigned hpteflags, unsigned bolted );
 extern void ppcdbg_initialize(void);
 extern void iSeries_pcibios_init(void);
 extern void iSeries_pcibios_fixup(void);
 extern void iSeries_pcibios_fixup_bus(int);
-static void iSeries_setup_dprofile(void);
 
 /* Global Variables */
 
@@ -77,10 +75,6 @@
 static unsigned long tbFreqMhz = 0;
 static unsigned long tbFreqMhzHundreths = 0;
 
-unsigned long dprof_shift = 0;
-unsigned long dprof_len = 0;
-unsigned int * dprof_buffer = NULL;
-
 int piranha_simulator = 0;
 
 extern char _end[];
@@ -257,7 +251,7 @@
 {
 	unsigned long i;
 	unsigned long mem_blocks = 0;
-	if ( __is_processor( PV_POWER4 ) )
+	if (__is_processor(PV_POWER4) || __is_processor(PV_POWER4p))
 		mem_blocks = iSeries_process_Regatta_mainstore_vpd( mb_array, max_entries );
 	else
 		mem_blocks = iSeries_process_Condor_mainstore_vpd( mb_array, max_entries );
@@ -283,8 +277,6 @@
 iSeries_init_early(void)
 {
 #ifdef CONFIG_PPC_ISERIES
-	ppcdbg_initialize();
-	
 #if defined(CONFIG_BLK_DEV_INITRD)
 	/*
 	 * If the init RAM disk has been configured and there is
@@ -397,28 +389,6 @@
 			*(p+1) = 0;
 	}
 
-        if (strstr(cmd_line, "dprofile=")) {
-                char *p, *q;
-
-                for (q = cmd_line; (p = strstr(q, "dprofile=")) != 0; ) {
-			unsigned long size, new_klimit;
-                        q = p + 9;
-                        if (p > cmd_line && p[-1] != ' ')
-                                continue;
-                        dprof_shift = simple_strtoul(q, &q, 0);
-			dprof_len = (unsigned long)&_etext - (unsigned long)&_stext;
-			dprof_len >>= dprof_shift;
-			size = ((dprof_len * sizeof(unsigned int)) + (PAGE_SIZE-1)) & PAGE_MASK;
-			dprof_buffer = (unsigned int *)((klimit + (PAGE_SIZE-1)) & PAGE_MASK);
-			new_klimit = ((unsigned long)dprof_buffer) + size;
-			lmb_reserve( __pa(klimit), (new_klimit-klimit));
-			klimit = new_klimit;
-			memset( dprof_buffer, 0, size );
-                }
-        }
-
-	iSeries_setup_dprofile();
-
 	iSeries_proc_early_init();	
 	mf_init();
 	mf_initialized = 1;
@@ -631,7 +601,6 @@
 /*
  * Bolt the kernel addr space into the HPT
  */
-
 static void __init iSeries_bolt_kernel(unsigned long saddr, unsigned long eaddr)
 {
 	unsigned long pa;
@@ -644,12 +613,13 @@
 		unsigned long va = ( vsid << 28 ) | ( pa & 0xfffffff );
 		unsigned long vpn = va >> PAGE_SHIFT;
 		unsigned long slot = HvCallHpt_findValid( &hpte, vpn );
-		if ( hpte.dw0.dw0.v ) {
+		if (hpte.dw0.dw0.v) {
 			/* HPTE exists, so just bolt it */
-			HvCallHpt_setSwBits( slot, 0x10, 0 );
+			HvCallHpt_setSwBits(slot, 0x10, 0);
 		} else {
 			/* No HPTE exists, so create a new bolted one */
-			build_valid_hpte(vsid, ea, pa, NULL, mode_rw, 1);
+			make_pte(NULL, va, (unsigned long)__v2a(ea), 
+				 mode_rw, 0, 0);
 		}
 	}
 }
@@ -852,7 +822,6 @@
 	 */
 	tb_ticks_per_sec   = tb_ticks_per_jiffy * HZ;
 	tb_ticks_per_usec = cyclesPerUsec;
-	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
 	div128_by_32( 1024*1024, 0, tb_ticks_per_sec, &divres );
 	tb_to_xs = divres.result_low;
 	setup_default_decr();
@@ -884,17 +853,3 @@
 	}
 }
 
-static void iSeries_setup_dprofile(void)
-{
-	if ( dprof_buffer ) {
-		unsigned i;
-		for (i=0; i<MAX_PACAS; ++i) {
-			paca[i].prof_shift = dprof_shift;
-			paca[i].prof_len = dprof_len-1;
-			paca[i].prof_buffer = dprof_buffer;
-			paca[i].prof_stext = (unsigned *)&_stext;
-			mb();
-			paca[i].prof_enabled = 1;
-		}
-	}
-}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/ioctl32.c linuxppc64_2_4/arch/ppc64/kernel/ioctl32.c
--- linux-2.4.19/arch/ppc64/kernel/ioctl32.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/ioctl32.c	Mon Aug 19 08:43:16 2002
@@ -44,6 +44,7 @@
 #include <linux/ppp_defs.h>
 #include <linux/if_ppp.h>
 #include <linux/if_pppox.h>
+#include <linux/if_tun.h>
 #include <linux/mtio.h>
 #include <linux/cdrom.h>
 #include <linux/loop.h>
@@ -102,6 +103,7 @@
 #include <linux/usb.h>
 #include <linux/usbdevice_fs.h>
 #include <linux/nbd.h>
+#include <linux/random.h>
 #include <asm/ppc32.h>
 #include <asm/ppcdebug.h>
 
@@ -469,6 +471,7 @@
 		return -ENODEV;
 
 	strcpy(ifr32.ifr_name, dev->name);
+	dev_put(dev);
 
 	err = copy_to_user((struct ifreq32 *)arg, &ifr32, sizeof(struct ifreq32));
 	return (err ? -EFAULT : 0);
@@ -1157,6 +1160,234 @@
 	return err;
 }
 
+typedef struct sg_io_hdr32 {
+	s32 interface_id;	/* [i] 'S' for SCSI generic (required) */
+	s32 dxfer_direction;	/* [i] data transfer direction  */
+	u8  cmd_len;		/* [i] SCSI command length ( <= 16 bytes) */
+	u8  mx_sb_len;		/* [i] max length to write to sbp */
+	u16 iovec_count;	/* [i] 0 implies no scatter gather */
+	u32 dxfer_len;		/* [i] byte count of data transfer */
+	u32 dxferp;		/* [i], [*io] points to data transfer memory
+					      or scatter gather list */
+	u32 cmdp;		/* [i], [*i] points to command to perform */
+	u32 sbp;		/* [i], [*o] points to sense_buffer memory */
+	u32 timeout;		/* [i] MAX_UINT->no timeout (unit: millisec) */
+	u32 flags;		/* [i] 0 -> default, see SG_FLAG... */
+	s32 pack_id;		/* [i->o] unused internally (normally) */
+	u32 usr_ptr;		/* [i->o] unused internally */
+	u8  status;		/* [o] scsi status */
+	u8  masked_status;	/* [o] shifted, masked scsi status */
+	u8  msg_status;		/* [o] messaging level data (optional) */
+	u8  sb_len_wr;		/* [o] byte count actually written to sbp */
+	u16 host_status;	/* [o] errors from host adapter */
+	u16 driver_status;	/* [o] errors from software driver */
+	s32 resid;		/* [o] dxfer_len - actual_transferred */
+	u32 duration;		/* [o] time taken by cmd (unit: millisec) */
+	u32 info;		/* [o] auxiliary information */
+} sg_io_hdr32_t;  /* 64 bytes long (on sparc32) */
+
+typedef struct sg_iovec32 {
+	u32 iov_base;
+	u32 iov_len;
+} sg_iovec32_t;
+
+static int alloc_sg_iovec(sg_io_hdr_t *sgp, u32 uptr32)
+{
+	sg_iovec32_t *uiov = (sg_iovec32_t *) A(uptr32);
+	sg_iovec_t *kiov;
+	int i;
+
+	sgp->dxferp = kmalloc(sgp->iovec_count *
+			      sizeof(sg_iovec_t), GFP_KERNEL);
+	if (!sgp->dxferp)
+		return -ENOMEM;
+	memset(sgp->dxferp, 0,
+	       sgp->iovec_count * sizeof(sg_iovec_t));
+
+	kiov = (sg_iovec_t *) sgp->dxferp;
+	for (i = 0; i < sgp->iovec_count; i++) {
+		u32 iov_base32;
+		if (__get_user(iov_base32, &uiov->iov_base) ||
+		    __get_user(kiov->iov_len, &uiov->iov_len))
+			return -EFAULT;
+
+		kiov->iov_base = kmalloc(kiov->iov_len, GFP_KERNEL);
+		if (!kiov->iov_base)
+			return -ENOMEM;
+		if (copy_from_user(kiov->iov_base,
+				   (void *) A(iov_base32),
+				   kiov->iov_len))
+			return -EFAULT;
+
+		uiov++;
+		kiov++;
+	}
+
+	return 0;
+}
+
+static int copy_back_sg_iovec(sg_io_hdr_t *sgp, u32 uptr32)
+{
+	sg_iovec32_t *uiov = (sg_iovec32_t *) A(uptr32);
+	sg_iovec_t *kiov = (sg_iovec_t *) sgp->dxferp;
+	int i;
+
+	for (i = 0; i < sgp->iovec_count; i++) {
+		u32 iov_base32;
+
+		if (__get_user(iov_base32, &uiov->iov_base))
+			return -EFAULT;
+
+		if (copy_to_user((void *) A(iov_base32),
+				 kiov->iov_base,
+				 kiov->iov_len))
+			return -EFAULT;
+
+		uiov++;
+		kiov++;
+	}
+
+	return 0;
+}
+
+static void free_sg_iovec(sg_io_hdr_t *sgp)
+{
+	sg_iovec_t *kiov = (sg_iovec_t *) sgp->dxferp;
+	int i;
+
+	for (i = 0; i < sgp->iovec_count; i++) {
+		if (kiov->iov_base) {
+			kfree(kiov->iov_base);
+			kiov->iov_base = NULL;
+		}
+		kiov++;
+	}
+	kfree(sgp->dxferp);
+	sgp->dxferp = NULL;
+}
+
+static int sg_ioctl_trans(unsigned int fd, unsigned int cmd, unsigned long arg)
+{
+	sg_io_hdr32_t *sg_io32;
+	sg_io_hdr_t sg_io64;
+	u32 dxferp32, cmdp32, sbp32;
+	mm_segment_t old_fs;
+	int err = 0;
+
+	sg_io32 = (sg_io_hdr32_t *)arg;
+	err = __get_user(sg_io64.interface_id, &sg_io32->interface_id);
+	err |= __get_user(sg_io64.dxfer_direction, &sg_io32->dxfer_direction);
+	err |= __get_user(sg_io64.cmd_len, &sg_io32->cmd_len);
+	err |= __get_user(sg_io64.mx_sb_len, &sg_io32->mx_sb_len);
+	err |= __get_user(sg_io64.iovec_count, &sg_io32->iovec_count);
+	err |= __get_user(sg_io64.dxfer_len, &sg_io32->dxfer_len);
+	err |= __get_user(sg_io64.timeout, &sg_io32->timeout);
+	err |= __get_user(sg_io64.flags, &sg_io32->flags);
+	err |= __get_user(sg_io64.pack_id, &sg_io32->pack_id);
+
+	sg_io64.dxferp = NULL;
+	sg_io64.cmdp = NULL;
+	sg_io64.sbp = NULL;
+
+	err |= __get_user(cmdp32, &sg_io32->cmdp);
+	sg_io64.cmdp = kmalloc(sg_io64.cmd_len, GFP_KERNEL);
+	if (!sg_io64.cmdp) {
+		err = -ENOMEM;
+		goto out;
+	}
+	if (copy_from_user(sg_io64.cmdp,
+			   (void *) A(cmdp32),
+			   sg_io64.cmd_len)) {
+		err = -EFAULT;
+		goto out;
+	}
+
+	err |= __get_user(sbp32, &sg_io32->sbp);
+	sg_io64.sbp = kmalloc(sg_io64.mx_sb_len, GFP_KERNEL);
+	if (!sg_io64.sbp) {
+		err = -ENOMEM;
+		goto out;
+	}
+	if (copy_from_user(sg_io64.sbp,
+			   (void *) A(sbp32),
+			   sg_io64.mx_sb_len)) {
+		err = -EFAULT;
+		goto out;
+	}
+
+	err |= __get_user(dxferp32, &sg_io32->dxferp);
+	if (sg_io64.iovec_count) {
+		int ret;
+
+		if ((ret = alloc_sg_iovec(&sg_io64, dxferp32))) {
+			err = ret;
+			goto out;
+		}
+	} else {
+		sg_io64.dxferp = kmalloc(sg_io64.dxfer_len, GFP_KERNEL);
+		if (!sg_io64.dxferp) {
+			err = -ENOMEM;
+			goto out;
+		}
+		if (copy_from_user(sg_io64.dxferp,
+				   (void *) A(dxferp32),
+				   sg_io64.dxfer_len)) {
+			err = -EFAULT;
+			goto out;
+		}
+	}
+
+	/* Unused internally, do not even bother to copy it over. */
+	sg_io64.usr_ptr = NULL;
+
+	if (err)
+		return -EFAULT;
+
+	old_fs = get_fs();
+	set_fs (KERNEL_DS);
+	err = sys_ioctl (fd, cmd, (unsigned long) &sg_io64);
+	set_fs (old_fs);
+
+	if (err < 0)
+		goto out;
+
+	err = __put_user(sg_io64.pack_id, &sg_io32->pack_id);
+	err |= __put_user(sg_io64.status, &sg_io32->status);
+	err |= __put_user(sg_io64.masked_status, &sg_io32->masked_status);
+	err |= __put_user(sg_io64.msg_status, &sg_io32->msg_status);
+	err |= __put_user(sg_io64.sb_len_wr, &sg_io32->sb_len_wr);
+	err |= __put_user(sg_io64.host_status, &sg_io32->host_status);
+	err |= __put_user(sg_io64.driver_status, &sg_io32->driver_status);
+	err |= __put_user(sg_io64.resid, &sg_io32->resid);
+	err |= __put_user(sg_io64.duration, &sg_io32->duration);
+	err |= __put_user(sg_io64.info, &sg_io32->info);
+	err |= copy_to_user((void *)A(sbp32), sg_io64.sbp, sg_io64.mx_sb_len);
+	if (sg_io64.dxferp) {
+		if (sg_io64.iovec_count)
+			err |= copy_back_sg_iovec(&sg_io64, dxferp32);
+		else
+			err |= copy_to_user((void *)A(dxferp32),
+					    sg_io64.dxferp,
+					    sg_io64.dxfer_len);
+	}
+	if (err)
+		err = -EFAULT;
+
+out:
+	if (sg_io64.cmdp)
+		kfree(sg_io64.cmdp);
+	if (sg_io64.sbp)
+		kfree(sg_io64.sbp);
+	if (sg_io64.dxferp) {
+		if (sg_io64.iovec_count) {
+			free_sg_iovec(&sg_io64);
+		} else {
+			kfree(sg_io64.dxferp);
+		}
+	}
+	return err;
+}
+
 struct ppp_option_data32 {
 	__kernel_caddr_t32	ptr;
 	__u32			length;
@@ -2161,12 +2392,11 @@
 		if (l->lv_block_exception) {
 			lbe32 = (lv_block_exception32_t *)A(ptr2);
 			memset(lbe, 0, size);
-                       for (i = 0; i < l->lv_remap_end; i++, lbe++, lbe32++) {
-                               err |= get_user(lbe->rsector_org, &lbe32->rsector_org);
-                               err |= __get_user(lbe->rdev_org, &lbe32->rdev_org);
-                               err |= __get_user(lbe->rsector_new, &lbe32->rsector_new);
-                               err |= __get_user(lbe->rdev_new, &lbe32->rdev_new);
-
+			for (i = 0; i < l->lv_remap_end; i++, lbe++, lbe32++) {
+				err |= get_user(lbe->rsector_org, &lbe32->rsector_org);
+				err |= __get_user(lbe->rdev_org, &lbe32->rdev_org);
+				err |= __get_user(lbe->rsector_new, &lbe32->rsector_new);
+				err |= __get_user(lbe->rdev_new, &lbe32->rdev_new);
 			}
 		}
 	}
@@ -2222,16 +2452,17 @@
 	switch (cmd) {
 	case VG_STATUS:
 		v = kmalloc(sizeof(vg_t), GFP_KERNEL);
-		if (!v) return -ENOMEM;
+		if (!v)
+			return -ENOMEM;
 		karg = v;
 		break;
 
 	case VG_CREATE_OLD:
 	case VG_CREATE:
 		v = kmalloc(sizeof(vg_t), GFP_KERNEL);
-		if (!v) return -ENOMEM;
-		if (copy_from_user(v, (void *)arg, (long)&((vg32_t *)0)->proc) ||
-		    __get_user(v->proc, &((vg32_t *)arg)->proc)) {
+		if (!v)
+			return -ENOMEM;
+		if (copy_from_user(v, (void *)arg, (long)&((vg32_t *)0)->proc)) {
 			kfree(v);
 			return -EFAULT;
 		}
@@ -2248,39 +2479,46 @@
 			return -EPERM;
 		for (i = 0; i < v->pv_max; i++) {
 			err = __get_user(ptr, &((vg32_t *)arg)->pv[i]);
-			if (err) break;
+			if (err)
+				break;
 			if (ptr) {
 				v->pv[i] = kmalloc(sizeof(pv_t), GFP_KERNEL);
 				if (!v->pv[i]) {
 					err = -ENOMEM;
 					break;
 				}
-				err = copy_from_user(v->pv[i], (void *)A(ptr), sizeof(pv32_t) - 8 - UUID_LEN+1);
+				err = copy_from_user(v->pv[i], (void *)A(ptr),
+						     sizeof(pv32_t) - 8 - UUID_LEN+1);
 				if (err) {
 					err = -EFAULT;
 					break;
 				}
-				err = copy_from_user(v->pv[i]->pv_uuid, ((pv32_t *)A(ptr))->pv_uuid, UUID_LEN+1);
+				err = copy_from_user(v->pv[i]->pv_uuid,
+						     ((pv32_t *)A(ptr))->pv_uuid,
+						     UUID_LEN+1);
 				if (err) {
 				        err = -EFAULT;
 					break;
 				}
 
-				
-				v->pv[i]->pe = NULL; v->pv[i]->inode = NULL;
+				v->pv[i]->pe = NULL;
+				v->pv[i]->bd = NULL;
 			}
 		}
 		if (!err) {
 			for (i = 0; i < v->lv_max; i++) {
 				err = __get_user(ptr, &((vg32_t *)arg)->lv[i]);
-				if (err) break;
+				if (err)
+					break;
 				if (ptr) {
 					v->lv[i] = get_lv_t(ptr, &err);
-					if (err) break;
+					if (err)
+						break;
 				}
 			}
 		}
 		break;
+
 	case LV_CREATE:
 	case LV_EXTEND:
 	case LV_REDUCE:
@@ -2288,54 +2526,70 @@
 	case LV_RENAME:
 	case LV_STATUS_BYNAME:
 	        err = copy_from_user(&u.pv_status, arg, sizeof(u.pv_status.pv_name));
-		if (err) return -EFAULT;
+		if (err)
+			return -EFAULT;
 		if (cmd != LV_REMOVE) {
 			err = __get_user(ptr, &((lv_req32_t *)arg)->lv);
-			if (err) return err;
+			if (err)
+				return err;
 			u.lv_req.lv = get_lv_t(ptr, &err);
 		} else
 			u.lv_req.lv = NULL;
 		break;
 
-
 	case LV_STATUS_BYINDEX:
-		err = get_user(u.lv_byindex.lv_index, &((lv_status_byindex_req32_t *)arg)->lv_index);
+		err = get_user(u.lv_byindex.lv_index,
+			       &((lv_status_byindex_req32_t *)arg)->lv_index);
 		err |= __get_user(ptr, &((lv_status_byindex_req32_t *)arg)->lv);
-		if (err) return err;
+		if (err)
+			return err;
 		u.lv_byindex.lv = get_lv_t(ptr, &err);
 		break;
+
 	case LV_STATUS_BYDEV:
 	        err = get_user(u.lv_bydev.dev, &((lv_status_bydev_req32_t *)arg)->dev);
+		err |= __get_user(ptr, &((lv_status_bydev_req32_t *)arg)->lv);
+		if (err)
+			return err;
 		u.lv_bydev.lv = get_lv_t(ptr, &err);
-		if (err) return err;
-		u.lv_bydev.lv = &p;
-		p.pe = NULL; p.inode = NULL;		
-		break;		
+		break;
+
 	case VG_EXTEND:
 		err = copy_from_user(&p, (void *)arg, sizeof(pv32_t) - 8 - UUID_LEN+1);
-		if (err) return -EFAULT;
+		if (err)
+			return -EFAULT;
 		err = copy_from_user(p.pv_uuid, ((pv32_t *)arg)->pv_uuid, UUID_LEN+1);
-		if (err) return -EFAULT;
-		p.pe = NULL; p.inode = NULL;
+		if (err)
+			return -EFAULT;
+		p.pe = NULL;
+		p.bd = NULL;
 		karg = &p;
 		break;
+
 	case PV_CHANGE:
 	case PV_STATUS:
 		err = copy_from_user(&u.pv_status, arg, sizeof(u.lv_req.lv_name));
-		if (err) return -EFAULT;
+		if (err)
+			return -EFAULT;
 		err = __get_user(ptr, &((pv_status_req32_t *)arg)->pv);
-		if (err) return err;
+		if (err)
+			return err;
 		u.pv_status.pv = &p;
 		if (cmd == PV_CHANGE) {
-			err = copy_from_user(&p, (void *)A(ptr), sizeof(pv32_t) - 8 - UUID_LEN+1);
-			if (err) return -EFAULT;
-			p.pe = NULL; p.inode = NULL;
+			err = copy_from_user(&p, (void *)A(ptr),
+					     sizeof(pv32_t) - 8 - UUID_LEN+1);
+			if (err)
+				return -EFAULT;
+			p.pe = NULL;
+			p.bd = NULL;
 		}
 		break;
-	}
+	};
+
         old_fs = get_fs(); set_fs (KERNEL_DS);
         err = sys_ioctl (fd, cmd, (unsigned long)karg);
         set_fs (old_fs);
+
 	switch (cmd) {
 	case VG_STATUS:
 		if (!err) {
@@ -2361,17 +2615,23 @@
 		}
 		kfree(v);
 		break;
+
 	case LV_STATUS_BYNAME:
-		if (!err && u.lv_req.lv) err = copy_lv_t(ptr, u.lv_req.lv);
+		if (!err && u.lv_req.lv)
+			err = copy_lv_t(ptr, u.lv_req.lv);
 		/* Fall through */
+
         case LV_CREATE:
 	case LV_EXTEND:
 	case LV_REDUCE:
-		if (u.lv_req.lv) put_lv_t(u.lv_req.lv);
+		if (u.lv_req.lv)
+			put_lv_t(u.lv_req.lv);
 		break;
+
 	case LV_STATUS_BYINDEX:
 		if (u.lv_byindex.lv) {
-			if (!err) err = copy_lv_t(ptr, u.lv_byindex.lv);
+			if (!err)
+				err = copy_lv_t(ptr, u.lv_byindex.lv);
 			put_lv_t(u.lv_byindex.lv);
 		}
 		break;
@@ -2387,9 +2647,11 @@
 	case PV_STATUS:
 		if (!err) {
 			err = copy_to_user((void *)A(ptr), &p, sizeof(pv32_t) - 8 - UUID_LEN+1);
-			if (err) return -EFAULT;
+			if (err)
+				return -EFAULT;
 			err = copy_to_user(((pv_t *)A(ptr))->pv_uuid, p.pv_uuid, UUID_LEN + 1);
-			if (err) return -EFAULT;
+			if (err)
+				return -EFAULT;
 		}
 		break;
 	};
@@ -2977,6 +3239,7 @@
 		set_fs (KERNEL_DS);
 		err = sys_ioctl(fd, cmd, (unsigned long)&a);
 		set_fs (old_fs);
+		break;
 	default:
 		return -EINVAL;
 	}                                        
@@ -3435,119 +3698,6 @@
 	return ((0 == ret) ? 0 : -EFAULT);
 }	
 
-struct sg_io_hdr_32
-{
-	int interface_id;
-	int dxfer_direction;
-	unsigned char cmd_len;
-	unsigned char mx_sb_len;
-	unsigned short iovec_count;
-	unsigned int dxfer_len;
-	u32 dxferp;
-	u32 cmdp;
-	u32 sbp;
-	unsigned int timeout;
-	unsigned int flags;
-	int pack_id;
-	u32 usr_ptr;
-	unsigned char status;
-	unsigned char masked_status;
-	unsigned char msg_status;
-	unsigned char sb_len_wr;
-	unsigned short host_status;
-	unsigned short driver_status;
-	int resid;
-	unsigned int duration;
-	unsigned int info;
-};
-
-static int do_sg_io(unsigned int fd, unsigned int cmd, unsigned long arg)
-{
-	struct sg_io_hdr *sg = kmalloc(sizeof(struct sg_io_hdr), GFP_KERNEL);
-	struct sg_io_hdr_32 *sg_32 = (struct sg_io_hdr_32 *)arg;
-	u32 dxferp_32;
-	u32 cmdp_32;
-	u32 sbp_32;
-	u32 usr_ptr_32;
-	int ret = -EFAULT;
-	int err;
-	mm_segment_t old_fs = get_fs();
-
-	if (!sg)
-		return -ENOMEM;
-
-	memset(sg, 0, sizeof(*sg));
-
-	err = copy_from_user(sg, sg_32, offsetof(struct sg_io_hdr, dxferp));
-	err |= __get_user(dxferp_32, &sg_32->dxferp);
-	err |= __get_user(cmdp_32, &sg_32->cmdp);
-	err |= __get_user(sbp_32, &sg_32->sbp);
-
-	if (err)
-		goto error;
-
-	sg->dxferp = (void *)A(dxferp_32);
-	sg->cmdp = (void *)A(cmdp_32);
-	sg->sbp = (void *)A(sbp_32);
-
-	err = __copy_from_user(&sg->timeout, &sg_32->timeout,
-			       (long)&sg->usr_ptr - (long)&sg->timeout);
-
-	err |= __get_user(usr_ptr_32, &sg_32->usr_ptr);
-
-	if (err)
-		goto error;
-
-	sg->usr_ptr = (void *)A(usr_ptr_32);
-
-	err = __copy_from_user(&sg->status, &sg_32->status,
-			       sizeof(struct sg_io_hdr) -
-			       offsetof(struct sg_io_hdr, status));
-
-	if (err)
-		goto error;
-
-	set_fs(KERNEL_DS);
-	ret = sys_ioctl(fd, cmd, (unsigned long)sg);
-	set_fs(old_fs);
-
-	err = copy_to_user(sg_32, sg, offsetof(struct sg_io_hdr, dxferp));
-
-	dxferp_32 = (unsigned long)sg->dxferp;
-	cmdp_32 = (unsigned long)sg->cmdp;
-	sbp_32 = (unsigned long)sg->sbp;
-	err |= __put_user(dxferp_32, &sg_32->dxferp);
-	err |= __put_user(cmdp_32, &sg_32->cmdp);
-	err |= __put_user(sbp_32, &sg_32->sbp);
-
-	if (err) {
-		ret = -EFAULT;
-		goto error;
-	}
-
-	err = __copy_to_user(&sg_32->timeout, &sg->timeout,
-			     (long)&sg->usr_ptr - (long)&sg->timeout);
-
-	usr_ptr_32 = (unsigned long)sg->usr_ptr;
-	err |= __put_user(usr_ptr_32, &sg_32->usr_ptr);
-
-	if (err) {
-		ret = -EFAULT;
-		goto error;
-	}
-
-	err = __copy_to_user(&sg_32->status, &sg->status,
-			      sizeof(struct sg_io_hdr) -
-			      offsetof(struct sg_io_hdr, status));
-
-	if (err)
-		ret = -EFAULT;
-
-error:
-	kfree(sg);
-	return ret;
-}
-
 struct ioctl_trans {
 	unsigned long cmd;
 	unsigned long handler;
@@ -3736,6 +3886,12 @@
 COMPATIBLE_IOCTL(SCSI_IOCTL_TAGGED_DISABLE),
 COMPATIBLE_IOCTL(SCSI_IOCTL_GET_BUS_NUMBER),
 COMPATIBLE_IOCTL(SCSI_IOCTL_SEND_COMMAND),
+/* Big T */
+COMPATIBLE_IOCTL(TUNSETNOCSUM),
+COMPATIBLE_IOCTL(TUNSETDEBUG),
+COMPATIBLE_IOCTL(TUNSETIFF),
+COMPATIBLE_IOCTL(TUNSETPERSIST),
+COMPATIBLE_IOCTL(TUNSETOWNER),
 /* Big V */
 COMPATIBLE_IOCTL(VT_SETMODE),
 COMPATIBLE_IOCTL(VT_GETMODE),
@@ -3812,6 +3968,8 @@
 COMPATIBLE_IOCTL(SIOCDRARP),
 COMPATIBLE_IOCTL(SIOCADDDLCI),
 COMPATIBLE_IOCTL(SIOCDELDLCI),
+COMPATIBLE_IOCTL(SIOCGIFVLAN),
+COMPATIBLE_IOCTL(SIOCSIFVLAN),
 /* SG stuff */
 COMPATIBLE_IOCTL(SG_SET_TIMEOUT),
 COMPATIBLE_IOCTL(SG_GET_TIMEOUT),
@@ -4145,6 +4303,13 @@
 COMPATIBLE_IOCTL(WDIOC_GETTEMP),
 COMPATIBLE_IOCTL(WDIOC_SETOPTIONS),
 COMPATIBLE_IOCTL(WDIOC_KEEPALIVE),
+/* Big R */
+COMPATIBLE_IOCTL(RNDGETENTCNT),
+COMPATIBLE_IOCTL(RNDADDTOENTCNT),
+COMPATIBLE_IOCTL(RNDGETPOOL),
+COMPATIBLE_IOCTL(RNDADDENTROPY),
+COMPATIBLE_IOCTL(RNDZAPENTCNT),
+COMPATIBLE_IOCTL(RNDCLEARPOOL),
 /* Bluetooth ioctls */
 COMPATIBLE_IOCTL(HCIDEVUP),
 COMPATIBLE_IOCTL(HCIDEVDOWN),
@@ -4264,6 +4429,7 @@
 HANDLE_IOCTL(FDPOLLDRVSTAT32, fd_ioctl_trans),
 HANDLE_IOCTL(FDGETFDCSTAT32, fd_ioctl_trans),
 HANDLE_IOCTL(FDWERRORGET32, fd_ioctl_trans),
+HANDLE_IOCTL(SG_IO,sg_ioctl_trans),
 HANDLE_IOCTL(PPPIOCGIDLE32, ppp_ioctl_trans),
 HANDLE_IOCTL(PPPIOCSCOMPRESS32, ppp_ioctl_trans),
 HANDLE_IOCTL(MTIOCGET32, mt_ioctl_trans),
@@ -4358,7 +4524,6 @@
 HANDLE_IOCTL(USBDEVFS_REAPURB32, do_usbdevfs_reapurb),
 HANDLE_IOCTL(USBDEVFS_REAPURBNDELAY32, do_usbdevfs_reapurb),
 HANDLE_IOCTL(USBDEVFS_DISCSIGNAL32, do_usbdevfs_discsignal),
-HANDLE_IOCTL(SG_IO, do_sg_io),
 };
 
 unsigned long ioctl32_hash_table[1024];
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/irq.c linuxppc64_2_4/arch/ppc64/kernel/irq.c
--- linux-2.4.19/arch/ppc64/kernel/irq.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/irq.c	Wed Jul  3 15:12:36 2002
@@ -1,6 +1,4 @@
 /*
- * 
- *
  *  arch/ppc/kernel/irq.c
  *
  *  Derived from arch/i386/kernel/irq.c
@@ -25,7 +23,6 @@
  * should be easier.
  */
 
-
 #include <linux/ptrace.h>
 #include <linux/errno.h>
 #include <linux/threads.h>
@@ -42,6 +39,7 @@
 #include <linux/delay.h>
 #include <linux/irq.h>
 #include <linux/proc_fs.h>
+#include <linux/seq_file.h>
 #include <linux/random.h>
 
 #include <asm/uaccess.h>
@@ -56,6 +54,7 @@
 #include <asm/iSeries/LparData.h>
 #include <asm/machdep.h>
 #include <asm/paca.h>
+#include <asm/perfmon.h>
 
 #include "local_irq.h"
 
@@ -99,7 +98,7 @@
  * this needs to be removed.
  * -- Cort
  */
-#define IRQ_KMALLOC_ENTRIES 8
+#define IRQ_KMALLOC_ENTRIES 16
 static int cache_bitmask = 0;
 static struct irqaction malloc_cache[IRQ_KMALLOC_ENTRIES];
 extern int mem_init_done;
@@ -554,58 +553,61 @@
 	spin_unlock(&desc->lock);
 }
 
-int do_IRQ(struct pt_regs *regs, int isfake)
+int do_IRQ(struct pt_regs *regs)
 {
 	int cpu = smp_processor_id();
-	int irq;
+	int irq, first = 1;
+#ifdef CONFIG_PPC_ISERIES
 	struct paca_struct *lpaca;
-	struct ItLpQueue * lpq;
-
-	/* if(cpu) udbg_printf("Entering do_IRQ\n");  */
-
-        irq_enter(cpu);
+	struct ItLpQueue *lpq;
+#endif
 
-	if (naca->platform != PLATFORM_ISERIES_LPAR) {
-	
-		/* every arch is required to have a get_irq -- Cort */
-		irq = ppc_md.get_irq( regs );
+	irq_enter(cpu);
 
-		if ( irq >= 0 ) {
-			ppc_irq_dispatch_handler( regs, irq );
-			if (ppc_md.post_irq)
-				ppc_md.post_irq( regs, irq );
-		} else {
-			/* -2 means ignore, already handled */
-			if (irq != -2) {
-				printk(KERN_DEBUG "Bogus interrupt %d from PC = %lx\n",
-					irq, regs->nip);
-				ppc_spurious_interrupts++;
-			}
-		}
-	}
-	/* if on iSeries partition */
-	else {
-		lpaca = get_paca();
+#ifdef CONFIG_PPC_ISERIES
+	lpaca = get_paca();
 #ifdef CONFIG_SMP
-		if ( lpaca->xLpPaca.xIntDword.xFields.xIpiCnt ) {
-			lpaca->xLpPaca.xIntDword.xFields.xIpiCnt = 0;
-			iSeries_smp_message_recv( regs );
-		}
-#endif /* CONFIG_SMP */
-		lpq = lpaca->lpQueuePtr;
-		if ( lpq && ItLpQueue_isLpIntPending( lpq ) )
-			lpEvent_count += ItLpQueue_process( lpq, regs );
+	if (lpaca->xLpPaca.xIntDword.xFields.xIpiCnt) {
+		lpaca->xLpPaca.xIntDword.xFields.xIpiCnt = 0;
+		iSeries_smp_message_recv(regs);
 	}
-		
+#endif /* CONFIG_SMP */
+	lpq = lpaca->lpQueuePtr;
+	if (lpq && ItLpQueue_isLpIntPending(lpq))
+		lpEvent_count += ItLpQueue_process(lpq, regs);
+#else
+	/*
+	 * Every arch is required to implement ppc_md.get_irq.
+	 * This function will either return an irq number or -1 to
+	 * indicate there are no more pending.  But the first time
+	 * through the loop this means there wasn't an IRQ pending.
+	 * The value -2 is for buggy hardware and means that this IRQ
+	 * has already been handled. -- Tom
+	 */
+	while ((irq = ppc_md.get_irq(regs)) >= 0) {
+		ppc_irq_dispatch_handler(regs, irq);
+		first = 0;
+	}
+	if (irq != -2 && first)
+		/* That's not SMP safe ... but who cares ? */
+		ppc_spurious_interrupts++;
+#endif
+
         irq_exit(cpu);
 
-	if (naca->platform == PLATFORM_ISERIES_LPAR) {
-		if ( lpaca->xLpPaca.xIntDword.xFields.xDecrInt ) {
-			lpaca->xLpPaca.xIntDword.xFields.xDecrInt = 0;
-			/* Signal a fake decrementer interrupt */
-			timer_interrupt( regs );
-		}
+#ifdef CONFIG_PPC_ISERIES
+	if (lpaca->xLpPaca.xIntDword.xFields.xDecrInt) {
+		lpaca->xLpPaca.xIntDword.xFields.xDecrInt = 0;
+		/* Signal a fake decrementer interrupt */
+		timer_interrupt(regs);
+	}
+
+	if (lpaca->xLpPaca.xIntDword.xFields.xPdcInt) {
+		lpaca->xLpPaca.xIntDword.xFields.xPdcInt = 0;
+		/* Signal a fake PMC interrupt */
+		PerformanceMonitorException();
 	}
+#endif
 
 	if (softirq_pending(cpu))
 		do_softirq();
@@ -902,9 +904,11 @@
 		unsigned i;
 		for (i=0; i<MAX_PACAS; ++i) {
 			if ( paca[i].prof_buffer && (new_value & 1) )
-				paca[i].prof_enabled = 1;
-			else
-				paca[i].prof_enabled = 0;
+				paca[i].prof_mode = PMC_STATE_DECR_PROFILE;
+			else {
+				if(paca[i].prof_mode != PMC_STATE_INITIAL) 
+					paca[i].prof_mode = PMC_STATE_READY;
+			}
 			new_value >>= 1;
 		}
 	}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/lmb.c linuxppc64_2_4/arch/ppc64/kernel/lmb.c
--- linux-2.4.19/arch/ppc64/kernel/lmb.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/lmb.c	Wed Aug  7 15:37:07 2002
@@ -12,6 +12,7 @@
  */
 
 #include <linux/config.h>
+#include <linux/kernel.h>
 #include <asm/types.h>
 #include <asm/page.h>
 #include <asm/prom.h>
@@ -27,7 +28,7 @@
 static long lmb_add_region(struct lmb_region *, unsigned long, unsigned long, unsigned long);
 
 struct lmb lmb = {
-	0,
+	0, 0,
 	{0,0,0,0,{{0,0,0}}},
 	{0,0,0,0,{{0,0,0}}}
 };
@@ -135,6 +136,10 @@
 	struct lmb *_lmb = PTRRELOC(&lmb);
 	struct lmb_region *_rgn = &(_lmb->memory);
 
+	/* On pSeries LPAR systems, the first LMB is our RMO region. */
+	if ( base == 0 )
+		_lmb->rmo_size = size;
+
 	return lmb_add_region(_rgn, base, size, LMB_MEMORY_AREA);
 
 }
@@ -242,12 +247,17 @@
 	return (i < rgn->cnt) ? i : -1;
 }
 
-
 unsigned long
 lmb_alloc(unsigned long size, unsigned long align)
 {
+	return lmb_alloc_base(size, align, LMB_ALLOC_ANYWHERE);
+}
+
+unsigned long
+lmb_alloc_base(unsigned long size, unsigned long align, unsigned long max_addr)
+{
 	long i, j;
-	unsigned long base;
+	unsigned long base = 0;
 	unsigned long offset = reloc_offset();
 	struct lmb *_lmb = PTRRELOC(&lmb);
 	struct lmb_region *_mem = &(_lmb->memory);
@@ -261,7 +271,12 @@
 		if ( lmbtype != LMB_MEMORY_AREA )
 			continue;
 
-		base = _ALIGN_DOWN(lmbbase+lmbsize-size, align);
+		if ( max_addr == LMB_ALLOC_ANYWHERE )
+			base = _ALIGN_DOWN(lmbbase+lmbsize-size, align);
+		else if ( lmbbase < max_addr )
+			base = _ALIGN_DOWN(min(lmbbase+lmbsize,max_addr)-size, align);
+		else
+			continue;
 
 		while ( (lmbbase <= base) &&
 			((j = lmb_overlaps_region(_rsv,base,size)) >= 0) ) {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/mf.c linuxppc64_2_4/arch/ppc64/kernel/mf.c
--- linux-2.4.19/arch/ppc64/kernel/mf.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/mf.c	Thu Aug  8 12:54:35 2002
@@ -410,7 +410,7 @@
 		mf_powerOff();
 	}
 	else
-		printk( KERN_ALERT "mf.c: init has been successfully notified to proceed with shutdown\n" );
+		printk( KERN_INFO "mf.c: init has been successfully notified to proceed with shutdown\n" );
 
 	return rc;
 }
@@ -436,7 +436,7 @@
 		case 0x5B:	/* power control notification */
 			if ( (event->xUnion.xCEMsgData.xCEMsg[5]&0x20) != 0 )
 			{
-				printk( KERN_ALERT "mf.c: Commencing partition shutdown\n" );
+				printk( KERN_INFO "mf.c: Commencing partition shutdown\n" );
 				if ( shutdown() == 0 )
 					signalCEMsg( "\x00\x00\x00\xDB\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
 			}
@@ -479,7 +479,7 @@
 			signalEvent( NULL );
 		break;
 	case 1:	/* IT sys shutdown */
-		printk( KERN_ALERT "mf.c: Commencing system shutdown\n" );
+		printk( KERN_INFO "mf.c: Commencing system shutdown\n" );
 		shutdown();
 		break;
 	}
@@ -666,7 +666,7 @@
  */
 void mf_powerOff( void )
 {
-	printk( KERN_ALERT "mf.c: Down it goes...\n" );
+	printk( KERN_INFO "mf.c: Down it goes...\n" );
 	signalCEMsg( "\x00\x00\x00\x4D\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
 	for (;;);
 }
@@ -677,7 +677,7 @@
  */
 void mf_reboot( void )
 {
-	printk( KERN_ALERT "mf.c: Preparing to bounce...\n" );
+	printk( KERN_INFO "mf.c: Preparing to bounce...\n" );
 	signalCEMsg( "\x00\x00\x00\x4E\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
 	for (;;);
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/misc.S linuxppc64_2_4/arch/ppc64/kernel/misc.S
--- linux-2.4.19/arch/ppc64/kernel/misc.S	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/misc.S	Fri Jun 28 11:29:14 2002
@@ -745,7 +745,8 @@
 	.llong .sys_fremovexattr	/* 220 */
 	.llong .sys_futex
 #endif
-	.rept NR_syscalls-221
+	.llong .sys_perfmonctl   /* Put this here for now ... */
+	.rept NR_syscalls-222
 		.llong .sys_ni_syscall
 	.endr
 #endif
@@ -975,6 +976,7 @@
 	.llong .sys_fremovexattr	/* 220 */
 	.llong .sys_futex
 #endif
-	.rept NR_syscalls-221
+	.llong .sys_perfmonctl   /* Put this here for now ... */
+	.rept NR_syscalls-222
 	.llong .sys_ni_syscall
 	.endr
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/mk_defs.c linuxppc64_2_4/arch/ppc64/kernel/mk_defs.c
--- linux-2.4.19/arch/ppc64/kernel/mk_defs.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/mk_defs.c	Wed Jul  3 15:12:36 2002
@@ -79,11 +79,33 @@
 	DEFINE(PACAPROCENABLED, offsetof(struct paca_struct, xProcEnabled));
 	DEFINE(PACAHRDWINTCOUNT, offsetof(struct paca_struct, xHrdIntCount));
 	DEFINE(PACADEFAULTDECR, offsetof(struct paca_struct, default_decr));
-	DEFINE(PACAPROFENABLED, offsetof(struct paca_struct, prof_enabled));
+
+	DEFINE(PACAPROFMODE, offsetof(struct paca_struct, prof_mode));
 	DEFINE(PACAPROFLEN, offsetof(struct paca_struct, prof_len));
 	DEFINE(PACAPROFSHIFT, offsetof(struct paca_struct, prof_shift));
 	DEFINE(PACAPROFBUFFER, offsetof(struct paca_struct, prof_buffer));
 	DEFINE(PACAPROFSTEXT, offsetof(struct paca_struct, prof_stext));
+	DEFINE(PACAPROFETEXT, offsetof(struct paca_struct, prof_etext));
+	DEFINE(PACAPMC1, offsetof(struct paca_struct, pmc[0]));
+	DEFINE(PACAPMC2, offsetof(struct paca_struct, pmc[1]));
+	DEFINE(PACAPMC3, offsetof(struct paca_struct, pmc[2]));
+	DEFINE(PACAPMC4, offsetof(struct paca_struct, pmc[3]));
+	DEFINE(PACAPMC5, offsetof(struct paca_struct, pmc[4]));
+	DEFINE(PACAPMC6, offsetof(struct paca_struct, pmc[5]));
+	DEFINE(PACAPMC7, offsetof(struct paca_struct, pmc[6]));
+	DEFINE(PACAPMC8, offsetof(struct paca_struct, pmc[7]));
+	DEFINE(PACAMMCR0, offsetof(struct paca_struct, pmc[8]));
+	DEFINE(PACAMMCR1, offsetof(struct paca_struct, pmc[9]));
+	DEFINE(PACAMMCRA, offsetof(struct paca_struct, pmc[10]));
+	DEFINE(PACAPMCC1, offsetof(struct paca_struct, pmcc[0]));
+	DEFINE(PACAPMCC2, offsetof(struct paca_struct, pmcc[1]));
+	DEFINE(PACAPMCC3, offsetof(struct paca_struct, pmcc[2]));
+	DEFINE(PACAPMCC4, offsetof(struct paca_struct, pmcc[3]));
+	DEFINE(PACAPMCC5, offsetof(struct paca_struct, pmcc[4]));
+	DEFINE(PACAPMCC6, offsetof(struct paca_struct, pmcc[5]));
+	DEFINE(PACAPMCC7, offsetof(struct paca_struct, pmcc[6]));
+	DEFINE(PACAPMCC8, offsetof(struct paca_struct, pmcc[7]));
+
 	DEFINE(PACALPPACA, offsetof(struct paca_struct, xLpPaca));
         DEFINE(LPPACA, offsetof(struct paca_struct, xLpPaca));
         DEFINE(PACAREGSAV, offsetof(struct paca_struct, xRegSav));
@@ -93,6 +115,7 @@
         DEFINE(LPPACASRR1, offsetof(struct ItLpPaca, xSavedSrr1));
 	DEFINE(LPPACAANYINT, offsetof(struct ItLpPaca, xIntDword.xAnyInt));
 	DEFINE(LPPACADECRINT, offsetof(struct ItLpPaca, xIntDword.xFields.xDecrInt));
+	DEFINE(LPPACAPDCINT, offsetof(struct ItLpPaca, xIntDword.xFields.xPdcInt));
         DEFINE(LPQCUREVENTPTR, offsetof(struct ItLpQueue, xSlicCurEventPtr));
         DEFINE(LPQOVERFLOW, offsetof(struct ItLpQueue, xPlicOverflowIntPending));
         DEFINE(LPEVENTFLAGS, offsetof(struct HvLpEvent, xFlags));
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/open_pic.c linuxppc64_2_4/arch/ppc64/kernel/open_pic.c
--- linux-2.4.19/arch/ppc64/kernel/open_pic.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/open_pic.c	Mon Aug  5 13:02:02 2002
@@ -33,7 +33,6 @@
 static volatile struct OpenPIC *OpenPIC = NULL;
 u_int OpenPIC_NumInitSenses __initdata = 0;
 u_char *OpenPIC_InitSenses __initdata = NULL;
-extern int use_of_interrupt_tree;
 
 void find_ISUs(void);
 
@@ -47,7 +46,6 @@
 OpenPIC_SourcePtr ISU[OPENPIC_MAX_ISU];
 
 static void openpic_end_irq(unsigned int irq_nr);
-static void openpic_ack_irq(unsigned int irq_nr);
 static void openpic_set_affinity(unsigned int irq_nr, unsigned long cpumask);
 
 struct hw_interrupt_type open_pic = {
@@ -56,14 +54,13 @@
 	NULL,
 	openpic_enable_irq,
 	openpic_disable_irq,
-	openpic_ack_irq,
+	NULL,
 	openpic_end_irq,
 	openpic_set_affinity
 };
 
 #ifdef CONFIG_SMP
 static void openpic_end_ipi(unsigned int irq_nr);
-static void openpic_ack_ipi(unsigned int irq_nr);
 static void openpic_enable_ipi(unsigned int irq_nr);
 static void openpic_disable_ipi(unsigned int irq_nr);
 
@@ -73,9 +70,9 @@
 	NULL,
 	openpic_enable_ipi,
 	openpic_disable_ipi,
-	openpic_ack_ipi,
+	NULL,
 	openpic_end_ipi,
-	0
+	NULL
 };
 #endif /* CONFIG_SMP */
 
@@ -293,7 +290,7 @@
 	}
 	OpenPIC = (volatile struct OpenPIC *)OpenPIC_Addr;
 
-	ppc_md.progress("openpic enter",0x122);
+	ppc64_boot_msg(0x20, "OpenPic Init");
 
 	t = openpic_read(&OpenPIC->Global.Feature_Reporting0);
 	switch (t & OPENPIC_FEATURE_VERSION_MASK) {
@@ -330,7 +327,7 @@
 	find_ISUs();
 
 	/* Initialize timer interrupts */
-	ppc_md.progress("openpic timer",0x3ba);
+	ppc64_boot_msg(0x21, "OpenPic Timer");
 	for (i = 0; i < OPENPIC_NUM_TIMERS; i++) {
 		/* Disabled, Priority 0 */
 		openpic_inittimer(i, 0, openpic_vec_timer+i);
@@ -340,7 +337,7 @@
 
 #ifdef CONFIG_SMP
 	/* Initialize IPI interrupts */
-	ppc_md.progress("openpic ipi",0x3bb);
+	ppc64_boot_msg(0x22, "OpenPic IPI");
 	openpic_test_broken_IPI();
 	for (i = 0; i < OPENPIC_NUM_IPI; i++) {
 		/* Disabled, Priority 10..13 */
@@ -352,7 +349,7 @@
 #endif
 
 	/* Initialize external interrupts */
-	ppc_md.progress("openpic ext",0x3bc);
+	ppc64_boot_msg(0x23, "OpenPic Ext");
 
 	openpic_set_priority(0xf);
 
@@ -385,7 +382,7 @@
 		irq_desc[i].handler = &open_pic;
 
 	/* Initialize the spurious interrupt */
-	ppc_md.progress("openpic spurious",0x3bd);
+	ppc64_boot_msg(0x24, "OpenPic Spurious");
 	openpic_set_spurious(openpic_vec_spurious);
 
 	/* Initialize the cascade */
@@ -397,7 +394,7 @@
 	openpic_set_priority(0);
 	openpic_disable_8259_pass_through();
 
-	ppc_md.progress("openpic exit",0x222);
+	ppc64_boot_msg(0x25, "OpenPic Done");
 }
 
 void openpic_setup_ISU(int isu_num, unsigned long addr)
@@ -756,13 +753,6 @@
 				(sense ? OPENPIC_SENSE_LEVEL : 0));
 }
 
-/* No spinlocks, should not be necessary with the OpenPIC
- * (1 register = 1 interrupt and we have the desc lock).
- */
-static void openpic_ack_irq(unsigned int irq_nr)
-{
-}
-
 static void openpic_end_irq(unsigned int irq_nr)
 {
 	if ((irq_desc[irq_nr].status & IRQ_LEVEL) != 0)
@@ -775,10 +765,6 @@
 }
 
 #ifdef CONFIG_SMP
-static void openpic_ack_ipi(unsigned int irq_nr)
-{
-}
-
 static void openpic_end_ipi(unsigned int irq_nr)
 {
 	/* IPIs are marked IRQ_PER_CPU. This has the side effect of
@@ -825,4 +811,3 @@
 		irq = -1;
 	return irq;
 }
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pSeries_lpar.c linuxppc64_2_4/arch/ppc64/kernel/pSeries_lpar.c
--- linux-2.4.19/arch/ppc64/kernel/pSeries_lpar.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pSeries_lpar.c	Mon Aug 19 09:28:36 2002
@@ -21,6 +21,7 @@
 
 #include <linux/config.h>
 #include <linux/kernel.h>
+#include <linux/fs.h>
 #include <asm/processor.h>
 #include <asm/semaphore.h>
 #include <asm/mmu.h>
@@ -33,128 +34,7 @@
 #include <asm/pci_dma.h>
 #include <linux/pci.h>
 #include <asm/naca.h>
-
-/* Status return values */
-#define H_Success	0
-#define H_Busy		1	/* Hardware busy -- retry later */
-#define H_Hardware	-1	/* Hardware error */
-#define H_Function	-2	/* Function not supported */
-#define H_Privilege	-3	/* Caller not privileged */
-#define H_Parameter	-4	/* Parameter invalid, out-of-range or conflicting */
-#define H_Bad_Mode	-5	/* Illegal msr value */
-#define H_PTEG_Full	-6	/* PTEG is full */
-#define H_Not_Found	-7	/* PTE was not found" */
-#define H_Reserved_DABR	-8	/* DABR address is reserved by the hypervisor on this processor" */
-
-/* Flags */
-#define H_LARGE_PAGE		(1UL<<(63-16))
-#define H_EXACT		    (1UL<<(63-24))	/* Use exact PTE or return H_PTEG_FULL */
-#define H_R_XLATE		(1UL<<(63-25))	/* include a valid logical page num in the pte if the valid bit is set */
-#define H_READ_4		(1UL<<(63-26))	/* Return 4 PTEs */
-#define H_AVPN			(1UL<<(63-32))	/* An avpn is provided as a sanity test */
-#define H_ICACHE_INVALIDATE	(1UL<<(63-40))	/* icbi, etc.  (ignored for IO pages) */
-#define H_ICACHE_SYNCHRONIZE	(1UL<<(63-41))	/* dcbst, icbi, etc (ignored for IO pages */
-#define H_ZERO_PAGE		(1UL<<(63-48))	/* zero the page before mapping (ignored for IO pages) */
-#define H_COPY_PAGE		(1UL<<(63-49))
-#define H_N			(1UL<<(63-61))
-#define H_PP1			(1UL<<(63-62))
-#define H_PP2			(1UL<<(63-63))
-
-
-
-/* pSeries hypervisor opcodes */
-#define H_REMOVE		0x04
-#define H_ENTER			0x08
-#define H_READ			0x0c
-#define H_CLEAR_MOD		0x10
-#define H_CLEAR_REF		0x14
-#define H_PROTECT		0x18
-#define H_GET_TCE		0x1c
-#define H_PUT_TCE		0x20
-#define H_SET_SPRG0		0x24
-#define H_SET_DABR		0x28
-#define H_PAGE_INIT		0x2c
-#define H_SET_ASR		0x30
-#define H_ASR_ON		0x34
-#define H_ASR_OFF		0x38
-#define H_LOGICAL_CI_LOAD	0x3c
-#define H_LOGICAL_CI_STORE	0x40
-#define H_LOGICAL_CACHE_LOAD	0x44
-#define H_LOGICAL_CACHE_STORE	0x48
-#define H_LOGICAL_ICBI		0x4c
-#define H_LOGICAL_DCBF		0x50
-#define H_GET_TERM_CHAR		0x54
-#define H_PUT_TERM_CHAR		0x58
-#define H_REAL_TO_LOGICAL	0x5c
-#define H_HYPERVISOR_DATA	0x60
-#define H_EOI			0x64
-#define H_CPPR			0x68
-#define H_IPI			0x6c
-#define H_IPOLL			0x70
-#define H_XIRR			0x74
-
-#define HSC			".long 0x44000022\n"
-#define H_ENTER_r3		"li	3, 0x08\n"
-
-/* plpar_hcall() -- Generic call interface using above opcodes
- *
- * The actual call interface is a hypervisor call instruction with
- * the opcode in R3 and input args in R4-R7.
- * Status is returned in R3 with variable output values in R4-R11.
- * Only H_PTE_READ with H_READ_4 uses R6-R11 so we ignore it for now
- * and return only two out args which MUST ALWAYS BE PROVIDED.
- */
-long plpar_hcall(unsigned long opcode,
-		 unsigned long arg1,
-		 unsigned long arg2,
-		 unsigned long arg3,
-		 unsigned long arg4,
-		 unsigned long *out1,
-		 unsigned long *out2,
-		 unsigned long *out3);
-
-/* Same as plpar_hcall but for those opcodes that return no values
- * other than status.  Slightly more efficient.
- */
-long plpar_hcall_norets(unsigned long opcode, ...);
-
-
-long plpar_pte_enter(unsigned long flags,
-		     unsigned long ptex,
-		     unsigned long new_pteh, unsigned long new_ptel,
-		     unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
-{
-	unsigned long dummy, ret;
-	ret = plpar_hcall(H_ENTER, flags, ptex, new_pteh, new_ptel,
-			   old_pteh_ret, old_ptel_ret, &dummy);
-	return(ret);
-}
-
-long plpar_pte_remove(unsigned long flags,
-		      unsigned long ptex,
-		      unsigned long avpn,
-		      unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
-{
-	unsigned long dummy;
-	return plpar_hcall(H_REMOVE, flags, ptex, avpn, 0,
-			   old_pteh_ret, old_ptel_ret, &dummy);
-}
-
-long plpar_pte_read(unsigned long flags,
-		    unsigned long ptex,
-		    unsigned long *old_pteh_ret, unsigned long *old_ptel_ret)
-{
-	unsigned long dummy;
-	return plpar_hcall(H_READ, flags, ptex, 0, 0,
-			   old_pteh_ret, old_ptel_ret, &dummy);
-}
-
-long plpar_pte_protect(unsigned long flags,
-		       unsigned long ptex,
-		       unsigned long avpn)
-{
-	return plpar_hcall_norets(H_PROTECT, flags, ptex);
-}
+#include <asm/hvcall.h>
 
 long plpar_tce_get(unsigned long liobn,
 		   unsigned long ioba,
@@ -222,357 +102,6 @@
 			   xirr_ret, mfrr_ret, &dummy);
 }
 
-/*
- * The following section contains code that ultimately should
- * be put in the relavent file (htab.c, xics.c, etc).  It has
- * been put here for the time being in order to ease maintainence
- * of the pSeries LPAR code until it can all be put into CVS.
- */
-static void hpte_invalidate_pSeriesLP(unsigned long slot)
-{
-	HPTE old_pte;
-	unsigned long lpar_rc;
-	unsigned long flags = 0;
-			
-	lpar_rc = plpar_pte_remove(flags,
-				   slot,
-				   0,
-				   &old_pte.dw0.dword0, 
-				   &old_pte.dw1.dword1);
-	if (lpar_rc != H_Success) BUG();
-}
-
-/* NOTE: for updatepp ops we are fortunate that the linux "newpp" bits and
- * the low 3 bits of flags happen to line up.  So no transform is needed.
- * We can probably optimize here and assume the high bits of newpp are
- * already zero.  For now I am paranoid.
- */
-static void hpte_updatepp_pSeriesLP(long slot, unsigned long newpp, unsigned long va)
-{
-	unsigned long lpar_rc;
-	unsigned long flags;
-	flags =   newpp & 3;
-	lpar_rc = plpar_pte_protect( flags,
-				     slot,
-				     0);
-	if (lpar_rc != H_Success) {
-		udbg_printf( " bad return code from pte protect rc = %lx \n", lpar_rc); 
-		for (;;);
-	}
-}
-
-static void hpte_updateboltedpp_pSeriesLP(unsigned long newpp, unsigned long ea)
-{
-	unsigned long lpar_rc;
-	unsigned long vsid,va,vpn,flags;
-	long slot;
-
-	vsid = get_kernel_vsid( ea );
-	va = ( vsid << 28 ) | ( ea & 0x0fffffff );
-	vpn = va >> PAGE_SHIFT;
-
-	slot = ppc_md.hpte_find( vpn );
-	flags =   newpp & 3;
-	lpar_rc = plpar_pte_protect( flags,
-				     slot,
-				     0);
-	if (lpar_rc != H_Success) {
-		udbg_printf( " bad return code from pte bolted protect rc = %lx \n", lpar_rc); 
-		for (;;);
-	}
-}
-
-
-static unsigned long hpte_getword0_pSeriesLP(unsigned long slot)
-{
-	unsigned long dword0;
-	unsigned long lpar_rc;
-	unsigned long dummy_word1;
-	unsigned long flags;
-	/* Read 1 pte at a time                        */
-	/* Do not need RPN to logical page translation */
-	/* No cross CEC PFT access                     */
-	flags = 0;
-	
-	lpar_rc = plpar_pte_read(flags,
-				 slot,
-				 &dword0, &dummy_word1);
-	if (lpar_rc != H_Success) {
-		udbg_printf(" error on pte read in get_hpte0 rc = %lx \n", lpar_rc);
-		for (;;);
-	}
-
-	return(dword0);
-}
-
-static long hpte_selectslot_pSeriesLP(unsigned long vpn)
-{
-	unsigned long primary_hash;
-	unsigned long hpteg_slot;
-	unsigned i, k;
-	unsigned long flags;
-	HPTE  pte_read;
-	unsigned long lpar_rc;
-
-	/* Search the primary group for an available slot */
-	primary_hash = hpt_hash(vpn, 0);
-
-	hpteg_slot = ( primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-
-	/* Read 1 pte at a time                        */
-	/* Do not need RPN to logical page translation */
-	/* No cross CEC PFT access                     */
-	flags = 0;
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		/* read the hpte entry from the slot */
-		lpar_rc = plpar_pte_read(flags,
-					 hpteg_slot + i,
-					 &pte_read.dw0.dword0, &pte_read.dw1.dword1);
-		if (lpar_rc != H_Success) {
-			udbg_printf(" read of hardware page table failed rc = %lx \n", lpar_rc); 
-			for (;;);
-		}
-		if ( pte_read.dw0.dw0.v == 0 ) {
-			/* If an available slot found, return it */
-			return hpteg_slot + i;
-		}
-
-	}
-
-
-	/* Search the secondary group for an available slot */
-	hpteg_slot = ( ~primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-
-
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		/* read the hpte entry from the slot */
-		lpar_rc = plpar_pte_read(flags,
-					 hpteg_slot + i,
-					 &pte_read.dw0.dword0, &pte_read.dw1.dword1);
-		if (lpar_rc != H_Success) {
-			udbg_printf(" read of hardware page table failed2 rc = %lx  \n", lpar_rc); 
-			for (;;);
-		}
-		if ( pte_read.dw0.dw0.v == 0 ) {
-			/* If an available slot found, return it */
-			return hpteg_slot + i;
-		}
-
-	}
-
-	/* No available entry found in secondary group */
-
-
-	/* Select an entry in the primary group to replace */
-
-	hpteg_slot = ( primary_hash & htab_data.htab_hash_mask ) * HPTES_PER_GROUP;
-
-	k = htab_data.next_round_robin++ & 0x7;
-
-	for (i=0; i<HPTES_PER_GROUP; ++i) {
-		if (k == HPTES_PER_GROUP)
-			k = 0;
-
-		lpar_rc = plpar_pte_read(flags,
-					 hpteg_slot + k,
-					 &pte_read.dw0.dword0, &pte_read.dw1.dword1);
-		if (lpar_rc != H_Success) {
-			udbg_printf( " pte read failed - rc = %lx", lpar_rc); 
-			for (;;);
-		}
-		if (  ! pte_read.dw0.dw0.bolted)
-		{
-			hpteg_slot += k;
-			/* Invalidate the current entry */
-			ppc_md.hpte_invalidate(hpteg_slot); 
-			return hpteg_slot;
-		}
-		++k;
-	}
-
-	/* No non-bolted entry found in primary group - time to panic */
-	udbg_printf("select_hpte_slot - No non-bolted HPTE in group 0x%lx! \n", hpteg_slot/HPTES_PER_GROUP);
-	udbg_printf("No non-bolted HPTE in group %lx", (unsigned long)hpteg_slot/HPTES_PER_GROUP);
-	for (;;);
-
-	/* never executes - avoid compiler errors */
-	return 0;
-}
-
-
-static void hpte_create_valid_pSeriesLP(unsigned long slot, unsigned long vpn,
-					unsigned long prpn, unsigned hash, 
-					void *ptep, unsigned hpteflags, 
-					unsigned bolted)
-{
-	/* Local copy of HPTE */
-	struct {
-		/* Local copy of first doubleword of HPTE */
-		union {
-			unsigned long d;
-			Hpte_dword0   h;
-		} dw0;
-		/* Local copy of second doubleword of HPTE */
-		union {
-			unsigned long     d;
-			Hpte_dword1       h;
-			Hpte_dword1_flags f;
-		} dw1;
-	} lhpte;
-	
-	unsigned long avpn = vpn >> 11;
-	unsigned long arpn = physRpn_to_absRpn( prpn );
-
-	unsigned long lpar_rc;
-	unsigned long flags;
-	HPTE ret_hpte;
-
-	/* Fill in the local HPTE with absolute rpn, avpn and flags */
-	lhpte.dw1.d        = 0;
-	lhpte.dw1.h.rpn    = arpn;
-	lhpte.dw1.f.flags  = hpteflags;
-
-	lhpte.dw0.d        = 0;
-	lhpte.dw0.h.avpn   = avpn;
-	lhpte.dw0.h.h      = hash;
-	lhpte.dw0.h.bolted = bolted;
-	lhpte.dw0.h.v      = 1;
-
-	/* Now fill in the actual HPTE */
-	/* Set CEC cookie to 0                  */
-	/* Large page = 0                       */
-	/* Zero page = 0                        */
-	/* I-cache Invalidate = 0               */
-	/* I-cache synchronize = 0              */
-	/* Exact = 1 - only modify exact entry  */
-	flags = H_EXACT;
-
-	if (hpteflags & (_PAGE_GUARDED|_PAGE_NO_CACHE))
-		lhpte.dw1.f.flags &= ~_PAGE_COHERENT;
-#if 1
-	__asm__ __volatile__ (
-		 H_ENTER_r3
-		 "mr	4, %1\n"
-		 "mr	5, %2\n"
-		 "mr	6, %3\n"
-		 "mr	7, %4\n"
-		 HSC
-		 "mr	%0, 3\n"
-		 : "=r" (lpar_rc)
-		 : "r" (flags), "r" (slot), "r" (lhpte.dw0.d), "r" (lhpte.dw1.d)
-		 : "r3", "r4", "r5", "r6", "r7", "cc");
-#else
-	lpar_rc =  plpar_pte_enter(flags,
-				   slot,
-				   lhpte.dw0.d,
-				   lhpte.dw1.d,
-				   &ret_hpte.dw0.dword0,
-				   &ret_hpte.dw1.dword1);
-#endif
-	if (lpar_rc != H_Success) {
-		udbg_printf("error on pte enter lapar rc = %ld\n",lpar_rc);
-		udbg_printf("ent: s=%lx, dw0=%lx, dw1=%lx\n", slot, lhpte.dw0.d, lhpte.dw1.d);
-		/* xmon_backtrace("backtrace"); */
-		for (;;);
-	}
-}
-
-static long hpte_find_pSeriesLP(unsigned long vpn)
-{
-	union {
-		unsigned long d;
-		Hpte_dword0   h;
-	} hpte_dw0;
-	long slot;
-	unsigned long hash;
-	unsigned long i,j;
-
-	hash = hpt_hash(vpn, 0);
-	for ( j=0; j<2; ++j ) {
-		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-		for ( i=0; i<HPTES_PER_GROUP; ++i ) {
-			hpte_dw0.d = hpte_getword0_pSeriesLP( slot );
-			if ( ( hpte_dw0.h.avpn == ( vpn >> 11 ) ) &&
-			     ( hpte_dw0.h.v ) &&
-			     ( hpte_dw0.h.h == j ) ) {
-				/* HPTE matches */
-				if ( j )
-					slot = -slot;
-				return slot;
-			}
-			++slot;
-		}
-		hash = ~hash;
-	}
-	return -1;
-} 
-
-/*
- * Create a pte - LPAR .  Used during initialization only.
- * We assume the PTE will fit in the primary PTEG.
- */
-void make_pte_LPAR(HPTE *htab,
-		   unsigned long va, unsigned long pa, int mode,
-		   unsigned long hash_mask, int large)
-{
-	HPTE  local_hpte, ret_hpte;
-	unsigned long hash, slot, flags,lpar_rc, vpn;
-
-	if (large)
-		vpn = va >> 24;
-	else
-		vpn = va >> 12;
-
-	hash = hpt_hash(vpn, large);
-
-	slot = ((hash & hash_mask)*HPTES_PER_GROUP);
-
-	local_hpte.dw1.dword1 = pa | mode;
-	local_hpte.dw0.dword0 = 0;
-	local_hpte.dw0.dw0.avpn = va >> 23;
-	local_hpte.dw0.dw0.bolted = 1;				/* bolted */
-	if (large)
-		local_hpte.dw0.dw0.l = 1;  /* large page */
-	local_hpte.dw0.dw0.v = 1;
-
-	/* Set CEC cookie to 0                   */
-	/* Zero page = 0                         */
-	/* I-cache Invalidate = 0                */
-	/* I-cache synchronize = 0               */
-	/* Exact = 0 - modify any entry in group */
-	flags = 0;
-#if 1
-	__asm__ __volatile__ (
-		 H_ENTER_r3
-		 "mr	4, %1\n"
-		 "mr	5, %2\n"
-		 "mr	6, %3\n"
-		 "mr	7, %4\n"
-		 HSC
-		 "mr	%0, 3\n"
-		 : "=r" (lpar_rc)
-		 : "r" (flags), "r" (slot), "r" (local_hpte.dw0.dword0), "r" (local_hpte.dw1.dword1)
-		 : "r3", "r4", "r5", "r6", "r7", "cc");
-#else
-	lpar_rc =  plpar_pte_enter(flags,
-				   slot,
-				   local_hpte.dw0.dword0,
-				   local_hpte.dw1.dword1,
-				   &ret_hpte.dw0.dword0,
-				   &ret_hpte.dw1.dword1);
-#endif
-#if 0 /* NOTE: we explicitly do not check return status here because it is
-       * "normal" for early boot code to map io regions for which a partition
-       * has no access.  However, we will die if we actually fault on these
-       * "permission denied" pages.
-       */
-	if (lpar_rc != H_Success) {
-		/* pSeriesLP_init_early(); */
-		udbg_printf("flags=%lx, slot=%lx, dword0=%lx, dword1=%lx, rc=%d\n", flags, slot, local_hpte.dw0.dword0,local_hpte.dw1.dword1, lpar_rc);
-		BUG();
-	}
-#endif
-}
 
 static void tce_build_pSeriesLP(struct TceTable *tbl, long tcenum, 
 				unsigned long uaddr, int direction )
@@ -746,55 +275,6 @@
 }
 
 
-/* This is called early in setup.c.
- * Use it to setup page table ppc_md stuff as well as udbg.
- */
-void pSeriesLP_init_early(void)
-{
-	ppc_md.hpte_invalidate   = hpte_invalidate_pSeriesLP;
-	ppc_md.hpte_updatepp     = hpte_updatepp_pSeriesLP;
-	ppc_md.hpte_updateboltedpp  = hpte_updateboltedpp_pSeriesLP;
-	ppc_md.hpte_getword0     = hpte_getword0_pSeriesLP;
-	ppc_md.hpte_selectslot   = hpte_selectslot_pSeriesLP;
-	ppc_md.hpte_create_valid = hpte_create_valid_pSeriesLP;
-	ppc_md.hpte_find	 = hpte_find_pSeriesLP;
-
-	ppc_md.tce_build	 = tce_build_pSeriesLP;
-	ppc_md.tce_free_one	 = tce_free_one_pSeriesLP;
-
-#ifdef CONFIG_SMP
-	smp_init_pSeries();
-#endif
-	pSeries_pcibios_init_early();
-
-	/* The keyboard is not useful in the LPAR environment.
-	 * Leave all the interfaces NULL.
-	 */
-
-	if (naca->serialPortAddr) {
-		void *comport = (void *)__ioremap(naca->serialPortAddr, 16, _PAGE_NO_CACHE);
-		udbg_init_uart(comport);
-		ppc_md.udbg_putc = udbg_putc;
-		ppc_md.udbg_getc = udbg_getc;
-		ppc_md.udbg_getc_poll = udbg_getc_poll;
-	} else {
-		/* lookup the first virtual terminal number in case we don't have a com port.
-		 * Zero is probably correct in case someone calls udbg before the init.
-		 * The property is a pair of numbers.  The first is the starting termno (the
-		 * one we use) and the second is the number of terminals.
-		 */
-		u32 *termno;
-		struct device_node *np = find_path_device("/rtas");
-		if (np) {
-			termno = (u32 *)get_property(np, "ibm,termno", 0);
-			if (termno)
-				vtermno = termno[0];
-		}
-		ppc_md.udbg_putc = udbg_putcLP;
-		ppc_md.udbg_getc = udbg_getcLP;
-		ppc_md.udbg_getc_poll = udbg_getc_pollLP;
-	}
-}
 
 /* Code for hvc_console.  Should move it back eventually. */
 
@@ -853,3 +333,43 @@
 	}
 	return 0;
 }
+
+#ifndef CONFIG_PPC_ISERIES
+void pSeries_lpar_mm_init(void);
+
+/* This is called early in setup.c.
+ * Use it to setup page table ppc_md stuff as well as udbg.
+ */
+void pSeriesLP_init_early(void)
+{
+	pSeries_lpar_mm_init();
+
+	ppc_md.tce_build	 = tce_build_pSeriesLP;
+	ppc_md.tce_free_one	 = tce_free_one_pSeriesLP;
+
+#ifdef CONFIG_SMP
+	smp_init_pSeries();
+#endif
+	pSeries_pcibios_init_early();
+
+	/* The keyboard is not useful in the LPAR environment.
+	 * Leave all the interfaces NULL.
+	 */
+
+	/* lookup the first virtual terminal number in case we don't have a com port.
+	 * Zero is probably correct in case someone calls udbg before the init.
+	 * The property is a pair of numbers.  The first is the starting termno (the
+	 * one we use) and the second is the number of terminals.
+	 */
+	u32 *termno;
+	struct device_node *np = find_path_device("/rtas");
+	if (np) {
+		termno = (u32 *)get_property(np, "ibm,termno", 0);
+		if (termno)
+			vtermno = termno[0];
+	}
+	ppc_md.udbg_putc = udbg_putcLP;
+	ppc_md.udbg_getc = udbg_getcLP;
+	ppc_md.udbg_getc_poll = udbg_getc_pollLP;
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pSeries_pci.c linuxppc64_2_4/arch/ppc64/kernel/pSeries_pci.c
--- linux-2.4.19/arch/ppc64/kernel/pSeries_pci.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pSeries_pci.c	Mon Aug 19 09:55:19 2002
@@ -22,13 +22,11 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/pci.h>
 #include <linux/delay.h>
 #include <linux/string.h>
 #include <linux/init.h>
-#include <linux/ide.h>
 #include <linux/bootmem.h>
 
 #include <asm/io.h>
@@ -80,14 +78,12 @@
 	 \
 	if (dn == NULL) { \
 		ret = -2; \
-	} else if (dn->status) { \
-		ret = -1; \
 	} else { \
 		addr = (dn->busno << 16) | (dn->devfn << 8) | offset; \
 		buid = dn->phb->buid; \
 		if (buid) { \
 			ret = rtas_call(ibm_read_pci_config, 4, 2, &returnval, addr, buid >> 32, buid & 0xffffffff, nbytes); \
-                        if (ret < 0) \
+                        if (ret < 0 || (returnval == 0xffffffff)) \
                                ret = rtas_fake_read(dn, offset, nbytes, &returnval); \
 		} else { \
 			ret = rtas_call(read_pci_config, 2, 2, &returnval, addr, nbytes); \
@@ -113,8 +109,6 @@
 	 \
 	if (dn == NULL) { \
 		ret = -2; \
-	} else if (dn->status) { \
-		ret = -1; \
 	} else { \
 		buid = dn->phb->buid; \
 		addr = (dn->busno << 16) | (dn->devfn << 8) | offset; \
@@ -491,7 +485,7 @@
 					 0x100000); 
 
 		/* 
-		 * Firmware doesnt always clear this bit which is critical
+		 * Firmware doesn't always clear this bit which is critical
 		 * for good performance - Anton
 		 */
 		{
@@ -659,13 +653,20 @@
 			continue;
 		}
 
+		if (dev->resource[i].start > dev->resource[i].end) {
+			/* Bogus resource.  Just clear it out. */
+			dev->resource[i].start = dev->resource[i].end = 0;
+			continue;
+		}
+
+
 		if (dev->resource[i].flags & IORESOURCE_IO) {
 			if (is_eeh_implemented()) {
 				unsigned int busno = dev->bus ? dev->bus->number : 0;
 				unsigned long size = dev->resource[i].end - dev->resource[i].start;
 				unsigned long addr = (unsigned long)__ioremap(dev->resource[i].start + phb->io_base_phys, size, _PAGE_NO_CACHE);
 				if (!addr)
-					panic("fixup_resources: ioremap failed!\n");
+					panic("fixup_resources: io ioremap failed!\n");
 				dev->resource[i].start = eeh_token(phb->global_number, busno, dev->devfn, addr) | eeh_disable_bit;
 				dev->resource[i].end = dev->resource[i].start + size;
 			} else {
@@ -685,7 +686,7 @@
 					unsigned long size = dev->resource[i].end - dev->resource[i].start;
 					unsigned long addr = (unsigned long)__ioremap(dev->resource[i].start + phb->pci_mem_offset, size, _PAGE_NO_CACHE);
 					if (!addr)
-						panic("fixup_resources: ioremap failed!\n");
+						panic("fixup_resources: mem ioremap failed!\n");
 					dev->resource[i].start = eeh_token(phb->global_number, busno, dev->devfn, addr) | eeh_disable_bit;
 					dev->resource[i].end = dev->resource[i].start + size;
 				} else {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pacaData.c linuxppc64_2_4/arch/ppc64/kernel/pacaData.c
--- linux-2.4.19/arch/ppc64/kernel/pacaData.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pacaData.c	Tue Jul 23 02:40:20 2002
@@ -7,7 +7,6 @@
  *      2 of the License, or (at your option) any later version.
  */
 
-#define __KERNEL__
 #include <asm/types.h>
 #include <asm/page.h>
 #include <stddef.h>
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pci.c linuxppc64_2_4/arch/ppc64/kernel/pci.c
--- linux-2.4.19/arch/ppc64/kernel/pci.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pci.c	Tue Aug 20 08:31:30 2002
@@ -74,7 +74,7 @@
 long   Pci_Cfg_Write_Count= 0;
 long   Pci_Error_Count    = 0;
 
-int    Pci_Retry_Max      = 3;	/* Only retry 3 times  */	
+int    Pci_Retry_Max      = 7;	/* Retry set to 7 times  */	
 int    Pci_Error_Flag     = 1;	/* Set Retry Error on. */
 int    Pci_Trace_Flag     = 0;
 
@@ -452,9 +452,9 @@
 	iSeries_pcibios_init(); 
 #endif
 
+	ppc64_boot_msg(0x40, "PCI Probe");
 	printk("PCI: Probing PCI hardware\n");
 	PPCDBG(PPCDBG_BUSWALK,"PCI: Probing PCI hardware\n");
-				
 
 	/* Scan all of the recorded PCI controllers.  */
 	for (next_busno = 0, hose = hose_head; hose; hose = hose->next) {
@@ -489,11 +489,13 @@
 #endif
 
 	/* Cache the location of the ISA bridge (if we have one) */
-	if (ppc64_isabridge_dev = pci_find_class(PCI_CLASS_BRIDGE_ISA << 8, NULL))
+	ppc64_isabridge_dev = pci_find_class(PCI_CLASS_BRIDGE_ISA << 8, NULL);
+	if (ppc64_isabridge_dev != NULL )
 		printk("ISA bridge at %s\n", ppc64_isabridge_dev->slot_name);
 
 	printk("PCI: Probing PCI hardware done\n");
 	PPCDBG(PPCDBG_BUSWALK,"PCI: Probing PCI hardware done.\n");
+	ppc64_boot_msg(0x41, "PCI Done");
 
 }
 
@@ -511,11 +513,11 @@
 
 void __init pcibios_fixup_bus(struct pci_bus *bus)
 {
+#ifndef CONFIG_PPC_ISERIES
 	struct pci_controller *phb = PCI_GET_PHB_PTR(bus);
 	struct resource *res;
 	int i;
 
-#ifndef CONFIG_PPC_ISERIES
 	if (bus->parent == NULL) {
 		/* This is a host bridge - fill in its resources */
 		phb->bus = bus;
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pci_dma.c linuxppc64_2_4/arch/ppc64/kernel/pci_dma.c
--- linux-2.4.19/arch/ppc64/kernel/pci_dma.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pci_dma.c	Fri Jul 26 10:30:02 2002
@@ -1389,10 +1389,11 @@
 	PPCDBG(PPCDBG_TCE, "pci_unmap_sg:\n");
 	PPCDBG(PPCDBG_TCE, "\thwdev = 0x%16.16lx, sg = 0x%16.16lx, direction = 0x%16.16lx, nelms = 0x%16.16lx\n", hwdev, sg, direction, nelms);	
 
-	if ( direction == PCI_DMA_NONE )
+	if ( direction == PCI_DMA_NONE || nelms == 0 )
 		BUG();
 
 	dma_start_page = sg->dma_address & PAGE_MASK;
+ 	dma_end_page   = 0;
 	for ( i=nelms; i>0; --i ) {
 		unsigned k = i - 1;
 		if ( sg[k].dma_length ) {
@@ -1408,6 +1409,7 @@
  	/* Client asked for way to much space.  This is checked later anyway */
 	/* It is easier to debug here for the drivers than in the tce tables.*/
  	if(order >= NUM_TCE_LEVELS) {
+		printk("PCI_DMA: dma_start_page:0x%lx  dma_end_page:0x%lx\n",dma_start_page,dma_end_page);
 		printk("PCI_DMA: pci_unmap_sg size to large: 0x%x \n",(numTces << PAGE_SHIFT));
  		return;
  	}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pci_dn.c linuxppc64_2_4/arch/ppc64/kernel/pci_dn.c
--- linux-2.4.19/arch/ppc64/kernel/pci_dn.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pci_dn.c	Mon Aug 19 02:13:56 2002
@@ -249,8 +249,9 @@
 	*eads = *other_eads;
 	eads->devfn &= ~7;	/* make it function zero */
 	eads->tce_table = NULL;
-	/* NOTE: share properties.  We could copy but for now this should suffice.
-	 * The full_name is also incorrect...but seems harmless.
+	/*
+	 * NOTE: share properties.  We could copy but for now this should
+	 * suffice.  The full_name is also incorrect...but seems harmless.
 	 */
 	eads->child = NULL;
 	eads->next = NULL;
@@ -285,21 +286,25 @@
 		dev->sysdata = dn;
 		/* ToDo: call some device init hook here */
 	} else {
-		/* Now it is very possible that we can't find the device because it is
-		 * not the zero'th device of a mutifunction device and we don't have
-		 * permission to read the zero'th device.  If this is the case, Linux
-		 * would ordinarily skip all the other functions.
+		/* Now it is very possible that we can't find the device
+		 * because it is not the zero'th device of a mutifunction
+		 * device and we don't have permission to read the zero'th
+		 * device.  If this is the case, Linux would ordinarily skip
+		 * all the other functions.
 		 */
 		if ((searchval & 0x7) == 0) {
 			struct device_node *thisdevdn;
 			/* Ok, we are looking for fn == 0.  Let's check for other functions. */
 			thisdevdn = (struct device_node *)traverse_pci_devices(phb_dn, is_devfn_sub_node, NULL, (void *)searchval);
 			if (thisdevdn) {
-				/* Ah ha!  There does exist a sub function.  Now this isn't an exact
-				 * match for searchval, but in order to get Linux to believe the sub
-				 * functions exist we will need to manufacture a fake device_node
-				 * for this zero'th function.  To keept this simple for now we only
-				 * handle pci bridges and we just hand back the found node which
+				/* Ah ha!  There does exist a sub function.
+				 * Now this isn't an exact match for
+				 * searchval, but in order to get Linux to
+				 * believe the sub functions exist we will
+				 * need to manufacture a fake device_node for
+				 * this zero'th function.  To keept this
+				 * simple for now we only handle pci bridges
+				 * and we just hand back the found node which
 				 * isn't correct, but Linux won't care.
 				 */
 				char *device_type = (char *)get_property(thisdevdn, "device_type", 0);
@@ -367,7 +372,6 @@
 		pci_fixup_bus_sysdata_list(&bus->children);
 	}
 }
-
 
 /******************************************************************
  * Fixup the bus->sysdata ptrs to point to the bus' device_node.
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/perfmon.c linuxppc64_2_4/arch/ppc64/kernel/perfmon.c
--- linux-2.4.19/arch/ppc64/kernel/perfmon.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kernel/perfmon.c	Wed Jul  3 15:34:07 2002
@@ -0,0 +1,404 @@
+/*
+ * This file contains the code to configure and utilize the ppc64 pmc hardware
+ * Copyright (C) 2002 David Engebretsen <engebret@us.ibm.com>
+ */
+
+#include <asm/proc_fs.h>
+#include <asm/paca.h>
+#include <asm/iSeries/ItLpPaca.h>
+#include <asm/iSeries/ItLpQueue.h>
+#include <asm/processor.h>
+#include <linux/proc_fs.h>
+#include <linux/spinlock.h>
+#include <asm/pmc.h>
+#include <asm/uaccess.h>
+#include <asm/naca.h>
+#include <asm/perfmon.h>
+
+extern char _stext[], _etext[], _end[];
+struct perfmon_base_struct perfmon_base = {0, 0, 0, 0, 0, PMC_STATE_INITIAL};
+
+int alloc_perf_buffer(int size);
+int free_perf_buffer(void);
+int clear_buffers(void);
+void pmc_stop(void *data);
+void pmc_start(void *data);
+void pmc_touch_bolted(void *data);
+void dump_pmc_struct(struct perfmon_struct *perfdata);
+void dump_hardware_pmc_struct(void *perfdata);
+int  decr_profile(struct perfmon_struct *perfdata);
+int  pmc_profile(struct perfmon_struct *perfdata);
+int  pmc_set_general(struct perfmon_struct *perfdata);
+int  pmc_set_user_general(struct perfmon_struct *perfdata);
+void pmc_configure(void *data);
+
+asmlinkage int
+sys_perfmonctl (int cmd, void *data) 
+{ 
+	struct perfmon_struct *pdata;
+	int err;
+
+	printk("sys_perfmonctl: cmd = 0x%x\n", cmd); 
+	pdata = kmalloc(sizeof(struct perfmon_struct), GFP_USER);
+	err = __copy_from_user(pdata, data, sizeof(struct perfmon_struct));
+	switch(cmd) {
+	case PMC_OP_ALLOC:
+		alloc_perf_buffer(0); 
+		break;
+	case PMC_OP_FREE:
+		free_perf_buffer(); 
+		break;
+	case PMC_OP_CLEAR:
+		clear_buffers();
+		break;
+	case PMC_OP_DUMP:
+		dump_pmc_struct(pdata);
+		copy_to_user(data, pdata, sizeof(struct perfmon_struct));
+		break;
+	case PMC_OP_DUMP_HARDWARE:
+		dump_hardware_pmc_struct(pdata);
+		smp_call_function(dump_hardware_pmc_struct, (void *)pdata, 0, 1);
+		break;
+	case PMC_OP_DECR_PROFILE: /* NIA time sampling */
+		decr_profile(pdata); 
+		break;
+	case PMC_OP_PMC_PROFILE:
+		pmc_profile(pdata); 
+		break;
+	case PMC_OP_SET:
+		pmc_set_general(pdata); 
+		break;
+	case PMC_OP_SET_USER:
+		pmc_set_user_general(pdata); 
+		break;
+	default:
+		printk("Perfmon: Unknown operation\n");
+		break;
+	}
+
+	kfree(pdata); 
+	return 0;
+}
+
+int alloc_perf_buffer(int size) 
+{
+	int i;
+
+	printk("Perfmon: allocate buffer\n");
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		perfmon_base.profile_length = (((unsigned long) &_etext - 
+				   (unsigned long) &_stext) >> 2) * sizeof(int);
+		perfmon_base.profile_buffer = (unsigned long)btmalloc(perfmon_base.profile_length);
+		perfmon_base.trace_length = 1024*1024*16;
+		perfmon_base.trace_buffer = (unsigned long)btmalloc(perfmon_base.trace_length);
+
+		if(perfmon_base.profile_buffer && perfmon_base.trace_buffer) {
+			memset((char *)perfmon_base.profile_buffer, 0, perfmon_base.profile_length);
+			printk("Profile buffer created at address 0x%lx of length 0x%lx\n",
+			       perfmon_base.profile_buffer, perfmon_base.profile_length); 
+		} else {
+			printk("Profile buffer creation failed\n");
+			return 0;
+		}
+
+		/* Fault in the first bolted segment - it then remains in the stab for all time */
+		pmc_touch_bolted(NULL); 
+		smp_call_function(pmc_touch_bolted, (void *)NULL, 0, 1);
+
+		for (i=0; i<MAX_PACAS; ++i) {
+			paca[i].prof_shift = 2;
+			paca[i].prof_len = perfmon_base.profile_length;
+			paca[i].prof_buffer = (unsigned *)(perfmon_base.profile_buffer);
+			paca[i].prof_stext = (unsigned *)&_stext;
+
+			paca[i].prof_etext = (unsigned *)&_etext;
+			mb();
+		} 
+
+		perfmon_base.state = PMC_STATE_READY; 
+	}
+
+	return 0;
+}
+
+int free_perf_buffer() 
+{
+	printk("Perfmon: free buffer\n");
+
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: free buffer failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+
+	btfree((void *)perfmon_base.profile_buffer); 
+	btfree((void *)perfmon_base.trace_buffer); 
+
+	perfmon_base.profile_length = 0;
+	perfmon_base.profile_buffer = 0;
+	perfmon_base.trace_buffer   = 0;
+	perfmon_base.trace_length   = 0;
+	perfmon_base.trace_end      = 0;
+	perfmon_base.state = PMC_STATE_INITIAL; 
+
+	return(0); 
+}
+
+int clear_buffers() 
+{
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: clear buffer failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+
+	printk("Perfmon: clear buffer\n");
+	
+	/* Stop counters on all processors -- blocking */
+	pmc_stop(NULL); 
+	smp_call_function(pmc_stop, (void *)NULL, 0, 1);
+	
+	/* Clear the buffers */
+	memset((char *)perfmon_base.profile_buffer, 0, perfmon_base.profile_length);
+	memset((char *)perfmon_base.trace_buffer, 0, perfmon_base.trace_length);
+	
+	/* Reset the trace buffer point */
+	perfmon_base.trace_end = 0;
+	
+	/* Restart counters on all processors -- blocking */
+	pmc_start(NULL); 
+	smp_call_function(pmc_start, (void *)NULL, 0, 1);
+
+	return(0); 
+}
+
+void pmc_stop(void *data) 
+{
+	/* Freeze all counters, leave everything else alone */
+	mtspr( MMCR0, mfspr( MMCR0 ) | 0x80000000 );
+}
+
+void pmc_start(void *data) 
+{
+	/* Free all counters, leave everything else alone */
+	mtspr( MMCR0, mfspr( MMCR0 ) & 0x7fffffff );
+}
+
+void pmc_touch_bolted(void *data) 
+{
+	volatile int touch;
+
+	/* Hack to fault the buffer into the segment table */
+	touch = *((int *)(perfmon_base.profile_buffer));
+}
+
+void dump_pmc_struct(struct perfmon_struct *perfdata) 
+{
+	unsigned int cpu = perfdata->vdata.pmc_info.cpu, i;
+
+	if(cpu > MAX_PACAS) return;
+
+	printk("PMC Control Mode: 0x%lx\n", perfmon_base.state);
+	printk("PMC[1 - 2] = 0x%16.16lx 0x%16.16lx\n",
+	       paca[cpu].pmcc[0], paca[cpu].pmcc[1]);
+	printk("PMC[3 - 4] = 0x%16.16lx 0x%16.16lx\n",
+	       paca[cpu].pmcc[2], paca[cpu].pmcc[3]);
+	printk("PMC[5 - 6] = 0x%16.16lx 0x%16.16lx\n",
+	       paca[cpu].pmcc[4], paca[cpu].pmcc[5]);
+	printk("PMC[7 - 8] = 0x%16.16lx 0x%16.16lx\n",
+	       paca[cpu].pmcc[6], paca[cpu].pmcc[7]);
+
+	perfdata->vdata.pmc_info.mode = perfmon_base.state;
+	for(i = 0; i < 11; i++) 
+		perfdata->vdata.pmc_info.pmc_base[i]  = paca[cpu].pmc[i];
+
+	for(i = 0; i < 8; i++) 
+		perfdata->vdata.pmc_info.pmc_cumulative[i]  = paca[cpu].pmcc[i];
+}
+
+void dump_hardware_pmc_struct(void *perfdata) 
+{
+	unsigned int cpu = smp_processor_id();
+
+	printk("PMC[%2.2d][1 - 4]  = 0x%8.8x 0x%8.8x 0x%8.8x 0x%8.8x\n",
+	       cpu, (u32) mfspr(PMC1),(u32) mfspr(PMC2),(u32) mfspr(PMC3),(u32) mfspr(PMC4));
+	printk("PMC[%2.2d][5 - 8]  = 0x%8.8x 0x%8.8x 0x%8.8x 0x%8.8x\n",
+	       cpu, (u32) mfspr(PMC5),(u32) mfspr(PMC6),(u32) mfspr(PMC7),(u32) mfspr(PMC8));
+	printk("MMCR[%2.2d][0,1,A] = 0x%8.8x 0x%8.8x 0x%8.8x\n",
+	       cpu, (u32) mfspr(MMCR0),(u32) mfspr(MMCR1),(u32) mfspr(MMCRA));
+}
+
+int decr_profile(struct perfmon_struct *perfdata) 
+{
+	int i;
+
+	printk("Perfmon: NIA decrementer profile\n");
+
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+	
+	/* Stop counters on all processors -- blocking */
+	pmc_stop(NULL); 
+	smp_call_function(pmc_stop, (void *)NULL, 0, 1);
+	
+	for (i=0; i<MAX_PACAS; ++i) {
+		paca[i].prof_mode = PMC_STATE_DECR_PROFILE;
+	}
+	
+	perfmon_base.state = PMC_STATE_DECR_PROFILE; 
+	mb(); 
+
+	return 0;
+}
+
+int pmc_profile(struct perfmon_struct *perfdata) 
+{
+	struct pmc_struct *pdata = &(perfdata->vdata.pmc);
+	int i;
+
+	printk("Perfmon: NIA PMC profile and CPI\n");
+
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+
+	/* Stop counters on all processors -- blocking */
+	pmc_stop(NULL); 
+	smp_call_function(pmc_stop, (void *)NULL, 0, 1);
+	
+	for (i=0; i<MAX_PACAS; ++i) {
+		paca[i].prof_mode = PMC_STATE_PROFILE_KERN;
+	}
+	perfmon_base.state = PMC_STATE_PROFILE_KERN; 
+
+	pdata->pmc[0] = 0x7f000000;
+	for(i = 1; i < 8; i++) 
+		pdata->pmc[i] = 0x0;
+	pdata->pmc[8] = 0x26000000 | (0x01 << (31 - 25) | (0x1));
+	pdata->pmc[9] = (0x3 << (31-4)); /* Instr completed */
+	pdata->pmc[10] = 0x00000000 | (0x1 << (31 - 30));
+
+	mb();
+
+	pmc_configure((void *)perfdata);
+	smp_call_function(pmc_configure, (void *)perfdata, 0, 0);
+
+	return 0;
+}
+
+int pmc_set_general(struct perfmon_struct *perfdata) 
+{
+	int i;
+
+	printk("Perfmon: PMC sampling - General\n");
+
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+
+	/* Stop counters on all processors -- blocking */
+	pmc_stop(NULL); 
+	smp_call_function(pmc_stop, (void *)NULL, 0, 1);
+	
+	for (i=0; i<MAX_PACAS; ++i) {
+		paca[i].prof_mode = PMC_STATE_TRACE_KERN;
+	}
+	perfmon_base.state = PMC_STATE_TRACE_KERN; 
+	mb();
+
+	pmc_configure((void *)perfdata);
+	smp_call_function(pmc_configure, (void *)perfdata, 0, 0);
+
+	return 0;
+}
+
+int pmc_set_user_general(struct perfmon_struct *perfdata) 
+{
+	struct pmc_struct *pdata = &(perfdata->vdata.pmc);
+	int pid = perfdata->header.pid;
+	struct task_struct *task;
+	int i;
+
+	printk("Perfmon: PMC sampling - general user\n");
+
+	if(perfmon_base.state == PMC_STATE_INITIAL) {
+		printk("Perfmon: failed - no buffer was allocated.\n"); 
+		return -1;
+	}
+
+	if(pid) {
+		printk("Perfmon: pid = 0x%x\n", pid);
+		read_lock(&tasklist_lock);
+		task = find_task_by_pid(pid);
+		if (task) {
+			printk("Perfmon: task = 0x%lx\n", (u64) task);
+			task->thread.regs->msr |= 0x4;
+#if 0
+			for(i = 0; i < 11; i++)
+				task->thread.pmc[i] = pdata->pmc[i];
+#endif
+		} else {
+			printk("Perfmon: task not found\n");
+			read_unlock(&tasklist_lock);
+			return -1;
+		}
+	}
+	read_unlock(&tasklist_lock);
+
+	/* Stop counters on all processors -- blocking */
+	pmc_stop(NULL); 
+	smp_call_function(pmc_stop, (void *)NULL, 0, 1);
+	
+	for (i=0; i<MAX_PACAS; ++i) {
+		paca[i].prof_mode = PMC_STATE_TRACE_USER;
+	}
+	perfmon_base.state = PMC_STATE_TRACE_USER; 
+	mb();
+
+	pmc_configure((void *)perfdata);
+	smp_call_function(pmc_configure, (void *)perfdata, 0, 0);
+
+	return 0;
+}
+
+void pmc_configure(void *data)
+{
+	struct paca_struct *lpaca = get_paca();
+	struct perfmon_struct *perfdata = (struct perfmon_struct *)data;
+	struct pmc_struct *pdata = &(perfdata->vdata.pmc);
+	unsigned long cmd_rec, i;
+
+	/* Indicate to hypervisor that we are using the PMCs */
+	if(naca->platform == PLATFORM_ISERIES_LPAR)
+		lpaca->xLpPacaPtr->xPMCRegsInUse = 1;
+
+	/* Freeze all counters */
+	mtspr( MMCR0, 0x80000000 ); mtspr( MMCR1, 0x00000000 );
+
+	cmd_rec = 0xFFUL << 56;
+	cmd_rec |= perfdata->header.type;
+	*((unsigned long *)(perfmon_base.trace_buffer + perfmon_base.trace_end)) = cmd_rec;
+	perfmon_base.trace_end += 8;
+
+	/* Clear all the PMCs */
+	mtspr( PMC1, 0 ); mtspr( PMC2, 0 ); mtspr( PMC3, 0 );
+	mtspr( PMC4, 0 ); mtspr( PMC5, 0 ); mtspr( PMC6, 0 );
+	mtspr( PMC7, 0 ); mtspr( PMC8, 0 );
+
+	for(i = 0; i < 11; i++)
+		lpaca->pmc[i]  = pdata->pmc[i];
+
+	mtspr(PMC1, lpaca->pmc[0]); mtspr(PMC2, lpaca->pmc[1]);
+	mtspr(PMC3, lpaca->pmc[2]); mtspr(PMC4, lpaca->pmc[3]);
+	mtspr(PMC5, lpaca->pmc[4]); mtspr(PMC6, lpaca->pmc[5]);
+	mtspr(PMC7, lpaca->pmc[6]); mtspr(PMC8, lpaca->pmc[7]);
+	mtspr(MMCR1, lpaca->pmc[9]); mtspr(MMCRA, lpaca->pmc[10]);
+
+	mb();
+	
+	/* Start all counters */
+	mtspr( MMCR0, lpaca->pmc[8]);
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/pmc.c linuxppc64_2_4/arch/ppc64/kernel/pmc.c
--- linux-2.4.19/arch/ppc64/kernel/pmc.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/pmc.c	Mon Aug 19 09:15:52 2002
@@ -31,7 +31,7 @@
 
 #include <linux/proc_fs.h>
 #include <linux/spinlock.h>
-#include <linux/malloc.h>
+#include <linux/slab.h>
 #include <linux/vmalloc.h>
 #include <asm/pmc.h>
 #include <asm/uaccess.h>
@@ -39,6 +39,10 @@
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
+#include <asm/page.h>
+#include <asm/machdep.h>
+#include <asm/lmb.h>
+#include <asm/abs_addr.h>
 #include <asm/ppcdebug.h>
 
 struct _pmc_sw pmc_sw_system = {
@@ -69,6 +73,8 @@
 struct mm_struct btmalloc_mm = {pgd             : bolted_dir,
                                 page_table_lock : SPIN_LOCK_UNLOCKED};
 
+extern spinlock_t hash_table_lock;
+
 char *
 ppc64_pmc_stab(int file)
 {
@@ -204,15 +210,16 @@
 void* btmalloc (unsigned long size) {
 	pgd_t *pgdp;
 	pmd_t *pmdp;
-	pte_t *ptep;
-	unsigned long ea_base, ea;
+	pte_t *ptep, pte;
+	unsigned long ea_base, ea, hpteflags;
 	struct vm_struct *area;
-	unsigned long pa, pg_count, page, vsid;
+	unsigned long pa, pg_count, page, vsid, slot, va, arpn, vpn;
   
 	size = PAGE_ALIGN(size);
 	if (!size || (size >> PAGE_SHIFT) > num_physpages) return NULL;
 
 	spin_lock(&btmalloc_mm.page_table_lock);
+	spin_lock(&hash_table_lock);
 
 	/* Get a virtual address region in the bolted space */
 	area = get_btm_area(size, 0);
@@ -228,6 +235,10 @@
 	for(page = 0; page < pg_count; page++) {
 		pa = get_free_page(GFP_KERNEL) - PAGE_OFFSET; 
 		ea = ea_base + (page * PAGE_SIZE);
+		vsid = get_kernel_vsid(ea);
+		va = ( vsid << 28 ) | ( pa & 0xfffffff );
+		vpn = va >> PAGE_SHIFT;
+		arpn = ((unsigned long)__v2a(ea)) >> PAGE_SHIFT;
 
 		/* Get a pointer to the linux page table entry for this page
 		 * allocating pmd or pte pages along the way as needed.  Note
@@ -236,15 +247,23 @@
 		pgdp = pgd_offset_b(ea);
 		pmdp = pmd_alloc(&btmalloc_mm, pgdp, ea);
 		ptep = pte_alloc(&btmalloc_mm, pmdp, ea);
+		pte = *ptep;
 
 		/* Clear any old hpte and set the new linux pte */
 		set_pte(ptep, mk_pte_phys(pa & PAGE_MASK, PAGE_KERNEL));
 
-		vsid = get_kernel_vsid(ea);
-		build_valid_hpte(vsid, ea, pa, ptep, 
-				  _PAGE_ACCESSED|_PAGE_COHERENT|PP_RWXX, 1);
+		hpteflags = _PAGE_ACCESSED|_PAGE_COHERENT|PP_RWXX;
+
+		pte_val(pte) &= ~_PAGE_HPTEFLAGS;
+		pte_val(pte) |= _PAGE_HASHPTE;
+
+		slot = ppc_md.hpte_insert(vpn, arpn, hpteflags, 1, 0);  
+
+		pte_val(pte) |= ((slot<<12) & 
+				 (_PAGE_GROUP_IX | _PAGE_SECONDARY));
 	}
 
+	spin_unlock(&hash_table_lock);
 	spin_unlock(&btmalloc_mm.page_table_lock);
 	return (void*)ea_base;
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/ppc_ksyms.c linuxppc64_2_4/arch/ppc64/kernel/ppc_ksyms.c
--- linux-2.4.19/arch/ppc64/kernel/ppc_ksyms.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/ppc_ksyms.c	Tue Aug 20 22:22:33 2002
@@ -40,15 +40,18 @@
 #include <asm/machdep.h>
 #include <asm/hw_irq.h>
 #include <asm/abs_addr.h>
-#ifdef CONFIG_SMP
 #include <asm/smplock.h>
-#endif /* CONFIG_SMP */
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iSeries/iSeries_pci.h>
 #include <asm/iSeries/iSeries_proc.h>
 #include <asm/iSeries/mf.h>
 #include <asm/iSeries/HvLpEvent.h>
 #include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/ItLpNaca.h>
+#include <asm/iSeries/ItExtVpdPanel.h>
+#include <asm/iSeries/LparData.h>
+#else
+#include <asm/rtas.h>
 #endif
 
 /* Tell string.h we don't want memcpy etc. as cpp defines */
@@ -61,6 +64,7 @@
 extern void AlignmentException(struct pt_regs *regs);
 extern void ProgramCheckException(struct pt_regs *regs);
 extern void SingleStepException(struct pt_regs *regs);
+extern int sys_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg);
 extern int sys_sigreturn(struct pt_regs *regs);
 extern int do_signal(sigset_t *, struct pt_regs *);
 extern int register_ioctl32_conversion(unsigned int cmd, int (*handler)(unsigned int, unsigned int, unsigned long, struct file *));
@@ -82,6 +86,7 @@
 EXPORT_SYMBOL(AlignmentException);
 EXPORT_SYMBOL(ProgramCheckException);
 EXPORT_SYMBOL(SingleStepException);
+EXPORT_SYMBOL(sys_ioctl);
 EXPORT_SYMBOL(sys_sigreturn);
 EXPORT_SYMBOL(enable_irq);
 EXPORT_SYMBOL(disable_irq);
@@ -160,6 +165,8 @@
 EXPORT_SYMBOL(mf_allocateLpEvents);
 EXPORT_SYMBOL(mf_deallocateLpEvents);
 EXPORT_SYMBOL(HvLpConfig_getLpIndex_outline);
+EXPORT_SYMBOL(itLpNaca);
+EXPORT_SYMBOL(xItExtVpdPanel);
 #endif
 
 EXPORT_SYMBOL(_insb);
@@ -189,6 +196,8 @@
 EXPORT_SYMBOL(iSeries_Read_Long);
 EXPORT_SYMBOL(iSeries_Device_ToggleReset);
 EXPORT_SYMBOL(iSeries_Write_Word);
+EXPORT_SYMBOL(iSeries_memset_io);
+EXPORT_SYMBOL(iSeries_memcpy_toio);
 EXPORT_SYMBOL(iSeries_memcpy_fromio);
 EXPORT_SYMBOL(iSeries_Read_Word);
 EXPORT_SYMBOL(iSeries_Read_Byte);
@@ -238,6 +247,13 @@
 EXPORT_SYMBOL(find_all_nodes);
 EXPORT_SYMBOL(get_property);
 
+#ifdef CONFIG_PPC_PSERIES
+EXPORT_SYMBOL(rtas_proc_dir);
+EXPORT_SYMBOL(rtas_firmware_flash_list);
+EXPORT_SYMBOL(rtas_token);
+EXPORT_SYMBOL(rtas_call);
+#endif
+
 #ifndef CONFIG_PPC_ISERIES
 EXPORT_SYMBOL(kd_mksound);
 #endif
@@ -288,3 +304,8 @@
 #endif
 
 EXPORT_SYMBOL(tb_ticks_per_usec);
+
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+extern void dump_send_ipi(int (*dump_ipi_callback)(struct pt_regs *));
+EXPORT_SYMBOL(dump_send_ipi);
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/proc_pmc.c linuxppc64_2_4/arch/ppc64/kernel/proc_pmc.c
--- linux-2.4.19/arch/ppc64/kernel/proc_pmc.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/proc_pmc.c	Fri Jun 28 11:29:40 2002
@@ -41,6 +41,8 @@
 #include <asm/pmc.h>
 #include <asm/uaccess.h>
 #include <asm/naca.h>
+#include <asm/rtas.h>
+#include <asm/perfmon.h>
 
 /* pci Flight Recorder AHT */
 extern void proc_pciFr_init(struct proc_dir_entry *proc_ppc64_root);
@@ -63,6 +65,10 @@
 			     int count, int *eof, void *data);
 int proc_ppc64_pmc_htab_read(char *page, char **start, off_t off,
 			     int count, int *eof, void *data);
+int proc_ppc64_pmc_profile_read(char *page, char **start, off_t off,
+				int count, int *eof, void *data);
+int proc_ppc64_pmc_profile_read(char *page, char **start, off_t off,
+				int count, int *eof, void *data);
 int proc_ppc64_pmc_hw_read(char *page, char **start, off_t off, 
 			   int count, int *eof, void *data);
 
@@ -88,6 +94,34 @@
 int proc_pmc_set_pmc7(  struct file *file, const char *buffer, unsigned long count, void *data);
 int proc_pmc_set_pmc8(  struct file *file, const char *buffer, unsigned long count, void *data);
 
+static loff_t  nacamap_seek( struct file *file, loff_t off, int whence);
+static ssize_t nacamap_read( struct file *file, char *buf, size_t nbytes, loff_t *ppos);
+static int     nacamap_mmap( struct file *file, struct vm_area_struct *vma );
+
+static struct file_operations nacamap_fops = {
+	llseek:	nacamap_seek,
+	read:	nacamap_read,
+	mmap:	nacamap_mmap
+};
+
+static ssize_t read_profile(struct file *file, char *buf, size_t count, loff_t *ppos);
+static ssize_t write_profile(struct file * file, const char * buf,
+			     size_t count, loff_t *ppos);
+static ssize_t read_trace(struct file *file, char *buf, size_t count, loff_t *ppos);
+static ssize_t write_trace(struct file * file, const char * buf,
+			     size_t count, loff_t *ppos);
+
+static struct file_operations proc_profile_operations = {
+	read:		read_profile,
+	write:		write_profile,
+};
+
+static struct file_operations proc_trace_operations = {
+	read:		read_trace,
+	write:		write_trace,
+};
+
+extern struct perfmon_base_struct perfmon_base;
 
 void proc_ppc64_init(void)
 {
@@ -107,8 +141,14 @@
 	if (!proc_ppc64_root) return;
 	spin_unlock(&proc_ppc64_lock);
 
-	/* /proc/ppc64/naca -- raw naca contents.  Only readable to root */
-	create_proc_read_entry("naca", S_IRUSR, proc_ppc64_root, proc_ppc64_page_read, naca);
+	ent = create_proc_entry("naca", S_IFREG|S_IRUGO, proc_ppc64_root);
+	if ( ent ) {
+		ent->nlink = 1;
+		ent->data = 0;
+		ent->size = 4096;
+		ent->proc_fops = &nacamap_fops;
+	}
+
 	/* /proc/ppc64/paca/XX -- raw paca contents.  Only readable to root */
 	ent = proc_mkdir("paca", proc_ppc64_root);
 	if (ent) {
@@ -116,6 +156,9 @@
 			proc_ppc64_create_paca(i, ent);
 	}
 
+	/* Placeholder for rtas interfaces. */
+	rtas_proc_dir = proc_mkdir("rtas", proc_ppc64_root);
+
 	/* Create the /proc/ppc64/pcifr for the Pci Flight Recorder.	 */
 	proc_pciFr_init(proc_ppc64_root);
 
@@ -167,6 +210,20 @@
 		ent->write_proc = (void *)proc_ppc64_pmc_htab_read;
 	}
 
+	ent = create_proc_entry("profile", S_IWUSR | S_IRUGO, proc_ppc64_pmc_system_root);
+	if (ent) {
+		ent->nlink = 1;
+		ent->proc_fops = &proc_profile_operations;
+		/* ent->size = (1+prof_len) * sizeof(unsigned int); */
+	}
+
+	ent = create_proc_entry("trace", S_IWUSR | S_IRUGO, proc_ppc64_pmc_system_root);
+	if (ent) {
+		ent->nlink = 1;
+		ent->proc_fops = &proc_trace_operations;
+		/* ent->size = (1+prof_len) * sizeof(unsigned int); */
+	}
+
 	/* Create directories for the hardware counters. */
 	for (i = 0; i < naca->processorCount; i++) {
 		ent = create_proc_entry("hardware", S_IRUGO | S_IWUSR, 
@@ -336,6 +393,60 @@
 	return n;
 }
 
+static ssize_t read_profile(struct file *file, char *buf,
+			    size_t count, loff_t *ppos)
+{
+	unsigned long p = *ppos;
+	ssize_t read;
+	char * pnt;
+	unsigned int sample_step = 4;
+
+	if (p >= (perfmon_base.profile_length+1)) return 0;
+	if (count > (perfmon_base.profile_length+1) - p)
+		count = (perfmon_base.profile_length+1) - p;
+	read = 0;
+
+	while (p < sizeof(unsigned int) && count > 0) {
+		put_user(*((char *)(&sample_step)+p),buf);
+		buf++; p++; count--; read++;
+	}
+	pnt = (char *)(perfmon_base.profile_buffer) + p - sizeof(unsigned int);
+	copy_to_user(buf,(void *)pnt,count);
+	read += count;
+	*ppos += read;
+	return read;
+}
+
+static ssize_t read_trace(struct file *file, char *buf,
+			    size_t count, loff_t *ppos)
+{
+	unsigned long p = *ppos;
+	ssize_t read;
+	char * pnt;
+	unsigned int sample_step = 4;
+
+	if (p >= (perfmon_base.trace_length)) return 0;
+	if (count > (perfmon_base.trace_length) - p)
+		count = (perfmon_base.trace_length) - p;
+	read = 0;
+
+	pnt = (char *)(perfmon_base.trace_buffer) + p; //  - sizeof(unsigned int);
+	copy_to_user(buf,(void *)pnt,count);
+	read += count;
+	*ppos += read;
+	return read;
+}
+
+static ssize_t write_trace(struct file * file, const char * buf,
+			     size_t count, loff_t *ppos)
+{
+}
+
+static ssize_t write_profile(struct file * file, const char * buf,
+			     size_t count, loff_t *ppos)
+{
+}
+
 int 
 proc_ppc64_pmc_hw_read(char *page, char **start, off_t off, 
 			     int count, int *eof, void *data)
@@ -377,6 +488,7 @@
     if (!ent) return;
     ent->nlink = 1;
     ent->data = (void *)0;
+    ent->size = 0;
     ent->read_proc = proc_get_titanTod;
     ent->write_proc = NULL;
 
@@ -836,5 +948,75 @@
 	mtspr( PMC8, v );
 
 	return count;
+}
+
+static loff_t nacamap_seek( struct file *file, loff_t off, int whence)
+{
+	loff_t new;
+	struct proc_dir_entry *dp;
+
+	dp = file->f_dentry->d_inode->u.generic_ip;
+
+	switch(whence) {
+	case 0:
+		new = off;
+		break;
+	case 1:
+		new = file->f_pos + off;
+		break;
+	case 2:
+		new = dp->size + off;
+		break;
+	default:
+		return -EINVAL;
+	}
+	if ( new < 0 || new > dp->size )
+		return -EINVAL;
+	return (file->f_pos = new);
+}
+
+static ssize_t nacamap_read( struct file *file, char *buf, size_t nbytes, loff_t *ppos)
+{
+	unsigned pos = *ppos;
+	unsigned size;
+	char * fromaddr;
+	struct proc_dir_entry *dp;
+
+	dp = file->f_dentry->d_inode->u.generic_ip;
+
+	size = dp->size;
+	if ( pos >= size )
+		return 0;
+	if ( nbytes >= size )
+		nbytes = size;
+	if ( pos + nbytes > size )
+		nbytes = size - pos;
+	fromaddr = (char *)(KERNELBASE + 0x4000 + pos);
+
+	copy_to_user( buf, fromaddr, nbytes );
+	*ppos = pos + nbytes;
+	return nbytes;
+}
+
+static int nacamap_mmap( struct file *file, struct vm_area_struct *vma )
+{
+	unsigned long pa;
+	long size;
+	long fsize;
+	struct proc_dir_entry *dp;
+
+	dp = file->f_dentry->d_inode->u.generic_ip;
+
+	pa = 0x4000;
+	fsize = 4096;
+
+	vma->vm_flags |= VM_SHM | VM_LOCKED;
+
+	size = vma->vm_end - vma->vm_start;
+	if ( size != 4096 )
+		return -EINVAL;
+
+	remap_page_range( vma->vm_start, pa, 4096, vma->vm_page_prot );
+	return 0;
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/process.c linuxppc64_2_4/arch/ppc64/kernel/process.c
--- linux-2.4.19/arch/ppc64/kernel/process.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/process.c	Tue Aug 20 14:46:58 2002
@@ -278,87 +278,45 @@
 	current->thread.fpscr = 0;
 }
 
-asmlinkage int sys_clone(int p1, int p2, int p3, int p4, int p5, int p6,
-			 struct pt_regs *regs)
+int sys_clone(int p1, int p2, int p3, int p4, int p5, int p6,
+	      struct pt_regs *regs)
 {
-	unsigned long clone_flags = p1;
-	int res;
-
-	PPCDBG(PPCDBG_SYS64, "sys_clone - entered - pid=%ld current=%lx comm=%s \n", current->pid, current, current->comm);
-
-	res = do_fork(clone_flags, regs->gpr[1], regs, 0);
-#ifdef CONFIG_SMP
-	/* When we clone the idle task we keep the same pid but
-	 * the return value of 0 for both causes problems.
-	 * -- Cort
-	 */
-	if ((current->pid == 0) && (current == &init_task))
-		res = 1;
-#endif /* CONFIG_SMP */
-
-	PPCDBG(PPCDBG_SYS64, "sys_clone - exited - pid=%ld current=%lx comm=%s \n", current->pid, current, current->comm);
-
-	return res;
+	return do_fork(p1, regs->gpr[1], regs, 0);
 }
 
-asmlinkage int sys_fork(int p1, int p2, int p3, int p4, int p5, int p6,
-			struct pt_regs *regs)
+int sys_fork(int p1, int p2, int p3, int p4, int p5, int p6,
+	     struct pt_regs *regs)
 {
-	int res;
-	
-	PPCDBG(PPCDBG_SYS64, "sys_fork - entered - pid=%ld comm=%s \n", current->pid, current->comm);
-
-	res = do_fork(SIGCHLD, regs->gpr[1], regs, 0);
-
-#ifdef CONFIG_SMP
-	/* When we clone the idle task we keep the same pid but
-	 * the return value of 0 for both causes problems.
-	 * -- Cort
-	 */
-	if ((current->pid == 0) && (current == &init_task))
-		res = 1;
-#endif /* CONFIG_SMP */
-	
-	PPCDBG(PPCDBG_SYS64, "sys_fork - exited - pid=%ld comm=%s \n", current->pid, current->comm);
-
-	return res;
+	return do_fork(SIGCHLD, regs->gpr[1], regs, 0);
 }
 
-asmlinkage int sys_vfork(int p1, int p2, int p3, int p4, int p5, int p6,
+int sys_vfork(int p1, int p2, int p3, int p4, int p5, int p6,
 			 struct pt_regs *regs)
 {
-  	PPCDBG(PPCDBG_SYS64, "sys_vfork - running - pid=%ld current=%lx comm=%s \n", current->pid, current, current->comm);
-
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->gpr[1], regs, 0);
 }
 
-asmlinkage int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
-			  unsigned long a3, unsigned long a4, unsigned long a5,
-			  struct pt_regs *regs)
+int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
+	       unsigned long a3, unsigned long a4, unsigned long a5,
+	       struct pt_regs *regs)
 {
 	int error;
 	char * filename;
-
-	PPCDBG(PPCDBG_SYS64, "sys_execve - entered - pid=%ld current=%lx comm=%s \n", current->pid, current, current->comm);
-
+	
 	filename = getname((char *) a0);
 	error = PTR_ERR(filename);
 	if (IS_ERR(filename))
 		goto out;
 	if (regs->msr & MSR_FP)
 		giveup_fpu(current);
-
-	PPCDBG(PPCDBG_SYS64, "sys_execve - before do_execve : filename = %s\n", filename);
-
+  
 	error = do_execve(filename, (char **) a1, (char **) a2, regs);
-
+  
 	if (error == 0)
 		current->ptrace &= ~PT_DTRACE;
 	putname(filename);
 
-  out:
-	PPCDBG(PPCDBG_SYS64, "sys_execve - exited - pid=%ld current=%lx comm=%s error = %lx\n", current->pid, current, current->comm, error);
-
+out:
 	return error;
 }
 
@@ -405,7 +363,7 @@
 	 * __get_free_pages() might give us a page > KERNBASE+256M which
 	 * is mapped with large ptes so we can't set up the guard page.
 	 */
-	if (__is_processor(PV_POWER4))
+	if (__is_processor(PV_POWER4) || __is_processor(PV_POWER4p))
 		return;
 
 	for (i=0; i < naca->processorCount; i++) {
@@ -469,10 +427,10 @@
 
 	printk("Call backtrace: \n");
 	while (sp) {
-		if (__get_user( i, &sp[2] ))
+		if (__get_user(i, &sp[2]))
 			break;
 		printk("%016lX ", i);
-		printk("%s\n", ppc_find_proc_name( (unsigned *)i, name_buf, 256 ));
+		printk("%s\n", ppc_find_proc_name((unsigned *)i, name_buf, 256));
 		if (cnt > 32) break;
 		if (__get_user(sp, (unsigned long **)sp))
 			break;
@@ -499,7 +457,7 @@
 	do {
 		sp = *(unsigned long *)sp;
 		if (sp < (stack_page + (2 * PAGE_SIZE)) ||
-		    sp >= (stack_page + (THREAD_SIZE * PAGE_SIZE)))
+		    sp >= (stack_page + THREAD_SIZE))
 			return 0;
 		if (count > 0) {
 			ip = *(unsigned long *)(sp + 16);
@@ -524,7 +482,7 @@
 	do {
 		sp = *(unsigned long *)sp;
 		if (sp < (stack_page + (2 * PAGE_SIZE)) ||
-		    sp >= (stack_page + (THREAD_SIZE * PAGE_SIZE)))
+		    sp >= (stack_page + THREAD_SIZE))
 			break;
 		if (count > 0) {
 			ip = *(unsigned long *)(sp + 16);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/prom.c linuxppc64_2_4/arch/ppc64/kernel/prom.c
--- linux-2.4.19/arch/ppc64/kernel/prom.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/prom.c	Thu Aug  8 08:08:48 2002
@@ -1,4 +1,4 @@
-/*
+/* - undefined for user space
  * 
  *
  * Procedures for interfacing to Open Firmware.
@@ -217,7 +217,10 @@
 
 void cacheable_memzero(void *, unsigned int);
 
-extern char cmd_line[512];	/* XXX */
+#ifndef CONFIG_CMDLINE
+#define CONFIG_CMDLINE ""
+#endif
+char cmd_line[512] = CONFIG_CMDLINE;
 unsigned long dev_tree_size;
 
 #ifdef CONFIG_HMT
@@ -340,6 +343,7 @@
 	struct prom_t *_prom = PTRRELOC(&prom);
         struct naca_struct *_naca = RELOC(naca);
 
+	/* NOTE: _naca->debug_switch is already initialized. */
 #ifdef DEBUG_PROM
 	prom_print(RELOC("prom_initialize_naca: start...\n"));
 #endif
@@ -358,23 +362,43 @@
 			 * d-cache and i-cache sizes... -Peter
 			 */
 			if ( num_cpus == 1 ) {
-				u32 size;
+				u32 size, lsize, sets;
 
 				call_prom(RELOC("getprop"), 4, 1, node,
-					  RELOC("d-cache-line-size"),
+					  RELOC("d-cache-size"),
 					  &size, sizeof(size));
 
-				_naca->dCacheL1LineSize     = size;
-				_naca->dCacheL1LogLineSize  = __ilog2(size);
-				_naca->dCacheL1LinesPerPage = PAGE_SIZE / size;
+				call_prom(RELOC("getprop"), 4, 1, node,
+					  RELOC("d-cache-line-size"),
+					  &lsize, sizeof(lsize));
+
+				call_prom(RELOC("getprop"), 4, 1, node,
+					  RELOC("d-cache-sets"),
+					  &sets, sizeof(sets));
+
+				_naca->dCacheL1Size         = size;
+				_naca->dCacheL1LineSize     = lsize;
+				_naca->dCacheL1LogLineSize  = __ilog2(lsize);
+				_naca->dCacheL1LinesPerPage = PAGE_SIZE/lsize;
+				_naca->dCacheL1Assoc = size / lsize / sets;
 
 				call_prom(RELOC("getprop"), 4, 1, node,
 					  RELOC("i-cache-line-size"),
 					  &size, sizeof(size));
 
-				_naca->iCacheL1LineSize     = size;
-				_naca->iCacheL1LogLineSize  = __ilog2(size);
-				_naca->iCacheL1LinesPerPage = PAGE_SIZE / size;
+				call_prom(RELOC("getprop"), 4, 1, node,
+					  RELOC("i-cache-line-size"),
+					  &lsize, sizeof(lsize));
+
+				call_prom(RELOC("getprop"), 4, 1, node,
+					  RELOC("i-cache-sets"),
+					  &sets, sizeof(sets));
+
+				_naca->iCacheL1Size         = size;
+				_naca->iCacheL1LineSize     = lsize;
+				_naca->iCacheL1LogLineSize  = __ilog2(lsize);
+				_naca->iCacheL1LinesPerPage = PAGE_SIZE/lsize;
+				_naca->iCacheL1Assoc = size / lsize / sets;
 
 				if (_naca->platform == PLATFORM_PSERIES_LPAR) {
 					u32 pft_size[2];
@@ -477,6 +501,11 @@
 	 */
 	_naca->slb_size = 64;
 
+	/* Add an eye catcher and the naca layout version number */
+	strcpy(_naca->eye_catcher, RELOC("PPC64"));
+	_naca->version     = 1;
+	_naca->processor   = _get_PVR() >> 16;
+
 #ifdef DEBUG_PROM
         prom_print(RELOC("naca->processorCount       = 0x"));
         prom_print_hex(_naca->processorCount);
@@ -620,8 +649,8 @@
 }
 
 
-static unsigned long __init
-prom_instantiate_rtas(unsigned long mem)
+static void __init
+prom_instantiate_rtas(void)
 {
 	unsigned long offset = reloc_offset();
 	struct prom_t *_prom = PTRRELOC(&prom);
@@ -654,16 +683,18 @@
 	        _rtas->size = getprop_rval;
 		prom_print(RELOC("instantiating rtas"));
 		if (_rtas->size != 0) {
-			/*
-			 * Ask OF for some space for RTAS.
-			 * Actually OF has bugs so we just arbitrarily
-			 * use memory at the 6MB point.
+			unsigned long rtas_region = RTAS_INSTANTIATE_MAX;
+
+			/* Grab some space within the first RTAS_INSTANTIATE_MAX bytes
+			 * of physical memory (or within the RMO region) because RTAS
+			 * runs in 32-bit mode and relocate off.
 			 */
-			// The new code...
-			mem = PAGE_ALIGN(mem);
-			_rtas->base = mem + offset - KERNELBASE;
+			if ( _naca->platform == PLATFORM_PSERIES_LPAR ) {
+				struct lmb *_lmb  = PTRRELOC(&lmb);
+				rtas_region = min(_lmb->rmo_size, RTAS_INSTANTIATE_MAX);
+			}
+			_rtas->base = lmb_alloc_base(_rtas->size, PAGE_SIZE, rtas_region);
 
-			mem += _rtas->size;
 			prom_print(RELOC(" at 0x"));
 			prom_print_hex(_rtas->base);
 
@@ -700,8 +731,6 @@
 #ifdef DEBUG_PROM
 	prom_print(RELOC("prom_instantiate_rtas: end...\n"));
 #endif
-
-	return mem;
 }
 
 unsigned long prom_strtoul(const char *cp)
@@ -912,8 +941,6 @@
 			    (strstr(model, RELOC("peedwagon")) == NULL) &&
 			    (strstr(model, RELOC("innipeg")) == NULL))
 				continue;
-		} else {
-			prom_print(RELOC("No known I/O bridge chip found.\n"));
 		}
 
 		if ((type[0] == 0) || (strstr(type, RELOC("pci")) == NULL)) {
@@ -1231,6 +1258,110 @@
 #endif
 }
 
+#ifdef CONFIG_PPCDBG
+extern char *trace_names[];	/* defined in udbg.c -- need a better interface */
+
+static void parse_ppcdbg_optionlist(const char *cmd,
+				    const char *cmdend)
+{
+	unsigned long offset = reloc_offset();
+	char **_trace_names = PTRRELOC(&trace_names[0]);
+	const char *all = RELOC("all");
+        struct naca_struct *_naca = RELOC(naca);
+	const char *p, *pend;
+	int onoff, i, cmdidx;
+	unsigned long mask;
+	char cmdbuf[30];
+
+	for (p = cmd, pend = strchr(p, ',');
+	     p < cmdend;
+	     pend = strchr(p, ',')) {
+		if (pend == NULL || pend > cmdend)
+			pend = cmdend;
+		onoff = 1;	/* default */
+		if (*p == '+' || *p == '-') {
+			/* explicit on or off */
+			onoff = (*p == '+');
+			p++;
+		}
+		/* parse out p..pend here */
+		if (pend - p < sizeof(cmdbuf)) {
+			strncpy(cmdbuf, p, pend - p);
+			cmdbuf[pend - p] = '\0';
+			for (cmdidx = -1, i = 0; i < PPCDBG_NUM_FLAGS; i++) {
+				if (_trace_names[i] &&
+				    (strcmp(PTRRELOC(_trace_names[i]), cmdbuf) == 0)) {
+					cmdidx = i;
+					break;
+				}
+			}
+			mask = 0;
+			if (cmdidx >= 0) {
+				mask = (1 << cmdidx);
+			} else if (strcmp(cmdbuf, all) == 0) {
+				mask = PPCDBG_ALL;
+			} else {
+				prom_print(RELOC("ppcdbg: unknown debug: "));
+				prom_print(cmdbuf);
+				prom_print_nl();
+			}
+			if (mask) {
+				if (onoff)
+					_naca->debug_switch |= mask;
+				else
+					_naca->debug_switch &= ~mask;
+			}
+		}
+		p = pend+1;
+	}
+}
+
+/*
+ * Parse ppcdbg= cmdline option.
+ *
+ * Option names are listed in <asm/ppcdebug.h> in the trace_names
+ * table.  Multiple names may be listed separated by commas (no whitespace),
+ * and each option may be preceeded by a + or - to force on or off state.
+ * The special option "all" may also be used.  They are processed strictly
+ * left to right.  Multiple ppcdbg= options are the command line are treated
+ * as a single option list.
+ *
+ * Examples:  ppcdbg=phb_init,buswalk
+ *            ppcdbg=all,-mm,-tce
+ *
+ * ToDo: add "group" names that map to common combinations of flags.
+ */
+void parse_ppcdbg_cmd_line(const char *line)
+{
+	unsigned long offset = reloc_offset();
+	const char *ppcdbgopt = RELOC("ppcdbg=");
+	struct naca_struct *_naca = RELOC(naca);
+	const char *cmd, *end;
+
+	_naca->debug_switch = PPC_DEBUG_DEFAULT; /* | PPCDBG_BUSWALK | PPCDBG_PHBINIT | PPCDBG_MM | PPCDBG_MMINIT | PPCDBG_TCEINIT | PPCDBG_TCE */
+	cmd = line;
+	while (cmd && (cmd = strstr(cmd, ppcdbgopt)) != NULL) {
+		cmd += 7;	/* skip ppcdbg= */
+		for (end = cmd;
+		     *end != '\0' && *end != '\t' && *end != ' ';
+		     end++)
+			; /* scan to whitespace or end */
+		parse_ppcdbg_optionlist(cmd, end);
+	}
+}
+#endif /* CONFIG_PPCDBG */
+
+
+/*
+ * Do minimal cmd_line parsing for early boot options.
+ */
+static void __init
+prom_parse_cmd_line(char *line)
+{
+#ifdef CONFIG_PPCDBG
+	parse_ppcdbg_cmd_line(line);
+#endif
+}
 
 /*
  * We enter here early on, when the Open Firmware prom is still
@@ -1246,13 +1377,14 @@
 	ihandle prom_mmu, prom_op, prom_root, prom_cpu;
 	phandle cpu_pkg;
 	unsigned long offset = reloc_offset();
-	long l;
+	long l, sz;
 	char *p, *d;
  	unsigned long phys;
         u32 getprop_rval;
         struct naca_struct   *_naca = RELOC(naca);
 	struct paca_struct *_xPaca = PTRRELOC(&paca[0]);
 	struct prom_t *_prom = PTRRELOC(&prom);
+	char *_cmd_line = PTRRELOC(&cmd_line[0]);
 
 	/* Default machine type. */
 	_naca->platform = PLATFORM_PSERIES;
@@ -1337,6 +1469,15 @@
 	}
 	_prom->encode_phys_size = (getprop_rval==1) ? 32 : 64;
 
+	/* Fetch the cmd_line */
+	sz = (long)call_prom(RELOC("getprop"), 4, 1, _prom->chosen,
+			    RELOC("bootargs"), _cmd_line,
+			    sizeof(cmd_line)-1);
+	if (sz > 0)
+		_cmd_line[sz] = '\0';
+
+	prom_parse_cmd_line(_cmd_line);
+
 #ifdef DEBUG_PROM
 	prom_print(RELOC("DRENG:    Detect OF version...\n"));
 #endif
@@ -1344,7 +1485,6 @@
 	prom_op = (ihandle)call_prom(RELOC("finddevice"), 1, 1, RELOC("/openprom"));
 	if (prom_op != (ihandle)-1) {
 		char model[64];
-		long sz;
 		sz = (long)call_prom(RELOC("getprop"), 4, 1, prom_op,
 				    RELOC("model"), model, 64);
 		if (sz > 0) {
@@ -1404,7 +1544,7 @@
 
 	mem = prom_bi_rec_reserve(mem);
 
-	mem = prom_instantiate_rtas(mem);
+	prom_instantiate_rtas();
         
         /* Initialize some system info into the Naca early... */
         mem = prom_initialize_naca(mem);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/ptrace.c linuxppc64_2_4/arch/ppc64/kernel/ptrace.c
--- linux-2.4.19/arch/ppc64/kernel/ptrace.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/ptrace.c	Tue Jul 23 03:10:22 2002
@@ -1,5 +1,5 @@
 /*
- *  linux/arch/ppc/kernel/ptrace.c
+ *  linux/arch/ppc64/kernel/ptrace.c
  *
  *  PowerPC version
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
@@ -126,14 +126,9 @@
 		ret = ptrace_attach(child);
 		goto out_tsk;
 	}
-	ret = -ESRCH;
-	if (!(child->ptrace & PT_PTRACED))
-		goto out_tsk;
-	if (child->state != TASK_STOPPED) {
-		if (request != PTRACE_KILL)
-			goto out_tsk;
-	}
-	if (child->p_pptr != current)
+
+	ret = ptrace_check_attach(child, request == PTRACE_KILL);
+	if (ret < 0)
 		goto out_tsk;
 
 	switch (request) {
@@ -153,16 +148,17 @@
 
 	/* read the word at location addr in the USER area. */
 	case PTRACE_PEEKUSR: {
-		unsigned long index, tmp;
+		unsigned long index;
+		unsigned long tmp;
 
 		ret = -EIO;
 		/* convert to index and check */
 		index = (unsigned long) addr >> 3;
-		if ((addr & 7) || index > PT_FPSCR)
+		if ((addr & 7) || (index > PT_FPSCR))
 			break;
 
 		if (index < PT_FPR0) {
-			tmp = get_reg(child, (int) index);
+			tmp = get_reg(child, (int)index);
 		} else {
 			if (child->thread.regs->msr & MSR_FP)
 				giveup_fpu(child);
@@ -176,7 +172,8 @@
 	case PTRACE_POKETEXT: /* write the word at location addr. */
 	case PTRACE_POKEDATA:
 		ret = 0;
-		if (access_process_vm(child, addr, &data, sizeof(data), 1) == sizeof(data))
+		if (access_process_vm(child, addr, &data, sizeof(data), 1)
+				== sizeof(data))
 			break;
 		ret = -EIO;
 		break;
@@ -188,7 +185,7 @@
 		ret = -EIO;
 		/* convert to index and check */
 		index = (unsigned long) addr >> 3;
-		if ((addr & 7) || index > PT_FPSCR)
+		if ((addr & 7) || (index > PT_FPSCR))
 			break;
 
 		if (index == PT_ORIG_R3)
@@ -221,11 +218,11 @@
 		break;
 	}
 
-/*
- * make the child exit.  Best I can do is send it a sigkill. 
- * perhaps it should be put in the status that it wants to 
- * exit.
- */
+	/*
+	 * make the child exit.  Best I can do is send it a sigkill.
+	 * perhaps it should be put in the status that it wants to
+	 * exit.
+	 */
 	case PTRACE_KILL: {
 		ret = 0;
 		if (child->state == TASK_ZOMBIE)	/* already dead */
@@ -254,56 +251,50 @@
 		ret = ptrace_detach(child, data);
 		break;
 
-	case PPC_PTRACE_GETREGS:
-	{ /* Get GPRs 0 - 31. */
+	case PPC_PTRACE_GETREGS: { /* Get GPRs 0 - 31. */
 		u64 tmp;
 		u64 cntr;
+
 		ret = 0; 
-		for (cntr=0; cntr<32 && ret==0; ++cntr)
-		{
+		for (cntr=0; cntr<32 && ret==0; ++cntr) {
 			tmp = ((u64*)child->thread.regs)[cntr];
 			ret = put_user(tmp, (u64*)(data+cntr));
 		}
 		break;
 	}
 
-	case PPC_PTRACE_SETREGS:
-	{ /* Set GPRs 0 - 31. */
+	case PPC_PTRACE_SETREGS: { /* Set GPRs 0 - 31. */
 		u64 cntr;
+
 		ret = 0; 
 		for (cntr=0; cntr<32 && ret==0; ++cntr)
-		{
 			ret = put_reg(child, cntr, *(u64*)(data+cntr));
-		}
 		break;
 	}
 
-	case PPC_PTRACE_GETFPREGS:
-	{ /* Get FPRs 0 - 31. */
+	case PPC_PTRACE_GETFPREGS: { /* Get FPRs 0 - 31. */
 		u64 tmp;
 		u64 cntr;
+
 		ret = -EIO;
 		if (child->thread.regs->msr & MSR_FP)
 			giveup_fpu(child);
 		ret = 0; 
-		for (cntr=0; cntr<32 && ret==0; ++cntr)
-		{
+		for (cntr=0; cntr<32 && ret==0; ++cntr) {
 			tmp = ((u64*)child->thread.fpr)[cntr];
 			ret = put_user(tmp, (u64*)(data+cntr));
 		}
 		break;
 	}
 
-	case PPC_PTRACE_SETFPREGS:
-	{ /* Get FPRs 0 - 31. */
+	case PPC_PTRACE_SETFPREGS: { /* Get FPRs 0 - 31. */
 		u64 cntr;
+
 		ret = -EIO;
 		if (child->thread.regs->msr & MSR_FP)
 			giveup_fpu(child);
 		for (cntr=0; cntr<32; ++cntr)
-		{
 			((u64*)child->thread.fpr)[cntr] = *(u64*)(data+cntr);
-		}
 		ret = 0; 
 		break;
 	}
@@ -338,4 +329,3 @@
 		current->exit_code = 0;
 	}
 }
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/ptrace32.c linuxppc64_2_4/arch/ppc64/kernel/ptrace32.c
--- linux-2.4.19/arch/ppc64/kernel/ptrace32.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/ptrace32.c	Tue Jul 23 03:10:22 2002
@@ -1,5 +1,5 @@
 /*
- *  linux/arch/ppc/kernel/ptrace32.c
+ *  linux/arch/ppc64/kernel/ptrace32.c
  *
  *  PowerPC version
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
@@ -86,8 +86,8 @@
 int sys32_ptrace(long request, long pid, unsigned long addr, unsigned long data)
 {
 	struct task_struct *child;
-	int ret  = -EPERM;
-  
+	int ret = -EPERM;
+
 	lock_kernel();
 	if (request == PTRACE_TRACEME) {
 		/* are we already being traced? */
@@ -115,18 +115,12 @@
 		ret = ptrace_attach(child);
 		goto out_tsk;
 	}
-	ret = -ESRCH;
-	if (!(child->ptrace & PT_PTRACED))
-		goto out_tsk;
-	if (child->state != TASK_STOPPED) {
-		if (request != PTRACE_KILL)
-			goto out_tsk;
-	}
-	if (child->p_pptr != current)
+
+	ret = ptrace_check_attach(child, request == PTRACE_KILL);
+	if (ret < 0)
 		goto out_tsk;
 
-	switch (request)
-	{
+	switch (request) {
 	/* Read word at location ADDR */
 	/* when I and D space are separate, these will need to be fixed. */
 	case PTRACE_PEEKTEXT: /* read word at location addr. */ 
@@ -143,11 +137,15 @@
 		break;
 	}
 
-	/* Read 4 bytes of the other process' storage */
-	/*  data is a pointer specifying where the user wants the 4 bytes copied into */
-	/*  addr is a pointer in the user's storage that contains an 8 byte address in the other process of the 4 bytes that is to be read */
-	/* (this is run in a 32-bit process looking at a 64-bit process) */
-	/* when I and D space are separate, these will need to be fixed. */
+	/*
+	 * Read 4 bytes of the other process' storage
+	 *  data is a pointer specifying where the user wants the
+	 *	4 bytes copied into
+	 *  addr is a pointer in the user's storage that contains an 8 byte
+	 *	address in the other process of the 4 bytes that is to be read
+	 * (this is run in a 32-bit process looking at a 64-bit process)
+	 * when I and D space are separate, these will need to be fixed.
+	 */
 	case PPC_PTRACE_PEEKTEXT_3264:
 	case PPC_PTRACE_PEEKDATA_3264: 
 	{
@@ -158,7 +156,7 @@
 		ret = -EIO;
 
 		/* Get the addr in the other process that we want to read */
-		if (get_user(addrOthers,(u32**)addr) != 0)
+		if (get_user(addrOthers, (u32**)addr) != 0)
 			break;
 
 		copied = access_process_vm(child, (u64)addrOthers, &tmp_mem_value, sizeof(tmp_mem_value), 0);
@@ -177,7 +175,7 @@
 		ret = -EIO;
 		/* convert to index and check */
 		index = (unsigned long) addr >> 2;
-		if ((addr & 3) || index > PT_FPSCR32)
+		if ((addr & 3) || (index > PT_FPSCR32))
 			break;
 
 		if (index < PT_FPR0) {
@@ -185,9 +183,10 @@
 		} else {
 			if (child->thread.regs->msr & MSR_FP)
 				giveup_fpu(child);
-			/* the user space code considers the floating point to be 
-			 *   an array of unsigned int (32 bits) - the index passed 
-			 *   in is based on this assumption.
+			/*
+			 * the user space code considers the floating point
+			 * to be an array of unsigned int (32 bits) - the
+			 * index passed in is based on this assumption.
 			 */
 			tmp_reg_value = ((unsigned int *)child->thread.fpr)[index - PT_FPR0];
 		}
@@ -196,12 +195,15 @@
 		break;
 	}
   
-	/* Read 4 bytes out of the other process' pt_regs area */
-	/*  data is a pointer specifying where the user wants the 4 bytes copied into */
-	/*  addr is the offset into the other process' pt_regs structure that is to be read */
-	/* (this is run in a 32-bit process looking at a 64-bit process) */
-	case PPC_PTRACE_PEEKUSR_3264:
-	{
+	/*
+	 * Read 4 bytes out of the other process' pt_regs area
+	 *  data is a pointer specifying where the user wants the
+	 *	4 bytes copied into
+	 *  addr is the offset into the other process' pt_regs structure
+	 *	that is to be read
+	 * (this is run in a 32-bit process looking at a 64-bit process)
+	 */
+	case PPC_PTRACE_PEEKUSR_3264: {
 		u32 index;
 		u32 reg32bits;
 		u64 tmp_reg_value;
@@ -222,8 +224,7 @@
 		if ((addr & 3) || numReg > PT_FPSCR)
 			break;
 
-		if (numReg >= PT_FPR0)
-		{
+		if (numReg >= PT_FPR0) {
 			if (child->thread.regs->msr & MSR_FP)
 				giveup_fpu(child);
 		        if (numReg == PT_FPSCR) 
@@ -251,11 +252,15 @@
 		break;
 	}
 
-	/* Write 4 bytes into the other process' storage */
-	/*  data is the 4 bytes that the user wants written */
-	/*  addr is a pointer in the user's storage that contains an 8 byte address in the other process where the 4 bytes that is to be written */
-	/* (this is run in a 32-bit process looking at a 64-bit process) */
-	/* when I and D space are separate, these will need to be fixed. */
+	/*
+	 * Write 4 bytes into the other process' storage
+	 *  data is the 4 bytes that the user wants written
+	 *  addr is a pointer in the user's storage that contains an
+	 *	8 byte address in the other process where the 4 bytes
+	 *	that is to be written
+	 * (this is run in a 32-bit process looking at a 64-bit process)
+	 * when I and D space are separate, these will need to be fixed.
+	 */
 	case PPC_PTRACE_POKETEXT_3264:
 	case PPC_PTRACE_POKEDATA_3264:
 	{
@@ -267,7 +272,6 @@
 		ret = -EIO;
 		if (get_user(addrOthers,(u32**)addr) != 0)
 			break;
-
 		ret = 0;
 		bytesWritten = access_process_vm(child, (u64)addrOthers, &tmp_value_to_write, sizeof(tmp_value_to_write), 1);
 		if (bytesWritten == sizeof(tmp_value_to_write))
@@ -281,63 +285,61 @@
 		unsigned long index;
 
 		ret = -EIO;
-
 		/* convert to index and check */
 		index = (unsigned long) addr >> 2;
-		if ((addr & 3) || index > PT_FPSCR32)
+		if ((addr & 3) || (index > PT_FPSCR32))
 			break;
 
 		if (index == PT_ORIG_R3)
 			break;
-
-
 		if (index < PT_FPR0) {
 			ret = put_reg(child, index, data);
 		} else {
 			if (child->thread.regs->msr & MSR_FP)
 				giveup_fpu(child);
-      /* the user space code considers the floating point to be 
-       *   an array of unsigned int (32 bits) - the index passed 
-       *   in is based on this assumption.
-       */
-
+			/*
+			 * the user space code considers the floating point
+			 * to be an array of unsigned int (32 bits) - the
+			 * index passed in is based on this assumption.
+			 */
 			((unsigned int *)child->thread.fpr)[index - PT_FPR0] = data;
 			ret = 0;
 		}
 		break;
 	}
 
-	/* Write 4 bytes into the other process' pt_regs area */
-	/*  data is the 4 bytes that the user wants written */
-	/*  addr is the offset into the other process' pt_regs structure that is to be written into */
-	/* (this is run in a 32-bit process looking at a 64-bit process) */
-	case PPC_PTRACE_POKEUSR_3264:
-	{
+	/*
+	 * Write 4 bytes into the other process' pt_regs area
+	 *  data is the 4 bytes that the user wants written
+	 *  addr is the offset into the other process' pt_regs structure
+	 *	that is to be written into
+	 * (this is run in a 32-bit process looking at a 64-bit process)
+	 */
+	case PPC_PTRACE_POKEUSR_3264: {
 		u32 index;
 		u32 numReg;
 
 		ret = -EIO;
-
 		/* Determine which register the user wants */
 		index = (u64)addr >> 2;  /* Divide addr by 4 */
 		numReg = index / 2;
-
-		/* Validate the input - check to see if address is on the wrong boundary or beyond the end of the user area */
-		if ((addr & 3) || numReg > PT_FPSCR)
+		/*
+		 * Validate the input - check to see if address is on the
+		 * wrong boundary or beyond the end of the user area
+		 */
+		if ((addr & 3) || (numReg > PT_FPSCR))
 			break;
 		/* Insure it is a register we let them change */
-		if ((numReg == PT_ORIG_R3) || ((numReg > PT_CCR) && (numReg < PT_FPR0)))
+		if ((numReg == PT_ORIG_R3)
+				|| ((numReg > PT_CCR) && (numReg < PT_FPR0)))
 			break;
-
-		if (numReg >= PT_FPR0)
-		{
+		if (numReg >= PT_FPR0) {
 			if (child->thread.regs->msr & MSR_FP)
 				giveup_fpu(child);
 		}
-
 		if (numReg == PT_MSR)
-			data = (data & MSR_DEBUGCHANGE) | (child->thread.regs->msr & ~MSR_DEBUGCHANGE);
-
+			data = (data & MSR_DEBUGCHANGE)
+				| (child->thread.regs->msr & ~MSR_DEBUGCHANGE);
 		((u32*)child->thread.regs)[index] = data;
 		ret = 0;
 		break;
@@ -361,8 +363,8 @@
 	}
 
 	/*
-	 * make the child exit.  Best I can do is send it a sigkill. 
-	 * perhaps it should be put in the status that it wants to 
+	 * make the child exit.  Best I can do is send it a sigkill.
+	 * perhaps it should be put in the status that it wants to
 	 * exit.
 	 */
 	case PTRACE_KILL: {
@@ -403,4 +405,3 @@
 	unlock_kernel();
 	return ret;
 }
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/rtas.c linuxppc64_2_4/arch/ppc64/kernel/rtas.c
--- linux-2.4.19/arch/ppc64/kernel/rtas.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/rtas.c	Mon Aug 19 09:15:52 2002
@@ -15,6 +15,7 @@
 #include <linux/kernel.h>
 #include <linux/types.h>
 #include <linux/spinlock.h>
+#include <linux/fs.h>
 
 #include <asm/init.h>
 #include <asm/prom.h>
@@ -24,8 +25,12 @@
 #include <asm/paca.h>
 #include <asm/page.h>
 #include <asm/system.h>
+#include <asm/abs_addr.h>
 #include <asm/udbg.h>
 
+struct proc_dir_entry *rtas_proc_dir;	/* /proc/ppc64/rtas dir */
+struct flash_block_list_header rtas_firmware_flash_list = {0, 0};
+
 /*
  * prom_init() is called very early on, before the kernel text
  * and data have been mapped to KERNELBASE.  At this point the code
@@ -103,18 +108,13 @@
 	enter_rtas((void *)__pa((unsigned long)rtas));	
 }
 
-#if 0
-#define DEBUG_RTAS
-#endif
 __openfirmware
 int
 rtas_token(const char *service)
 {
 	int *tokp;
 	if (rtas.dev == NULL) {
-#ifdef DEBUG_RTAS
-		udbg_printf("\tNo rtas device in device-tree...\n");
-#endif /* DEBUG_RTAS */
+		PPCDBG(PPCDBG_RTAS,"\tNo rtas device in device-tree...\n");
 		return RTAS_UNKNOWN_SERVICE;
 	}
 	tokp = (int *) get_property(rtas.dev, service, NULL);
@@ -131,13 +131,11 @@
 	unsigned long s;
 	struct rtas_args *rtas_args = &(get_paca()->xRtas);
 
-#ifdef DEBUG_RTAS
-	udbg_printf("Entering rtas_call\n");
-	udbg_printf("\ttoken    = 0x%x\n", token);
-	udbg_printf("\tnargs    = %d\n", nargs);
-	udbg_printf("\tnret     = %d\n", nret);
-	udbg_printf("\t&outputs = 0x%lx\n", outputs);
-#endif /* DEBUG_RTAS */
+	PPCDBG(PPCDBG_RTAS, "Entering rtas_call\n");
+	PPCDBG(PPCDBG_RTAS, "\ttoken    = 0x%x\n", token);
+	PPCDBG(PPCDBG_RTAS, "\tnargs    = %d\n", nargs);
+	PPCDBG(PPCDBG_RTAS, "\tnret     = %d\n", nret);
+	PPCDBG(PPCDBG_RTAS, "\t&outputs = 0x%lx\n", outputs);
 	if (token == RTAS_UNKNOWN_SERVICE)
 		return -1;
 
@@ -147,10 +145,8 @@
 	rtas_args->rets  = (rtas_arg_t *)&(rtas_args->args[nargs]);
 	va_start(list, outputs);
 	for (i = 0; i < nargs; ++i) {
-	  rtas_args->args[i] = (rtas_arg_t)LONG_LSW(va_arg(list, ulong));
-#ifdef DEBUG_RTAS
-	  udbg_printf("\tnarg[%d] = 0x%lx\n", i, rtas_args->args[i]);
-#endif /* DEBUG_RTAS */
+		rtas_args->args[i] = (rtas_arg_t)LONG_LSW(va_arg(list, ulong));
+		PPCDBG(PPCDBG_RTAS, "\tnarg[%d] = 0x%lx\n", i, rtas_args->args[i]);
 	}
 	va_end(list);
 
@@ -162,22 +158,19 @@
 #else
 	spin_lock_irqsave(&rtas.lock, s);
 #endif
-#ifdef DEBUG_RTAS
-	udbg_printf("\tentering rtas with 0x%lx\n", (void *)__pa((unsigned long)rtas_args));
-#endif /* DEBUG_RTAS */
+	PPCDBG(PPCDBG_RTAS, "\tentering rtas with 0x%lx\n",
+		(void *)__pa((unsigned long)rtas_args));
 	enter_rtas((void *)__pa((unsigned long)rtas_args));
-#ifdef DEBUG_RTAS
-	udbg_printf("\treturned from rtas ...\n");
-#endif /* DEBUG_RTAS */
+	PPCDBG(PPCDBG_RTAS, "\treturned from rtas ...\n");
 #if 0   /* Gotta do something different here, use global lock for now... */
 	spin_unlock_irqrestore(&rtas_args->lock, s);
 #else
 	spin_unlock_irqrestore(&rtas.lock, s);
 #endif
-#ifdef DEBUG_RTAS
-	for(i=0; i < nret ;i++)
-	  udbg_printf("\tnret[%d] = 0x%lx\n", i, (ulong)rtas_args->rets[i]);
-#endif /* DEBUG_RTAS */
+	ifppcdebug(PPCDBG_RTAS) {
+		for(i=0; i < nret ;i++)
+			udbg_printf("\tnret[%d] = 0x%lx\n", i, (ulong)rtas_args->rets[i]);
+	}
 
 	if (nret > 1 && outputs != NULL)
 		for (i = 0; i < nret-1; ++i)
@@ -185,9 +178,87 @@
 	return (ulong)((nret > 0) ? rtas_args->rets[0] : 0);
 }
 
+#define FLASH_BLOCK_LIST_VERSION (1UL)
+static void
+rtas_flash_firmware(void)
+{
+	unsigned long image_size;
+	struct flash_block_list *f, *next, *flist;
+	unsigned long rtas_block_list;
+	int i, status, update_token;
+
+	update_token = rtas_token("ibm,update-flash-64-and-reboot");
+	if (update_token == RTAS_UNKNOWN_SERVICE) {
+		printk(KERN_ALERT "FLASH: ibm,update-flash-64-and-reboot is not available -- not a service partition?\n");
+		printk(KERN_ALERT "FLASH: firmware will not be flashed\n");
+		return;
+	}
+
+	/* NOTE: the "first" block list is a global var with no data
+	 * blocks in the kernel data segment.  We do this because
+	 * we want to ensure this block_list addr is under 4GB.
+	 */
+	rtas_firmware_flash_list.num_blocks = 0;
+	flist = (struct flash_block_list *)&rtas_firmware_flash_list;
+	rtas_block_list = virt_to_absolute((unsigned long)flist);
+	if (rtas_block_list >= (4UL << 20)) {
+		printk(KERN_ALERT "FLASH: kernel bug...flash list header addr above 4GB\n");
+		return;
+	}
+
+	printk(KERN_ALERT "FLASH: preparing saved firmware image for flash\n");
+	/* Update the block_list in place. */
+	image_size = 0;
+	for (f = flist; f; f = next) {
+		/* Translate data addrs to absolute */
+		for (i = 0; i < f->num_blocks; i++) {
+			f->blocks[i].data = (char *)virt_to_absolute((unsigned long)f->blocks[i].data);
+			image_size += f->blocks[i].length;
+		}
+		next = f->next;
+		f->next = (struct flash_block_list *)virt_to_absolute((unsigned long)f->next);
+		/* make num_blocks into the version/length field */
+		f->num_blocks = (FLASH_BLOCK_LIST_VERSION << 56) | ((f->num_blocks+1)*16);
+	}
+
+	printk(KERN_ALERT "FLASH: flash image is %ld bytes\n", image_size);
+	printk(KERN_ALERT "FLASH: performing flash and reboot\n");
+	ppc_md.progress("Flashing        \n", 0x0);
+	ppc_md.progress("Please Wait...  ", 0x0);
+	printk(KERN_ALERT "FLASH: this will take several minutes.  Do not power off!\n");
+	status = rtas_call(update_token, 1, 1, NULL, rtas_block_list);
+	switch (status) {	/* should only get "bad" status */
+	    case 0:
+		printk(KERN_ALERT "FLASH: success\n");
+		break;
+	    case -1:
+		printk(KERN_ALERT "FLASH: hardware error.  Firmware may not be not flashed\n");
+		break;
+	    case -3:
+		printk(KERN_ALERT "FLASH: image is corrupt or not correct for this platform.  Firmware not flashed\n");
+		break;
+	    case -4:
+		printk(KERN_ALERT "FLASH: flash failed when partially complete.  System may not reboot\n");
+		break;
+	    default:
+		printk(KERN_ALERT "FLASH: unknown flash return code %d\n", status);
+		break;
+	}
+}
+
+void rtas_flash_bypass_warning(void)
+{
+	printk(KERN_ALERT "FLASH: firmware flash requires a reboot\n");
+	printk(KERN_ALERT "FLASH: the firmware image will NOT be flashed\n");
+}
+
+
 void __chrp
 rtas_restart(char *cmd)
 {
+	if (rtas_firmware_flash_list.next)
+		rtas_flash_firmware();
+
         printk("RTAS system-reboot returned %ld\n",
 	       rtas_call(rtas_token("system-reboot"), 0, 1, NULL));
         for (;;);
@@ -196,6 +267,8 @@
 void __chrp
 rtas_power_off(void)
 {
+	if (rtas_firmware_flash_list.next)
+		rtas_flash_bypass_warning();
         /* allow power on only with power button press */
         printk("RTAS power-off returned %ld\n",
                rtas_call(rtas_token("power-off"), 2, 1, NULL,0xffffffff,0xffffffff));
@@ -205,5 +278,7 @@
 void __chrp
 rtas_halt(void)
 {
+	if (rtas_firmware_flash_list.next)
+		rtas_flash_bypass_warning();
         rtas_power_off();
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/rtas_flash.c linuxppc64_2_4/arch/ppc64/kernel/rtas_flash.c
--- linux-2.4.19/arch/ppc64/kernel/rtas_flash.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/arch/ppc64/kernel/rtas_flash.c	Mon May  6 14:29:20 2002
@@ -0,0 +1,240 @@
+/*
+ *  c 2001 PPC 64 Team, IBM Corp
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ *
+ * /proc/ppc64/rtas/firmware_flash interface
+ *
+ * This file implements a firmware_flash interface to pump a firmware
+ * image into the kernel.  At reboot time rtas_restart() will see the
+ * firmware image and flash it as it reboots (see rtas.c).
+ */
+
+#include <linux/module.h>
+
+#include <linux/config.h>
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <asm/rtas.h>
+
+#define MODULE_VERSION "1.0"
+#define MODULE_NAME "rtas_flash"
+
+#define FIRMWARE_FLASH_NAME "firmware_flash"
+
+/* Local copy of the flash block list.
+ * We only allow one open of the flash proc file and create this
+ * list as we go.  This list will be put in the kernel's
+ * rtas_firmware_flash_list global var once it is fully read.
+ *
+ * For convenience as we build the list we use virtual addrs,
+ * we do not fill in the version number, and the length field
+ * is treated as the number of entries currently in the block
+ * (i.e. not a byte count).  This is all fixed on release.
+ */
+static struct flash_block_list *flist;
+static char *flash_msg;
+static int flash_possible;
+
+static int rtas_flash_open(struct inode *inode, struct file *file)
+{
+	if ((file->f_mode & FMODE_WRITE) && flash_possible) {
+		if (flist)
+			return -EBUSY;
+		flist = (struct flash_block_list *)get_free_page(GFP_KERNEL);
+		if (!flist)
+			return -ENOMEM;
+	}
+	return 0;
+}
+
+/* Do simple sanity checks on the flash image. */
+static int flash_list_valid(struct flash_block_list *flist)
+{
+	struct flash_block_list *f;
+	int i;
+	unsigned long block_size, image_size;
+
+	flash_msg = NULL;
+	/* Paranoid self test here.  We also collect the image size. */
+	image_size = 0;
+	for (f = flist; f; f = f->next) {
+		for (i = 0; i < f->num_blocks; i++) {
+			if (f->blocks[i].data == NULL) {
+				flash_msg = "error: internal error null data\n";
+				return 0;
+			}
+			block_size = f->blocks[i].length;
+			if (block_size <= 0 || block_size > PAGE_SIZE) {
+				flash_msg = "error: internal error bad length\n";
+				return 0;
+			}
+			image_size += block_size;
+		}
+	}
+	if (image_size < (256 << 10)) {
+		if (image_size < 2)
+			flash_msg = NULL;	/* allow "clear" of image */
+		else
+			flash_msg = "error: flash image short\n";
+		return 0;
+	}
+	printk(KERN_INFO "FLASH: flash image with %ld bytes stored for hardware flash on reboot\n", image_size);
+	return 1;
+}
+
+static void free_flash_list(struct flash_block_list *f)
+{
+	struct flash_block_list *next;
+	int i;
+
+	while (f) {
+		for (i = 0; i < f->num_blocks; i++)
+			free_page((unsigned long)(f->blocks[i].data));
+		next = f->next;
+		free_page((unsigned long)f);
+		f = next;
+	}
+}
+
+static int rtas_flash_release(struct inode *inode, struct file *file)
+{
+	if (flist) {
+		/* Always clear saved list on a new attempt. */
+		if (rtas_firmware_flash_list.next) {
+			free_flash_list(rtas_firmware_flash_list.next);
+			rtas_firmware_flash_list.next = NULL;
+		}
+
+		if (flash_list_valid(flist))
+			rtas_firmware_flash_list.next = flist;
+		else
+			free_flash_list(flist);
+		flist = NULL;
+	}
+	return 0;
+}
+
+/* Reading the proc file will show status (not the firmware contents) */
+static ssize_t rtas_flash_read(struct file *file, char *buf,
+			       size_t count, loff_t *ppos)
+{
+	int error;
+	char *msg;
+	int msglen;
+
+	if (!flash_possible) {
+		msg = "error: this partition does not have service authority\n";
+	} else if (flist) {
+		msg = "info: this file is busy for write by some process\n";
+	} else if (flash_msg) {
+		msg = flash_msg;	/* message from last flash attempt */
+	} else if (rtas_firmware_flash_list.next) {
+		msg = "ready: firmware image ready for flash on reboot\n";
+	} else {
+		msg = "info: no firmware image for flash\n";
+	}
+	msglen = strlen(msg);
+	if (msglen > count)
+		msglen = count;
+
+	if (ppos && *ppos != 0)
+		return 0;	/* be cheap */
+
+	error = verify_area(VERIFY_WRITE, buf, msglen);
+	if (error)
+		return -EINVAL;
+
+	copy_to_user(buf, msg, msglen);
+
+	if (ppos)
+		*ppos = msglen;
+	return msglen;
+}
+
+/* We could be much more efficient here.  But to keep this function
+ * simple we allocate a page to the block list no matter how small the
+ * count is.  If the system is low on memory it will be just as well
+ * that we fail....
+ */
+static ssize_t rtas_flash_write(struct file *file, const char *buffer,
+				size_t count, loff_t *off)
+{
+	size_t len = count;
+	char *p;
+	int next_free;
+	struct flash_block_list *fl = flist;
+
+	if (!flash_possible || len == 0)
+		return len;	/* discard data */
+
+	while (fl->next)
+		fl = fl->next; /* seek to last block_list for append */
+	next_free = fl->num_blocks;
+	if (next_free == FLASH_BLOCKS_PER_NODE) {
+		/* Need to allocate another block_list */
+		fl->next = (struct flash_block_list *)get_free_page(GFP_KERNEL);
+		if (!fl->next)
+			return -ENOMEM;
+		fl = fl->next;
+		next_free = 0;
+	}
+
+	if (len > PAGE_SIZE)
+		len = PAGE_SIZE;
+	p = (char *)get_free_page(GFP_KERNEL);
+	if (!p)
+		return -ENOMEM;
+	if(copy_from_user(p, buffer, len)) {
+		free_page((unsigned long)p);
+		return -EFAULT;
+	}
+	fl->blocks[next_free].data = p;
+	fl->blocks[next_free].length = len;
+	fl->num_blocks++;
+
+	return len;
+}
+
+static struct file_operations rtas_flash_operations = {
+	read:		rtas_flash_read,
+	write:		rtas_flash_write,
+	open:		rtas_flash_open,
+	release:	rtas_flash_release,
+};
+
+
+int __init rtas_flash_init(void)
+{
+	struct proc_dir_entry *ent = NULL;
+
+	if (!rtas_proc_dir) {
+		printk(KERN_WARNING "rtas proc dir does not already exist");
+		return -ENOENT;
+	}
+
+	if (rtas_token("ibm,update-flash-64-and-reboot") != RTAS_UNKNOWN_SERVICE)
+		flash_possible = 1;
+
+	if ((ent = create_proc_entry(FIRMWARE_FLASH_NAME, S_IRUSR | S_IWUSR, rtas_proc_dir)) != NULL) {
+		ent->nlink = 1;
+		ent->proc_fops = &rtas_flash_operations;
+		ent->owner = THIS_MODULE;
+	}
+	return 0;
+}
+
+void __exit rtas_flash_cleanup(void)
+{
+	if (!rtas_proc_dir)
+		return;
+	remove_proc_entry(FIRMWARE_FLASH_NAME, rtas_proc_dir);
+}
+
+module_init(rtas_flash_init);
+module_exit(rtas_flash_cleanup);
+MODULE_LICENSE("GPL");
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/rtasd.c linuxppc64_2_4/arch/ppc64/kernel/rtasd.c
--- linux-2.4.19/arch/ppc64/kernel/rtasd.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/rtasd.c	Tue Jul 23 02:56:54 2002
@@ -98,9 +98,7 @@
 	rtas_log_size -= 1;
 	spin_unlock(&rtas_log_lock);
 
-	copy_to_user(buf, tmp, count);
-	error = count;
-
+	error = copy_to_user(buf, tmp, count) ? -EFAULT : count;
 out:
 	kfree(tmp);
 	return error;
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/setup.c linuxppc64_2_4/arch/ppc64/kernel/setup.c
--- linux-2.4.19/arch/ppc64/kernel/setup.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/setup.c	Thu Aug 22 11:27:34 2002
@@ -21,6 +21,8 @@
 #include <linux/ide.h>
 #include <linux/seq_file.h>
 #include <linux/ioport.h>
+#include <linux/console.h>
+#include <linux/version.h>
 #include <asm/init.h>
 #include <asm/io.h>
 #include <asm/prom.h>
@@ -40,6 +42,7 @@
 /* extern void *stab; */
 extern HTAB htab_data;
 extern unsigned long loops_per_jiffy;
+extern int preferred_console;	/* from kernel/printk.c */
 
 extern unsigned long embedded_sysmap_start;
 extern unsigned long embedded_sysmap_end;
@@ -68,6 +71,10 @@
 extern void xmon_map_scc(void);
 #endif
 
+#ifdef CONFIG_KDB
+extern void kdb_map_scc(void);
+#endif
+
 char saved_command_line[256];
 unsigned char aux_device_present;
 
@@ -109,15 +116,12 @@
 int icache_bsize;
 int ucache_bsize;
 
-/*
- * Initialize the PPCDBG state.  Called before relocation has been enabled.
- */
-void ppcdbg_initialize(void) {
-	unsigned long offset = reloc_offset();
-	struct naca_struct *_naca = RELOC(naca);
-
-	_naca->debug_switch = PPC_DEBUG_DEFAULT; /* | PPCDBG_BUSWALK | PPCDBG_PHBINIT | PPCDBG_MM | PPCDBG_MMINIT | PPCDBG_TCEINIT | PPCDBG_TCE */;
-}
+static struct console udbg_console = {
+	name:	"udbg",
+	write:	udbg_console_write,
+	flags:	CON_PRINTBUFFER,
+	index:	-1,
+};
 
 /*
  * Do some initial setup of the system.  The paramters are those which 
@@ -157,66 +161,29 @@
 #endif
 	}
 
-	udbg_puts("\n-----------------------------------------------------\n");
-	udbg_puts("Naca Info...\n\n");
-	udbg_puts("naca                       = 0x");
-	udbg_puthex((unsigned long)naca);
-	udbg_putc('\n');
-
-	udbg_puts("naca->processorCount       = 0x");
-	udbg_puthex(naca->processorCount);
-	udbg_putc('\n');
-
-	udbg_puts("naca->physicalMemorySize   = 0x");
-	udbg_puthex(naca->physicalMemorySize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->dCacheL1LineSize     = 0x");
-	udbg_puthex(naca->dCacheL1LineSize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->dCacheL1LogLineSize  = 0x");
-	udbg_puthex(naca->dCacheL1LogLineSize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->dCacheL1LinesPerPage = 0x");
-	udbg_puthex(naca->dCacheL1LinesPerPage);
-	udbg_putc('\n');
-
-	udbg_puts("naca->iCacheL1LineSize     = 0x");
-	udbg_puthex(naca->iCacheL1LineSize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->iCacheL1LogLineSize  = 0x");
-	udbg_puthex(naca->iCacheL1LogLineSize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->iCacheL1LinesPerPage = 0x");
-	udbg_puthex(naca->iCacheL1LinesPerPage);
-	udbg_putc('\n');
-
-	udbg_puts("naca->pftSize              = 0x");
-	udbg_puthex(naca->pftSize);
-	udbg_putc('\n');
-
-	udbg_puts("naca->serialPortAddr       = 0x");
-	udbg_puthex(naca->serialPortAddr);
-	udbg_putc('\n');
-
-	udbg_puts("naca->interrupt_controller = 0x");
-	udbg_puthex(naca->interrupt_controller);
-	udbg_putc('\n');
-
-	udbg_printf("\nHTAB Info ...\n\n"); 
-	udbg_puts("htab_data.htab             = 0x");
-	udbg_puthex((unsigned long)htab_data.htab);
-	udbg_putc('\n');
-	udbg_puts("htab_data.num_ptegs        = 0x");
-	udbg_puthex(htab_data.htab_num_ptegs);
-	udbg_putc('\n');
+	if (naca->platform & PLATFORM_PSERIES) {
+		register_console(&udbg_console);
+		preferred_console = -1;
+	}
 
-	udbg_puts("\n-----------------------------------------------------\n");
+	printk("Starting Linux PPC64 %s\n", UTS_RELEASE);
 
+	printk("-----------------------------------------------------\n");
+	printk("naca                       = 0x%p\n", naca);
+	printk("naca->processorCount       = 0x%x\n", naca->processorCount);
+	printk("naca->physicalMemorySize   = 0x%lx\n", naca->physicalMemorySize);
+	printk("naca->dCacheL1LineSize     = 0x%x\n", naca->dCacheL1LineSize);
+	printk("naca->dCacheL1LogLineSize  = 0x%x\n", naca->dCacheL1LogLineSize);
+	printk("naca->dCacheL1LinesPerPage = 0x%x\n", naca->dCacheL1LinesPerPage);
+	printk("naca->iCacheL1LineSize     = 0x%x\n", naca->iCacheL1LineSize);
+	printk("naca->iCacheL1LogLineSize  = 0x%x\n", naca->iCacheL1LogLineSize);
+	printk("naca->iCacheL1LinesPerPage = 0x%x\n", naca->iCacheL1LinesPerPage);
+	printk("naca->pftSize              = 0x%lx\n", naca->pftSize);
+	printk("naca->debug_switch         = 0x%lx\n", naca->debug_switch);
+	printk("naca->interrupt_controller = 0x%lx\n", naca->interrupt_controller);
+	printk("htab_data.htab             = 0x%p\n", htab_data.htab);
+	printk("htab_data.num_ptegs        = 0x%lx\n", htab_data.htab_num_ptegs);
+	printk("-----------------------------------------------------\n");
 
 	if (naca->platform & PLATFORM_PSERIES) {
 		finish_device_tree();
@@ -226,14 +193,28 @@
 	mm_init_ppc64();
 
 	switch (naca->platform) {
-	case PLATFORM_ISERIES_LPAR:
+	    case PLATFORM_ISERIES_LPAR:
 		iSeries_init();
 		break;
-	default:
+	    default:
 		/* The following relies on the device tree being */
 		/* fully configured.                             */
 		parse_cmd_line(r3, r4, r5, r6, r7);
 	}
+	ppc64_boot_msg(0x10, "Setup System");
+}
+
+/* This is called just before console_init().
+ * It will be obsolete when Linux gets real early console support (2.5?)
+ * We need to hack preferred_console to retain the correct behavior
+ */
+void setup_before_console_init(void)
+{
+	if (naca->platform & PLATFORM_PSERIES) {
+		int save = preferred_console;
+		unregister_console(&udbg_console);
+		preferred_console = save;
+	}
 }
 
 void machine_restart(char *cmd)
@@ -284,6 +265,9 @@
 	pvr = paca[cpu_id].pvr;
 
 	switch (PVR_VER(pvr)) {
+	case PV_NORTHSTAR:
+		seq_printf(m, "RS64-II (northstar)\n");
+		break;
 	case PV_PULSAR:
 		seq_printf(m, "RS64-III (pulsar)\n");
 		break;
@@ -302,6 +286,9 @@
 	case PV_630p:
 		seq_printf(m, "POWER3 (630+)\n");
 		break;
+	case PV_POWER4p:
+		seq_printf(m, "POWER4+ (gq)\n");
+		break;
 	default:
 		seq_printf(m, "Unknown (%08x)\n", pvr);
 		break;
@@ -328,7 +315,7 @@
 	if (ppc_md.setup_residual != NULL)
 		ppc_md.setup_residual(m, cpu_id);
 
-	seq_printf(m, "revision\t: %hd.%hd\n", maj, min);
+	seq_printf(m, "revision\t: %hd.%hd\n\n", maj, min);
 	
 	return 0;
 }
@@ -369,18 +356,9 @@
 	}
 #endif
 
-	cmd_line[0] = 0;
-	chosen = find_devices("chosen");
-	if (chosen != NULL) {
-		p = get_property(chosen, "bootargs", NULL);
-		if (p != NULL)
-			strncpy(cmd_line, p, sizeof(cmd_line));
-	}
-	cmd_line[sizeof(cmd_line) - 1] = 0;
-
 	/* Look for mem= option on command line */
 	if (strstr(cmd_line, "mem=")) {
-		char *p, *q;
+		char *q;
 		unsigned long maxmem = 0;
 		extern unsigned long __max_memory;
 
@@ -399,7 +377,6 @@
 		}
 		__max_memory = maxmem;
 	}
-	ppc_md.progress("id mach: done", 0x200);
 }
 
 
@@ -477,7 +454,6 @@
 	printk("Calibrating delay loop... %lu.%02lu BogoMips\n",
 			       loops_per_jiffy/(500000/HZ),
 			       loops_per_jiffy/(5000/HZ) % 100);
-
 }	
 
 extern void (*calibrate_delay)(void);
@@ -495,15 +471,18 @@
 
 	calibrate_delay = ppc64_calibrate_delay;
 
+	ppc64_boot_msg(0x12, "Setup Arch");
 #ifdef CONFIG_XMON
 	xmon_map_scc();
 	if (strstr(cmd_line, "xmon"))
 		xmon(0);
 #endif /* CONFIG_XMON */
+
 #ifdef CONFIG_KDB
-	xmon_map_scc();	/* in kdb/start.c --need to rename TAI */
+	kdb_map_scc();	
+	if (strstr(cmd_line, "kdb=early"))
+		kdb(KDB_REASON_CALL,0,0);
 #endif
-	ppc_md.progress("setup_arch:enter", 0x3eab);
 
 #if defined(CONFIG_KGDB)
 	kgdb_map_scc();
@@ -532,12 +511,11 @@
 
 	/* set up the bootmem stuff with available memory */
 	do_init_bootmem();
-	ppc_md.progress("setup_arch:bootmem", 0x3eab);
 
 	ppc_md.setup_arch();
 
 	paging_init();
-	ppc_md.progress("setup_arch: exit", 0x3eab);
+	ppc64_boot_msg(0x15, "Setup Done");
 }
 
 #ifdef CONFIG_IDE
@@ -637,6 +615,53 @@
 	id->integrity_word=__le16_to_cpu(id->integrity_word);
 }
 #endif
+
+/* ToDo: do something useful if ppc_md is not yet setup. */
+#define PPC64_LINUX_FUNCTION 0x0f000000
+#define PPC64_IPL_MESSAGE 0xc0000000
+#define PPC64_TERM_MESSAGE 0xb0000000
+#define PPC64_ATTN_MESSAGE 0xa0000000
+#define PPC64_DUMP_MESSAGE 0xd0000000
+
+static void ppc64_do_msg(unsigned int src, const char *msg)
+{
+	if (ppc_md.progress) {
+		char buf[32];
+
+		sprintf(buf, "%08x        \n", src);
+		ppc_md.progress(buf, 0);
+		sprintf(buf, "%-16s", msg);
+		ppc_md.progress(buf, 0);
+	}
+}
+
+/* Print a boot progress message. */
+void ppc64_boot_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_IPL_MESSAGE|src, msg);
+	printk("[boot]%04x %s\n", src, msg);
+}
+
+/* Print a termination message (print only -- does not stop the kernel) */
+void ppc64_terminate_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_TERM_MESSAGE|src, msg);
+	printk("[terminate]%04x %s\n", src, msg);
+}
+
+/* Print something that needs attention (device error, etc) */
+void ppc64_attention_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_ATTN_MESSAGE|src, msg);
+	printk("[attention]%04x %s\n", src, msg);
+}
+
+/* Print a dump progress message. */
+void ppc64_dump_msg(unsigned int src, const char *msg)
+{
+	ppc64_do_msg(PPC64_LINUX_FUNCTION|PPC64_DUMP_MESSAGE|src, msg);
+	printk("[dump]%04x %s\n", src, msg);
+}
 
 
 void exception_trace(unsigned long trap)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/signal.c linuxppc64_2_4/arch/ppc64/kernel/signal.c
--- linux-2.4.19/arch/ppc64/kernel/signal.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/signal.c	Tue Jul 23 02:59:14 2002
@@ -1,8 +1,6 @@
 /*
  *  linux/arch/ppc64/kernel/signal.c
  *
- *  
- *
  *  PowerPC version 
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
  *
@@ -637,11 +635,6 @@
 	 */
 	if (current->thread.flags & PPC_FLAG_32BIT)
 		return do_signal32(oldset, regs);
-  
-        PPCDBG(PPCDBG_SIGNAL, "do_signal - running - pid=%ld current=%lx comm=%s \n", 
-               current->pid, current, current->comm);
-
-  
 
         if (!oldset)
 		oldset = &current->blocked;
@@ -716,7 +709,7 @@
 				continue;
 
 			switch (signr) {
-			case SIGCONT: case SIGCHLD: case SIGWINCH:
+			case SIGCONT: case SIGCHLD: case SIGWINCH: case SIGURG:
 				continue;
 
 			case SIGTSTP: case SIGTTIN: case SIGTTOU:
@@ -724,13 +717,16 @@
 					continue;
 				/* FALLTHRU */
 
-			case SIGSTOP:
+			case SIGSTOP: {
+				struct signal_struct *sig;
 				current->state = TASK_STOPPED;
 				current->exit_code = signr;
-				if (!(current->p_pptr->sig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))
+				sig = current->p_pptr->sig;
+				if (sig && !(sig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))
 					notify_parent(current, SIGCHLD);
 				schedule();
 				continue;
+			}
 
 			case SIGQUIT: case SIGILL: case SIGTRAP:
 			case SIGABRT: case SIGFPE: case SIGSEGV:
@@ -740,10 +736,7 @@
 				/* FALLTHRU */
 
 			default:
-				sigaddset(&current->pending.signal, signr);
-				recalc_sigpending(current);
-				current->flags |= PF_SIGNALED;
-				do_exit(exit_code);
+				sig_exit(signr, exit_code, &info);
 				/* NOTREACHED */
 			}
 		}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/signal32.c linuxppc64_2_4/arch/ppc64/kernel/signal32.c
--- linux-2.4.19/arch/ppc64/kernel/signal32.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/signal32.c	Thu May  9 00:49:40 2002
@@ -1406,7 +1406,7 @@
 				continue;
 
 			switch (signr) {
-			case SIGCONT: case SIGCHLD: case SIGWINCH:
+			case SIGCONT: case SIGCHLD: case SIGWINCH: case SIGURG:
 				continue;
 
 			case SIGTSTP: case SIGTTIN: case SIGTTOU:
@@ -1430,10 +1430,7 @@
 				/* FALLTHRU */
 
 			default:
-				sigaddset(&current->pending.signal, signr);
-				recalc_sigpending(current);
-				current->flags |= PF_SIGNALED;
-				do_exit(exit_code);
+				sig_exit(signr, exit_code, &info);
 				/* NOTREACHED */
 			}
 		}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/smp.c linuxppc64_2_4/arch/ppc64/kernel/smp.c
--- linux-2.4.19/arch/ppc64/kernel/smp.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/smp.c	Wed Aug  7 11:09:14 2002
@@ -52,6 +52,14 @@
 #include <asm/ppcdebug.h>
 #include "open_pic.h"
 #include <asm/machdep.h>
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+int (*dump_ipi_function_ptr)(struct pt_regs *);
+#include <linux/dump.h>
+#endif
+
+#ifdef CONFIG_KDB
+#include <linux/kdb.h>
+#endif
 
 int smp_threads_ready = 0;
 volatile int smp_commenced = 0;
@@ -93,9 +101,21 @@
        } while(0)
 
 #ifdef CONFIG_KDB
+/* save regs here before calling kdb_ipi */
+struct pt_regs *kdb_smp_regs[NR_CPUS]; 
+
+/* called for each processor.. drop each into kdb. */
+void smp_kdb_stop_proc(void)
+{
+    kdb_ipi(kdb_smp_regs[smp_processor_id()], NULL);
+}
+
 void smp_kdb_stop(void)
 {
+  int ret=0;
+  ret =    smp_call_function(smp_kdb_stop_proc, NULL, 1, 0);
 }
+
 #endif
 
 static inline void set_tb(unsigned int upper, unsigned int lower)
@@ -161,7 +181,7 @@
 			paca[i].next_jiffy_update_tb = paca[0].next_jiffy_update_tb;
 		}
 	}
-	
+
 	smp_tb_synchronized = 1;
 	return np;
 }
@@ -263,8 +283,6 @@
 	paca[nr].xProcStart = 1;
 }
 
-extern struct gettimeofday_struct do_gtod;
-
 static void smp_space_timers( unsigned nr )
 {
 	unsigned long offset, i;
@@ -285,8 +303,8 @@
 		/* timebases already synced under the hypervisor. */
 		paca[cpu_nr].next_jiffy_update_tb = tb_last_stamp = get_tb();
 		if (cpu_nr == 0) {
-			do_gtod.tb_orig_stamp = tb_last_stamp;
-			/* Should update do_gtod.stamp_xsec.
+			naca->tb_orig_stamp = tb_last_stamp;
+			/* Should update naca->stamp_xsec.
 			 * For now we leave it which means the time can be some
 			 * number of msecs off until someone does a settimeofday()
 			 */
@@ -312,7 +330,7 @@
 			mb();
 			frozen = 0;
 			tb_last_stamp = get_tb();
-			do_gtod.tb_orig_stamp = tb_last_stamp;
+			naca->tb_orig_stamp = tb_last_stamp;
 			smp_tb_synchronized = 1;
 		} else {
 			atomic_inc(&ready);
@@ -346,7 +364,7 @@
 			set_bit(msg, &xics_ipi_message[i]);
 			mb();
 			xics_cause_IPI(i);
-			}
+		}
 	}
 }
 
@@ -387,6 +405,9 @@
 	
 	switch( msg ) {
 	case PPC_MSG_CALL_FUNCTION:
+#ifdef CONFIG_KDB
+		kdb_smp_regs[smp_processor_id()]=regs;
+#endif
 		smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE: 
@@ -394,15 +415,16 @@
 		break;
 #ifdef CONFIG_XMON
 	case PPC_MSG_XMON_BREAK:
-		xmon(regs);
+	        /* ToDo: need a nmi way to handle this.  Soft disable? */
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+	        if (dump_ipi_function_ptr) {
+			printk(KERN_ALERT "got dump ipi...\n");
+			dump_ipi_function_ptr(regs);
+		} else
+#endif
+			xmon(regs);
 		break;
 #endif /* CONFIG_XMON */
-#ifdef CONFIG_KDB
-	case PPC_MSG_XMON_BREAK:
-	        /* This isn't finished yet, obviously -TAI */
-		kdb(KDB_REASON_KEYBOARD,0, (kdb_eframe_t) regs);
-		break;
-#endif
 	default:
 		printk("SMP %d: smp_message_recv(): unknown msg %d\n",
 		       smp_processor_id(), msg);
@@ -422,6 +444,18 @@
 }
 #endif /* CONFIG_XMON */
 
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+void dump_send_ipi(int (*dump_ipi_callback)(struct pt_regs *))
+{
+	dump_ipi_function_ptr = dump_ipi_callback;
+	if (dump_ipi_callback) {
+		printk(KERN_ALERT "dump_send_ipi...\n");
+		mb();
+		smp_message_pass(MSG_ALL_BUT_SELF, PPC_MSG_XMON_BREAK, 0, 0);
+	}
+}
+#endif
+
 static void stop_this_cpu(void *dummy)
 {
 	__cli();
@@ -463,7 +497,7 @@
  * remote CPUs are nearly ready to execute <<func>> or are or have executed.
  *
  * You must not call this function with disabled interrupts or from a
- * hardware interrupt handler, you may call it from a bottom half handler.
+ * hardware interrupt handler or from a bottom half handler.
  */
 int smp_call_function (void (*func) (void *info), void *info, int nonatomic,
 			int wait)
@@ -498,6 +532,10 @@
 #ifdef CONFIG_XMON
                         xmon(0);
 #endif
+#ifdef CONFIG_KDB
+			kdb(KDB_REASON_CALL,0, (kdb_eframe_t) 0);
+#endif
+
 #ifdef CONFIG_PPC_ISERIES
 			HvCall_terminateMachineSrc();
 #endif
@@ -550,6 +588,7 @@
 		atomic_inc(&call_data->finished);
 }
 
+
 extern unsigned long decr_overclock;
 
 void __init smp_boot_cpus(void)
@@ -577,7 +616,7 @@
 	init_idle();
 
 	for (i = 0; i < NR_CPUS; i++) {
-		paca[i].prof_counter=1;
+		paca[i].prof_counter = 1;
 		paca[i].prof_multiplier = 1;
 		if(i != 0) {
 		        /*
@@ -611,7 +650,7 @@
 	if (cpu_nr > max_cpus)
 		cpu_nr = max_cpus;
 
-#ifdef CONFIG_ISERIES
+#ifdef CONFIG_PPC_ISERIES
 	smp_space_timers( cpu_nr );
 #endif
 
@@ -742,7 +781,7 @@
 {
 }
 
-int __init setup_profiling_timer(unsigned int multiplier)
+int setup_profiling_timer(unsigned int multiplier)
 {
 	return 0;
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/stab.c linuxppc64_2_4/arch/ppc64/kernel/stab.c
--- linux-2.4.19/arch/ppc64/kernel/stab.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/stab.c	Tue Aug 20 14:46:58 2002
@@ -35,7 +35,7 @@
 	esid = GET_ESID(KERNELBASE);
 	vsid = get_kernel_vsid(esid << SID_SHIFT); 
 
-	if (!__is_processor(PV_POWER4)) {
+	if (!__is_processor(PV_POWER4) && !__is_processor(PV_POWER4p)) {
                 __asm__ __volatile__("isync; slbia; isync":::"memory");
 		make_ste(stab, esid, vsid);
 	} else {
@@ -305,7 +305,7 @@
 	unsigned char *segments = get_paca()->xSegments;
 	unsigned long flags, i;
 
-	if(!__is_processor(PV_POWER4)) {
+	if (!__is_processor(PV_POWER4) && !__is_processor(PV_POWER4p)) {
 		unsigned long entry;
 		STE *ste;
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/sys_ppc32.c linuxppc64_2_4/arch/ppc64/kernel/sys_ppc32.c
--- linux-2.4.19/arch/ppc64/kernel/sys_ppc32.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/sys_ppc32.c	Tue Jul 23 03:12:30 2002
@@ -81,47 +81,6 @@
  */
 #define MSR_USERCHANGE	(MSR_FE0 | MSR_FE1)
 
-/* In order to reduce some races, while at the same time doing additional
- * checking and hopefully speeding things up, we copy filenames to the
- * kernel data space before using them..
- *
- * POSIX.1 2.4: an empty pathname is invalid (ENOENT).
- */
-static inline int do_getname32(const char *filename, char *page)
-{
-	int retval;
-
-	/* 32bit pointer will be always far below TASK_SIZE :)) */
-	retval = strncpy_from_user((char *)page, (char *)filename, PAGE_SIZE);
-	if (retval > 0) {
-		if (retval < PAGE_SIZE)
-			return 0;
-		return -ENAMETOOLONG;
-	} else if (!retval)
-		retval = -ENOENT;
-	return retval;
-}
-
-char * getname32(const char *filename)
-{
-	char *tmp, *result;
-
-	result = ERR_PTR(-ENOMEM);
-  tmp =  __getname();
-	if (tmp)  {
-		int retval = do_getname32(filename, tmp);
-
-		result = tmp;
-		if (retval < 0) {
-			putname(tmp);
-			result = ERR_PTR(retval);
-		}
-	}
-	return result;
-}
-
-
-
 extern asmlinkage long sys_utime(char * filename, struct utimbuf * times);
 
 struct utimbuf32 {
@@ -141,7 +100,7 @@
 		return sys_utime(filename, NULL);
 	if (get_user(t.actime, &times->actime) || __get_user(t.modtime, &times->modtime))
 		return -EFAULT;
-	filenam = getname32(filename);
+	filenam = getname(filename);
 
 	ret = PTR_ERR(filenam);
 	if (!IS_ERR(filenam)) {
@@ -554,7 +513,7 @@
 		return sys_quotactl(cmd, special,
 				    id, (caddr_t)addr);
 	}
-	spec = getname32 (special);
+	spec = getname (special);
 	err = PTR_ERR(spec);
 	if (IS_ERR(spec)) return err;
 	old_fs = get_fs ();
@@ -899,15 +858,6 @@
 	return ret;
 }
 
-
-
-
-/*
- * Due to some executables calling the wrong select we sometimes
- * get wrong args.  This determines how the args are being passed
- * (a single ptr to them all args passed) then calls
- * sys_select() with the appropriate args. -- Cort
- */
 /* Note: it is necessary to treat n as an unsigned int, 
  * with the corresponding cast to a signed int to insure that the 
  * proper conversion (sign extension) between the register representation of a signed int (msr in 32-bit mode)
@@ -915,14 +865,9 @@
  */
 asmlinkage int ppc32_select(u32 n, u32* inp, u32* outp, u32* exp, u32 tvp_x)
 {
-	if ((unsigned int)n >= 4096)
-		panic("ppc32_select - wrong arguments were passed in \n");
-
 	return sys32_select((int)n, inp, outp, exp, tvp_x);
 }
 
-
-
 static int cp_new_stat32(struct inode *inode, struct stat32 *statbuf)
 {
 	unsigned long ino, blksize, blocks;
@@ -1090,7 +1035,7 @@
 	
 	PPCDBG(PPCDBG_SYS32X, "sys32_statfs - entered - pid=%ld current=%lx comm=%s\n", current->pid, current, current->comm);
 	
-	pth = getname32 (path);
+	pth = getname (path);
 	ret = PTR_ERR(pth);
 	if (!IS_ERR(pth)) {
 		set_fs (KERNEL_DS);
@@ -2883,9 +2828,6 @@
 		}
 		break;
 	}
-#if 0
-	udbg_printf("  err3 = 0x%lx\n", err); 
-#endif
 	return err;
 }
 
@@ -4000,11 +3942,6 @@
 	int error;
 	char * filename;
 	
-	ifppcdebug(PPCDBG_SYS32) {
-		udbg_printf("sys32_execve - entered - pid=%ld, comm=%s \n", current->pid, current->comm);
-		//PPCDBG(PPCDBG_SYS32NI, "               a0=%lx, a1=%lx, a2=%lx, a3=%lx, a4=%lx, a5=%lx, regs=%p \n", a0, a1, a2, a3, a4, a5, regs);
-	}
-
 	filename = getname((char *) a0);
 	error = PTR_ERR(filename);
 	if (IS_ERR(filename))
@@ -4019,10 +3956,6 @@
 	putname(filename);
 
 out:
-	ifppcdebug(PPCDBG_SYS32) {
-		udbg_printf("sys32_execve - exited - returning %x - pid=%ld \n", error, current->pid);
-		//udbg_printf("sys32_execve - at exit - regs->gpr[1]=%lx, gpr[3]=%lx, gpr[4]=%lx, gpr[5]=%lx, gpr[6]=%lx \n", regs->gpr[1], regs->gpr[3], regs->gpr[4], regs->gpr[5], regs->gpr[6]);
-	}
 	return error;
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/time.c linuxppc64_2_4/arch/ppc64/kernel/time.c
--- linux-2.4.19/arch/ppc64/kernel/time.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/time.c	Mon Aug  5 13:02:02 2002
@@ -47,6 +47,7 @@
 #include <linux/time.h>
 #include <linux/init.h>
 
+#include <asm/naca.h>
 #include <asm/segment.h>
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -64,6 +65,8 @@
 
 void smp_local_timer_interrupt(struct pt_regs *);
 
+extern void setup_before_console_init();
+
 /* keep track of when we need to update the rtc */
 time_t last_rtc_update;
 extern rwlock_t xtime_lock;
@@ -83,12 +86,9 @@
 unsigned long next_xtime_sync_tb;
 unsigned long xtime_sync_interval;
 unsigned long tb_to_xs;
-unsigned      tb_to_us;
 unsigned long processor_freq;
 spinlock_t rtc_lock = SPIN_LOCK_UNLOCKED;
 
-struct gettimeofday_struct do_gtod;
-
 extern unsigned long wall_jiffies;
 extern unsigned long lpEvent_count;
 extern int smp_tb_synchronized;
@@ -99,15 +99,14 @@
 extern unsigned long prof_shift;
 extern char _stext;
 
+extern struct timezone sys_tz;
+
 void ppc_adjtimex(void);
 
 static unsigned adjusting_time = 0;
 
-static inline void ppc_do_profile (unsigned long nip)
+static void ppc_do_profile (unsigned long nip)
 {
-	if (!prof_buffer)
-		return;
-
 	/*
 	 * Only measure the CPUs specified by /proc/irq/prof_cpu_mask.
 	 * (default is all CPUs.)
@@ -181,9 +180,9 @@
 #ifdef CONFIG_PPC_ISERIES
 
 /* 
- * This function recalibrates the timebase based on the 49-bit time-of-day value in the Titan chip.
- * The Titan is much more accurate than the value returned by the service processor for the
- * timebase frequency.  
+ * This function recalibrates the timebase based on the 49-bit time-of-day
+ * value in the Titan chip.  The Titan is much more accurate than the value
+ * returned by the service processor for the timebase frequency.  
  */
 
 static void iSeries_tb_recal(void)
@@ -213,9 +212,9 @@
 				tb_ticks_per_jiffy = new_tb_ticks_per_jiffy;
 				tb_ticks_per_sec   = new_tb_ticks_per_sec;
 				div128_by_32( XSEC_PER_SEC, 0, tb_ticks_per_sec, &divres );
-				do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
+				naca->tb_ticks_per_sec = tb_ticks_per_sec;
 				tb_to_xs = divres.result_low;
-				do_gtod.varp->tb_to_xs = tb_to_xs;
+				naca->tb_to_xs = tb_to_xs;
 			}
 			else {
 				printk( "Titan recalibrate: FAILED (difference > 4 percent)\n"
@@ -256,10 +255,8 @@
 
 	irq_enter(cpu);
 
-#ifndef CONFIG_PPC_ISERIES
-	if (!user_mode(regs))
+	if ((!user_mode(regs)) && (prof_buffer))
 		ppc_do_profile(instruction_pointer(regs));
-#endif
 
 	lpaca->xLpPaca.xIntDword.xFields.xDecrInt = 0;
 
@@ -306,17 +303,62 @@
 {
         unsigned long sec, usec, tb_ticks;
 	unsigned long xsec, tb_xsec;
-	struct gettimeofday_vars * temp_varp;
 	unsigned long temp_tb_to_xs, temp_stamp_xsec;
+	unsigned long tb_count_1, tb_count_2;
+	unsigned long always_zero;
+	struct naca_struct *gtdp;
+	
+	gtdp = (struct naca_struct *)0xC000000000004000;
+	/* 
+	 * The following loop guarantees that we see a consistent view of the
+	 * tb_to_xs and stamp_xsec variables.  These two variables can change
+	 * (eg. when xntpd adjusts the clock frequency) and an inconsistent
+	 * view (one variable changed, the other not) could result in a wildly
+	 * wrong result for do_gettimeofday. 
+	 *
+	 * The code which updates these variables (ppc_adjtimex below)
+	 * increments tb_update_count, then updates the two variables and then
+	 * increments tb_update_count again.  This code reads tb_update_count,
+	 * reads the two variables and then reads tb_update_count again.  It
+	 * loops doing this until the two reads of tb_update_count yield the
+	 * same value and that value is even.  This ensures a consistent view
+	 * of the two variables.
+	 *
+	 * The strange looking assembler code below causes the hardware to
+	 * think that reading the two variables is dependent on the first read
+	 * of tb_update_count and that the second reading of tb_update_count is
+	 * dependent on reading the two variables.  This assures ordering
+	 * without the need for a lwsync, which is much more expensive.
+	 */
+	do {
+		tb_ticks = get_tb() - gtdp->tb_orig_stamp;
+
+		tb_count_1 = gtdp->tb_update_count;
+
+		__asm__ __volatile__ (
+"		andc 	%0,%2,%2\n\
+		add	%1,%3,%0\n\
+"		: "=&r"(always_zero), "=r"(gtdp)
+		: "r"(tb_count_1), "r"(gtdp) );
+
+		temp_tb_to_xs = gtdp->tb_to_xs;
+		temp_stamp_xsec = gtdp->stamp_xsec;
+
+		__asm__ __volatile__ (
+"		add	%0,%2,%3\n\
+		andc	%0,%0,%0\n\
+		add	%1,%4,%0\n\
+"		: "=&r"(always_zero), "=r"(gtdp)
+		: "r"(temp_stamp_xsec), "r"(temp_tb_to_xs), "r"(gtdp) );
+
+		tb_count_2 = gtdp->tb_update_count;
+
+	} while ( tb_count_2 - ( tb_count_1 & 0xfffffffffffffffe ) ); 
 
 	/* These calculations are faster (gets rid of divides)
 	 * if done in units of 1/2^20 rather than microseconds.
 	 * The conversion to microseconds at the end is done
 	 * without a divide (and in fact, without a multiply) */
-	tb_ticks = get_tb() - do_gtod.tb_orig_stamp;
-	temp_varp = do_gtod.varp;
-	temp_tb_to_xs = temp_varp->tb_to_xs;
-	temp_stamp_xsec = temp_varp->stamp_xsec;
 	tb_xsec = mulhdu( tb_ticks, temp_tb_to_xs );
 	xsec = temp_stamp_xsec + tb_xsec;
 	sec = xsec / XSEC_PER_SEC;
@@ -370,21 +412,24 @@
 	time_maxerror = NTP_PHASE_LIMIT;
 	time_esterror = NTP_PHASE_LIMIT;
 
-	delta_xsec = mulhdu( (tb_last_stamp-do_gtod.tb_orig_stamp), do_gtod.varp->tb_to_xs );
+	delta_xsec = mulhdu( (tb_last_stamp-naca->tb_orig_stamp), naca->tb_to_xs );
 	new_xsec = (new_usec * XSEC_PER_SEC) / USEC_PER_SEC;
 	new_xsec += new_sec * XSEC_PER_SEC;
 	if ( new_xsec > delta_xsec ) {
-		do_gtod.varp->stamp_xsec = new_xsec - delta_xsec;
+		naca->stamp_xsec = new_xsec - delta_xsec;
 	}
 	else {
 		/* This is only for the case where the user is setting the time
 		 * way back to a time such that the boot time would have been
-		 * before 1970 ... eg. we booted ten days ago, and we are setting
-		 * the time to Jan 5, 1970 */
-		do_gtod.varp->stamp_xsec = new_xsec;
-		do_gtod.tb_orig_stamp = tb_last_stamp;
+		 * before 1970 ... eg. we booted ten days ago, and we are
+		 * setting the time to Jan 5, 1970 */
+		naca->stamp_xsec = new_xsec;
+		naca->tb_orig_stamp = tb_last_stamp;
 	}
 
+	naca->tz_minuteswest = sys_tz.tz_minuteswest;
+	naca->tz_dsttime = sys_tz.tz_dsttime;
+
 	write_unlock_irqrestore(&xtime_lock, flags);
 }
 
@@ -453,13 +498,11 @@
 	xtime.tv_sec = mktime(tm.tm_year + 1900, tm.tm_mon + 1, tm.tm_mday,
 			      tm.tm_hour, tm.tm_min, tm.tm_sec);
 	tb_last_stamp = get_tb();
-	do_gtod.tb_orig_stamp = tb_last_stamp;
-	do_gtod.varp = &do_gtod.vars[0];
-	do_gtod.var_idx = 0;
-	do_gtod.varp->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
-	do_gtod.tb_ticks_per_sec = tb_ticks_per_sec;
-	do_gtod.varp->tb_to_xs = tb_to_xs;
-	do_gtod.tb_to_us = tb_to_us;
+	naca->tb_orig_stamp = tb_last_stamp;
+	naca->tb_update_count = 0;
+	naca->tb_ticks_per_sec = tb_ticks_per_sec;
+	naca->stamp_xsec = xtime.tv_sec * XSEC_PER_SEC;
+	naca->tb_to_xs = tb_to_xs;
 
 	xtime_sync_interval = tb_ticks_per_sec - (tb_ticks_per_sec/8);
 	next_xtime_sync_tb = tb_last_stamp + xtime_sync_interval;
@@ -470,23 +513,21 @@
 	last_rtc_update = xtime.tv_sec;
 	write_unlock_irqrestore(&xtime_lock, flags);
 
-#ifdef CONFIG_PPC_ISERIES
-	/* HACK HACK This allows the iSeries profiling to use /proc/profile */
-	prof_shift = 0;
-#endif
-
 	/* Not exact, but the timer interrupt takes care of this */
 	set_dec(tb_ticks_per_jiffy);
+
+	/* This horrible hack gives setup a hook just before console_init */
+	setup_before_console_init();
 }
 
 /* 
  * After adjtimex is called, adjust the conversion of tb ticks
  * to microseconds to keep do_gettimeofday synchronized 
  * with ntpd.
-
+ *
  * Use the time_adjust, time_freq and time_offset computed by adjtimex to 
  * adjust the frequency.
-*/
+ */
 
 /* #define DEBUG_PPC_ADJTIMEX 1 */
 
@@ -497,8 +538,6 @@
 	long delta_freq, ltemp;
 	struct div_result divres; 
 	unsigned long flags;
-	struct gettimeofday_vars * temp_varp;
-	unsigned temp_idx;
 	long singleshot_ppm = 0;
 
 	/* Compute parts per million frequency adjustment to accomplish the time adjustment
@@ -529,8 +568,11 @@
 		
 		/* Compute parts per million frequency adjustment to match time_adjust */
 		singleshot_ppm = tickadj * HZ;	
-		/* The adjustment should be tickadj*HZ to match the code in linux/kernel/timer.c, but
-		   experiments show that this is too large. 3/4 of tickadj*HZ seems about right */
+		/*
+		 * The adjustment should be tickadj*HZ to match the code in
+		 * linux/kernel/timer.c, but experiments show that this is too
+		 * large. 3/4 of tickadj*HZ seems about right
+		 */
 		singleshot_ppm -= singleshot_ppm / 4;
 		/* Use SHIFT_USEC to get it into the same units as time_freq */	
 		singleshot_ppm <<= SHIFT_USEC;
@@ -564,36 +606,38 @@
 	printk("ppc_adjtimex: tb_ticks_per_sec - base = %ld  new = %ld\n", tb_ticks_per_sec, new_tb_ticks_per_sec);
 #endif
 				
-	/* Compute a new value of tb_to_xs (used to convert tb to microseconds and a new value of 
-	   stamp_xsec which is the time (in 1/2^20 second units) corresponding to tb_orig_stamp.  This 
-	   new value of stamp_xsec compensates for the change in frequency (implied by the new tb_to_xs)
-	   which guarantees that the current time remains the same */ 
-	tb_ticks = get_tb() - do_gtod.tb_orig_stamp;
+	/*
+	 * Compute a new value of tb_to_xs (used to convert tb to microseconds
+	 * and a new value of stamp_xsec which is the time (in 1/2^20 second
+	 * units) corresponding to tb_orig_stamp.  This new value of stamp_xsec
+	 * compensates for the change in frequency (implied by the new
+	 * tb_to_xs) and so guarantees that the current time remains the same
+	 *
+	 */ 
+	tb_ticks = get_tb() - naca->tb_orig_stamp;
 	div128_by_32( 1024*1024, 0, new_tb_ticks_per_sec, &divres );
 	new_tb_to_xs = divres.result_low;
 	new_xsec = mulhdu( tb_ticks, new_tb_to_xs );
 
 	write_lock_irqsave( &xtime_lock, flags );
-	old_xsec = mulhdu( tb_ticks, do_gtod.varp->tb_to_xs );
-	new_stamp_xsec = do_gtod.varp->stamp_xsec + old_xsec - new_xsec;
+	old_xsec = mulhdu( tb_ticks, naca->tb_to_xs );
+	new_stamp_xsec = naca->stamp_xsec + old_xsec - new_xsec;
 
-	/* There are two copies of tb_to_xs and stamp_xsec so that no lock is needed to access and use these
-	   values in do_gettimeofday.  We alternate the copies and as long as a reasonable time elapses between
-	   changes, there will never be inconsistent values.  ntpd has a minimum of one minute between updates */
-
-	if (do_gtod.var_idx == 0) {
-		temp_varp = &do_gtod.vars[1];
-		temp_idx  = 1;
-	}
-	else {
-		temp_varp = &do_gtod.vars[0];
-		temp_idx  = 0;
-	}
-	temp_varp->tb_to_xs = new_tb_to_xs;
-	temp_varp->stamp_xsec = new_stamp_xsec;
-	mb();
-	do_gtod.varp = temp_varp;
-	do_gtod.var_idx = temp_idx;
+	/*
+	 * tb_update_count is used to allow the problem state gettimeofday code
+	 * to assure itself that it sees a consistent view of the tb_to_xs and
+	 * stamp_xsec variables.  It reads the tb_update_count, then reads
+	 * tb_to_xs and stamp_xsec and then reads tb_update_count again.  If
+	 * the two values of tb_update_count match and are even then the
+	 * tb_to_xs and stamp_xsec values are consistent.  If not, then it
+	 * loops back and reads them again until this criteria is met.
+	 */
+	++(naca->tb_update_count);
+	wmb();
+	naca->tb_to_xs = new_tb_to_xs;
+	naca->stamp_xsec = new_stamp_xsec;
+	wmb();
+	++(naca->tb_update_count);
 
 	write_unlock_irqrestore( &xtime_lock, flags );
 
@@ -691,6 +735,7 @@
 	GregorianDay(tm);
 }
 
+#if 0
 /* Auxiliary function to compute scaling factors */
 /* Actually the choice of a timebase running at 1/4 the of the bus
  * frequency giving resolution of a few tens of nanoseconds is quite nice.
@@ -720,6 +765,7 @@
         if (err <= inscale/2) mlt++;
         return mlt;
   }
+#endif
 
 /*
  * Divide a 128-bit dividend by a 32-bit divisor, leaving a 128 bit
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/traps.c linuxppc64_2_4/arch/ppc64/kernel/traps.c
--- linux-2.4.19/arch/ppc64/kernel/traps.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/traps.c	Tue Jun 25 08:47:30 2002
@@ -29,6 +29,8 @@
 #include <linux/interrupt.h>
 #include <linux/config.h>
 #include <linux/init.h>
+#include <asm/iSeries/HvCall.h>
+#include <asm/iSeries/HvCallCfg.h>
 
 #ifdef CONFIG_KDB
 #include <linux/kdb.h>
@@ -71,8 +73,23 @@
 int (*debugger_iabr_match)(struct pt_regs *regs);
 int (*debugger_dabr_match)(struct pt_regs *regs);
 void (*debugger_fault_handler)(struct pt_regs *regs);
-#endif
-#endif
+#else
+#ifdef CONFIG_KDB
+/* kdb has different call parms */
+/* ... */
+void (*debugger)(struct pt_regs *regs) = kdb;
+int (*debugger_bpt)(struct pt_regs *regs);
+int (*debugger_sstep)(struct pt_regs *regs);
+int (*debugger_iabr_match)(struct pt_regs *regs);
+int (*debugger_dabr_match)(struct pt_regs *regs);
+/* - only defined during (xmon) mread */
+void (*debugger_fault_handler)(struct pt_regs *regs);
+#endif /* kdb */
+#endif /* kgdb */
+#endif /* xmon */
+
+void set_local_DABR(void *valp);
+
 /*
  * Trap & Exception support
  */
@@ -151,7 +168,16 @@
 	xmon(regs);
 	udbg_printf("leaving xmon...\n");
 #endif
+#if defined(CONFIG_KDB)
+	kdb(KDB_REASON_ENTER, regs->trap, (kdb_eframe_t) regs);
+	udbg_printf("leaving kdb...\n");
+#endif
+
+/*
+allow system to resume after reset.
 	for(;;);
+*/
+
 }
 
 
@@ -272,14 +298,6 @@
 	_exception(SIGTRAP, regs);	
 }
 
-/* Dummy handler for Performance Monitor */
-
-void
-PerformanceMonitorException(struct pt_regs *regs)
-{
-	_exception(SIGTRAP, regs);
-}
-
 void
 AlignmentException(struct pt_regs *regs)
 {
@@ -306,4 +324,21 @@
 
 void __init trap_init(void)
 {
+}
+
+/*
+ * Set the DABR on all processors in the system.  The value is defined as:
+ * DAB(0:60), Break Translate(61), Write(62), Read(63)
+ */
+void
+set_all_DABR(unsigned long val) {
+	set_local_DABR(&val); 
+	smp_call_function(set_local_DABR, &val, 0, 0);
+}
+
+void
+set_local_DABR(void *valp) {
+	unsigned long val = *((unsigned long *)valp); 
+
+	HvCall_setDABR(val);
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/udbg.c linuxppc64_2_4/arch/ppc64/kernel/udbg.c
--- linux-2.4.19/arch/ppc64/kernel/udbg.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/udbg.c	Thu Aug 22 11:27:34 2002
@@ -52,8 +52,6 @@
 
 volatile struct NS16550 *udbg_comport;
 
-spinlock_t udbg_lock = SPIN_LOCK_UNLOCKED;
-
 void
 udbg_init_uart(void *comport)
 {
@@ -161,6 +159,12 @@
 }
 
 void
+udbg_console_write(struct console *con, const char *s, unsigned int n)
+{
+	udbg_write(s, n);
+}
+
+void
 udbg_puthex(unsigned long val)
 {
 	int i, nibbles = sizeof(val)*2;
@@ -190,16 +194,13 @@
 void
 udbg_printf(const char *fmt, ...)
 {
-	unsigned long flags;
 	unsigned char buf[256];
 
 	va_list args;
 	va_start(args, fmt);
 
-	spin_lock_irqsave(&udbg_lock, flags);
 	vsprintf(buf, fmt, args);
 	udbg_puts(buf);
-	spin_unlock_irqrestore(&udbg_lock, flags);
 
 	va_end(args);
 }
@@ -208,7 +209,6 @@
 void
 udbg_ppcdbg(unsigned long debug_flags, const char *fmt, ...)
 {
-	unsigned long flags;
 	unsigned long active_debugs = debug_flags & naca->debug_switch;
 
 	if ( active_debugs ) {
@@ -216,7 +216,6 @@
 		unsigned char buf[256];
 		unsigned long i, len = 0;
 
-		spin_lock_irqsave(&udbg_lock, flags);
 		for(i=0; i < PPCDBG_NUM_FLAGS ;i++) {
 			if (((1U << i) & active_debugs) && 
 			    trace_names[i]) {
@@ -237,7 +236,6 @@
 		va_start(ap, fmt);
 		vsprintf(buf, fmt, ap);
 		udbg_puts(buf);
-		spin_unlock_irqrestore(&udbg_lock, flags);
 		
 		va_end(ap);
 	}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/kernel/xics.c linuxppc64_2_4/arch/ppc64/kernel/xics.c
--- linux-2.4.19/arch/ppc64/kernel/xics.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/kernel/xics.c	Mon Aug  5 13:02:02 2002
@@ -231,9 +231,9 @@
 		}
 	} else if( vec == XICS_IRQ_SPURIOUS ) {
 		irq = -1;
-		printk("spurious PPC interrupt!\n");
-	} else
+	} else {
 		irq = real_irq_to_virt(vec) + XICS_IRQ_OFFSET;
+	}
 	return irq;
 }
 
@@ -285,6 +285,8 @@
 	struct device_node *np;
 	uint *ireg, ilen, indx=0;
 
+	ppc64_boot_msg(0x20, "XICS Init");
+
 	ibm_get_xive = rtas_token("ibm,get-xive");
 	ibm_set_xive = rtas_token("ibm,set-xive");
 	ibm_int_off = rtas_token("ibm,int-off");
@@ -347,6 +349,7 @@
 	np = find_type_devices("interrupt-controller");
 	if (!np) {
 		printk(KERN_WARNING "xics:  no ISA Interrupt Controller\n");
+		xics_irq_8259_cascade_real = -1;
 		xics_irq_8259_cascade = -1;
 	} else {
 		ireg = (uint *) get_property(np, "interrupts", 0);
@@ -399,6 +402,7 @@
 	request_irq(XICS_IPI + XICS_IRQ_OFFSET, xics_ipi_action, 0, "IPI", 0);
 	irq_desc[XICS_IPI+XICS_IRQ_OFFSET].status |= IRQ_PER_CPU;
 #endif
+	ppc64_boot_msg(0x21, "XICS Done");
 }
 
 void xics_isa_init(void)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/lib/string.S linuxppc64_2_4/arch/ppc64/lib/string.S
--- linux-2.4.19/arch/ppc64/lib/string.S	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/lib/string.S	Fri May 31 10:29:57 2002
@@ -351,9 +351,9 @@
 #endif /* MAX_COPY_PREFETCH */
 
 	mtctr	r0
-53:
 	dcbt	r3,r4
-	dcbz	r11,r6
+53:
+54:	dcbz	r11,r6
 /* had to move these to keep extable in order */
 	.section __ex_table,"a"
 	.align	3
@@ -361,7 +361,7 @@
 	.llong	71b,101f
 	.llong	72b,102f
 	.llong	73b,103f
-	.llong	53b,105f
+	.llong	54b,105f
 	.text
 /* the main body of the cacheline loop */
 	COPY_16_BYTES_WITHEX(0)
@@ -504,11 +504,11 @@
 	add	r4,r0,r4
 	subf	r6,r0,r6
 	srwi	r0,r4,2
+	andi.	r4,r4,3
 	mtctr	r0
-	bdz	6f
+	bdz	7f
 1:	stwu	r5,4(r6)
 	bdnz	1b
-6:	andi.	r4,r4,3
 	/* clear byte sized chunks */
 7:	cmpwi	0,r4,0
 	beqlr
@@ -517,14 +517,20 @@
 8:	stbu	r5,1(r6)
 	bdnz	8b
 	blr
-99:	li	r3,-EFAULT
+90:	mr	r3,r4
+	blr
+91:	mfctr	r3
+	slwi	r3,r3,2
+	add	r3,r3,r4
+	blr
+92:	mfctr	r3
 	blr
 
 	.section __ex_table,"a"
 	.align	3
-	.llong	11b,99b
-	.llong	1b,99b
-	.llong	8b,99b
+	.llong	11b,90b
+	.llong	1b,91b
+	.llong	8b,92b
 	.text
 
 _GLOBAL(__strncpy_from_user)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/mm/fault.c linuxppc64_2_4/arch/ppc64/mm/fault.c
--- linux-2.4.19/arch/ppc64/mm/fault.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/mm/fault.c	Tue Jul 23 02:44:17 2002
@@ -38,7 +38,11 @@
 
 #include <asm/ppcdebug.h>
 
-#if defined(CONFIG_XMON) || defined(CONFIG_KGDB)
+#if defined(CONFIG_KDB)
+#include <linux/kdb.h>
+#endif
+	
+#if defined(CONFIG_XMON) || defined(CONFIG_KGDB) || defined(CONFIG_KDB)
 extern void (*debugger)(struct pt_regs *);
 extern void (*debugger_fault_handler)(struct pt_regs *);
 extern int (*debugger_dabr_match)(struct pt_regs *);
@@ -118,7 +122,7 @@
 
 good_area:
 	code = SEGV_ACCERR;
-	
+
 	/* a write */
 	if (is_write) {
 		if (!(vma->vm_flags & VM_WRITE))
@@ -159,7 +163,7 @@
 
 bad_area:
 	up_read(&mm->mmap_sem);
-	
+
 	/* User mode accesses cause a SIGSEGV */
 	if (user_mode(regs)) {
 		info.si_signo = SIGSEGV;
@@ -224,8 +228,10 @@
 	if (debugger_kernel_faults)
 		debugger(regs);
 #endif
+#if defined(CONFIG_KDB)
+	kdb(KDB_REASON_FAULT, regs->trap, regs);
+#endif	
 	print_backtrace( (unsigned long *)regs->gpr[1] );
 	panic("kernel access of bad area pc %lx lr %lx address %lX tsk %s/%d",
 	      regs->nip,regs->link,address,current->comm,current->pid);
 }
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/mm/init.c linuxppc64_2_4/arch/ppc64/mm/init.c
--- linux-2.4.19/arch/ppc64/mm/init.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/mm/init.c	Tue Jul 23 02:44:17 2002
@@ -1,6 +1,4 @@
 /*
- *  
- *
  *  PowerPC version 
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
  *
@@ -214,7 +212,7 @@
 	else {
 		ea = ioremap_bot;
 		ioremap_bot += size;
-        }
+	}
 
 	if ((flags & _PAGE_PRESENT) == 0)
 		flags |= pgprot_val(PAGE_KERNEL);
@@ -231,9 +229,9 @@
 void iounmap(void *addr) 
 {
 #ifdef CONFIG_PPC_ISERIES
-     /* iSeries I/O Remap is a noop              */
+	/* iSeries I/O Remap is a noop              */
 	return;
-#else 
+#else
 	/* DRENG / PPPBBB todo */
 	return;
 #endif
@@ -264,7 +262,7 @@
 		/* If the mm subsystem is not fully up, we cannot create a
 		 * linux page table entry for this mapping.  Simply bolt an
 		 * entry in the hardware page table. 
- 		 */
+		 */
 		vsid = get_kernel_vsid(ea);
 		make_pte(htab_data.htab,
 			(vsid << 28) | (ea & 0xFFFFFFF), // va (NOT the ea)
@@ -434,12 +432,11 @@
 }
 #endif
 
-
-
 /*
  * Do very early mm setup.
  */
-void __init mm_init_ppc64(void) {
+void __init mm_init_ppc64(void)
+{
 	struct paca_struct *lpaca;
 	unsigned long guard_page, index;
 
@@ -467,8 +464,6 @@
 	ppc_md.progress("MM:exit", 0x211);
 }
 
-
-
 /*
  * Initialize the bootmem system and give it all the memory we
  * have available.
@@ -488,7 +483,7 @@
 	bootmap_pages = bootmem_bootmap_pages(total_pages);
 
 	start = (unsigned long)__a2p(lmb_alloc(bootmap_pages<<PAGE_SHIFT, PAGE_SIZE));
-	if( start == 0 ) {
+	if (start == 0) {
 		udbg_printf("do_init_bootmem: failed to allocate a bitmap.\n");
 		udbg_printf("\tbootmap_pages = 0x%lx.\n", bootmap_pages);
 		PPCDBG_ENTER_DEBUGGER(); 
@@ -502,7 +497,7 @@
 	PPCDBG(PPCDBG_MMINIT, "\tboot_mapsize        = 0x%lx\n", boot_mapsize);
 
 	/* add all physical memory to the bootmem map */
-	for (i=0; i < lmb.memory.cnt ;i++) {
+	for (i=0; i < lmb.memory.cnt; i++) {
 		unsigned long physbase, size;
 		unsigned long type = lmb.memory.region[i].type;
 
@@ -514,7 +509,7 @@
 		free_bootmem(physbase, size);
 	}
 	/* reserve the sections we're already using */
-	for (i=0; i < lmb.reserved.cnt ;i++) {
+	for (i=0; i < lmb.reserved.cnt; i++) {
 		unsigned long physbase = lmb.reserved.region[i].physbase;
 		unsigned long size = lmb.reserved.region[i].size;
 #if 0 /* PPPBBB */
@@ -538,19 +533,12 @@
 	/*
 	 * All pages are DMA-able so we put them all in the DMA zone.
 	 */
-	zones_size[0] = lmb_end_of_DRAM() >> PAGE_SHIFT;
+	zones_size[ZONE_DMA] = lmb_end_of_DRAM() >> PAGE_SHIFT;
 	for (i = 1; i < MAX_NR_ZONES; i++)
 		zones_size[i] = 0;
 	free_area_init(zones_size);
 }
 
-extern unsigned long prof_shift;
-extern unsigned long prof_len;
-extern unsigned int * prof_buffer;
-extern unsigned long dprof_shift;
-extern unsigned long dprof_len;
-extern unsigned int * dprof_buffer;
-
 void initialize_paca_hardware_interrupt_stack(void);
 
 void __init mem_init(void)
@@ -609,10 +597,6 @@
 
 #ifdef CONFIG_PPC_ISERIES
 	create_virtual_bus_tce_table();
-	/* HACK HACK This allows the iSeries profiling to use /proc/profile */
-	prof_shift = dprof_shift;
-	prof_len = dprof_len;
-	prof_buffer = dprof_buffer;
 #endif
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/xmon/privinst.h linuxppc64_2_4/arch/ppc64/xmon/privinst.h
--- linux-2.4.19/arch/ppc64/xmon/privinst.h	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/xmon/privinst.h	Tue Jul 23 02:44:18 2002
@@ -6,7 +6,6 @@
  *      as published by the Free Software Foundation; either version
  *      2 of the License, or (at your option) any later version.
  */
-#include <linux/config.h>
 
 #define GETREG(reg)		\
     static inline unsigned long get_ ## reg (void)	\
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/xmon/start.c linuxppc64_2_4/arch/ppc64/xmon/start.c
--- linux-2.4.19/arch/ppc64/xmon/start.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/xmon/start.c	Mon Aug 19 09:15:52 2002
@@ -9,6 +9,7 @@
 #include <linux/string.h>
 #include <linux/kernel.h>
 #include <linux/sysrq.h>
+#include <linux/fs.h>
 #include <asm/machdep.h>
 #include <asm/io.h>
 #include <asm/page.h>
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/arch/ppc64/xmon/xmon.c linuxppc64_2_4/arch/ppc64/xmon/xmon.c
--- linux-2.4.19/arch/ppc64/xmon/xmon.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/arch/ppc64/xmon/xmon.c	Tue Aug 20 14:46:58 2002
@@ -123,12 +123,10 @@
 static void mem_check(void);
 static void mem_find_real(void);
 static void mem_find_vsid(void);
-static void mem_check_full_group(void);
 static void mem_check_pagetable_vsids (void);
 
 static void mem_map_check_slab(void);
 static void mem_map_lock_pages(void);
-static void mem_map_check_hash(void);
 static void mem_check_dup_rpn (void);
 static void debug_trace(void);
 
@@ -191,13 +189,6 @@
 	asm volatile("sync; isync");
 }
 
-extern inline void __delay(unsigned int loops)
-{
-	if (loops != 0)
-		__asm__ __volatile__("mtctr %0; 1: bdnz 1b" : :
-				     "r" (loops) : "ctr");
-}
-
 /* (Ref: 64-bit PowerPC ELF ABI Spplement; Ian Lance Taylor, Zembu Labs).
  A PPC stack frame looks like this:
 
@@ -544,7 +535,7 @@
 		}
 	}
 
-	if (!__is_processor(PV_POWER4)) {
+	if (!__is_processor(PV_POWER4) && !__is_processor(PV_POWER4p)) {
 		if (dabr.enabled)
 			set_dabr(dabr.address);
 		if (iabr.enabled)
@@ -561,7 +552,7 @@
 
 	if (naca->platform != PLATFORM_PSERIES)
 		return;
-	if (!__is_processor(PV_POWER4)) {
+	if (!__is_processor(PV_POWER4) && !__is_processor(PV_POWER4p)) {
 		set_dabr(0);
 		set_iabr(0);
 	}
@@ -634,15 +625,9 @@
 			case 'c':
 				mem_check();
 				break;
-			case 'g':
-				mem_check_full_group();
-				break;
 			case 'j':
 				mem_map_check_slab();
 				break;
-			case 'h':
-				mem_map_check_hash();
-				break;
 			case 'f':
 				mem_find_real();
 				break;
@@ -864,7 +849,7 @@
 	cmd = inchar();
 	switch (cmd) {
 	case 'd':	/* bd - hardware data breakpoint */
-		if (__is_processor(PV_POWER4)) {
+		if (__is_processor(PV_POWER4) || __is_processor(PV_POWER4p)) {
 			printf("Not implemented on POWER4\n");
 			break;
 		}
@@ -884,8 +869,7 @@
 			dabr.address = (dabr.address & ~7) | mode;
 		break;
 	case 'i':	/* bi - hardware instr breakpoint */
-		if (__is_processor(PV_POWER4)) {
-			printf("Not implemented on POWER4\n");
+		if (__is_processor(PV_POWER4) || __is_processor(PV_POWER4p)) {
 			break;
 		}
 		iabr.address = 0;
@@ -2490,34 +2474,6 @@
 	printf(" count of locked pages = %d \n", lock_count); 
 }
 
-
-
-void mem_map_check_hash()
-{
-	int i = max_mapnr;
-	
-	while (i-- > 0)  {
-		/* skip the reserved */
-		if (!PageReserved(mem_map+i)) {
-			if (((mem_map+i)->next_hash) != NULL) {
-				if ( REGION_ID((mem_map+i)->next_hash) != KERNEL_REGION_ID ) {
-					printf(" mem_map check hash - non c0 entry - "
-					       "address/value = %p %lx\n", mem_map+i,(mem_map+i)->next_hash);
-				} 
-				if ((unsigned long)((mem_map+i)->next_hash) ==  KERNELBASE){
-					printf(" mem_map check hash - 0x%lx  entry = %p \n",
-					       KERNELBASE, mem_map+i);
-				} 
-			}
-		} else {
-			if (page_count(mem_map+i) < 0) {
-				printf(" reserved page with negative count- entry = %lx \n", mem_map+i);
-			}
-		}
-	}
-	printf(" mem_map check hash completed \n");
-}
-
 void mem_check_dup_rpn ()
 {
 	unsigned long htab_size_bytes;
@@ -2725,70 +2681,6 @@
 
 	printf("\nDone -------------------\n");
 
-}
-
-
-void mem_check_full_group()
-{
-	unsigned long htab_size_bytes;
-	unsigned count;
-	unsigned count_array[] = {0,0,0,0,0,0,0,0,0}; 
-	unsigned i;
-	unsigned long htab_end;
-	HPTE *hpte1, *hpte2, *hpte3;
-	u64  rpn = 0;
-
-	htab_size_bytes = htab_data.htab_num_ptegs * 128; // 128B / PTEG
-	htab_end = (unsigned long)htab_data.htab + htab_size_bytes;
-
-	printf("\nHardware Page Find full groups \n-------------------\n");
-	printf("htab base      : %.16lx\n", htab_data.htab);
-	printf("htab size      : %.16lx\n", htab_size_bytes);
-
-	for (hpte1 = htab_data.htab; (unsigned long)hpte1 < htab_end; hpte1= hpte1 + 8)
-	{
-		count = 0;
-		hpte2 = hpte1;
-		for (i=0; i<8; ++i)
-		{
-			if ( hpte2->dw0.dw0.v != 0 )
-			{
-				count++;
-			}
-			hpte2++;
-		}
-		if (count == 8 )
-		{
-			printf("  full group starting with entry %lx \n", hpte1);
-			hpte3 = hpte1;
-			for (i=0; i<8; ++i)
-			{
-				if ( hpte3->dw0.dw0.v != 0 )
-				{
-					printf(" entry number %d   \n",i);
-					printf("          vsid: %.13lx   api: %.2lx  hash: %.1lx\n", 
-					       (hpte3->dw0.dw0.avpn)>>5, 
-					       (hpte3->dw0.dw0.avpn) & 0x1f,
-					       (hpte3->dw0.dw0.h));
-					printf("          rpn: %.13lx \n", (hpte3->dw1.dw1.rpn)); 
-					// Dump out the memmap array entry address, corresponding virtual address, and reference count.
-					rpn = hpte3->dw1.dw1.rpn;
-					printf("          mem_map+rpn=%p, virtual@=%p, count=%lx \n", mem_map+rpn, (mem_map+rpn)->virtual, (mem_map+rpn)->count);
-				}
-				hpte3++;
-			}
-			if (xmon_interrupted())
-				return;
-		}
-
-		count_array[count]++;
-	}
-	for (i=1; i<9; ++i)
-	{
-		printf("  group count for size  %i = %lx \n", i, count_array[i]);
-	}
-
-	printf("\nDone -------------------\n");
 }
 
 static void debug_trace(void) {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/Makefile linuxppc64_2_4/drivers/Makefile
--- linux-2.4.19/drivers/Makefile	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/Makefile	Mon Aug 19 09:15:52 2002
@@ -7,8 +7,8 @@
 
 
 mod-subdirs :=	dio mtd sbus video macintosh usb input telephony sgi ide \
-		message/i2o message/fusion scsi md ieee1394 pnp isdn atm \
-		fc4 net/hamradio i2c acpi bluetooth
+		message/i2o message/fusion scsi md ieee1394 pnp isdn dump atm \
+		fc4 net/hamradio i2c acpi bluetooth iseries
 
 subdir-y :=	parport char block net sound misc media cdrom hotplug
 subdir-m :=	$(subdir-y)
@@ -24,7 +24,8 @@
 subdir-$(CONFIG_TC)		+= tc
 subdir-$(CONFIG_VT)		+= video
 subdir-$(CONFIG_MAC)		+= macintosh
-subdir-$(CONFIG_PPC)		+= macintosh
+subdir-$(CONFIG_PPC32)         	+= macintosh
+subdir-$(CONFIG_PPC_ISERIES)	+= iseries
 subdir-$(CONFIG_USB)		+= usb
 subdir-$(CONFIG_INPUT)		+= input
 subdir-$(CONFIG_PHONE)		+= telephony
@@ -38,6 +39,7 @@
 subdir-$(CONFIG_PNP)		+= pnp
 subdir-$(CONFIG_ISDN_BOOL)	+= isdn
 subdir-$(CONFIG_ATM)		+= atm
+subdir-$(CONFIG_DUMP)		+= dump
 subdir-$(CONFIG_FC4)		+= fc4
 
 # CONFIG_HAMRADIO can be set without CONFIG_NETDEVICE being set  -- ch
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/block/genhd.c linuxppc64_2_4/drivers/block/genhd.c
--- linux-2.4.19/drivers/block/genhd.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/block/genhd.c	Fri Jul 26 13:41:00 2002
@@ -213,6 +213,10 @@
 #ifdef CONFIG_VT
 	console_map_init();
 #endif
+#ifdef CONFIG_VIODASD
+       viodasd_init();
+#endif
+
 	return 0;
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/block/ll_rw_blk.c linuxppc64_2_4/drivers/block/ll_rw_blk.c
--- linux-2.4.19/drivers/block/ll_rw_blk.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/block/ll_rw_blk.c	Mon Aug 19 08:35:12 2002
@@ -1345,6 +1345,9 @@
 #ifdef CONFIG_BLK_DEV_XD
 	xd_init();
 #endif
+#ifdef CONFIG_VIOCD
+	viocd_init();
+#endif
 #ifdef CONFIG_BLK_DEV_MFM
 	mfm_init();
 #endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/cdrom/Makefile linuxppc64_2_4/drivers/cdrom/Makefile
--- linux-2.4.19/drivers/cdrom/Makefile	Fri Dec 29 16:07:21 2000
+++ linuxppc64_2_4/drivers/cdrom/Makefile	Thu Oct 11 11:10:49 2001
@@ -27,6 +27,7 @@
 obj-$(CONFIG_BLK_DEV_IDECD)	+=              cdrom.o
 obj-$(CONFIG_BLK_DEV_SR)	+=              cdrom.o
 obj-$(CONFIG_PARIDE_PCD)	+=		cdrom.o
+obj-$(CONFIG_VIOCD)		+=		cdrom.o
 
 obj-$(CONFIG_AZTCD)		+= aztcd.o
 obj-$(CONFIG_CDU31A)		+= cdu31a.o     cdrom.o
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/Config.in linuxppc64_2_4/drivers/char/Config.in
--- linux-2.4.19/drivers/char/Config.in	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/char/Config.in	Fri Aug  9 12:47:04 2002
@@ -45,6 +45,10 @@
    if [ "$CONFIG_EXPERIMENTAL" = "y" ]; then
       dep_tristate '  Multi-Tech multiport card support (EXPERIMENTAL)' CONFIG_ISI m
    fi
+   tristate '  IBM Multiport Serial Adapter' CONFIG_ICOM
+   if [ "$CONFIG_ICOM" = "y" ]; then
+      string 'Modem Country Code (Internal Modem Users only)' CONFIG_ICOM_MODEM_CC ""
+   fi
    tristate '  Microgate SyncLink card support' CONFIG_SYNCLINK
    tristate '  HDLC line discipline support' CONFIG_N_HDLC
    tristate '  SDL RISCom/8 card support' CONFIG_RISCOM8
@@ -134,6 +138,7 @@
    fi
    dep_tristate 'Support for user-space parallel port device drivers' CONFIG_PPDEV $CONFIG_PARPORT
 fi
+dep_bool 'pSeries Hypervisor Virtual Console support' CONFIG_HVC_CONSOLE $CONFIG_PPC64
 
 source drivers/i2c/Config.in
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/Makefile linuxppc64_2_4/drivers/char/Makefile
--- linux-2.4.19/drivers/char/Makefile	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/char/Makefile	Tue Apr 23 09:37:26 2002
@@ -164,6 +164,7 @@
 obj-$(CONFIG_COMPUTONE) += ip2.o ip2main.o
 obj-$(CONFIG_RISCOM8) += riscom8.o
 obj-$(CONFIG_ISI) += isicom.o
+obj-$(CONFIG_ICOM) += icom.o
 obj-$(CONFIG_ESPSERIAL) += esp.o
 obj-$(CONFIG_SYNCLINK) += synclink.o
 obj-$(CONFIG_N_HDLC) += n_hdlc.o
@@ -177,6 +178,7 @@
 obj-$(CONFIG_MVME147_SCC) += generic_serial.o vme_scc.o
 obj-$(CONFIG_MVME162_SCC) += generic_serial.o vme_scc.o
 obj-$(CONFIG_BVME6000_SCC) += generic_serial.o vme_scc.o
+obj-$(CONFIG_HVC_CONSOLE) += hvc_console.o
 obj-$(CONFIG_SERIAL_TX3912) += generic_serial.o serial_tx3912.o
 obj-$(CONFIG_TXX927_SERIAL) += serial_txx927.o
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/hvc_console.c linuxppc64_2_4/drivers/char/hvc_console.c
--- linux-2.4.19/drivers/char/hvc_console.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/char/hvc_console.c	Mon Aug 19 09:15:52 2002
@@ -0,0 +1,364 @@
+/*
+ * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
+ * Copyright (C) 2001 Paul Mackerras <paulus@au.ibm.com>, IBM
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ * 
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/console.h>
+#include <linux/major.h>
+#include <linux/kernel.h>
+#include <linux/sysrq.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/sched.h>
+#include <linux/kbd_kern.h>
+#include <asm/uaccess.h>
+#include <linux/spinlock.h>
+
+extern int hvc_count(int *);
+extern int hvc_get_chars(int index, char *buf, int count);
+extern int hvc_put_chars(int index, const char *buf, int count);
+
+#define HVC_MAJOR	229
+#define HVC_MINOR	0
+
+#define MAX_NR_HVC_CONSOLES	4
+
+#define TIMEOUT		((HZ + 99) / 100)
+
+struct tty_driver hvc_driver;
+static int hvc_refcount;
+static struct tty_struct *hvc_table[MAX_NR_HVC_CONSOLES];
+static struct termios *hvc_termios[MAX_NR_HVC_CONSOLES];
+static struct termios *hvc_termios_locked[MAX_NR_HVC_CONSOLES];
+static int hvc_offset;
+#ifdef CONFIG_MAGIC_SYSRQ
+static int sysrq_pressed;
+#endif
+
+#define N_OUTBUF	16
+
+#define __ALIGNED__	__attribute__((__aligned__(8)))
+
+struct hvc_struct {
+	spinlock_t lock;
+	int index;
+	struct tty_struct *tty;
+	unsigned int count;
+	int do_wakeup;
+	char outbuf[N_OUTBUF] __ALIGNED__;
+	int n_outbuf;
+};
+
+struct hvc_struct hvc_struct[MAX_NR_HVC_CONSOLES];
+
+static int hvc_open(struct tty_struct *tty, struct file * filp)
+{
+	int line = MINOR(tty->device) - tty->driver.minor_start;
+	struct hvc_struct *hp;
+	unsigned long flags;
+
+	if (line < 0 || line >= MAX_NR_HVC_CONSOLES)
+		return -ENODEV;
+	hp = &hvc_struct[line];
+
+	tty->driver_data = hp;
+	spin_lock_irqsave(&hp->lock, flags);
+	hp->tty = tty;
+	hp->count++;
+	spin_unlock_irqrestore(&hp->lock, flags);
+
+	return 0;
+}
+
+static void hvc_close(struct tty_struct *tty, struct file * filp)
+{
+	struct hvc_struct *hp = tty->driver_data;
+	unsigned long flags;
+
+	if (tty_hung_up_p(filp))
+		return;
+	spin_lock_irqsave(&hp->lock, flags);
+	if (--hp->count == 0)
+		hp->tty = NULL;
+	else if (hp->count < 0)
+		printk(KERN_ERR "hvc_close %lu: oops, count is %d\n",
+		       hp - hvc_struct, hp->count);
+	spin_unlock_irqrestore(&hp->lock, flags);
+}
+
+static void hvc_hangup(struct tty_struct *tty)
+{
+	struct hvc_struct *hp = tty->driver_data;
+
+	hp->count = 0;
+	hp->tty = NULL;
+}
+
+/* called with hp->lock held */
+static void hvc_push(struct hvc_struct *hp)
+{
+	int n;
+
+	n = hvc_put_chars(hp->index + hvc_offset, hp->outbuf, hp->n_outbuf);
+	if (n <= 0) {
+		if (n == 0)
+			return;
+		/* throw away output on error; this happens when
+		   there is no session connected to the vterm. */
+		hp->n_outbuf = 0;
+	} else
+		hp->n_outbuf -= n;
+	if (hp->n_outbuf > 0)
+		memmove(hp->outbuf, hp->outbuf + n, hp->n_outbuf);
+	else
+		hp->do_wakeup = 1;
+}
+
+static int hvc_write(struct tty_struct *tty, int from_user,
+		     const unsigned char *buf, int count)
+{
+	struct hvc_struct *hp = tty->driver_data;
+	char *p;
+	int todo, written = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&hp->lock, flags);
+	while (count > 0 && (todo = N_OUTBUF - hp->n_outbuf) > 0) {
+		if (todo > count)
+			todo = count;
+		p = hp->outbuf + hp->n_outbuf;
+		if (from_user) {
+			todo -= copy_from_user(p, buf, todo);
+			if (todo == 0) {
+				if (written == 0)
+					written = -EFAULT;
+				break;
+			}
+		} else
+			memcpy(p, buf, todo);
+		count -= todo;
+		buf += todo;
+		hp->n_outbuf += todo;
+		written += todo;
+		hvc_push(hp);
+	}
+	spin_unlock_irqrestore(&hp->lock, flags);
+
+	return written;
+}
+
+static int hvc_write_room(struct tty_struct *tty)
+{
+	struct hvc_struct *hp = tty->driver_data;
+
+	return N_OUTBUF - hp->n_outbuf;
+}
+
+static int hvc_chars_in_buffer(struct tty_struct *tty)
+{
+	struct hvc_struct *hp = tty->driver_data;
+
+	return hp->n_outbuf;
+}
+
+static void hvc_poll(int index)
+{
+	struct hvc_struct *hp = &hvc_struct[index];
+	struct tty_struct *tty;
+	int i, n;
+	char buf[16] __ALIGNED__;
+	unsigned long flags;
+
+	spin_lock_irqsave(&hp->lock, flags);
+
+	if (hp->n_outbuf > 0)
+		hvc_push(hp);
+
+	tty = hp->tty;
+	if (tty) {
+		for (;;) {
+			if (TTY_FLIPBUF_SIZE - tty->flip.count < sizeof(buf))
+				break;
+			n = hvc_get_chars(index + hvc_offset, buf, sizeof(buf));
+			if (n <= 0)
+				break;
+			for (i = 0; i < n; ++i) {
+#ifdef CONFIG_MAGIC_SYSRQ		/* Handle the SysRq Hack */
+				if (buf[i] == '\x0f') {	/* ^O -- should support a sequence */
+					sysrq_pressed = 1;
+					continue;
+				} else if (sysrq_pressed) {
+					handle_sysrq(buf[i], NULL, NULL, tty);
+					sysrq_pressed = 0;
+					continue;
+				}
+#endif
+				tty_insert_flip_char(tty, buf[i], 0);
+			}
+		}
+		if (tty->flip.count)
+			tty_schedule_flip(tty);
+
+		if (hp->do_wakeup) {
+			hp->do_wakeup = 0;
+			if ((tty->flags & (1 << TTY_DO_WRITE_WAKEUP))
+			    && tty->ldisc.write_wakeup)
+				(tty->ldisc.write_wakeup)(tty);
+			wake_up_interruptible(&tty->write_wait);
+		}
+	}
+
+	spin_unlock_irqrestore(&hp->lock, flags);
+}
+
+int khvcd(void *unused)
+{
+	int i;
+
+	daemonize();
+	reparent_to_init();
+	strcpy(current->comm, "khvcd");
+	sigfillset(&current->blocked);
+
+	for (;;) {
+		for (i = 0; i < MAX_NR_HVC_CONSOLES; ++i)
+			hvc_poll(i);
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(TIMEOUT);
+	}
+}
+
+int __init hvc_init(void)
+{
+	int i;
+
+	memset(&hvc_driver, 0, sizeof(struct tty_driver));
+
+	hvc_driver.magic = TTY_DRIVER_MAGIC;
+	hvc_driver.driver_name = "hvc";
+	hvc_driver.name = "hvc/%d";
+	hvc_driver.major = HVC_MAJOR;
+	hvc_driver.minor_start = HVC_MINOR;
+	hvc_driver.num = hvc_count(&hvc_offset);
+	if (hvc_driver.num > MAX_NR_HVC_CONSOLES)
+		hvc_driver.num = MAX_NR_HVC_CONSOLES;
+	hvc_driver.type = TTY_DRIVER_TYPE_SYSTEM;
+	hvc_driver.init_termios = tty_std_termios;
+	hvc_driver.flags = TTY_DRIVER_REAL_RAW;
+	hvc_driver.refcount = &hvc_refcount;
+	hvc_driver.table = hvc_table;
+	hvc_driver.termios = hvc_termios;
+	hvc_driver.termios_locked = hvc_termios_locked;
+
+	hvc_driver.open = hvc_open;
+	hvc_driver.close = hvc_close;
+	hvc_driver.write = hvc_write;
+	hvc_driver.hangup = hvc_hangup;
+	hvc_driver.write_room = hvc_write_room;
+	hvc_driver.chars_in_buffer = hvc_chars_in_buffer;
+
+	for (i = 0; i < hvc_driver.num; i++) {
+		hvc_struct[i].lock = SPIN_LOCK_UNLOCKED;
+		hvc_struct[i].index = i;
+		tty_register_devfs(&hvc_driver, 0, hvc_driver.minor_start + i);
+	}
+
+	if (tty_register_driver(&hvc_driver))
+		panic("Couldn't register hvc console driver\n");
+
+	if (hvc_driver.num > 0)
+		kernel_thread(khvcd, NULL, CLONE_FS | CLONE_FILES | CLONE_SIGNAL);
+
+	return 0;
+}
+
+static void __exit hvc_exit(void)
+{
+}
+
+void hvc_console_print(struct console *co, const char *b, unsigned count)
+{
+	char c[16] __ALIGNED__;
+	unsigned i, n;
+	int r, donecr = 0;
+
+	i = n = 0;
+	while (count > 0 || i > 0) {
+		if (count > 0 && i < sizeof(c)) {
+			if (b[n] == '\n' && !donecr) {
+				c[i++] = '\r';
+				donecr = 1;
+			} else {
+				c[i++] = b[n++];
+				donecr = 0;
+				--count;
+			}
+		} else {
+			r = hvc_put_chars(co->index + hvc_offset, c, i);
+			if (r < 0) {
+				/* throw away chars on error */
+				i = 0;
+			} else if (r > 0) {
+				i -= r;
+				if (i > 0)
+					memmove(c, c+r, i);
+			}
+		}
+	}
+}
+
+static kdev_t hvc_console_device(struct console *c)
+{
+	return MKDEV(HVC_MAJOR, HVC_MINOR + c->index);
+}
+
+int hvc_wait_for_keypress(struct console *co)
+{
+	char c[16] __ALIGNED__;
+
+	while (hvc_get_chars(co->index, &c[0], 1) < 1)
+		;
+	return 0;
+}
+
+static int __init hvc_console_setup(struct console *co, char *options)
+{
+	if (co->index < 0 || co->index >= MAX_NR_HVC_CONSOLES
+	    || co->index >= hvc_count(&hvc_offset))
+		return -1;
+	return 0;
+}
+
+struct console hvc_con_driver = {
+	name:		"hvc",
+	write:		hvc_console_print,
+	device:		hvc_console_device,
+	setup:		hvc_console_setup,
+	flags:		CON_PRINTBUFFER,
+	index:		-1,
+};
+
+int __init hvc_console_init(void)
+{
+	register_console(&hvc_con_driver);
+	return 0;
+}
+
+module_init(hvc_init);
+module_exit(hvc_exit);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/icom.c linuxppc64_2_4/drivers/char/icom.c
--- linux-2.4.19/drivers/char/icom.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/char/icom.c	Fri Aug  9 10:40:16 2002
@@ -0,0 +1,5687 @@
+/*
+ * icom.c
+ *
+ * Copyright (C) 2001 Michael Anderson, IBM Corporation
+ *
+ * Serial device driver.
+ *
+ * Based on code from serial.c
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ *
+ * NOTE:  Only for users with internal modem cards (eg. iSeries 2771, 2772)
+ *        find the decimal number associated with your country below
+ *        and set country code operations using module parameter at install
+ *        time, eg.  for Argentina the command would be (be sure number is in "")
+ *                      insmod icom.o iCom_country_code="52"
+ *
+ * Module paramter values for
+ *   iCom_country_code:
+                          52     * AR Argentina *
+                          52     * AW Aruba *
+                           1     * AU Australia *
+                          52     * AT Austria *
+                          52     * BH Bahrain *
+                          52     * BE Belgium *
+                          52     * BR Brazil *
+                          52     * BN Brunei Darussalam *
+                          52     * CA Canada *
+                          52     * KY Cayman Islands *
+                          52     * CL Chile *
+                          52     * CN China *
+                          52     * CO Colombia *
+                          52     * CR Costa Rica *
+                          52     * HR Croatia *
+                          52     * CY Cyprus *
+                          37     * CZ Czech Republic *
+                          52     * DK Denmark *
+                          52     * EC Ecuador *
+                          52     * EG Egypt *
+                          52     * FI Finland *
+                          52     * FR France *
+                          52     * DE Germany *
+                          52     * GR Greece *
+                          52     * GT Guatemala *
+                          48     * HK China (Hong Kong S.A.R.) *
+                          48     * HU Hungary *
+                          52     * IS Iceland *
+                          48     * IN India *
+                          48     * ID Indonesia *
+                          52     * IE Ireland *
+                          48     * IL Israel *
+                          52     * IT Italy *
+                          52     * JM Jamaica *
+                          16     * JP Japan *
+                          52     * KR Korea, Republic of *
+                          52     * LU Luxembourg *
+                          52     * MO China (Macau S.A.R.) *
+                          48     * MY Malaysia *
+                          52     * MX Mexico *
+                          52     * MA Morocco *
+                          52     * NL Netherlands *
+                          52     * AN Netherlands Antilles *
+                           9     * NZ New Zealand *
+                          52     * NO Norway *
+                          52     * PK Pakistan *
+                          52     * PA Panama *
+                          52     * PE Peru *
+                          48     * PH Philippines *
+                          48     * PL Poland *
+                          52     * PT Portugal *
+                          52     * QA Qatar *
+                          52     * RO Romania *
+                          52     * RU Russia *
+                          52     * SA Saudi Arabia *
+                          48     * SG Singapore *
+                          52     * SK Slovakia *
+                          48     * SI Slovenia *
+                          53     * ZA South Africa *
+                          52     * ES Spain *
+                          52     * LK Sri Lanka *
+                          52     * SE Sweden *
+                          52     * CH Switzerland *
+                          52     * TW Taiwan *
+                          52     * TH Thailand *
+                          52     * TT Trinidad and Tobago *
+                          52     * TR Turkey *
+                          52     * UA Ukraine *
+                          52     * AE United Arab Emirates *
+                          52     * GB United Kingdom *
+                          52     * US United States of America*
+                          52     * UY Uruguay *
+                          52     * VE Venezuela *
+                          48     * VN Vietnam *
+*/
+#define SERIAL_DO_RESTART
+#ifdef	MODVERSIONS
+#include <linux/modversions.h>
+#endif
+#include <linux/module.h>
+
+MODULE_AUTHOR ("Michael Anderson <mjanders@us.ibm.com>");
+MODULE_DESCRIPTION ("IBM iSeries Serial IOA driver");
+MODULE_SUPPORTED_DEVICE("IBM iSeries 2745, 2771, 2772, 2742, 2793 and 2805 Communications adapters");
+MODULE_LICENSE("GPL");
+
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <linux/interrupt.h>
+#include <linux/tty.h>
+#include <linux/termios.h>
+#include <linux/fs.h>
+#include <linux/tty_flip.h>
+#include <linux/serial.h>
+#include <linux/serial_reg.h>
+#include <linux/major.h>
+#include <linux/string.h>
+#include <linux/fcntl.h>
+#include <linux/ptrace.h>
+#include <linux/ioport.h>
+#include <linux/mm.h>
+#include <linux/malloc.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/pci.h>
+#include <linux/vmalloc.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/spinlock.h>
+
+#include <asm/system.h>
+#include <asm/segment.h>
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+#include <asm/bitops.h>
+#include <asm/serial.h>
+
+#ifdef CONFIG_PPC_ISERIES
+
+#ifndef CONFIG_PPC64
+#include <asm/iSeries/iSeries_VpdInfo.h>
+#endif
+
+#include <asm/iSeries/iSeries_pci.h>
+#include <asm/iSeries/iSeries_dma.h>
+#endif
+
+/* adapter code loads */
+#include "icom.h"
+
+#ifdef MODULE
+#define CONFIG_ICOM_MODEM_CC ""
+#endif
+
+static char *iCom_country_code = CONFIG_ICOM_MODEM_CC;
+MODULE_PARM(iCom_country_code, "s");
+MODULE_PARM_DESC(iCom_country_code, "Modem country code configuration");
+
+#define ICOM_TRACE /* enable port trace capabalities */
+
+#define ICOM_DRIVER_NAME "icom"
+#define ICOM_VERSION_STR "1.1.0"
+#define VENDOR_ID        0x1014
+#define DEVICE_ID        0x0031
+#define DEVICE_ID2       0x0219
+#define MAX_ADAPTERS     40
+#define NR_PORTS	 (active_adapters * 4)
+#define MAX_PORTS        (MAX_ADAPTERS * 4)
+#define ASYNC_CLOSING    0x08000000 /* Serial port is closing */
+#define ASYNC_HUP_NOTIFY 0x0001 /* Notify getty on hangups and closes 
+				   on the callout port */
+
+#ifdef MODULE
+static const struct pci_device_id iCom_pci_table[] __initdata =
+{
+	{
+		vendor: VENDOR_ID,
+		device: DEVICE_ID,
+		subvendor: 0xFFFF,
+		subdevice: 0xFFFF,
+	},
+	{
+		vendor: VENDOR_ID,
+		device: DEVICE_ID2,
+		subvendor: VENDOR_ID,
+		subdevice: 0x021a,
+	},
+	{
+		vendor: VENDOR_ID,
+		device: DEVICE_ID2,
+		subvendor: VENDOR_ID,
+		subdevice: 0x0251,
+	},
+	{
+		vendor: VENDOR_ID,
+		device: DEVICE_ID2,
+		subvendor: VENDOR_ID,
+		subdevice: 0x0252,
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(pci, iCom_pci_table);
+#endif
+
+/*
+ * adapter defines and structures
+ */
+#define ICOM_CONTROL_START_A         0x00000008
+#define ICOM_CONTROL_STOP_A          0x00000004
+#define ICOM_CONTROL_START_B         0x00000002
+#define ICOM_CONTROL_STOP_B          0x00000001
+#define ICOM_CONTROL_START_C         0x00000008
+#define ICOM_CONTROL_STOP_C          0x00000004
+#define ICOM_CONTROL_START_D         0x00000002
+#define ICOM_CONTROL_STOP_D          0x00000001
+#define ICOM_IRAM_OFFSET             0x1000
+#define ICOM_DCE_IRAM_OFFSET         0x0A00
+#define ICOM_CABLE_ID_VALID          0x01
+#define ICOM_CABLE_ID_MASK           0xF0
+#define ICOM_DISABLE                 0x80
+#define CMD_XMIT_RCV_ENABLE          0xC0
+#define CMD_XMIT_ENABLE              0x40
+#define CMD_RCV_DISABLE              0x00
+#define CMD_RCV_ENABLE               0x80
+#define CMD_RESTART                  0x01
+#define CMD_HOLD_XMIT                0x02
+#define CMD_SND_BREAK                0x04
+#define RS232_CABLE                  0x06
+#define V24_CABLE                    0x0E
+#define V35_CABLE                    0x0C
+#define V36_CABLE                    0x02
+#define NO_CABLE                     0x00
+#define START_DOWNLOAD               0x80
+#define ICOM_INT_MASK_PRC_A          0x00003FFF
+#define ICOM_INT_MASK_PRC_B          0x3FFF0000
+#define ICOM_INT_MASK_PRC_C          0x00003FFF
+#define ICOM_INT_MASK_PRC_D          0x3FFF0000
+#define INT_RCV_COMPLETED            0x1000
+#define INT_XMIT_COMPLETED           0x2000
+#define INT_IDLE_DETECT              0x0800
+#define INT_RCV_DISABLED             0x0400
+#define INT_XMIT_DISABLED            0x0200
+#define INT_RCV_XMIT_SHUTDOWN        0x0100
+#define INT_FATAL_ERROR              0x0080
+#define INT_CABLE_PULL               0x0020
+#define INT_SIGNAL_CHANGE            0x0010
+#define HDLC_PPP_PURE_ASYNC          0x02
+#define HDLC_FF_FILL                 0x00
+#define HDLC_HDW_FLOW                0x01
+#define START_XMIT                   0x80
+#define ICOM_ACFG_DRIVE1             0x20
+#define ICOM_ACFG_NO_PARITY          0x00
+#define ICOM_ACFG_PARITY_ENAB        0x02
+#define ICOM_ACFG_PARITY_ODD         0x01
+#define ICOM_ACFG_8BPC               0x00
+#define ICOM_ACFG_7BPC               0x04
+#define ICOM_ACFG_6BPC               0x08
+#define ICOM_ACFG_5BPC               0x0C
+#define ICOM_ACFG_1STOP_BIT          0x00
+#define ICOM_ACFG_2STOP_BIT          0x10
+#define ICOM_DTR                     0x80
+#define ICOM_RTS                     0x40
+#define ICOM_RI                      0x08
+#define ICOM_DSR                     0x80
+#define ICOM_DCD                     0x20
+#define ICOM_CTS                     0x40
+
+#define BAUD_TABLE_LIMIT             20
+static int icom_acfg_baud[] = {
+  300,
+  600,
+  900,
+  1200,
+  1800,
+  2400,
+  3600,
+  4800,
+  7200,
+  9600,
+  14400,
+  19200,
+  28800,
+  38400,
+  57600,
+  76800,
+  115200,
+  153600,
+  230400,
+  307200,
+  460800};
+
+static int                active_adapters;
+static struct tty_driver  serial_driver;
+static int                serial_refcount = 0;
+static struct tty_struct *serial_table[MAX_PORTS];
+static struct termios    *serial_termios[MAX_PORTS];
+static struct termios    *serial_termios_locked[MAX_PORTS];
+
+struct iCom_regs {
+  u32                  control;        /* Adapter Control Register     */
+  u32                  interrupt;      /* Adapter Interrupt Register   */
+  u32                  int_mask;       /* Adapter Interrupt Mask Reg   */
+  u32                  int_pri;        /* Adapter Interrupt Priority r */
+  u32                  int_reg_b;      /* Adapter non-masked Interrupt */
+  u32                  resvd01;
+  u32                  resvd02;
+  u32                  resvd03;
+  u32                  control_2;      /* Adapter Control Register 2   */
+  u32                  interrupt_2;    /* Adapter Interrupt Register 2 */
+  u32                  int_mask_2;     /* Adapter Interrupt Mask 2     */
+  u32                  int_pri_2;      /* Adapter Interrupt Prior 2    */
+  u32                  int_reg_2b;     /* Adapter non-masked 2         */
+};
+
+struct func_dram {
+  u32                 reserved[108];          /* 0-1B0   reserved by personality code */
+  u32                 RcvStatusAddr;          /* 1B0-1B3 Status Address for Next rcv */
+  u8                  RcvStnAddr;             /* 1B4     Receive Station Addr */
+  u8                  IdleState;              /* 1B5     Idle State */
+  u8                  IdleMonitor;            /* 1B6     Idle Monitor */
+  u8                  FlagFillIdleTimer;      /* 1B7     Flag Fill Idle Timer */
+  u32                 XmitStatusAddr;         /* 1B8-1BB Transmit Status Address */
+  u8                  StartXmitCmd;           /* 1BC     Start Xmit Command */
+  u8                  HDLCConfigReg;          /* 1BD     Reserved */
+  u8                  CauseCode;              /* 1BE     Cause code for fatal error */
+  u8                  xchar;                  /* 1BF     High priority send */
+  u32                 reserved3;              /* 1C0-1C3 Reserved */
+  u8                  PrevCmdReg;             /* 1C4     Reserved */
+  u8                  CmdReg;                 /* 1C5     Command Register */
+  u8                  async_config2;          /* 1C6     Async Config Byte 2*/
+  u8                  async_config3;          /* 1C7     Async Config Byte 3*/
+  u8                  dce_resvd[20];          /* 1C8-1DB DCE Rsvd           */
+  u8                  dce_resvd21;            /* 1DC     DCE Rsvd (21st byte*/
+  u8                  misc_flags;             /* 1DD     misc flags         */
+#define V2_HARDWARE     0x40
+#define ICOM_HDW_ACTIVE 0x01
+  u8                  call_length;            /* 1DE     Phone #/CFI buff ln*/
+  u8                  call_length2;           /* 1DF     Upper byte (unused)*/
+  u32                 call_addr;              /* 1E0-1E3 Phn #/CFI buff addr*/
+  u16                 timer_value;            /* 1E4-1E5 general timer value*/
+  u8                  timer_command;          /* 1E6     general timer cmd  */
+  u8                  dce_command;            /* 1E7     dce command reg    */
+  u8                  dce_cmd_status;         /* 1E8     dce command stat   */
+  u8                  x21_r1_ioff;            /* 1E9     dce ready counter  */
+  u8                  x21_r0_ioff;            /* 1EA     dce not ready ctr  */
+  u8                  x21_ralt_ioff;          /* 1EB     dce CNR counter    */
+  u8                  x21_r1_ion;             /* 1EC     dce ready I on ctr */
+  u8                  rsvd_ier;               /* 1ED     Rsvd for IER (if ne*/
+  u8                  ier;                    /* 1EE     Interrupt Enable   */
+  u8                  isr;                    /* 1EF     Input Signal Reg   */
+  u8                  osr;                    /* 1F0     Output Signal Reg  */
+  u8                  reset;                  /* 1F1     Reset/Reload Reg   */
+  u8                  disable;                /* 1F2     Disable Reg        */
+  u8                  sync;                   /* 1F3     Sync Reg           */
+  u8                  error_stat;             /* 1F4     Error Status       */
+  u8                  cable_id;               /* 1F5     Cable ID           */
+  u8                  cs_length;              /* 1F6     CS Load Length     */
+  u8                  mac_length;             /* 1F7     Mac Load Length    */
+  u32                 cs_load_addr;           /* 1F8-1FB Call Load PCI Addr */
+  u32                 mac_load_addr;          /* 1FC-1FF Mac Load PCI Addr  */
+};
+
+#define NUM_XBUFFS 1
+#define NUM_RBUFFS 2
+#define RCV_BUFF_SZ 0x0200
+#define XMIT_BUFF_SZ 0x1000
+struct statusArea
+{
+  /**********************************************/
+  /* Transmit Status Area                       */
+  /**********************************************/
+  struct {
+    u32                    leNext;         /* Next entry in Little Endian on Adapter */
+    u32                    leNextASD;
+    u32                    leBuffer;       /* Buffer for entry in LE for Adapter */
+    u16                    leLengthASD;
+    u16                    leOffsetASD;
+    u16                    leLength;       /* Length of data in segment */
+    u16                    flags;
+#define SA_FLAGS_DONE           0x0080          /* Done with Segment */
+#define SA_FLAGS_CONTINUED      0x8000          /* More Segments */
+#define SA_FLAGS_IDLE           0x4000          /* Mark IDLE after frm */
+#define SA_FLAGS_READY_TO_XMIT  0x0800
+#define SA_FLAGS_STAT_MASK      0x007F
+  } xmit[NUM_XBUFFS];
+    
+  /**********************************************/
+  /* Receive Status Area                        */
+  /**********************************************/
+  struct {
+    u32                    leNext;         /* Next entry in Little Endian on Adapter */
+    u32                    leNextASD;
+    u32                    leBuffer;       /* Buffer for entry in LE for Adapter */
+    u16                    WorkingLength;  /* size of segment */
+    u16                    reserv01;
+    u16                    leLength;       /* Length of data in segment */
+    u16                    flags;
+#define SA_FL_RCV_DONE           0x0010          /* Data ready */
+#define SA_FLAGS_OVERRUN         0x0040
+#define SA_FLAGS_PARITY_ERROR    0x0080 
+#define SA_FLAGS_FRAME_ERROR     0x0001
+#define SA_FLAGS_FRAME_TRUNC     0x0002
+#define SA_FLAGS_BREAK_DET       0x0004    /* set conditionally by device driver, not hardware */
+#define SA_FLAGS_RCV_MASK        0xFFE6
+  } rcv[NUM_RBUFFS];
+};
+
+struct iCom_port {
+  u8			imbed_modem;
+#define   ICOM_UNKNOWN       1
+#define   ICOM_RVX           2
+#define   ICOM_IMBED_MODEM   3
+  unsigned char         cable_id;
+  int                   open_active_count;
+  struct tty_struct 	*tty;
+  unsigned long	int     event;
+  struct tq_struct	tqueue;
+  int                   flags;
+  int                   xmit_fifo_size;
+  int                   baud_base;
+  wait_queue_head_t	close_wait;
+  wait_queue_head_t	open_wait;
+  wait_queue_head_t	delta_msr_wait;
+  int                   blocked_open;
+  unsigned short        close_delay;
+  unsigned short        closing_wait;
+  unsigned long int     timeout;
+  long			session; /* Session of opening process */
+  long			pgrp; /* pgrp of opening process */
+  unsigned char         read_status_mask;
+  unsigned char         ignore_status_mask;
+  struct async_icount	icount;	
+  struct termios	normal_termios;
+  struct termios	callout_termios;
+  unsigned long int     int_reg;
+  struct iCom_regs      *global_reg;
+  struct func_dram      *dram;
+  int                   adapter;
+  int                   port;
+  int                   minor_number;
+  struct statusArea     *statStg;
+  dma_addr_t            statStg_pci;
+  u32                   *xmitRestart;
+  dma_addr_t            xmitRestart_pci;
+  unsigned char         *xmit_buf;
+  dma_addr_t            xmit_buf_pci;
+  unsigned char         *recv_buf;
+  dma_addr_t            recv_buf_pci;
+  int                   next_rcv;
+  int                   put_length;
+  int                   passed_diags;
+  int                   status;
+#define ICOM_PORT_ACTIVE  1 /* Port exists. */
+#define ICOM_PORT_OFF     0 /* Port does not exist. */
+  int                   load_in_progress;
+  u32                   tpr;
+#define NUM_ERROR_ENTRIES 16
+  u32                   error_data[NUM_ERROR_ENTRIES];
+  u32                   thread_status;
+#define STATUS_INIT 0x99999999
+#define STATUS_PASS 0 
+  unsigned long         *trace_blk;
+};
+
+static struct iCom_adapter {
+  unsigned long int  base_addr;
+  unsigned char      slot;
+  unsigned char      irq_number;
+  struct pci_dev     *pci_dev;
+  struct iCom_port   port_info[4];
+  int                version;
+#define ADAPTER_V1   0x0001
+#define ADAPTER_V2   0x0002
+  u32		     subsystem_id;
+#define FOUR_PORT_MODEL				0x02521014
+#define V2_TWO_PORTS_RVX			0x021A1014
+#define V2_ONE_PORT_RVX_ONE_PORT_IMBED_MDM	0x02511014
+  int                numb_ports;
+  unsigned int       saved_bar;
+  unsigned int       saved_command_reg;
+  u32	             tpr;
+  u32                error_data[NUM_ERROR_ENTRIES];
+  u32                diag_int1;
+  u32                diag_int2;
+  u32                diag_int_pri1;
+  u32                diag_int_pri2;
+  u32                diag_int_reset1;
+  u32                diag_int_reset2;
+  u32                resources;
+#define HAVE_MALLOC_1	        0x00000001
+#define HAVE_INT_HANDLE		0x00000010
+  u32                *malloc_addr_1;
+} *iCom_adapter_info;
+
+struct mthread
+{
+    void       *thread;
+    u32		task;
+    u8		port;
+    int         status;
+#define STATUS_INIT 0x99999999
+#define STATUS_PASS 0 
+    u32         tpr;
+    u32         error_data[NUM_ERROR_ENTRIES];
+};
+
+#define	 IOA_FAILURE	0xFF
+
+static spinlock_t iComlock;
+/*
+   Utility functions
+*/
+static void diag_return_resources(struct iCom_adapter *iCom_adapter_ptr);
+static void return_port_memory(struct iCom_port *iCom_port_info);
+static void iCom_wait_until_sent(struct tty_struct *tty, int timeout);
+static void do_softint(void *);
+static void iCom_start(struct tty_struct * tty);
+static void iCom_flush_buffer(struct tty_struct * tty);
+static void iCom_set_code(struct iCom_port *iCom_port_info);
+static void mdm_send(struct iCom_port *iCom_port_info, char *mdm_cmnd,
+                     int cmnd_length);
+static int mdm_rcv(struct iCom_port *iCom_port_info, char *exp_str);
+#ifdef ICOM_TRACE
+static void TRACE(struct iCom_port *,u32 , u32);
+#else
+#define TRACE(x,y,z) /* nub out calls to TRACE function */
+#endif
+
+#ifdef CONFIG_PPC64
+extern int register_ioctl32_conversion(unsigned int cmd,
+				       int (*handler)(unsigned int, unsigned int, unsigned long, struct file *));
+extern int unregister_ioctl32_conversion(unsigned int cmd);
+#else
+static inline int register_ioctl32_conversion(unsigned int cmd,
+					      int (*handler)(unsigned int,
+							     unsigned int, unsigned long, struct file *))
+{
+	return 0;
+}
+static inline int unregister_ioctl32_conversion(unsigned int cmd)
+{
+	return 0;
+}
+#endif
+
+static u8 iCom_readb(void *address)
+{
+	/* Issue a 'write memory barrier' prior to the mmio to ensure ordering */
+	/* This translates to an eieio instruction on ppc */
+	wmb();
+	return readb(address);
+}
+
+static void iCom_writeb(u8 value, void *address)
+{
+	/* Issue a 'memory barrier' prior to the mmio to ensure data is flushed
+	 to memory. This translates to a sync instruction on ppc */
+	mb();
+	writeb(value, address);
+}
+
+static u16 iCom_readw(void *address)
+{
+	/* Issue a 'write memory barrier' prior to the mmio to ensure ordering */
+	/* This translates to an eieio instruction on ppc */
+	wmb();
+	return readw(address);
+}
+
+static void iCom_writew(u16 value, void *address)
+{
+	/* Issue a 'memory barrier' prior to the mmio to ensure data is flushed
+	 to memory. This translates to a sync instruction on ppc */
+	mb();
+	writew(value, address);
+}
+
+static u32 iCom_readl(void *address)
+{
+	/* Issue a 'write memory barrier' prior to the mmio to ensure ordering */
+	/* This translates to an eieio instruction on ppc */
+	wmb();
+	return readl(address);
+}
+
+static void iCom_writel(u32 value, void *address)
+{
+	/* Issue a 'memory barrier' prior to the mmio to ensure data is flushed
+	 to memory. This translates to a sync instruction on ppc */
+	mb();
+	writel(value, address);
+}
+
+static int get_port_memory(struct iCom_port *iCom_port_info)
+{
+  int index;
+  int number_of_buffs;
+  unsigned long int stgAddr;
+  unsigned long int startStgAddr;
+  unsigned long int offset;
+
+  TRACE(iCom_port_info,TRACE_GET_PORT_MEM,0);
+
+  iCom_port_info->xmit_buf = (unsigned char *)kmalloc(4096,GFP_KERNEL | GFP_DMA);
+  iCom_port_info->xmit_buf_pci = pci_map_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+						(void *)iCom_port_info->xmit_buf,
+						4096,
+						PCI_DMA_BIDIRECTIONAL);
+
+  if (!iCom_port_info->xmit_buf) {
+    printk("iCom:  ERROR, Can not allocate Transmit buffer\n");
+    return -ENOMEM;
+  }
+  TRACE(iCom_port_info,TRACE_GET_PORT_MEM,(unsigned long)iCom_port_info->xmit_buf);
+
+  iCom_port_info->recv_buf = (unsigned char *)kmalloc(4096,GFP_KERNEL | GFP_DMA);
+  iCom_port_info->recv_buf_pci = pci_map_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+						(void *)iCom_port_info->recv_buf,
+						4096,
+						PCI_DMA_BIDIRECTIONAL);
+
+  if (!iCom_port_info->recv_buf) {
+    printk("iCom:  ERROR, Can not allocate Receive buffer\n");
+    return_port_memory(iCom_port_info);
+    return -ENOMEM;
+  }
+  TRACE(iCom_port_info,TRACE_GET_PORT_MEM,(unsigned long)iCom_port_info->recv_buf);
+
+  iCom_port_info->statStg = (struct statusArea *)kmalloc(4096,GFP_KERNEL | GFP_DMA);
+  iCom_port_info->statStg_pci = pci_map_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+					       (void *)iCom_port_info->statStg,
+					       4096,
+					       PCI_DMA_BIDIRECTIONAL);
+
+  if (!iCom_port_info->statStg) {
+    printk("iCom:  ERROR, Can not allocate Status buffer\n");
+    return_port_memory(iCom_port_info);
+    return -ENOMEM;
+  }
+  TRACE(iCom_port_info,TRACE_GET_PORT_MEM,(unsigned long)iCom_port_info->statStg);
+
+  iCom_port_info->xmitRestart = (u32 *)kmalloc(128,GFP_KERNEL | GFP_DMA);/*only 4 bytes needed */
+  iCom_port_info->xmitRestart_pci = pci_map_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+						   iCom_port_info->xmitRestart,
+						   4,
+						   PCI_DMA_BIDIRECTIONAL);
+
+  if (!iCom_port_info->xmitRestart) {
+    printk("iCom:  ERROR, Can not allocate xmit Restart buffer\n");
+    return_port_memory(iCom_port_info);
+    return -ENOMEM;
+  }
+
+  memset(iCom_port_info->statStg, 0,4096);
+
+  /* FODs */
+  number_of_buffs = NUM_XBUFFS;
+  stgAddr = (unsigned long int)iCom_port_info->statStg;
+  startStgAddr = stgAddr;
+  for (index = 0; index < number_of_buffs; index++)
+  {
+    TRACE(iCom_port_info,TRACE_FOD_ADDR,stgAddr);
+    stgAddr = stgAddr + sizeof(iCom_port_info->statStg->xmit[0]);
+    if (index < (number_of_buffs - 1))
+    {
+      iCom_port_info->statStg->xmit[index].flags = 0;
+      iCom_port_info->statStg->xmit[index].leNext = 0;
+      iCom_port_info->statStg->xmit[index].leNextASD = 0;
+      iCom_port_info->statStg->xmit[index].leLengthASD = (unsigned short int)cpu_to_le16(XMIT_BUFF_SZ);
+      iCom_port_info->statStg->xmit[index].leOffsetASD = 0;
+      TRACE(iCom_port_info,TRACE_FOD_ADDR,stgAddr);
+      TRACE(iCom_port_info,TRACE_FOD_XBUFF,(unsigned long)iCom_port_info->xmit_buf);
+      iCom_port_info->statStg->xmit[index].leBuffer = cpu_to_le32(iCom_port_info->xmit_buf_pci);
+    }
+    else if (index == (number_of_buffs - 1))
+    {
+      iCom_port_info->statStg->xmit[index].flags = 0;
+      iCom_port_info->statStg->xmit[index].leNext = 0;
+      iCom_port_info->statStg->xmit[index].leNextASD = 0;
+      iCom_port_info->statStg->xmit[index].leLengthASD = (unsigned short int)cpu_to_le16(XMIT_BUFF_SZ);
+      iCom_port_info->statStg->xmit[index].leOffsetASD = 0;
+      TRACE(iCom_port_info,TRACE_FOD_XBUFF,(unsigned long)iCom_port_info->xmit_buf);
+      iCom_port_info->statStg->xmit[index].leBuffer = cpu_to_le32(iCom_port_info->xmit_buf_pci);
+    }
+    else
+    {
+      iCom_port_info->statStg->xmit[index].flags = 0;
+      iCom_port_info->statStg->xmit[index].leNext = 0;
+      iCom_port_info->statStg->xmit[index].leNextASD = 0;
+      iCom_port_info->statStg->xmit[index].leLengthASD = 0;
+      iCom_port_info->statStg->xmit[index].leOffsetASD = 0;
+      iCom_port_info->statStg->xmit[index].leBuffer = 0;
+    }
+  }
+  /* FIDs */
+  startStgAddr = stgAddr;
+
+  /* fill in every entry, even if no buffer */
+  number_of_buffs = NUM_RBUFFS;
+  for (index = 0; index < number_of_buffs; index++)
+  {
+    TRACE(iCom_port_info,TRACE_FID_ADDR,stgAddr);
+    stgAddr = stgAddr + sizeof(iCom_port_info->statStg->rcv[0]);
+    iCom_port_info->statStg->rcv[index].leLength = 0;
+    iCom_port_info->statStg->rcv[index].WorkingLength = (unsigned short int)cpu_to_le16(RCV_BUFF_SZ); 
+    if (index < (number_of_buffs - 1))
+    {
+      offset = stgAddr - (unsigned long)iCom_port_info->statStg;
+      iCom_port_info->statStg->rcv[index].leNext = (unsigned long)cpu_to_le32(iCom_port_info->statStg_pci + offset);
+      TRACE(iCom_port_info,TRACE_FID_RBUFF,(unsigned long)iCom_port_info->recv_buf);
+      iCom_port_info->statStg->rcv[index].leBuffer = cpu_to_le32(iCom_port_info->recv_buf_pci);
+    }
+    else if (index == (number_of_buffs - 1))
+    {
+      offset = startStgAddr - (unsigned long)iCom_port_info->statStg;
+      iCom_port_info->statStg->rcv[index].leNext = (unsigned long)cpu_to_le32(iCom_port_info->statStg_pci + offset);
+      TRACE(iCom_port_info,TRACE_FID_RBUFF,(unsigned long)iCom_port_info->recv_buf + 2048);
+      iCom_port_info->statStg->rcv[index].leBuffer = cpu_to_le32(iCom_port_info->recv_buf_pci + 2048);
+    }
+    else
+    {
+      iCom_port_info->statStg->rcv[index].leNext = 0;
+      iCom_port_info->statStg->rcv[index].leBuffer = 0;
+    }
+  }
+
+  return 0;
+}
+
+static void return_port_memory(struct iCom_port *iCom_port_info)
+{
+  TRACE(iCom_port_info, TRACE_RET_PORT_MEM,0);
+  if (iCom_port_info->recv_buf) {
+    pci_unmap_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+		     iCom_port_info->recv_buf_pci,
+		     4096,
+		     PCI_DMA_BIDIRECTIONAL);
+    kfree((void *)iCom_port_info->recv_buf);
+    iCom_port_info->recv_buf = 0;
+  }
+  if (iCom_port_info->xmit_buf) {
+    pci_unmap_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+		     iCom_port_info->xmit_buf_pci,
+		     4096,
+		     PCI_DMA_BIDIRECTIONAL);
+    kfree((void *)iCom_port_info->xmit_buf);
+    iCom_port_info->xmit_buf = 0;
+  }
+  if (iCom_port_info->statStg) {
+    pci_unmap_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+		     iCom_port_info->statStg_pci,
+		     4096,
+		     PCI_DMA_BIDIRECTIONAL);
+    kfree((void *)iCom_port_info->statStg);
+    iCom_port_info->statStg = 0;
+  }
+
+  if (iCom_port_info->xmitRestart) {
+    pci_unmap_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,
+		     iCom_port_info->xmitRestart_pci,
+		     4,
+		     PCI_DMA_BIDIRECTIONAL);
+    kfree(iCom_port_info->xmitRestart);
+    iCom_port_info->xmitRestart = 0;
+  }
+  TRACE(iCom_port_info,TRACE_RET_MEM,0);
+}
+
+static void stop_processor(struct iCom_port *iCom_port_info)
+{
+  unsigned long      temp;
+
+  switch (iCom_port_info->port) {
+  case 0:
+    temp = iCom_readl(&iCom_port_info->global_reg->control);
+    temp = (temp & ~ICOM_CONTROL_START_A) | ICOM_CONTROL_STOP_A;
+    iCom_writel(temp,&iCom_port_info->global_reg->control);
+    TRACE(iCom_port_info,TRACE_STOP_PROC_A,0);
+    break;
+  case 1:
+    temp = iCom_readl(&iCom_port_info->global_reg->control);
+    temp = (temp & ~ICOM_CONTROL_START_B) | ICOM_CONTROL_STOP_B;
+    iCom_writel(temp,&iCom_port_info->global_reg->control);
+    TRACE(iCom_port_info,TRACE_STOP_PROC_B,0);
+    break;
+  case 2:
+    temp = iCom_readl(&iCom_port_info->global_reg->control_2);
+    temp = (temp & ~ICOM_CONTROL_START_C) | ICOM_CONTROL_STOP_C;
+    iCom_writel(temp,&iCom_port_info->global_reg->control_2);
+    TRACE(iCom_port_info,TRACE_STOP_PROC_C,0);
+    break;
+  case 3:
+    temp = iCom_readl(&iCom_port_info->global_reg->control_2);
+    temp = (temp & ~ICOM_CONTROL_START_D) | ICOM_CONTROL_STOP_D;
+    iCom_writel(temp,&iCom_port_info->global_reg->control_2);
+    TRACE(iCom_port_info,TRACE_STOP_PROC_D,0);
+    break;
+  default:
+    printk("iCom:  ERROR:  invalid port assignment\n");
+  }
+}
+
+static void start_processor(struct iCom_port *iCom_port_info)
+{
+  unsigned long      temp;
+
+  switch (iCom_port_info->port) {
+  case 0:
+    temp = iCom_readl(&iCom_port_info->global_reg->control);
+    temp = (temp & ~ICOM_CONTROL_STOP_A) | ICOM_CONTROL_START_A;
+    iCom_writel(temp,&iCom_port_info->global_reg->control);
+    TRACE(iCom_port_info,TRACE_START_PROC_A,0);
+    break;
+  case 1:
+    temp = iCom_readl(&iCom_port_info->global_reg->control);
+    temp = (temp & ~ICOM_CONTROL_STOP_B) | ICOM_CONTROL_START_B;
+    iCom_writel(temp,&iCom_port_info->global_reg->control);
+    TRACE(iCom_port_info,TRACE_START_PROC_B,0);
+    break;
+  case 2:
+    temp = iCom_readl(&iCom_port_info->global_reg->control_2);
+    temp = (temp & ~ICOM_CONTROL_STOP_C) | ICOM_CONTROL_START_C;
+    iCom_writel(temp,&iCom_port_info->global_reg->control_2);
+    TRACE(iCom_port_info,TRACE_START_PROC_C,0);
+    break;
+  case 3:
+    temp = iCom_readl(&iCom_port_info->global_reg->control_2);
+    temp = (temp & ~ICOM_CONTROL_STOP_D) | ICOM_CONTROL_START_D;
+    iCom_writel(temp,&iCom_port_info->global_reg->control_2);
+    TRACE(iCom_port_info,TRACE_START_PROC_D,0);
+    break;
+  default:
+    printk("iCom:  ERROR: invalid port assignment\n");
+  }
+}
+
+/*
+ * irq = no lock
+ */
+static void loadCode (struct iCom_port *iCom_port_info)
+{
+  char               *iram_ptr;
+  int                index;
+  int                status = 0;
+  char               *dram_ptr = (char *)iCom_port_info->dram;
+  unsigned long int  temp;
+  unsigned char      *new_page;
+  unsigned char      cable_id;
+
+  TRACE(iCom_port_info,TRACE_GET_MEM,0); /* this really gets memory for trace */
+  TRACE(iCom_port_info,TRACE_LOAD_MEM,(u32)dram_ptr);
+
+  /* Clear out any pending interrupts */
+  iCom_writew(0x3FFF,(void *)iCom_port_info->int_reg);
+
+  TRACE(iCom_port_info,TRACE_CLEAR_INTERRUPTS,0);
+
+  /* Stop processor */
+  stop_processor(iCom_port_info);
+
+  /* Zero out DRAM */
+  for (index = 0; index < 512; index++)
+  {
+    iCom_writeb(0x00,&dram_ptr[index]);
+  }
+
+  /* Load Call Setup into Adapter */
+  iram_ptr = (char *)iCom_port_info->dram + ICOM_IRAM_OFFSET;
+  for (index = 0; index < sizeof(callSetup); index++)
+  {
+    iCom_writeb(callSetup[index],&iram_ptr[index]);
+  }
+
+  /* Load Resident DCE portion of Adapter */
+  iram_ptr = (char *) iCom_port_info->dram + ICOM_IRAM_OFFSET +
+    ICOM_DCE_IRAM_OFFSET;
+
+  /* Load the RV dce code */
+  for (index = 0; index < sizeof(resRVdce); index++) {
+    iCom_writeb(resRVdce[index],&iram_ptr[index]);
+  }
+
+  /* Set Hardware level */
+  if ((iCom_adapter_info[iCom_port_info->adapter].version | ADAPTER_V2) == ADAPTER_V2) {
+    iCom_writeb(V2_HARDWARE,&(iCom_port_info->dram->misc_flags));
+  }
+
+  /* Start the processor in Adapter */
+  start_processor(iCom_port_info);
+
+  iCom_writeb((HDLC_PPP_PURE_ASYNC | HDLC_FF_FILL),&(iCom_port_info->dram->HDLCConfigReg));
+  iCom_writeb(0x04,&(iCom_port_info->dram->FlagFillIdleTimer)); /* 0.5 seconds */
+  iCom_writeb(0x00,&(iCom_port_info->dram->CmdReg));
+  iCom_writeb(0x10,&(iCom_port_info->dram->async_config3));
+  iCom_writeb((ICOM_ACFG_DRIVE1 | ICOM_ACFG_NO_PARITY | ICOM_ACFG_8BPC | ICOM_ACFG_1STOP_BIT),&(iCom_port_info->dram->async_config2));
+
+  /*Set up data in iCom DRAM to indicate where personality
+   *code is located and its length.
+   */
+  new_page = (unsigned char *)kmalloc(4096,GFP_KERNEL | GFP_DMA);
+  for (index = 0; index < sizeof(funcLoad); index++) {
+    new_page[index] = funcLoad[index];
+  }
+  temp = pci_map_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,new_page,4096,PCI_DMA_BIDIRECTIONAL);
+
+  iCom_writeb((char)(sizeof(funcLoad)/16),&iCom_port_info->dram->mac_length);
+  iCom_writel(temp,&iCom_port_info->dram->mac_load_addr);
+
+  /*Setting the syncReg to 0x80 causes adapter to start downloading
+    the personality code into adapter instruction RAM.
+    Once code is loaded, it will begin executing and, based on
+    information provided above, will start DMAing data from
+    shared memory to adapter DRAM.
+  */
+  iCom_writeb(START_DOWNLOAD,&iCom_port_info->dram->sync);
+
+  /* Wait max 1 Sec for data download and processor to start */
+  for (index = 0; index < 100; index++ ) {
+      current->state = TASK_INTERRUPTIBLE;
+      schedule_timeout(HZ/100);
+      if (iCom_readb(&iCom_port_info->dram->misc_flags) & ICOM_HDW_ACTIVE) break;
+  }
+
+  if (index == 100) status = -1;
+
+  pci_unmap_single(iCom_adapter_info[iCom_port_info->adapter].pci_dev,temp,4096,PCI_DMA_BIDIRECTIONAL);
+  kfree(new_page);
+  
+  /*
+   * check Cable ID
+   */
+  cable_id = iCom_readb(&iCom_port_info->dram->cable_id);
+  if (cable_id & ICOM_CABLE_ID_VALID)
+  {
+      /* Get cable ID into the lower 4 bits (standard form) */
+      cable_id = (cable_id & ICOM_CABLE_ID_MASK) >> 4;
+      iCom_port_info->cable_id = cable_id;
+  }
+  else {
+      iCom_port_info->cable_id = NO_CABLE;
+  }
+
+  if (status != 0)
+  {
+    /* Clear out any pending interrupts */
+    iCom_writew(0x3FFF,(void *)iCom_port_info->int_reg);
+
+    /* Turn off port */
+    iCom_writeb(ICOM_DISABLE,&(iCom_port_info->dram->disable));
+
+    /* Stop processor */
+    stop_processor(iCom_port_info);
+
+    /* Fail the port */
+    if (cable_id != NO_CABLE) {
+	printk("iCom:  Error, minor number %d port not opertional\n", iCom_port_info->minor_number);
+	iCom_port_info->passed_diags = 0;
+    }
+  }
+}
+
+/*
+ * This routine is called to set the port to match
+ * the specified baud rate for a serial port.
+ * irq = locked
+ */
+static void change_speed(struct iCom_port *iCom_port_info,
+			 struct termios *old_termios, unsigned long flags)
+{
+  int	         baud;
+  unsigned       cflag;
+  int	         bits;
+  char           new_config2;
+  char           new_config3;
+  char           tmp_byte;
+  int            index;
+  int            rcv_buff,xmit_buff;
+  unsigned long int offset;
+
+  TRACE(iCom_port_info,TRACE_CHANGE_SPEED | TRACE_TIME,jiffies);
+
+  if (!iCom_port_info->tty || !iCom_port_info->tty->termios)
+    return;
+  cflag = iCom_port_info->tty->termios->c_cflag;
+
+  new_config2 = ICOM_ACFG_DRIVE1;
+
+  /* byte size and parity */
+  switch (cflag & CSIZE) {
+    case CS5: /* 5 bits/char */
+      new_config2 |= ICOM_ACFG_5BPC;
+      bits = 7;
+      break;
+    case CS6: /* 6 bits/char */
+      new_config2 |= ICOM_ACFG_6BPC;
+      bits = 8;
+      break;
+    case CS7: /* 7 bits/char */
+      new_config2 |= ICOM_ACFG_7BPC;
+      bits = 9;
+      break;
+    case CS8: /* 8 bits/char */
+      new_config2 |= ICOM_ACFG_8BPC;
+      bits = 10;
+      break;
+    default:  bits = 10;  break;
+  }
+  if (cflag & CSTOPB) {
+    /* 2 stop bits */
+    new_config2 |= ICOM_ACFG_2STOP_BIT;
+    bits++;
+  }
+  if (cflag & PARENB) {
+    /* parity bit enabled */
+    new_config2 |= ICOM_ACFG_PARITY_ENAB;
+    TRACE(iCom_port_info, TRACE_PARENB,0);
+    bits++;
+  }
+  if (cflag & PARODD) {
+    /* odd parity */
+    new_config2 |= ICOM_ACFG_PARITY_ODD;
+    TRACE(iCom_port_info, TRACE_PARODD,0);
+  }
+
+  /* Determine divisor based on baud rate */
+  baud = tty_get_baud_rate(iCom_port_info->tty);
+  if (!baud)
+    baud = 9600;	/* B0 transition handled in rs_set_termios */
+
+  for (index = 0; index < BAUD_TABLE_LIMIT; index++) {
+    if (icom_acfg_baud[index] == baud) {
+      new_config3 = index;
+      break;
+    }
+  }
+
+  iCom_port_info->timeout = XMIT_BUFF_SZ*HZ*bits/baud;
+  iCom_port_info->timeout += HZ/50;		/* Add .02 seconds of slop */
+
+  /* CTS flow control flag and modem status interrupts */
+  if (cflag & CRTSCTS) {
+    iCom_port_info->flags |= ASYNC_CTS_FLOW;
+    tmp_byte = iCom_readb(&(iCom_port_info->dram->HDLCConfigReg));
+    tmp_byte |= HDLC_HDW_FLOW;
+    iCom_writeb(tmp_byte, &(iCom_port_info->dram->HDLCConfigReg));
+  } else {
+    iCom_port_info->flags &= ~ASYNC_CTS_FLOW;
+    tmp_byte = iCom_readb(&(iCom_port_info->dram->HDLCConfigReg));
+    tmp_byte &= ~HDLC_HDW_FLOW;
+    iCom_writeb(tmp_byte, &(iCom_port_info->dram->HDLCConfigReg));
+  }
+  if (cflag & CLOCAL)
+    iCom_port_info->flags &= ~ASYNC_CHECK_CD;
+  else {
+    iCom_port_info->flags |= ASYNC_CHECK_CD;
+  }
+
+  /*
+   * Set up parity check flag
+   */
+  iCom_port_info->read_status_mask = SA_FLAGS_OVERRUN | SA_FL_RCV_DONE;
+  if (I_INPCK(iCom_port_info->tty))
+    iCom_port_info->read_status_mask |= SA_FLAGS_FRAME_ERROR | SA_FLAGS_PARITY_ERROR;
+
+  if (I_BRKINT(iCom_port_info->tty) || I_PARMRK(iCom_port_info->tty))
+    iCom_port_info->read_status_mask |= SA_FLAGS_BREAK_DET;
+
+  /*
+   * Characters to ignore
+   */
+  iCom_port_info->ignore_status_mask = 0;
+  if (I_IGNPAR(iCom_port_info->tty))
+    iCom_port_info->ignore_status_mask |= SA_FLAGS_PARITY_ERROR | SA_FLAGS_FRAME_ERROR;
+  if (I_IGNBRK(iCom_port_info->tty)) {
+    iCom_port_info->ignore_status_mask |= SA_FLAGS_BREAK_DET;
+    /*
+     * If we're ignore parity and break indicators, ignore 
+     * overruns too.  (For real raw support).
+     */
+    if (I_IGNPAR(iCom_port_info->tty))
+      iCom_port_info->ignore_status_mask |= SA_FLAGS_OVERRUN;
+  }
+
+  /*
+   * !!! ignore all characters if CREAD is not set
+   */
+  if ((cflag & CREAD) == 0)
+    iCom_port_info->ignore_status_mask |= SA_FL_RCV_DONE;
+
+  /* Turn off Receiver to prepare for reset */
+  iCom_writeb(CMD_RCV_DISABLE,&iCom_port_info->dram->CmdReg);
+
+  spin_unlock_irqrestore(&iComlock,flags);
+  for (index = 0; index < 10; index++) {
+    /* Wait 0.1 Sec for receive operations to complete*/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/10);
+
+    if (iCom_readb(&iCom_port_info->dram->PrevCmdReg) == 0x00) {
+      break;
+    }
+  }
+  spin_lock_irqsave(&iComlock, flags);
+
+  /* clear all current buffers of data */
+  for (rcv_buff = 0; rcv_buff < NUM_RBUFFS; rcv_buff++) {
+    iCom_port_info->statStg->rcv[rcv_buff].flags = 0;
+    iCom_port_info->statStg->rcv[rcv_buff].leLength = 0;
+    iCom_port_info->statStg->rcv[rcv_buff].WorkingLength = (unsigned short int)cpu_to_le16(RCV_BUFF_SZ);
+  }
+
+  for (xmit_buff = 0; xmit_buff < NUM_XBUFFS;  xmit_buff++) {
+    iCom_port_info->statStg->xmit[xmit_buff].flags = 0;
+  }
+
+  /* activate changes and start xmit and receiver here */
+  /* Enable the receiver */
+  iCom_writeb(new_config3,&(iCom_port_info->dram->async_config3));
+  iCom_writeb(new_config2,&(iCom_port_info->dram->async_config2));
+  tmp_byte = iCom_readb(&(iCom_port_info->dram->HDLCConfigReg));
+  tmp_byte |= HDLC_PPP_PURE_ASYNC | HDLC_FF_FILL;
+  iCom_writeb(tmp_byte,&(iCom_port_info->dram->HDLCConfigReg));
+  iCom_writeb(0x04, &(iCom_port_info->dram->FlagFillIdleTimer)); /* 0.5 seconds */
+  iCom_writeb(0xFF, &(iCom_port_info->dram->ier)); /* enable modem signal interrupts */
+
+  /* reset processor */
+  iCom_writeb(CMD_RESTART,&iCom_port_info->dram->CmdReg);
+  spin_unlock_irqrestore(&iComlock, flags);
+  for (index = 0; index < 10; index++) {
+    /* Wait for reset operation */
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/10);
+
+    if (iCom_readb(&iCom_port_info->dram->CmdReg) == 0x00) {
+      break;
+    }
+  }
+  spin_lock_irqsave(&iComlock, flags);
+
+  /* Enable Transmitter and Reciever */
+  offset = (unsigned long int)&iCom_port_info->statStg->rcv[0] - (unsigned long int)iCom_port_info->statStg;
+  iCom_writel(iCom_port_info->statStg_pci + offset,&iCom_port_info->dram->RcvStatusAddr);
+  iCom_port_info->next_rcv = 0;
+  iCom_port_info->put_length = 0;
+  *iCom_port_info->xmitRestart = 0;
+  iCom_writel(iCom_port_info->xmitRestart_pci,&iCom_port_info->dram->XmitStatusAddr);
+  TRACE(iCom_port_info,TRACE_XR_ENAB,0);
+  iCom_writeb(CMD_XMIT_RCV_ENABLE,&iCom_port_info->dram->CmdReg);
+}
+
+static int block_til_ready(struct tty_struct *tty, struct file * filp,
+                           struct iCom_port *iCom_port_info, long int flags)
+{
+  DECLARE_WAITQUEUE(wait, current);
+  int		retval;
+  int		do_clocal = 0, extra_count = 0;
+
+  /*
+   * If the device is in the middle of being closed, then block
+   * until it's done, and then try again.
+   */
+  if (tty_hung_up_p(filp) ||
+      (iCom_port_info->flags & ASYNC_CLOSING)) {
+      if (iCom_port_info->flags & ASYNC_CLOSING) {
+	  spin_unlock_irqrestore(&iComlock,flags);
+	  interruptible_sleep_on(&iCom_port_info->close_wait);
+	  spin_lock_irqsave(&iComlock,flags);
+      }    
+#ifdef SERIAL_DO_RESTART
+    return ((iCom_port_info->flags & ASYNC_HUP_NOTIFY) ?
+            -EAGAIN : -ERESTARTSYS);
+#else
+    return -EAGAIN;
+#endif
+  }
+
+  /*
+   * If this is a callout device, then just make sure the normal
+   * device isn't being used.
+   */
+  if (tty->driver.subtype == SERIAL_TYPE_CALLOUT) {
+    if (iCom_port_info->flags & ASYNC_NORMAL_ACTIVE)
+      return -EBUSY;
+    if ((iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) &&
+        (iCom_port_info->flags & ASYNC_SESSION_LOCKOUT) &&
+        (iCom_port_info->session != current->session))
+      return -EBUSY;
+    if ((iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) &&
+        (iCom_port_info->flags & ASYNC_PGRP_LOCKOUT) &&
+        (iCom_port_info->pgrp != current->pgrp))
+      return -EBUSY;
+    iCom_port_info->flags |= ASYNC_CALLOUT_ACTIVE;
+    return 0;
+  }
+
+  /*
+   * If non-blocking mode is set, or the port is not enabled,
+   * then make the check up front and then exit.
+   */
+  if ((filp->f_flags & O_NONBLOCK) ||
+      (tty->flags & (1 << TTY_IO_ERROR))) {
+    if (iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE)
+      return -EBUSY;
+    iCom_port_info->flags |= ASYNC_NORMAL_ACTIVE;
+    return 0;
+  }
+
+  if (iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) {
+    if (iCom_port_info->normal_termios.c_cflag & CLOCAL)
+      do_clocal = 1;
+  } else {
+    if (tty->termios->c_cflag & CLOCAL)
+      do_clocal = 1;
+  }
+
+  /*
+   * Block waiting for the carrier detect and the line to become
+   * free (i.e., not in use by the callout).  While we are in
+   * this loop, open_active_count is dropped by one, so that
+   * rs_close() knows when to free things.  We restore it upon
+   * exit, either normal or abnormal.
+   */
+  retval = 0;
+  add_wait_queue(&iCom_port_info->open_wait, &wait);
+
+  if (!tty_hung_up_p(filp)) {
+    extra_count = 1;
+    iCom_port_info->open_active_count--;
+  }
+  iCom_port_info->blocked_open++;
+  while (1) {
+    if (!(iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) &&
+        (tty->termios->c_cflag & CBAUD)) {
+      /* raise DTR and RTS */
+      TRACE(iCom_port_info,TRACE_RAISE_DTR_RTS,0);
+      iCom_writeb(0xC0,&iCom_port_info->dram->osr);
+    }
+    current->state = TASK_INTERRUPTIBLE;
+    if (tty_hung_up_p(filp) ||
+        !(iCom_port_info->flags & ASYNC_INITIALIZED)) {
+#ifdef SERIAL_DO_RESTART
+      if (iCom_port_info->flags & ASYNC_HUP_NOTIFY)
+        retval = -EAGAIN;
+      else
+        retval = -ERESTARTSYS;	
+#else
+      retval = -EAGAIN;
+#endif
+      break;
+    }
+
+    if (!(iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) &&
+        !(iCom_port_info->flags & ASYNC_CLOSING) &&
+        (do_clocal || (iCom_readb(&iCom_port_info->dram->isr) & ICOM_DCD))) /* Carrier Detect */
+      break;
+    if (signal_pending(current)) {
+      retval = -ERESTARTSYS;
+      break;
+    }
+    spin_unlock_irqrestore(&iComlock,flags);
+    printk("iCom:  WAIT for CD\n");
+    schedule();
+    spin_lock_irqsave(&iComlock,flags);
+  }
+  current->state = TASK_RUNNING;
+  remove_wait_queue(&iCom_port_info->open_wait, &wait);
+  if (extra_count)
+    iCom_port_info->open_active_count++;
+  iCom_port_info->blocked_open--;
+
+  if (retval)
+    return retval;
+  iCom_port_info->flags |= ASYNC_NORMAL_ACTIVE;
+
+  return 0;
+}
+
+/*
+  irq = locked
+*/
+static int startup(struct iCom_port *iCom_port_info, unsigned long flags)
+{
+  int	            retval=0;
+  unsigned long int temp;
+  unsigned char     cable_id, raw_cable_id;
+
+  TRACE(iCom_port_info,TRACE_STARTUP,0);
+
+  if (iCom_port_info->flags & ASYNC_INITIALIZED) {
+    goto errout;
+  }
+
+  if (iCom_port_info->dram == 0x00000000) {
+      /* should NEVER be zero */
+      return -ENODEV;
+  }
+
+  /*
+   * check Cable ID
+   */
+  raw_cable_id = iCom_readb(&iCom_port_info->dram->cable_id);
+  TRACE(iCom_port_info,TRACE_CABLE_ID,raw_cable_id);
+
+  /* Get cable ID into the lower 4 bits (standard form) */
+  cable_id = (raw_cable_id & ICOM_CABLE_ID_MASK) >> 4;
+
+  /* Check Cable ID is RS232 */
+  if (!(raw_cable_id & ICOM_CABLE_ID_VALID) || (cable_id != RS232_CABLE))
+  {
+      /* reload adapter code, pick up any potential changes in cable id */
+      if (iCom_port_info->load_in_progress) {
+	  printk("iCom:  Unusable Port, minor number (%d) currently being initialized by another task\n", iCom_port_info->minor_number);
+	  return -ENODEV;
+      }
+      iCom_port_info->load_in_progress = 1;
+      spin_unlock_irqrestore(&iComlock,flags);
+      loadCode(iCom_port_info);
+      spin_lock_irqsave(&iComlock, flags);
+      iCom_port_info->load_in_progress = 0;
+
+      /* still no sign of RS232, error out */
+      if (iCom_port_info->cable_id != RS232_CABLE) {
+	  printk("iCom:  Unusable Port, minor number (%d) incorrect cable attached, only RS232 cables permitted\n",iCom_port_info->minor_number);
+	  return -ENODEV;
+      }
+  }
+
+  /*
+   * set appropriate modem signals
+   */
+  if (iCom_port_info->tty->termios->c_cflag & CBAUD) {
+    /* raise DTR and RTS */
+    TRACE(iCom_port_info,TRACE_RAISE_DTR_RTS,0);
+    iCom_writeb(0xC0,&iCom_port_info->dram->osr);
+  }
+
+  /*
+   * Finally, clear and  enable interrupts
+   */
+  switch (iCom_port_info->port) {
+  case 0:
+    /* Clear out any pending interrupts */
+    iCom_writew(0x00FF,(void *)iCom_port_info->int_reg);
+
+    /* Enable interrupts for first port */
+    TRACE(iCom_port_info,TRACE_ENABLE_INTERRUPTS_PA,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask);
+    iCom_writel((temp & ~ICOM_INT_MASK_PRC_A),&iCom_port_info->global_reg->int_mask);
+    break;
+  case 1:
+    /* Clear out any pending interrupts */
+    iCom_writew(0x3F00,(void *)iCom_port_info->int_reg);
+
+    /* Enable interrupts for second port */
+    TRACE(iCom_port_info,TRACE_ENABLE_INTERRUPTS_PB,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask);
+    iCom_writel((temp & ~ICOM_INT_MASK_PRC_B),&iCom_port_info->global_reg->int_mask);
+    break;
+  case 2:
+    /* Clear out any pending interrupts */
+    iCom_writew(0x00FF,(void *)iCom_port_info->int_reg);
+
+    /* Enable interrupts for first port */
+    TRACE(iCom_port_info,TRACE_ENABLE_INTERRUPTS_PC,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask_2);
+    iCom_writel((temp & ~ICOM_INT_MASK_PRC_C),&iCom_port_info->global_reg->int_mask_2);
+    break;
+  case 3:
+    /* Clear out any pending interrupts */
+    iCom_writew(0x3F00,(void *)iCom_port_info->int_reg);
+
+    /* Enable interrupts for second port */
+    TRACE(iCom_port_info,TRACE_ENABLE_INTERRUPTS_PD,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask_2);
+    iCom_writel((temp & ~ICOM_INT_MASK_PRC_D),&iCom_port_info->global_reg->int_mask_2);
+    break;
+  default:
+    printk("iCom:  ERROR:  Invalid port defined\n");
+  }
+
+  if (iCom_port_info->tty)
+    clear_bit(TTY_IO_ERROR, &iCom_port_info->tty->flags);
+
+  /*
+   * Set up the tty->alt_speed kludge
+   */
+  if (iCom_port_info->tty) {
+    if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_HI)
+      iCom_port_info->tty->alt_speed = 57600;
+    if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_VHI)
+      iCom_port_info->tty->alt_speed = 115200;
+    if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_SHI)
+      iCom_port_info->tty->alt_speed = 230400;
+    if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_WARP)
+      iCom_port_info->tty->alt_speed = 460800;
+  }
+
+  /*
+   * and set the speed of the serial port
+   */
+  change_speed(iCom_port_info, 0, flags);
+
+  iCom_port_info->flags |= ASYNC_INITIALIZED;
+  return 0;
+
+  errout:
+  return retval;
+}
+
+/*
+ * This routine will shutdown a serial port; interrupts are disabled, and
+ * DTR is dropped if the hangup on close termio flag is on.
+ *
+ * irq = locked
+ */
+static void shutdown(struct iCom_port * iCom_port_info)
+{
+  unsigned long int temp;
+  unsigned char     cmdReg;
+
+  TRACE(iCom_port_info,TRACE_SHUTDOWN | TRACE_TIME,jiffies);
+
+  if (!(iCom_port_info->flags & ASYNC_INITIALIZED))
+    return;
+
+  /*
+   * clear delta_msr_wait queue to avoid mem leaks: we may free the irq
+   * here so the queue might never be waken up
+   */
+  wake_up_interruptible(&iCom_port_info->delta_msr_wait);
+
+  /*
+   * disable all interrupts
+   */
+  switch (iCom_port_info->port) {
+  case 0:
+    TRACE(iCom_port_info,TRACE_DIS_INTERRUPTS_PA,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask);
+    iCom_writel((temp | ICOM_INT_MASK_PRC_A),&iCom_port_info->global_reg->int_mask);
+    break;
+  case 1:
+    TRACE(iCom_port_info,TRACE_DIS_INTERRUPTS_PB,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask);
+    iCom_writel((temp | ICOM_INT_MASK_PRC_B),&iCom_port_info->global_reg->int_mask);
+    break;
+  case 2:
+    TRACE(iCom_port_info,TRACE_DIS_INTERRUPTS_PC,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask_2);
+    iCom_writel((temp | ICOM_INT_MASK_PRC_C),&iCom_port_info->global_reg->int_mask_2);
+    break;
+  case 3:
+    TRACE(iCom_port_info,TRACE_DIS_INTERRUPTS_PD,0);
+    temp = iCom_readl(&iCom_port_info->global_reg->int_mask_2);
+    iCom_writel((temp | ICOM_INT_MASK_PRC_D),&iCom_port_info->global_reg->int_mask_2);
+    break;
+  default:
+    printk("iCom:  ERROR:  Invalid port assignment\n");
+  }
+
+  /*
+   * disable break condition
+   */
+  cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+  if ((cmdReg | CMD_SND_BREAK) == CMD_SND_BREAK) {
+    iCom_writeb(cmdReg & ~CMD_SND_BREAK,&iCom_port_info->dram->CmdReg);
+  }
+
+  if (!iCom_port_info->tty || (iCom_port_info->tty->termios->c_cflag & HUPCL)) {
+    /* drop DTR and RTS */
+    TRACE(iCom_port_info,TRACE_DROP_DTR_RTS,0);
+    iCom_writeb(0x00,&iCom_port_info->dram->osr);
+  }
+  
+  if (iCom_port_info->tty)
+    set_bit(TTY_IO_ERROR, &iCom_port_info->tty->flags);
+
+  iCom_port_info->flags &= ~ASYNC_INITIALIZED;
+}
+
+/*
+   Primary interface routines to iCom Driver
+*/
+static int iCom_open(struct tty_struct * tty, struct file * filp)
+{
+  int               line;
+  int               adapter_entry;
+  int               port_entry;
+  struct iCom_port *iCom_port_info;
+  int               retval;
+  unsigned long     flags;
+
+  /*
+      Minor Number
+      _ _ _ _ b (lower nibble)
+      ___ ___
+       |   |
+       |   - port number (lowest 2 bits is port identifier)
+       - adapter number (remaining higher order bits identify adapter #)
+  */
+
+  MOD_INC_USE_COUNT;
+  line = MINOR(tty->device) - tty->driver.minor_start;
+  if ((line < 0) || (line >= NR_PORTS)) {
+      printk("iCom:  Invalid Minor number (%d) on open\n",MINOR(tty->device));
+      MOD_DEC_USE_COUNT;
+      return -ENODEV;
+  }
+
+  adapter_entry = (line & 0xFFFC) >> 2; /* shift adapter # into position */
+  port_entry = line & 0x0003; /* mask of port number */
+
+  if ((port_entry == 1) &&
+      (iCom_adapter_info[adapter_entry].version == ADAPTER_V2) &&
+      (iCom_adapter_info[adapter_entry].subsystem_id != FOUR_PORT_MODEL)) {
+      port_entry = 2;
+  }
+  iCom_port_info = &iCom_adapter_info[adapter_entry].port_info[port_entry];
+
+  if (iCom_port_info->status != ICOM_PORT_ACTIVE) {
+      printk("iCom:  Unusable Port, minor number (%d) invalid minor number on open\n",MINOR(tty->device));
+      MOD_DEC_USE_COUNT;
+      return -ENODEV;
+  }
+
+  if (!iCom_port_info->passed_diags) {
+      printk("iCom:  Unusable Port, minor number (%d) failed diagnostic checks, see previously logged messages\n",MINOR(tty->device));
+      MOD_DEC_USE_COUNT;
+      return -ENODEV;
+  }
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_DEVICE_NUMB,tty->device);
+  tty->driver_data = iCom_port_info;
+  iCom_port_info->tty = tty;
+  iCom_port_info->open_active_count++;
+
+  /*
+   * If the port is the middle of closing, bail out now
+   */
+  if (tty_hung_up_p(filp) ||
+      (iCom_port_info->flags & ASYNC_CLOSING)) {
+
+      MOD_DEC_USE_COUNT;
+      iCom_port_info->open_active_count--;
+      spin_unlock_irqrestore(&iComlock,flags);
+      if (iCom_port_info->flags & ASYNC_CLOSING)
+	  interruptible_sleep_on(&iCom_port_info->close_wait);
+#ifdef SERIAL_DO_RESTART
+      return ((iCom_port_info->flags & ASYNC_HUP_NOTIFY) ?
+            -EAGAIN : -ERESTARTSYS);
+#else
+      return -EAGAIN;
+#endif
+  }
+
+  /*
+   * Start up serial port
+   */
+  retval = startup(iCom_port_info, flags);
+  if (retval) {
+    /* reset open variables */
+    iCom_port_info->open_active_count--;
+    TRACE(iCom_port_info,TRACE_STARTUP_ERROR,0);
+    spin_unlock_irqrestore(&iComlock,flags);
+    return retval;
+  }
+
+  retval = block_til_ready(tty, filp, iCom_port_info, flags);
+  if (retval) {
+    MOD_DEC_USE_COUNT;
+    iCom_port_info->open_active_count--;
+    spin_unlock_irqrestore(&iComlock,flags);
+    return retval;
+  }
+
+  if ((iCom_port_info->open_active_count == 1) &&
+      (iCom_port_info->flags & ASYNC_SPLIT_TERMIOS)) {
+    if (tty->driver.subtype == SERIAL_TYPE_NORMAL)
+      *tty->termios = iCom_port_info->normal_termios;
+    else 
+      *tty->termios = iCom_port_info->callout_termios;
+    change_speed(iCom_port_info, 0, flags);
+  }
+
+  iCom_port_info->session = current->session;
+  iCom_port_info->pgrp = current->pgrp;
+
+  spin_unlock_irqrestore(&iComlock,flags);
+  return 0;
+}
+
+/*
+ * ------------------------------------------------------------
+ * iCom_close()
+ * 
+ * This routine is called when the serial port gets closed.  First, we
+ * wait for the last remaining data to be sent.  Then, we unlink its
+ * async structure from the interrupt chain if necessary.
+ * ------------------------------------------------------------
+ */
+static void iCom_close(struct tty_struct * tty, struct file * filp)
+{
+  struct iCom_port *iCom_port_info;
+  unsigned long     flags;
+  unsigned char     cmdReg;
+
+
+  if (!tty) {
+    printk("iCom:  iCom_close - no tty\n");
+    return;
+  }
+
+  iCom_port_info = (struct iCom_port *)tty->driver_data;
+  if (!iCom_port_info) {
+    printk("iCom:  iCom_close - no tty->driver_data\n");
+    return;
+  }
+  
+  TRACE(iCom_port_info,TRACE_CLOSE,0);
+  spin_lock_irqsave(&iComlock,flags);
+
+  if (tty_hung_up_p(filp)) {
+    TRACE(iCom_port_info,TRACE_CLOSE_HANGUP,0);
+    MOD_DEC_USE_COUNT;
+    spin_unlock_irqrestore(&iComlock,flags);
+    return;
+  }
+
+  if ((tty->count == 1) && (iCom_port_info->open_active_count != 1)) {
+    /*
+     * Uh, oh.  tty->count is 1, which means that the tty
+     * structure will be freed.  open_active_count should always
+     * be one in these conditions.  If it's greater than
+     * one, we've got real problems, since it means the
+     * serial port won't be shutdown.
+     */
+    iCom_port_info->open_active_count = 1;
+  }
+
+  if (--iCom_port_info->open_active_count < 0) {
+    iCom_port_info->open_active_count = 0;
+  }
+
+  if (iCom_port_info->open_active_count) {
+    TRACE(iCom_port_info,TRACE_OPEN_ACTIVE,0);
+    MOD_DEC_USE_COUNT;
+    spin_unlock_irqrestore(&iComlock,flags);
+    return;
+  }
+  iCom_port_info->flags |= ASYNC_CLOSING;
+
+  /*
+   * Save the termios structure, since this port may have
+   * separate termios for callout and dialin.
+   */
+  if (iCom_port_info->flags & ASYNC_NORMAL_ACTIVE)
+    iCom_port_info->normal_termios = *tty->termios;
+  if (iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE)
+    iCom_port_info->callout_termios = *tty->termios;
+
+  /*
+   * Now we wait for the transmit buffer to clear; and we notify 
+   * the line discipline to only process XON/XOFF characters.
+   */
+  tty->closing = 1;
+  if (iCom_port_info->closing_wait != ASYNC_CLOSING_WAIT_NONE) {
+    spin_unlock_irqrestore(&iComlock,flags);
+    tty_wait_until_sent(tty, iCom_port_info->closing_wait);
+    spin_lock_irqsave(&iComlock,flags);
+  }
+
+  /*
+   * At this point we stop accepting input.  To do this, we
+   * disable the receive line status interrupts, and tell the
+   * interrupt driver to stop checking the data ready bit in the
+   * line status register.
+   */
+  if (iCom_port_info->flags & ASYNC_INITIALIZED) {
+    cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+    iCom_writeb(cmdReg & (unsigned char)~CMD_RCV_ENABLE,&iCom_port_info->dram->CmdReg);
+
+   /*
+     * Before we drop DTR, make sure the UART transmitter
+     * has completely drained; this is especially
+     * important if there is a transmit FIFO!
+     */
+    spin_unlock_irqrestore(&iComlock,flags);
+    iCom_wait_until_sent(tty, iCom_port_info->timeout);
+    spin_lock_irqsave(&iComlock,flags);
+  }
+
+  shutdown(iCom_port_info);
+
+  spin_unlock_irqrestore(&iComlock,flags);
+  if (tty->driver.flush_buffer)
+    tty->driver.flush_buffer(tty);
+  if (tty->ldisc.flush_buffer)
+    tty->ldisc.flush_buffer(tty);
+  spin_lock_irqsave(&iComlock,flags);
+  tty->closing = 0;
+  iCom_port_info->event = 0;
+  iCom_port_info->tty = 0;
+
+  if (iCom_port_info->blocked_open) {
+    if (iCom_port_info->close_delay) {
+	current->state = TASK_INTERRUPTIBLE;
+	spin_unlock_irqrestore(&iComlock,flags);
+	schedule_timeout(iCom_port_info->close_delay);
+	spin_lock_irqsave(&iComlock,flags);
+    }
+    wake_up_interruptible(&iCom_port_info->open_wait);
+  }
+  iCom_port_info->flags &= ~(ASYNC_NORMAL_ACTIVE|ASYNC_CALLOUT_ACTIVE|
+                   ASYNC_CLOSING);
+
+  wake_up_interruptible(&iCom_port_info->close_wait);
+
+  MOD_DEC_USE_COUNT;
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static int iCom_write(struct tty_struct * tty, int from_user,
+			const unsigned char * buf, int count)
+{
+  struct iCom_port  *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned long     data_count = count;
+  unsigned char     *data;
+  unsigned char     cmdReg;
+  unsigned long int offset;
+  unsigned long int flags;
+
+
+  if (!tty) {
+    printk("iCom:  iCom_write - no tty\n");
+    return 0;
+  }
+
+  spin_lock_irqsave(&iComlock,flags);
+
+  iCom_port_info = (struct iCom_port *)tty->driver_data;
+  TRACE(iCom_port_info,TRACE_WRITE | TRACE_TIME,jiffies);
+
+  if (cpu_to_le16(iCom_port_info->statStg->xmit[0].flags) & SA_FLAGS_READY_TO_XMIT) {
+      TRACE(iCom_port_info,TRACE_WRITE_FULL,0);
+      spin_unlock_irqrestore(&iComlock,flags);
+      return 0;
+  }
+
+  if (data_count > XMIT_BUFF_SZ)
+    data_count = XMIT_BUFF_SZ;
+
+  if (from_user) {
+    data_count -= copy_from_user(iCom_port_info->xmit_buf, buf, data_count);
+    if (!data_count) {
+      TRACE(iCom_port_info,TRACE_WRITE_NODATA,0);
+      spin_unlock_irqrestore(&iComlock,flags);
+      return -EFAULT;
+    }
+  } else {
+    memcpy(iCom_port_info->xmit_buf, buf, data_count);
+  }
+
+  data = iCom_port_info->xmit_buf;
+
+  if (data_count) {
+      iCom_port_info->statStg->xmit[0].flags = (unsigned short int)cpu_to_le16(SA_FLAGS_READY_TO_XMIT);
+      iCom_port_info->statStg->xmit[0].leLength = (unsigned short int)cpu_to_le16(data_count);
+      offset = (unsigned long int)&iCom_port_info->statStg->xmit[0] - (unsigned long int)iCom_port_info->statStg;
+      *iCom_port_info->xmitRestart = cpu_to_le32(iCom_port_info->statStg_pci + offset);
+      cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+      iCom_writeb(cmdReg | CMD_XMIT_RCV_ENABLE,&iCom_port_info->dram->CmdReg);
+      iCom_writeb(START_XMIT,&iCom_port_info->dram->StartXmitCmd);
+      TRACE(iCom_port_info,TRACE_WRITE_START,data_count);
+  }
+
+  spin_unlock_irqrestore(&iComlock,flags);
+
+  return data_count;
+}
+
+static void iCom_put_char(struct tty_struct * tty, unsigned char ch)
+{
+  /* iCom_put_char adds the character to the current buffer, the
+   * data is not actually sent until iCom_flush_chars is called.
+   * Per definition iCom_flush_chars MUST be called after
+   * iCom_put_char
+   */
+
+  unsigned char     *data;
+  struct iCom_port  *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned long int flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_PUT_CHAR, ch);
+
+  if (cpu_to_le16(iCom_port_info->statStg->xmit[0].flags) & SA_FLAGS_READY_TO_XMIT) {
+      TRACE(iCom_port_info,TRACE_PUT_FULL,0);
+      spin_unlock_irqrestore(&iComlock,flags);
+      return;
+  }
+
+  data = iCom_port_info->xmit_buf;
+  data[iCom_port_info->put_length] = ch;
+
+  if (!tty->stopped && !tty->hw_stopped) {
+    iCom_port_info->put_length++;
+  }
+
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static void iCom_flush_chars(struct tty_struct * tty)
+{
+  struct iCom_port  *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char     cmdReg;
+  unsigned long int offset;
+  unsigned long int flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_FLUSH_CHAR | TRACE_TIME,jiffies);
+  if (iCom_port_info->put_length) {
+      TRACE(iCom_port_info,TRACE_START_FLUSH,iCom_port_info->put_length);
+      iCom_port_info->statStg->xmit[0].flags = (unsigned short int)cpu_to_le16(SA_FLAGS_READY_TO_XMIT);
+      iCom_port_info->statStg->xmit[0].leLength = (unsigned short int)cpu_to_le16(iCom_port_info->put_length);
+      offset = (unsigned long int)&iCom_port_info->statStg->xmit[0] - (unsigned long int)iCom_port_info->statStg;
+      *iCom_port_info->xmitRestart = cpu_to_le32(iCom_port_info->statStg_pci + offset);
+      cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+      iCom_writeb(cmdReg | CMD_XMIT_RCV_ENABLE,&iCom_port_info->dram->CmdReg);
+      iCom_writeb(START_XMIT,&iCom_port_info->dram->StartXmitCmd);
+  }
+  iCom_port_info->put_length = 0;
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static int iCom_write_room(struct tty_struct * tty)
+{
+  int bytes_avail;
+  struct iCom_port *iCom_port_info = tty->driver_data;
+  
+  if (cpu_to_le16(iCom_port_info->statStg->xmit[0].flags) & SA_FLAGS_READY_TO_XMIT)
+      bytes_avail = 0;
+  else
+      bytes_avail = XMIT_BUFF_SZ;
+
+  TRACE(iCom_port_info,TRACE_WRITE_ROOM,bytes_avail);
+  return bytes_avail;
+}
+
+static int iCom_chars_in_buffer(struct tty_struct * tty)
+{
+  unsigned long int dram;
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  int number_remaining = 0;
+
+  TRACE(iCom_port_info,TRACE_CHARS_IN_BUFF,0);
+  if (cpu_to_le16(iCom_port_info->statStg->xmit[0].flags) & SA_FLAGS_READY_TO_XMIT) {
+      dram = (unsigned long int)iCom_port_info->dram;
+      number_remaining = iCom_readw((void *)(dram + 0x168));
+      TRACE(iCom_port_info,TRACE_CHARS_REMAIN,number_remaining);
+  }
+  return number_remaining;
+}
+
+static int get_modem_info(struct iCom_port * iCom_port_info, unsigned int *value)
+{
+  unsigned char status,control;
+  unsigned int result;
+
+  TRACE(iCom_port_info,TRACE_GET_MODEM,0);
+
+  status = iCom_readb(&iCom_port_info->dram->isr);
+  control = iCom_readb(&iCom_port_info->dram->osr);
+
+  result =  ((control & 0x40) ? TIOCM_RTS : 0)
+    | ((control & ICOM_DTR) ? TIOCM_DTR : 0)
+    | ((status  & ICOM_DCD) ? TIOCM_CAR : 0)
+    | ((status  & ICOM_RI ) ? TIOCM_RNG : 0)
+    | ((status  & ICOM_DSR) ? TIOCM_DSR : 0)
+    | ((status  & ICOM_CTS) ? TIOCM_CTS : 0);
+  return put_user(result,value);
+}
+
+static int set_modem_info(struct iCom_port * iCom_port_info, unsigned int cmd,
+			  unsigned int *value)
+{
+  int error;
+  unsigned int arg;
+  unsigned char local_osr;
+
+  TRACE(iCom_port_info,TRACE_SET_MODEM,0);
+  local_osr = iCom_readb(&iCom_port_info->dram->osr);
+
+  error = get_user(arg, value);
+  if (error)
+    return error;
+  switch (cmd) {
+    case TIOCMBIS: 
+      if (arg & TIOCM_RTS) {
+	TRACE(iCom_port_info,TRACE_RAISE_RTS,0);
+        local_osr |= ICOM_RTS;
+      }
+      if (arg & TIOCM_DTR) {
+	TRACE(iCom_port_info,TRACE_RAISE_DTR,0);
+        local_osr |= ICOM_DTR;
+      }
+      break;
+    case TIOCMBIC:
+      if (arg & TIOCM_RTS) {
+        TRACE(iCom_port_info,TRACE_LOWER_RTS,0);
+        local_osr &= ~ICOM_RTS;
+      }
+      if (arg & TIOCM_DTR) {
+	TRACE(iCom_port_info,TRACE_LOWER_DTR,0);
+        local_osr &= ~ICOM_DTR;
+      }
+      break;
+    case TIOCMSET:
+      local_osr = ((local_osr & ~(ICOM_RTS | ICOM_DTR))
+                   | ((arg & TIOCM_RTS) ? ICOM_RTS : 0)
+                   | ((arg & TIOCM_DTR) ? ICOM_DTR : 0));
+      break;
+    default:
+      return -EINVAL;
+  }
+
+  iCom_writeb(local_osr,&iCom_port_info->dram->osr);
+  return 0;
+}
+
+static int get_serial_info(struct iCom_port * iCom_port_info,
+			   struct serial_struct * retinfo)
+{
+  struct serial_struct tmp;
+
+  TRACE(iCom_port_info,TRACE_GET_SERIAL,0);
+
+  if (!retinfo)
+    return -EFAULT;
+  memset(&tmp, 0, sizeof(tmp));
+  tmp.type = 0x00; /* device specific, PORT_UNKNOWN */
+  tmp.line = iCom_port_info->adapter; /* adapter number */
+  tmp.port = iCom_port_info->port; /* port number on adapter */
+  tmp.irq = iCom_adapter_info[iCom_port_info->adapter].irq_number;
+  tmp.flags = iCom_port_info->flags;
+  tmp.xmit_fifo_size = XMIT_BUFF_SZ;
+  tmp.baud_base = 0x00; /* device specific */
+  tmp.close_delay = iCom_port_info->close_delay;
+  tmp.closing_wait = iCom_port_info->closing_wait;
+  tmp.custom_divisor = 0x00;  /* device specific */
+  tmp.hub6 = 0x00; /* device specific */
+  if (copy_to_user(retinfo,&tmp,sizeof(*retinfo)))
+    return -EFAULT;
+  return 0;
+}
+
+static int set_serial_info(struct iCom_port * iCom_port_info,
+                           struct serial_struct * new_info)
+{
+  struct serial_struct new_serial;
+  int                  old_flags;
+  int 		       retval = 0;
+  unsigned long        flags;
+
+  TRACE(iCom_port_info,TRACE_SET_SERIAL,0);
+
+  if (copy_from_user(&new_serial,new_info,sizeof(new_serial)))
+    return -EFAULT;
+
+  old_flags = iCom_port_info->flags;
+  /* new_serial.irq --- irq of adapter will not change, PCI only */
+  /* new_serial.xmit_fifo_size -- can not change on this device */
+  /* new_serial.baud_base -- ??? */
+  /* new_serial.custom_divisor -- device specific */
+  /* new_serial.hub6 -- device specific */
+  /* new_serial.type -- device specific */
+  /* new_serial.port -- address of port will not change, PCI only */
+
+  if (!capable(CAP_SYS_ADMIN)) {
+    if ((new_serial.baud_base != iCom_port_info->baud_base) ||
+        (new_serial.close_delay != iCom_port_info->close_delay) ||
+        ((new_serial.flags & ~ASYNC_USR_MASK) !=
+         (iCom_port_info->flags & ~ASYNC_USR_MASK)))
+      return -EPERM;
+    iCom_port_info->flags = ((iCom_port_info->flags & ~ASYNC_USR_MASK) |
+                             (new_serial.flags & ASYNC_USR_MASK));
+    goto check_and_exit;
+  }
+
+  if (new_serial.baud_base < 9600) {
+    return -EINVAL;
+  }
+
+  /*
+   * OK, past this point, all the error checking has been done.
+   * At this point, we start making changes.....
+   */
+  iCom_port_info->baud_base = new_serial.baud_base;
+  iCom_port_info->flags = ((iCom_port_info->flags & ~ASYNC_FLAGS) |
+                           (new_serial.flags & ASYNC_FLAGS));
+  iCom_port_info->close_delay = new_serial.close_delay * HZ/100;
+  iCom_port_info->closing_wait = new_serial.closing_wait * HZ/100;
+  iCom_port_info->tty->low_latency = (iCom_port_info->flags & ASYNC_LOW_LATENCY) ? 1 : 0;
+
+  check_and_exit:
+  spin_lock_irqsave(&iComlock,flags);
+  if (iCom_port_info->flags & ASYNC_INITIALIZED) {
+    if (((iCom_port_info->flags & ASYNC_SPD_MASK) !=
+         (old_flags & ASYNC_SPD_MASK))) {
+      if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_HI)
+        iCom_port_info->tty->alt_speed = 57600;
+      if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_VHI)
+        iCom_port_info->tty->alt_speed = 115200;
+      if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_SHI)
+        iCom_port_info->tty->alt_speed = 230400;
+      if ((iCom_port_info->flags & ASYNC_SPD_MASK) == ASYNC_SPD_WARP)
+        iCom_port_info->tty->alt_speed = 460800;
+      change_speed(iCom_port_info, 0, flags);
+    }
+  } else
+    retval = startup(iCom_port_info, flags);
+
+  spin_unlock_irqrestore(&iComlock,flags);
+
+  return retval;
+}
+
+/*
+ * get_lsr_info - get line status register info
+ *
+ * Purpose: Let user call ioctl() to get info when the UART physically
+ * 	    is emptied.  On bus types like RS485, the transmitter must
+ * 	    release the bus after transmitting. This must be done when
+ * 	    the transmit shift register is empty, not be done when the
+ * 	    transmit holding register is empty.  This functionality
+ * 	    allows an RS485 driver to be written in user space. 
+ */
+static int get_lsr_info(struct iCom_port * info, unsigned int *value)
+{
+  unsigned char status;
+  unsigned int result;
+
+  TRACE(info,TRACE_SET_LSR,0);
+
+  status = cpu_to_le16(info->statStg->xmit[0].flags);
+  result = ((status & SA_FLAGS_DONE) ? TIOCSER_TEMT : 0);
+  return put_user(result,value);
+}
+
+static int iCom_ioctl(struct tty_struct * tty, struct file * filp,
+			unsigned int cmd, unsigned long arg) 
+{
+  int error;
+  struct iCom_port * iCom_port_info = (struct iCom_port *)tty->driver_data;
+  struct async_icount cprev, cnow;	/* kernel counter temps */
+  struct serial_icounter_struct *p_cuser;	/* user space */
+  unsigned long flags;
+
+  TRACE(iCom_port_info,TRACE_IOCTL | TRACE_TIME,jiffies);
+  if ((cmd != TIOCGSERIAL) && (cmd != TIOCSSERIAL) &&
+      (cmd != TIOCSERCONFIG) && (cmd != TIOCSERGSTRUCT) &&
+      (cmd != TIOCMIWAIT) && (cmd != TIOCGICOUNT)) {
+    if (tty->flags & (1 << TTY_IO_ERROR))
+      return -EIO;
+  }
+
+  switch (cmd) {
+    case 0x4300:
+      if (copy_to_user((void *)arg,iCom_port_info->trace_blk,TRACE_BLK_SZ))
+	  return -EFAULT;
+      return 0;
+    case TIOCMGET:
+      return get_modem_info(iCom_port_info, (unsigned int *) arg);
+    case TIOCMBIS:
+    case TIOCMBIC:
+    case TIOCMSET:
+      return set_modem_info(iCom_port_info, cmd, (unsigned int *) arg);
+    case TIOCGSERIAL:
+      return get_serial_info(iCom_port_info,
+                             (struct serial_struct *) arg);
+    case TIOCSSERIAL:
+      return set_serial_info(iCom_port_info,
+                             (struct serial_struct *) arg);
+
+    case TIOCSERGETLSR: /* Get line status register */
+      return get_lsr_info(iCom_port_info, (unsigned int *) arg);
+
+      /*
+       * Wait for any of the 4 modem inputs (DCD,RI,DSR,CTS) to change
+       * - mask passed in arg for lines of interest
+       *   (use |'ed TIOCM_RNG/DSR/CD/CTS for masking)
+       * Caller should use TIOCGICOUNT to see which one it was
+       */
+    case TIOCMIWAIT:
+      spin_lock_irqsave(&iComlock,flags);
+      /* note the counters on entry */
+      cprev = iCom_port_info->icount;
+      spin_unlock_irqrestore(&iComlock,flags);
+      while (1) {
+        interruptible_sleep_on(&iCom_port_info->delta_msr_wait);
+        /* see if a signal did it */
+        if (signal_pending(current))
+          return -ERESTARTSYS;
+        spin_lock_irqsave(&iComlock,flags);
+        cnow = iCom_port_info->icount; /* atomic copy */
+        spin_unlock_irqrestore(&iComlock,flags);
+        if (cnow.rng == cprev.rng && cnow.dsr == cprev.dsr && 
+            cnow.dcd == cprev.dcd && cnow.cts == cprev.cts)
+          return -EIO; /* no change => error */
+        if ( ((arg & TIOCM_RNG) && (cnow.rng != cprev.rng)) ||
+             ((arg & TIOCM_DSR) && (cnow.dsr != cprev.dsr)) ||
+             ((arg & TIOCM_CD)  && (cnow.dcd != cprev.dcd)) ||
+             ((arg & TIOCM_CTS) && (cnow.cts != cprev.cts)) ) {
+          return 0;
+        }
+        cprev = cnow;
+      }
+      /* NOTREACHED */
+
+      /* 
+       * Get counter of input serial line interrupts (DCD,RI,DSR,CTS)
+       * Return: write counters to the user passed counter struct
+       * NB: both 1->0 and 0->1 transitions are counted except for
+       *     RI where only 0->1 is counted.
+       */
+    case TIOCGICOUNT:
+      spin_lock_irqsave(&iComlock,flags);
+      cnow = iCom_port_info->icount;
+      spin_unlock_irqrestore(&iComlock,flags);
+      p_cuser = (struct serial_icounter_struct *) arg;
+      error = put_user(cnow.cts, &p_cuser->cts);
+      if (error) return error;
+      error = put_user(cnow.dsr, &p_cuser->dsr);
+      if (error) return error;
+      error = put_user(cnow.rng, &p_cuser->rng);
+      if (error) return error;
+      error = put_user(cnow.dcd, &p_cuser->dcd);
+      if (error) return error;
+      error = put_user(cnow.rx, &p_cuser->rx);
+      if (error) return error;
+      error = put_user(cnow.tx, &p_cuser->tx);
+      if (error) return error;
+      error = put_user(cnow.frame, &p_cuser->frame);
+      if (error) return error;
+      error = put_user(cnow.overrun, &p_cuser->overrun);
+      if (error) return error;
+      error = put_user(cnow.parity, &p_cuser->parity);
+      if (error) return error;
+      error = put_user(cnow.brk, &p_cuser->brk);
+      if (error) return error;
+      error = put_user(cnow.buf_overrun, &p_cuser->buf_overrun);
+      if (error) return error;			
+      return 0;
+
+    case TIOCSERGWILD:
+    case TIOCSERSWILD:
+      /* "setserial -W" is called in Debian boot */
+      printk ("TIOCSER?WILD ioctl obsolete, ignored.\n");
+      return 0;
+
+    default:
+      TRACE(iCom_port_info,TRACE_IOCTL_IGNORE,cmd);
+      return -ENOIOCTLCMD;
+  }
+  return 0;
+}
+
+static void iCom_send_xchar(struct tty_struct * tty, char ch)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    xdata;
+  int              index;
+  unsigned long    flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_SEND_XCHAR,ch);
+  /* attempt sending char for a period of .1 second */
+  for (index = 0; index < 10; index++ ) {
+      xdata = iCom_readb(&iCom_port_info->dram->xchar);
+      if (xdata == 0x00) {
+	  TRACE(iCom_port_info,TRACE_QUICK_WRITE,0);
+	  iCom_writeb(ch,&iCom_port_info->dram->xchar);
+	  break;
+      }
+      current->state = TASK_INTERRUPTIBLE;
+      spin_unlock_irqrestore(&iComlock,flags);
+      schedule_timeout(HZ/100);
+      spin_lock_irqsave(&iComlock,flags);
+  }
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static void iCom_throttle(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    osr;
+
+  TRACE(iCom_port_info,TRACE_THROTTLE,0);
+  if (I_IXOFF(tty))
+    iCom_send_xchar(tty, STOP_CHAR(tty));
+
+  if (tty->termios->c_cflag & CRTSCTS) {
+    osr = iCom_readb(&iCom_port_info->dram->osr);
+    iCom_writeb(osr & ~ICOM_RTS,&iCom_port_info->dram->osr);
+  }
+}
+
+static void iCom_unthrottle(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    osr;
+
+  TRACE(iCom_port_info,TRACE_UNTHROTTLE,0);
+  if (I_IXOFF(tty)) {
+    iCom_send_xchar(tty, START_CHAR(tty));
+  }
+  if (tty->termios->c_cflag & CRTSCTS) {
+    osr = iCom_readb(&iCom_port_info->dram->osr);
+    iCom_writeb(osr | ICOM_RTS,&iCom_port_info->dram->osr);
+  }
+}
+
+static void iCom_set_termios(struct tty_struct * tty, struct termios * old_termios)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned int     cflag = tty->termios->c_cflag;
+  unsigned char    osr;
+  unsigned long    flags;
+#define RELEVANT_IFLAG(iflag) (iflag & (IGNBRK|BRKINT|IGNPAR|PARMRK|INPCK))
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_SET_TERMIOS,0);
+  if ((cflag == old_termios->c_cflag)
+      && (RELEVANT_IFLAG(tty->termios->c_iflag) 
+	  == RELEVANT_IFLAG(old_termios->c_iflag))) {
+      spin_unlock_irqrestore(&iComlock,flags);
+      return;
+  }
+
+  change_speed(iCom_port_info, old_termios, flags);
+
+  /* Handle transition to B0 status */
+  if ((old_termios->c_cflag & CBAUD) &&
+      !(cflag & CBAUD)) {
+    osr = iCom_readb(&iCom_port_info->dram->osr);
+    TRACE(iCom_port_info,TRACE_DROP_DTR_RTS,0);
+    iCom_writeb(osr & ~(ICOM_DTR|ICOM_RTS),&iCom_port_info->dram->osr);
+  }
+
+  /* Handle transition away from B0 status */
+  if (!(old_termios->c_cflag & CBAUD) &&
+      (cflag & CBAUD)) {
+    osr = iCom_readb(&iCom_port_info->dram->osr);
+    TRACE(iCom_port_info,TRACE_RAISE_DTR,0);
+    osr |= ICOM_DTR;
+    if (!(tty->termios->c_cflag & CRTSCTS) || 
+        !test_bit(TTY_THROTTLED, &tty->flags)) {
+      TRACE(iCom_port_info,TRACE_RAISE_RTS,0);
+      osr |= ICOM_RTS;
+    }
+    iCom_writeb(osr,&iCom_port_info->dram->osr);
+  }
+
+  spin_unlock_irqrestore(&iComlock,flags);
+
+  /* Handle turning off CRTSCTS */
+  if ((old_termios->c_cflag & CRTSCTS) &&
+      !(tty->termios->c_cflag & CRTSCTS)) {
+    tty->hw_stopped = 0;
+    iCom_start(tty);
+  }
+
+#if 0
+  /*
+   * No need to wake up processes in open wait, since they
+   * sample the CLOCAL flag once, and don't recheck it.
+   * XXX  It's not clear whether the current behavior is correct
+   * or not.  Hence, this may change.....
+   */
+  if (!(old_termios->c_cflag & CLOCAL) &&
+      (tty->termios->c_cflag & CLOCAL))
+    wake_up_interruptible(&iCom_port_info->open_wait);
+#endif
+}
+
+static void iCom_stop(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    cmdReg;
+  unsigned long    flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_STOP,0);
+  cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+  iCom_writeb(cmdReg | CMD_HOLD_XMIT,&iCom_port_info->dram->CmdReg);
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static void iCom_start(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    cmdReg;
+  unsigned long    flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_START,0);
+  cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+  iCom_writeb(cmdReg & ~CMD_HOLD_XMIT,&iCom_port_info->dram->CmdReg);
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static void iCom_hangup(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned long    flags;
+
+  TRACE(iCom_port_info,TRACE_HANGUP,0);
+  iCom_flush_buffer(tty);
+  spin_lock_irqsave(&iComlock,flags);
+  shutdown(iCom_port_info);
+  iCom_port_info->open_active_count = 0;
+  iCom_port_info->flags &= ~(ASYNC_NORMAL_ACTIVE|ASYNC_CALLOUT_ACTIVE);
+  iCom_port_info->tty = 0;
+  wake_up_interruptible(&iCom_port_info->open_wait);
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+static void iCom_break(struct tty_struct *tty, int break_state)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    cmdReg;
+  unsigned long    flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_BREAK,0);
+  cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+  if (break_state == -1) {
+    iCom_writeb(cmdReg | CMD_SND_BREAK,&iCom_port_info->dram->CmdReg);
+  }
+  else{
+    iCom_writeb(cmdReg & ~CMD_SND_BREAK,&iCom_port_info->dram->CmdReg);
+  }
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+/*
+ * iCom_wait_until_sent() --- wait until the transmitter is empty
+ */
+static void iCom_wait_until_sent(struct tty_struct *tty, int timeout)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned long orig_jiffies, char_time;
+  int status;
+
+  TRACE(iCom_port_info,TRACE_WAIT_UNTIL_SENT,0);
+
+  orig_jiffies = jiffies;
+  /*
+   * Set the check interval to be 1/5 of the estimated time to
+   * send a single character, and make it at least 1.  The check
+   * interval should also be less than the timeout.
+   * 
+   * Note: we have to use pretty tight timings here to satisfy
+   * the NIST-PCTS.
+   */
+  char_time = (iCom_port_info->timeout - HZ/50) / iCom_port_info->xmit_fifo_size;
+  char_time = char_time / 5;
+  if (char_time == 0)
+      char_time = 1;
+  if (timeout) {
+      if (timeout < char_time)
+	char_time = timeout;
+  }
+  /*
+   * If the transmitter hasn't cleared in twice the approximate
+   * amount of time to send the entire FIFO, it probably won't
+   * ever clear.  This assumes the UART isn't doing flow
+   * control, which is currently the case.  Hence, if it ever
+   * takes longer than iCom_port_info->timeout, this is probably due to a
+   * UART bug of some kind.  So, we clamp the timeout parameter at
+   * 2*iCom_port_info->timeout.
+   */
+  if (!timeout || timeout > 2*iCom_port_info->timeout)
+      timeout = 2*iCom_port_info->timeout;
+
+  status = cpu_to_le16(iCom_port_info->statStg->xmit[0].flags);
+  while (status & SA_FLAGS_DONE ) {  /*data still transmitting*/
+
+      current->state = TASK_INTERRUPTIBLE;
+      current->counter = 0;	/* make us low-priority */
+      schedule_timeout(char_time);
+      if (signal_pending(current))
+	  break;
+      if (timeout && time_after(jiffies, orig_jiffies + timeout))
+	  break;
+      status = cpu_to_le16(iCom_port_info->statStg->xmit[0].flags);
+  }
+  current->state = TASK_RUNNING;
+}
+
+/*
+ * /proc fs routines....
+ */
+static inline int line_info(char *buf, int free_space, struct iCom_port *iCom_port_info)
+{
+  char	line[72], control, status;
+  int	ret, baud_index, len;
+  int   port;
+
+  if ((iCom_port_info->port == 2) &&
+      (iCom_adapter_info[iCom_port_info->adapter].subsystem_id != FOUR_PORT_MODEL))
+      port = 1;
+  else
+      port = iCom_port_info->port;
+
+  memset(line, 0,sizeof(line));
+  len = sprintf(line,"   port:%d      maj:%d min:%d cable:",
+                port,
+                243,
+                iCom_port_info->minor_number);
+  if (!iCom_port_info->passed_diags) {
+    len += sprintf(line+len," PortFailed\n");
+  }
+  else if (iCom_port_info->imbed_modem == ICOM_IMBED_MODEM) {
+    len += sprintf(line+len," InternalModem\n");
+  }
+  else if (iCom_port_info->cable_id == RS232_CABLE) {
+    len += sprintf(line+len," RS232\n");
+  }
+  else {
+    len += sprintf(line+len," ----\n");
+  }
+
+  if (len > free_space) return 0;
+  ret = sprintf(buf, "%s", line);
+
+  memset(line, 0,sizeof(line));
+  baud_index = iCom_readb(&iCom_port_info->dram->async_config3);
+  len = sprintf(line, "     baud:%d",icom_acfg_baud[baud_index]);
+
+  len += sprintf(line+len, " tx:%d rx:%d",
+                 iCom_port_info->icount.tx, iCom_port_info->icount.rx);
+
+  if (iCom_port_info->icount.frame)
+    len += sprintf(line+len, " fe:%d", iCom_port_info->icount.frame);
+
+  if (iCom_port_info->icount.parity)
+    len += sprintf(line+len, " pe:%d", iCom_port_info->icount.parity);
+
+  if (iCom_port_info->icount.brk)
+    len += sprintf(line+len, " brk:%d", iCom_port_info->icount.brk);	
+
+  if (iCom_port_info->icount.overrun)
+    len += sprintf(line+len, " oe:%d", iCom_port_info->icount.overrun);
+
+  if ((ret + len) > free_space) return 0;
+  ret += sprintf(buf+ret, "%s", line);
+
+  /*
+   * Last thing is the RS-232 status lines
+   */
+  memset(line, 0,sizeof(line));
+  status = iCom_readb(&iCom_port_info->dram->isr);
+  control = iCom_readb(&iCom_port_info->dram->osr);
+
+  line[0] = 0;
+  line[1] = 0;
+  len = 0;
+  if (control & ICOM_RTS)
+    len += sprintf(line+len, "|RTS");
+  if (status & ICOM_CTS)
+    len += sprintf(line+len, "|CTS");
+  if (control & ICOM_DTR)
+    len += sprintf(line+len, "|DTR");
+  if (status & ICOM_DSR)
+    len += sprintf(line+len, "|DSR");
+  if (status & ICOM_DCD)
+    len += sprintf(line+len, "|CD");
+  if (status & ICOM_RI)
+    len += sprintf(line+len, "|RI");
+
+  if ((ret + len) > free_space) return 0;
+  ret += sprintf(buf+ret, "   %s\n", line+1);
+
+  return ret;
+}
+
+int iCom_read_proc(char *page, char **start, off_t off, int count,
+		   int *eof, void *data)
+{
+  int i, j, len = 0, l;
+  off_t	begin = 0;
+  char line[72];
+  struct LocationDataStruct *p_location_data = NULL;
+
+  len += sprintf(page, "%s: %s\n",ICOM_DRIVER_NAME, ICOM_VERSION_STR);
+  for (i = 0; i < active_adapters; i++) {
+    memset(line, 0,sizeof(line));
+#ifdef CONFIG_PPC_ISERIES
+    p_location_data = iSeries_GetLocationData(iCom_adapter_info[i].pci_dev);
+
+    if (p_location_data != NULL)
+    {
+      l = sprintf(line,"Frame ID: %d", p_location_data->FrameId);
+      l += sprintf(line+l," Card Position: %s", p_location_data->CardLocation);
+      kfree(p_location_data);
+    }
+    else
+      l = sprintf(line, "Adapter %d", i);
+#else
+    l = sprintf(line, "Adapter %d", i);
+#endif
+    switch (iCom_adapter_info[i].subsystem_id) {
+      case FOUR_PORT_MODEL:
+        l += sprintf(line+l," CCIN: 2805\n");
+        break;
+      case V2_TWO_PORTS_RVX:
+        l += sprintf(line+l," CCIN: 2742\n");
+        break;
+      case V2_ONE_PORT_RVX_ONE_PORT_IMBED_MDM:
+        l += sprintf(line+l," CCIN: 2793\n");
+        break;
+      default:
+        l += sprintf(line+l, "\n");
+    }
+
+    if ((l + len) < count) {
+      len += sprintf(page + len,"%s",line);
+      if (len+begin <= off) {
+        begin += len;
+        len = 0;
+      }
+    }
+    else
+      goto done;
+
+    for (j= 0; j < 4; j++) {
+	if (iCom_adapter_info[i].port_info[j].status == ICOM_PORT_ACTIVE) {
+	    l = line_info(page + len, count - len, &iCom_adapter_info[i].port_info[j]);
+	    /* l == 0 means line_info could not fit new data into remaining space */
+	    if (l == 0)
+		goto done;
+	    len += l;
+	    if (len+begin <= off) {
+		begin += len;
+		len = 0;
+	    }
+	}
+    }
+  }
+  *eof = 1;
+  done:
+    if (off >= len+begin)
+      return 0;
+  /*
+     if returning multiple pages, the time delta between sending pages
+     may experience changes in the data size represented in the pages
+     already sent.  The logic as is may duplicate a line but should
+     never miss a line
+  */
+  *start = page;
+  return len;
+}
+
+static void iCom_flush_buffer(struct tty_struct * tty)
+{
+  struct iCom_port *iCom_port_info = (struct iCom_port *)tty->driver_data;
+  unsigned char    cmdReg;
+  unsigned long    flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+  TRACE(iCom_port_info,TRACE_FLUSH_BUFFER,0);
+  /*
+   * with no CMD_XMIT_ENABLE is same as disabling xmitter.  This should
+   * result in an interrupt if currently transmitting
+   */
+  cmdReg = iCom_readb(&iCom_port_info->dram->CmdReg);
+  iCom_writeb(cmdReg & ~CMD_XMIT_ENABLE,&iCom_port_info->dram->CmdReg);  
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+/*
+ * This routine is used by the interrupt handler to schedule
+ * processing in the software interrupt portion of the driver.
+ */
+static inline void rs_sched_event(struct iCom_port *info,
+				  int event)
+{
+  info->event |= 1 << event;
+  queue_task(&info->tqueue, &tq_immediate);
+  mark_bh(IMMEDIATE_BH);
+}
+
+static inline void check_modem_status(struct iCom_port *iCom_port_info)
+{
+  static char old_status = 0;
+  char delta_status;
+  unsigned char status;
+
+  /*modem input register */
+  status = iCom_readb(&iCom_port_info->dram->isr);
+  TRACE(iCom_port_info,TRACE_CHECK_MODEM,status);
+  delta_status = status ^ old_status;
+  if (delta_status) {
+    if (delta_status & ICOM_RI)
+      iCom_port_info->icount.rng++;
+    if (delta_status & ICOM_DSR)
+      iCom_port_info->icount.dsr++;
+    if (delta_status & ICOM_DCD)
+      iCom_port_info->icount.dcd++;
+    if (delta_status & ICOM_CTS)
+      iCom_port_info->icount.cts++;
+  
+    wake_up_interruptible(&iCom_port_info->delta_msr_wait);
+    old_status = status;
+  }
+
+  if ((iCom_port_info->flags & ASYNC_CHECK_CD) && (status & ICOM_DCD)) {
+    if (status & ICOM_DCD) /* Carrier Detect up */
+      wake_up_interruptible(&iCom_port_info->open_wait);
+    else if (!((iCom_port_info->flags & ASYNC_CALLOUT_ACTIVE) &&
+               (iCom_port_info->flags & ASYNC_CALLOUT_NOHUP))) {
+      if (iCom_port_info->tty)
+        tty_hangup(iCom_port_info->tty);
+    }
+  }
+
+  if (iCom_port_info->flags & ASYNC_CTS_FLOW) {
+    if (iCom_port_info->tty->hw_stopped) {
+      if (status & ICOM_CTS) {  /* CTS up */
+        iCom_port_info->tty->hw_stopped = 0;
+	TRACE(iCom_port_info,TRACE_CTS_UP,0);
+        rs_sched_event(iCom_port_info, 0);
+        return;
+      }
+    } else {
+      if (!(status & ICOM_CTS)) { /* CTS down */
+        iCom_port_info->tty->hw_stopped = 1;
+	TRACE(iCom_port_info,TRACE_CTS_DOWN,0);
+      }
+    }
+  }
+}
+
+static void process_interrupt(u16 port_int_reg, struct iCom_port *iCom_port_info)
+{
+  short int           count, rcv_buff;
+  struct tty_struct   *tty = iCom_port_info->tty;
+  unsigned char       *data;
+  unsigned short int  status;
+  struct async_icount *icount;
+  unsigned long int   offset;
+
+
+  TRACE(iCom_port_info,TRACE_INTERRUPT | TRACE_TIME,jiffies);
+
+  if (port_int_reg & (INT_XMIT_COMPLETED | INT_XMIT_DISABLED)) {
+    if (port_int_reg & (INT_XMIT_COMPLETED))
+	TRACE(iCom_port_info,TRACE_XMIT_COMPLETE,0);
+    else
+	TRACE(iCom_port_info,TRACE_XMIT_DISABLED,0);
+
+    /* clear buffer in use bit */
+    iCom_port_info->statStg->xmit[0].flags &= cpu_to_le16(~SA_FLAGS_READY_TO_XMIT);
+    iCom_port_info->icount.tx += (unsigned short int)cpu_to_le16(iCom_port_info->statStg->xmit[0].leLength);
+
+    /* activate write queue */
+    rs_sched_event(iCom_port_info, 0);
+  }
+
+  if (port_int_reg & INT_RCV_COMPLETED) {
+
+    TRACE(iCom_port_info,TRACE_RCV_COMPLETE,0);
+    rcv_buff = iCom_port_info->next_rcv;
+
+    status = cpu_to_le16(iCom_port_info->statStg->rcv[rcv_buff].flags);
+    while (status & SA_FL_RCV_DONE) {
+
+      TRACE(iCom_port_info,TRACE_FID_STATUS,status);
+
+      count = cpu_to_le16(iCom_port_info->statStg->rcv[rcv_buff].leLength);
+	
+      TRACE(iCom_port_info,TRACE_RCV_COUNT,count);
+      if (count > (TTY_FLIPBUF_SIZE - tty->flip.count))
+        count = TTY_FLIPBUF_SIZE - tty->flip.count;
+
+      TRACE(iCom_port_info,TRACE_REAL_COUNT,count);
+
+      offset = cpu_to_le32(iCom_port_info->statStg->rcv[rcv_buff].leBuffer) - iCom_port_info->recv_buf_pci;
+
+      memcpy(tty->flip.char_buf_ptr,(unsigned char *)((unsigned long int)iCom_port_info->recv_buf + offset),count);
+
+      data = (unsigned char *)tty->flip.char_buf_ptr;
+
+      if (count > 0) {
+	tty->flip.count += count - 1;
+	tty->flip.char_buf_ptr += count - 1;
+	
+	memset(tty->flip.flag_buf_ptr, 0, count);
+	tty->flip.flag_buf_ptr += count - 1;
+      }
+
+      icount = &iCom_port_info->icount;
+      icount->rx += count;
+
+      /* Break detect logic */
+      if ((status & SA_FLAGS_FRAME_ERROR) && (tty->flip.char_buf_ptr[0] == 0x00)) {
+	status &= ~SA_FLAGS_FRAME_ERROR;
+	status |= SA_FLAGS_BREAK_DET;
+        TRACE(iCom_port_info,TRACE_BREAK_DET,0);
+      }
+
+      if (status & (SA_FLAGS_BREAK_DET | SA_FLAGS_PARITY_ERROR |
+                    SA_FLAGS_FRAME_ERROR | SA_FLAGS_OVERRUN)) {
+
+        if (status & SA_FLAGS_BREAK_DET)
+          icount->brk++;
+        if (status & SA_FLAGS_PARITY_ERROR)
+          icount->parity++;
+        if (status & SA_FLAGS_FRAME_ERROR)
+          icount->frame++;
+        if (status & SA_FLAGS_OVERRUN)
+          icount->overrun++;
+
+        /*
+         * Now check to see if character should be
+         * ignored, and mask off conditions which
+         * should be ignored.
+         */ 
+        if (status & iCom_port_info->ignore_status_mask) {
+	  TRACE(iCom_port_info,TRACE_IGNORE_CHAR,0);
+          goto ignore_char;
+        }
+
+        status &= iCom_port_info->read_status_mask;
+
+        if (status & SA_FLAGS_BREAK_DET) {
+          *tty->flip.flag_buf_ptr = TTY_BREAK;
+          if (iCom_port_info->flags & ASYNC_SAK)
+            do_SAK(tty);
+        } else if (status & SA_FLAGS_PARITY_ERROR) {
+	  TRACE(iCom_port_info,TRACE_PARITY_ERROR,0);
+          *tty->flip.flag_buf_ptr = TTY_PARITY;
+	}
+        else if (status & SA_FLAGS_FRAME_ERROR)
+          *tty->flip.flag_buf_ptr = TTY_FRAME;
+        if (status & SA_FLAGS_OVERRUN) {
+          /*
+           * Overrun is special, since it's
+           * reported immediately, and doesn't
+           * affect the current character
+           */
+          if (tty->flip.count < TTY_FLIPBUF_SIZE) {
+            tty->flip.count++;
+            tty->flip.flag_buf_ptr++;
+            tty->flip.char_buf_ptr++;
+            *tty->flip.flag_buf_ptr = TTY_OVERRUN;
+          }
+        }
+      }
+
+      tty->flip.flag_buf_ptr++;
+      tty->flip.char_buf_ptr++;
+      tty->flip.count++;
+      ignore_char:
+      iCom_port_info->statStg->rcv[rcv_buff].flags = 0;
+      iCom_port_info->statStg->rcv[rcv_buff].leLength = 0;
+      iCom_port_info->statStg->rcv[rcv_buff].WorkingLength = (unsigned short int)cpu_to_le16(RCV_BUFF_SZ);
+
+      rcv_buff++;
+      if (rcv_buff == NUM_RBUFFS) rcv_buff = 0;
+
+      status = cpu_to_le16(iCom_port_info->statStg->rcv[rcv_buff].flags);
+    }
+    iCom_port_info->next_rcv = rcv_buff;
+    tty_flip_buffer_push(tty);
+  }
+}
+
+static void iCom_interrupt(int irq, void * dev_id, struct pt_regs * regs)
+{
+  unsigned long int   int_reg;
+  u32                 adapter_interrupts;
+  u16                 port_int_reg;
+  struct iCom_adapter *iCom_adapter_ptr;
+  struct iCom_port    *iCom_port_info;
+  unsigned long       flags;
+
+  spin_lock_irqsave(&iComlock,flags);
+
+  /* find iCom_port_info for this interrupt */
+  iCom_adapter_ptr = (struct iCom_adapter *)dev_id;
+
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    int_reg = iCom_adapter_ptr->base_addr + 0x8024;
+
+    adapter_interrupts = iCom_readl((void *)int_reg);
+    
+    if (adapter_interrupts & 0x00003FFF) {
+      /* port 2 interrupt,  NOTE:  for all ADAPTER_V2, port 2 will be active */
+      iCom_port_info = &iCom_adapter_ptr->port_info[2];
+      port_int_reg = (u16)adapter_interrupts;
+      process_interrupt(port_int_reg, iCom_port_info);
+      check_modem_status(iCom_port_info);
+    }
+    if (adapter_interrupts & 0x3FFF0000) {
+      /* port 3 interrupt */
+      iCom_port_info = &iCom_adapter_ptr->port_info[3];
+      if (iCom_port_info->status == ICOM_PORT_ACTIVE) {
+	  port_int_reg = (u16)(adapter_interrupts >> 16);
+	  process_interrupt(port_int_reg, iCom_port_info);
+	  check_modem_status(iCom_port_info);
+      }
+    }
+    
+    /* Clear out any pending interrupts */
+    iCom_writel(adapter_interrupts,(void *)int_reg);
+    
+    int_reg = iCom_adapter_ptr->base_addr + 0x8004;
+  }
+  else {
+    int_reg = iCom_adapter_ptr->base_addr + 0x4004;
+  }
+
+  adapter_interrupts = iCom_readl((void *)int_reg);
+
+  if (adapter_interrupts & 0x00003FFF) {
+    /* port 0 interrupt, NOTE:  for all adapters, port 0 will be active */
+    iCom_port_info = &iCom_adapter_ptr->port_info[0];
+    port_int_reg = (u16)adapter_interrupts;
+    process_interrupt(port_int_reg, iCom_port_info);
+    check_modem_status(iCom_port_info);
+  }
+  if (adapter_interrupts & 0x3FFF0000) {
+    /* port 1 interrupt */
+    iCom_port_info = &iCom_adapter_ptr->port_info[1];
+    if (iCom_port_info->status == ICOM_PORT_ACTIVE) {
+	port_int_reg = (u16)(adapter_interrupts >> 16);
+	process_interrupt(port_int_reg, iCom_port_info);
+	check_modem_status(iCom_port_info);
+    }
+  }
+  
+  /* Clear out any pending interrupts */
+  iCom_writel(adapter_interrupts,(void *)int_reg);
+  spin_unlock_irqrestore(&iComlock,flags);
+}
+
+/*
+ * -------------------------------------------------------------------
+ * Here ends the serial interrupt routines.
+ * -------------------------------------------------------------------
+ */
+
+/*
+ * This routine is used to handle the "bottom half" processing for the
+ * serial driver, known also the "software interrupt" processing.
+ * This processing is done at the kernel interrupt level, after the
+ * iCom_interrupt() has returned, BUT WITH INTERRUPTS TURNED ON.  This
+ * is where time-consuming activities which can not be done in the
+ * interrupt driver proper are done; the interrupt driver schedules
+ * them using rs_sched_event(), and they get done here.
+ */
+static void do_softint(void *private_)
+{
+  struct iCom_port	*info = (struct iCom_port *) private_;
+  struct tty_struct	*tty;
+
+  tty = info->tty;
+  if (!tty)
+    return;
+
+  if (test_and_clear_bit(0, &info->event)) {
+    if ((tty->flags & (1 << TTY_DO_WRITE_WAKEUP)) && tty->ldisc.write_wakeup)
+      (tty->ldisc.write_wakeup)(tty);
+    wake_up_interruptible(&tty->write_wait);
+    TRACE(info,TRACE_WAKEUP,0);
+  }
+}
+
+/*******************************************/
+/* Diagnostic Check for previous Error Log */
+/*******************************************/
+static u8 __init NoErrorYet(struct iCom_adapter *iCom_adapter_ptr,
+		u8	port)  /* IOA_FAILURE indicates an Adapter failure */
+{
+    if(port == IOA_FAILURE)
+    {
+	if(iCom_adapter_ptr->error_data[0] == 0)
+	    return -1; /* return non zero (TRUE) */
+    }
+    else
+    {
+	if(iCom_adapter_ptr->port_info[port].tpr == 0)
+	   return -1; /* return non zero (TRUE) */
+    }
+
+/* must be an error so return 0 (FALSE) */
+return 0;
+}
+
+/************************************/
+/* Diagnostic Error Logging Routine */
+/************************************/
+void __init diag_error(struct iCom_adapter *iCom_adapter_ptr,
+		u8	port,  /* IOA_FAILURE indicates an Adapter failure */
+		char*	text, /* error message */
+		u32	tpr,
+		u32*	error_data_ptr,
+		u32	num_error_entries)
+
+{
+    u32	i,j;
+    u32 error_data[NUM_ERROR_ENTRIES];
+    u32 *temp32ptr;
+    int version;
+    char port_letter = '0';
+    struct LocationDataStruct *p_location_data = NULL;
+
+    
+    if((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2)
+	version = 1;
+    else
+	version = 2;
+
+    if(port == 0)
+	port_letter = 'A';
+    else if(port == 1)
+	port_letter = 'B';
+    else if(port == 2)
+	port_letter = 'C';
+    else if(port == 3)
+	port_letter = 'D';
+
+    if(NoErrorYet(iCom_adapter_ptr, port))
+    {
+	if(port == IOA_FAILURE)
+	{
+	    iCom_adapter_ptr->tpr = tpr;
+	    temp32ptr = &iCom_adapter_ptr->error_data[0];
+
+	    /* set all ports to failed */
+	    for(j=0;j<NUM_ERROR_ENTRIES;j++)
+	    {
+		iCom_adapter_ptr->port_info[j].passed_diags = 0; /* failed */
+	    }
+
+	    /* find the MIN of num_error_entries and NUM_ERROR_ENTRIES */
+	    if(num_error_entries < NUM_ERROR_ENTRIES)
+		j=num_error_entries;
+	    else
+		j=NUM_ERROR_ENTRIES;
+	    for(i=0;i<j;i++)
+	    {
+		temp32ptr[i] = error_data_ptr[i];
+	    }
+	}
+	else
+	{
+	    iCom_adapter_ptr->port_info[port].tpr = tpr;
+	    temp32ptr = &iCom_adapter_ptr->port_info[port].error_data[0];
+	    iCom_adapter_ptr->port_info[port].passed_diags = 0;/* failed */
+
+	    /* find the MIN of num_error_entries and NUM_ERROR_ENTRIES */
+	    if(num_error_entries < NUM_ERROR_ENTRIES)
+		j=num_error_entries;
+	    else
+		j=NUM_ERROR_ENTRIES;
+	    for(i=0;i<j;i++)
+	    {
+		temp32ptr[i] = error_data_ptr[i];
+	    }
+	}
+
+	/* copy error data into local array */
+	for(i=0;i<j;i++)
+	    error_data[i] = error_data_ptr[i];
+
+	/* set the rest of the error data array to zero */
+	for(;i<NUM_ERROR_ENTRIES;i++)
+	    error_data[i] = 0;
+
+	printk("iCom: ERROR #################################################\n");
+#ifdef CONFIG_PPC_ISERIES
+	p_location_data = iSeries_GetLocationData(iCom_adapter_ptr->pci_dev);
+
+	if (p_location_data != NULL)
+	{
+	    printk("iCom: Frame ID: %d Card Position: %s.\n",p_location_data->FrameId, p_location_data->CardLocation);
+	    kfree(p_location_data);
+	}
+#endif
+	if(port == IOA_FAILURE)
+	    printk("iCom: Version %d adapter failed.\n",version);
+	else
+	    printk("iCom: Version %d adapter port %c failed.\n",version, port_letter);
+	printk("iCom: ");
+	printk(text); /* print text */
+	/* print sixteen words of error data */
+	printk("iCom: %.8x %.8x %.8x %.8x\n", tpr, error_data[0], error_data[1], error_data[2]);
+	printk("iCom: %.8x %.8x %.8x %.8x\n", error_data[3], error_data[4], error_data[5], error_data[6]);
+	printk("iCom: %.8x %.8x %.8x %.8x\n", error_data[7], error_data[8], error_data[9], error_data[10]);
+	printk("iCom: %.8x %.8x %.8x %.8x\n", error_data[11], error_data[12], error_data[13], error_data[14]);
+
+	printk("iCom: ERROR #################################################\n");
+    }
+    return;
+}
+
+/**************************************************/
+/* Diagnostic Interrupt register checking routine */
+/**************************************************/
+static u32 __init diag_check_ints(struct iCom_adapter *iCom_adapter_ptr,
+		    u32 exp_int1,		/* expected int one value */
+		    u32 exp_int2,		/* expected int two value */
+		    char *msg,			/* message to put in error log */
+		    u32 tpr)			/* TPR to put in error log */
+{
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+
+#define PORT_A_INT_MASK 0x0000FFFF
+#define PORT_B_INT_MASK 0xFFFF0000
+
+
+    u32	status = 0;	/* pass/fail */
+
+    if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2){
+    /* check port A */
+	if((iCom_adapter_ptr->diag_int_reset1 & PORT_A_INT_MASK )
+	   != (exp_int1 & PORT_A_INT_MASK))
+	{
+	    error_data[0] = exp_int1;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset1;
+
+	    diag_error(iCom_adapter_ptr,
+		       0,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+    /* check port B */
+	if((iCom_adapter_ptr->diag_int_reset1 & PORT_B_INT_MASK )
+	   != (exp_int1 & PORT_B_INT_MASK))
+	{
+	    error_data[0] = exp_int1;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset1;
+
+	    diag_error(iCom_adapter_ptr,
+		       1,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+    }
+    else { /* this is a four port adapter */
+	if((iCom_adapter_ptr->diag_int_reset1 & PORT_A_INT_MASK )
+	   != (exp_int1 & PORT_A_INT_MASK))
+	{
+	    error_data[0] = exp_int1;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset1;
+
+	    diag_error(iCom_adapter_ptr,
+		       0,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+    /* check port B */
+	if((iCom_adapter_ptr->diag_int_reset1 & PORT_B_INT_MASK )
+	   != (exp_int1 & PORT_B_INT_MASK))
+	{
+	    error_data[0] = exp_int1;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset1;
+
+	    diag_error(iCom_adapter_ptr,
+		       1,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+        /* check port C */
+	if((iCom_adapter_ptr->diag_int_reset2 & PORT_A_INT_MASK )
+	   != (exp_int2 & PORT_A_INT_MASK))
+	{
+	    error_data[0] = exp_int2;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset2;
+
+	    diag_error(iCom_adapter_ptr,
+		       2,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+    /* check port D */
+	if((iCom_adapter_ptr->diag_int_reset2 & PORT_B_INT_MASK )
+	   != (exp_int2 & PORT_B_INT_MASK))
+	{
+	    error_data[0] = exp_int2;
+	    error_data[1] = iCom_adapter_ptr->diag_int_reset2;
+
+	    diag_error(iCom_adapter_ptr,
+		       3,  /* IOA_FAILURE indicates an Adapter failure */
+		       msg, /* error message */
+		       tpr,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       2);	/* number of error words. */
+	    status = -1;
+	}
+    }
+    return status;
+}
+
+/*************************************/
+/* Diagnostic Initialization Routine */
+/*************************************/
+static u32 __init diag_init(struct iCom_adapter *iCom_adapter_ptr)
+{
+u8	i,j;
+u32	rc;
+
+/* init rc to zero */
+rc = 0;
+
+iCom_adapter_ptr->tpr = DIAG_INIT_ERROR_DATA;
+/* set the TPR and error data to zero for the adapter */
+for(j=0;j<NUM_ERROR_ENTRIES;j++)
+{
+    iCom_adapter_ptr->error_data[j] = 0;
+}
+
+/* set the TPR and error data to zero for all ports */
+for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+{
+    iCom_adapter_ptr->port_info[i].tpr = 0;
+    iCom_adapter_ptr->port_info[i].passed_diags = 0; /* false */
+    for(j=0;j<NUM_ERROR_ENTRIES;j++)
+    {
+	iCom_adapter_ptr->port_info[i].error_data[j] = 0;
+    }
+}
+
+/* Set the global register value */
+iCom_adapter_ptr->tpr = DIAG_INIT_SET_GLOBAL_REGS;
+if (iCom_adapter_ptr->version == ADAPTER_V1) {
+    iCom_adapter_ptr->port_info[0].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x4000);
+    iCom_adapter_ptr->port_info[1].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x4000);
+}
+else {
+    iCom_adapter_ptr->port_info[0].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x8000);
+    iCom_adapter_ptr->port_info[1].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x8000);
+    iCom_adapter_ptr->port_info[2].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x8000);
+    iCom_adapter_ptr->port_info[3].global_reg = (struct iCom_regs *)((char *)iCom_adapter_ptr->base_addr + 0x8000);
+}
+
+iCom_adapter_ptr->tpr = DIAG_INIT_END;
+return rc;
+}
+
+
+/********************************/
+/* Diagnostic Interrupt handler */
+/********************************/
+void __init diag_int_handler(int irq, void * dev_id, struct pt_regs * regs)
+{
+  u8                  temp_8;
+  u32                 temp_32;
+  struct iCom_adapter *iCom_adapter_ptr;
+  /* find iCom_port_info for this interrupt */
+  iCom_adapter_ptr = (struct iCom_adapter *)dev_id;
+
+
+  /* if BAR0 is 0, then this must be a BIST */
+  /* int so reset the config regs           */
+  pci_read_config_dword(iCom_adapter_ptr->pci_dev, PCI_COMMAND, &temp_32);
+
+  /* if memory space is not enabled, this must be the BIST interrupt */
+  /* note that version 2 adapters will already be set up so it will  */
+  /* skip this piece of code.                                        */
+  if(!(temp_32 & 0x00000002))
+  {
+      /* restore the config registers */
+      pci_write_config_dword(iCom_adapter_ptr->pci_dev,PCI_COMMAND, iCom_adapter_ptr->saved_command_reg);
+      pci_write_config_dword(iCom_adapter_ptr->pci_dev,0x44, 0x8300830A);
+      pci_write_config_dword(iCom_adapter_ptr->pci_dev, PCI_BASE_ADDRESS_0, iCom_adapter_ptr->saved_bar);
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_INTHANDLE_START;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    iCom_adapter_ptr->diag_int1 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+    iCom_adapter_ptr->diag_int2 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+    iCom_adapter_ptr->diag_int_pri1 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x800C));
+    iCom_adapter_ptr->diag_int_pri2 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x802C));
+  }
+  else{
+    iCom_adapter_ptr->diag_int1 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+    iCom_adapter_ptr->diag_int2 = 0; /* does not exist */
+    iCom_adapter_ptr->diag_int_pri1 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x400C));
+    iCom_adapter_ptr->diag_int_pri2 = 0x80; /* no int */
+  }
+
+  /* if pri1 has an int set, clear that int */
+  iCom_adapter_ptr->tpr = DIAG_INTHANDLE_CHECK_FOR_INTS;
+  if(iCom_adapter_ptr->diag_int_pri1 != 0x80) { /* 0x80 means "no int" */
+    temp_8 = iCom_adapter_ptr->diag_int_pri1 >> 2; /* determine how many bits to shift left */
+    temp_32 = 1;
+    temp_32 <<= temp_8; /* shift a 1 that many bits */
+    iCom_adapter_ptr->diag_int_reset1 |= temp_32; /* record the int being reset */
+    if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2)
+    {
+	iCom_writel(temp_32,(void*)iCom_adapter_ptr->base_addr+0x8004); /* reset int */
+    }
+    else
+      iCom_writel(temp_32,(void*)iCom_adapter_ptr->base_addr+0x4004); /* reset int */
+  }
+  else /* clear the int from pri2 */{
+    temp_8 = iCom_adapter_ptr->diag_int_pri2 >> 2; /* determine how many bits to shift left */
+    temp_32 = 1;
+    temp_32 <<= temp_8; /* shift a 1 that many bits */
+    iCom_adapter_ptr->diag_int_reset2 |= temp_32; /* record the int being reset */
+    iCom_writel(temp_32,(void*)iCom_adapter_ptr->base_addr+0x8024); /* reset int */
+  }
+  iCom_adapter_ptr->tpr = DIAG_INTHANDLE_END;
+  return;
+
+}
+
+
+/************************/
+/* Diagnostic BIST Test */
+/************************/
+static u32 __init diag_bist(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           rc;
+  u32           temp_32;
+  u8            i;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  int		done;
+
+  iCom_adapter_ptr->tpr = DIAG_BIST_CLEAR_INTS;
+  /* make sure the all ints are cleared before we start */
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+    iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x8024));
+  }
+  else{
+    iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+  }
+
+  /* version 2 adapters must not cause a real int because */
+  /* accesses to any register immediately after BIST will */
+  /* fail. We must first mask the ints, cause BIST to     */
+  /* happen, then unmask the int to get to the interrupt  */
+  /* handler. Note that version 1 adapters can not mask   */
+  /* off the BIST int.                                    */
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+      temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8008));
+      temp_32 |= 0x80000000;
+      iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8008));
+  }
+
+  /**** save registers that will have to be restored ****/
+  
+  /* get the control reg */
+  iCom_adapter_ptr->tpr = DIAG_BIST_SAVE_CMD_REG;
+  if((rc = pci_read_config_dword(iCom_adapter_ptr->pci_dev, PCI_COMMAND, &iCom_adapter_ptr->saved_command_reg)))
+  {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Failed to read command reg.\n", /* error message */
+		 DIAG_BIST_READ_CMD_REG_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+
+  /* get the memory bar reg */
+  iCom_adapter_ptr->tpr = DIAG_BIST_SAVE_BAR0_REG;
+  if((rc = pci_read_config_dword(iCom_adapter_ptr->pci_dev, PCI_BASE_ADDRESS_0, &iCom_adapter_ptr->saved_bar)))
+  {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Failed to read BAR 0.\n", /* error message */
+		 DIAG_BIST_READ_BAR0_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+
+  /**** hook in the interrupt handler ****/
+
+  iCom_adapter_ptr->diag_int1 = 0; /* init */
+  
+  /* save off irq and request irq line */
+  iCom_adapter_ptr->tpr = DIAG_BIST_HOOK_INT_LINE;
+  if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+                  SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Failed to hook int handler.\n", /* error message */
+		 DIAG_BIST_HOOK_INT_HANDLER_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+  else
+      iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /**** start bist ****/
+  iCom_adapter_ptr->tpr = DIAG_BIST_START_BIST;
+  if((rc = pci_write_config_dword(iCom_adapter_ptr->pci_dev, 0x0C, 0x40000000))) {
+    printk("iCom: Diagnostic failed. CD700230, %8x.\n", rc);
+    error_data[0] = rc;
+
+    diag_error(iCom_adapter_ptr,
+	       IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+	       "iCom: Bist Test. Failed to write BIST reg.\n", /* error message */
+	       DIAG_BIST_START_BIST_FAIL, /* tpr */
+	       error_ptr,		/* error data pointer */
+	       1);	/* number of error words. */
+    return 0x0000000f;
+  }
+
+  /* set done to zero before the check for version 2 below */
+  done = 0;
+
+  /* restore the config registers on version 2 adapters */
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+
+    /**** wait 2 seconds ****/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ*2); /* HZ = 1 second */
+
+    /* restore the config registers */
+    pci_write_config_dword(iCom_adapter_ptr->pci_dev,PCI_COMMAND, iCom_adapter_ptr->saved_command_reg);
+    pci_write_config_dword(iCom_adapter_ptr->pci_dev,0x44, 0x42004200);
+    pci_write_config_dword(iCom_adapter_ptr->pci_dev,0x48, 0x42004200);
+    pci_write_config_dword(iCom_adapter_ptr->pci_dev, PCI_BASE_ADDRESS_0, iCom_adapter_ptr->saved_bar);
+
+    /* now clear the mask bit to force the interrupt to happen */
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8008));
+    temp_32 = temp_32 & 0x7FFFFFFF;
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8008));
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/10); /* HZ = 1 second */
+    done = 1;
+  }
+
+  /**** wait for 6 seconds or int ****/
+  iCom_adapter_ptr->tpr = DIAG_BIST_WAIT_FOR_INT;
+  for(i=0;((i<60) && (done==0));i++)
+  {
+    if(iCom_adapter_ptr->diag_int1 == 0)
+    {
+      /* wait 100ms */
+      current->state = TASK_INTERRUPTIBLE;
+      schedule_timeout(HZ/10); /* HZ = 1 second */
+    }
+    else
+    {
+	done = 1;
+    }
+  }
+
+  /* if not passed */
+  iCom_adapter_ptr->tpr = DIAG_BIST_CHECK_INT_REGS;
+  if(iCom_adapter_ptr->diag_int1 != 0x80000000) {
+      error_data[0] = iCom_adapter_ptr->diag_int1;
+      error_data[1] = i;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Bist int did not occur.\n", /* error message */
+		 DIAG_BIST_NO_BIST_INT_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 2);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+  
+  /**** see if bist completed successfully ****/
+  iCom_adapter_ptr->tpr = DIAG_CHECK_READ_BIST_REG;
+  if((rc = pci_read_config_dword(iCom_adapter_ptr->pci_dev, 0x0C, &temp_32)))
+  {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Unable to read config offset 0xC.\n", /* error message */
+		 DIAG_BIST_READ_CONFIG_OFFSET_0C_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+
+  /**** check tha the MISR and PRPG are not set ****/
+  iCom_adapter_ptr->tpr = DIAG_BIST_CHECK_MISR_AND_PRPG;
+  if(temp_32 & 0x03000000) {
+      error_data[0] = temp_32;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Bist Test. Incorrect MISR or PRPG.\n", /* error message */
+		 DIAG_BIST_INCORRECT_MISR_OR_PRPG,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f; /* fail the whole adapter */
+  }
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_BIST_RETURN_INT_HANDLE;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  return 0; /* pass */
+}
+
+/***************/
+/* memory test */
+/***************/
+static u32 __init mem_test(struct iCom_adapter *iCom_adapter_ptr,
+	     u32 port,
+	     unsigned long addr,
+	     u32 len)
+{
+  u8            data;
+  u8		actual_data;
+  u32           i;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+
+  /**** addr tag test ****/
+
+  /* set the memory */
+  iCom_adapter_ptr->tpr = DIAG_MEM_WRITE_ADDR_TAG;
+  data = 0;
+  for(i=0;i<len;i++)
+  {
+    iCom_writeb(data, (void*)(addr + i));
+    ++data;
+    if(data == 254)
+      data = 0;
+  }
+
+  /* check the memory */
+  iCom_adapter_ptr->tpr = DIAG_MEM_CHECK_ADDR_TAG;
+  data = 0;
+  for(i=0;i<len;i++)
+  {
+      actual_data = iCom_readb((void*)(addr + i));
+
+      if(actual_data != data)
+      {
+	  error_data[0] = (u32)data;
+	  error_data[1] = (u32)actual_data;
+	  error_data[2] = (u32)addr;
+	  error_data[3] = (u32)addr+i;
+
+	  diag_error(iCom_adapter_ptr,
+		     port,  /* IOA_FAILURE indicates an Adapter failure */
+		     "iCom: Memory Test. Address Tag Test Failed.\n", /* error message */
+		     DIAG_MEM_ADDR_TAG_TEST_FAILED,		/* tpr */
+		     error_ptr,		/* error data pointer */
+		     4);	/* number of error words. */
+	  return -1; /* fail the whole adapter */
+      }
+      ++data;
+      if(data == 254)
+	  data = 0;
+  }
+
+  /**** write A's ****/
+  
+  iCom_adapter_ptr->tpr = DIAG_MEM_WRITE_AA;
+  for(i=0;i<len;i++)
+  {
+    iCom_writeb(0xAA, (void*)(addr + i));
+  }
+  
+  iCom_adapter_ptr->tpr = DIAG_MEM_CHECK_AA;
+  for(i=0;i<len;i++)
+  {
+    actual_data = iCom_readb((void*)(addr + i));
+    if(actual_data != 0xAA)
+    {
+	error_data[0] = 0x000000AA;
+	error_data[1] = (u32)actual_data;
+	error_data[2] = (u32)addr;
+	error_data[3] = (u32)addr+i;
+
+	diag_error(iCom_adapter_ptr,
+		   port,  /* IOA_FAILURE indicates an Adapter failure */
+		   "iCom: Memory Test. 0xAA Test Failed.\n", /* error message */
+		   DIAG_MEM_AA_TEST_FAIL,		/* tpr */
+		   error_ptr,		/* error data pointer */
+		   4);	/* number of error words. */
+	return -1;
+    }
+    iCom_writeb(0x55, (void*)(addr + i));
+  }
+
+  /**** check for 55's ****/
+  
+  iCom_adapter_ptr->tpr = DIAG_MEM_CHECK_55;
+  for(i=0;i<len;i++)
+  {
+    actual_data = iCom_readb((void*)(addr + i));
+    if(actual_data != 0x55)
+    {
+	error_data[0] = 0x00000055;
+	error_data[1] = (u32)actual_data;
+	error_data[2] = (u32)addr;
+	error_data[3] = (u32)addr+i;
+
+	diag_error(iCom_adapter_ptr,
+		   port,  /* IOA_FAILURE indicates an Adapter failure */
+		   "iCom: Memory Test. 0x55 Test Failed.\n", /* error message */
+		   DIAG_MEM_55_TEST_FAIL,		/* tpr */
+		   error_ptr,		/* error data pointer */
+		   4);	/* number of error words. */
+	return -1;
+    }
+    iCom_writeb(0, (void*)(addr + i));
+  }
+
+  /**** check for 00's ****/
+  
+  iCom_adapter_ptr->tpr = DIAG_MEM_CHECK_00;
+  for(i=0;i<len;i++)
+  {
+    actual_data = iCom_readb((void*)(addr + i));
+    if(actual_data != 0)
+    {
+	error_data[0] = 0x00000000;
+	error_data[1] = (u32)actual_data;
+	error_data[2] = (u32)addr;
+	error_data[3] = (u32)addr+i;
+
+	diag_error(iCom_adapter_ptr,
+		   port,  /* IOA_FAILURE indicates an Adapter failure */
+		   "iCom: Memory Test. 0x00 Test Failed.\n", /* error message */
+		   DIAG_MEM_00_TEST_FAIL,		/* tpr */
+		   error_ptr,		/* error data pointer */
+		   4);	/* number of error words. */
+	return -1;
+    }
+  }
+return 0;
+}
+
+
+/**************************/
+/* Diagnostic Memory test */
+/**************************/
+static u32 __init diag_memory(struct iCom_adapter *iCom_adapter_ptr)
+{
+  u8            i;
+  u32           port;
+  u32           rc;
+  unsigned long mem1_addr, mem2_addr;
+  u32           mem1_len, mem2_len;
+  struct iCom_port    *iCom_port_info;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+
+
+  /* for the number of ports on adapter */
+  iCom_adapter_ptr->tpr = DIAG_MEM_FOR_EVERY_PORT;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+  {
+
+  /* Stop processor to make sure the instruction ram can be tested */
+      iCom_adapter_ptr->tpr = DIAG_MEM_STOP_PROC;
+      iCom_port_info = &iCom_adapter_ptr->port_info[i];
+      stop_processor(iCom_port_info);
+
+      switch (i)
+      {
+	  case 0: /* this port exists on all adpaters */
+	      port = 1; /* bit for port 0 */
+	      mem1_addr = iCom_adapter_ptr->base_addr;
+	      mem1_len = 0x200;
+	      mem2_addr = iCom_adapter_ptr->base_addr + 0x1000;
+	      if(iCom_adapter_ptr->version == ADAPTER_V1)
+		  mem2_len = 0xc00;
+	      else
+		  mem2_len = 0x1000;
+	      break;
+
+	  case 1:
+	      if(iCom_adapter_ptr->port_info[1].status == ICOM_PORT_ACTIVE) {
+		  port = 2; /* bit for port 1 */
+		  mem1_addr = iCom_adapter_ptr->base_addr + 0x2000;
+		  mem1_len = 0x200;
+		  mem2_addr = iCom_adapter_ptr->base_addr + 0x3000;
+		  if(iCom_adapter_ptr->version == ADAPTER_V1)
+		      mem2_len = 0xc00;
+		  else
+		      mem2_len = 0x1000;
+	      }
+	      else
+		  continue;
+	      break;
+
+	  case 2: /* if four ports exist, this one will be valid */
+	      port = 4; /* bit for port 2 */
+	      mem1_addr = iCom_adapter_ptr->base_addr + 0x4000;
+	      mem1_len = 0x200;
+	      mem2_addr = iCom_adapter_ptr->base_addr + 0x5000;
+	      mem2_len = 0x1000;
+	      break;
+
+	  case 3:
+	      if(iCom_adapter_ptr->port_info[3].status == ICOM_PORT_ACTIVE) {
+		  port = 8; /* bit for port 3 */
+		  mem1_addr = iCom_adapter_ptr->base_addr + 0x6000;
+		  mem1_len = 0x200;
+		  mem2_addr = iCom_adapter_ptr->base_addr + 0x7000;
+		  mem2_len = 0x1000;
+	      }
+	      else
+		  continue;
+	      break;
+	  default:
+	      {
+		  error_data[0] = i;
+
+		  diag_error(iCom_adapter_ptr,
+			     IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+			     "iCom: Memory Test. Invalid Port Select.\n", /* error message */
+			     DIAG_MEM_INVALID_PORT_SELECT,		/* tpr */
+			     error_ptr,		/* error data pointer */
+			     1);	/* number of error words. */
+		  return -1;
+	      }
+
+      } /* end of switch */
+
+    /* test the memory */
+      iCom_adapter_ptr->tpr = DIAG_MEM_CALL_MEM_TEST;
+      if((rc = mem_test(iCom_adapter_ptr, port, mem1_addr, mem1_len)))
+	  return port;
+      if((rc = mem_test(iCom_adapter_ptr, port, mem2_addr, mem2_len)))
+	  return port;
+
+  } /* end of for loop */
+
+  /* pass */
+  return 0;
+
+} /* end of function */
+
+/*****************************/
+/* Diagnostic Port Load Code */
+/*****************************/
+void __init diag_port_load(int port,
+		    struct iCom_port *iCom_port_info,
+		    long int base_addr,
+		    unsigned char *code,
+		    int size)
+{
+    char               *iram_ptr;
+    int                index;
+    char              *dram_ptr;
+
+
+    /* point to the DRAM */
+    dram_ptr = (char*)(base_addr + 0x2000*port);
+
+
+  /* Stop processor */
+    iCom_port_info->tpr = DIAG_LOAD_STOP_PROC;
+    stop_processor(iCom_port_info);
+
+    /* Zero out DRAM */
+    iCom_port_info->tpr = DIAG_LOAD_ZERO_DRAM;
+    for (index = 0; index < 512; index++)
+    {
+	iCom_writeb(0x00,&dram_ptr[index]);
+    }
+
+    /* Load Code into Adapter */
+    iCom_port_info->tpr = DIAG_LOAD_LOAD_CODE;
+    iram_ptr = (char *)dram_ptr + ICOM_IRAM_OFFSET;
+    for (index = 0; index < size; index++)
+    {
+	iCom_writeb(code[index],&iram_ptr[index]);
+    }
+
+    iCom_port_info->tpr = DIAG_LOAD_DONE;
+    iCom_port_info->tpr = 0; /* must set this to zero when done */
+                             /* for IfNoError to work.          */
+    return ;
+} 
+ 
+/*****************************/
+/* Diagnostic Interrupt Test */
+/*****************************/
+static u32 __init diag_int(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           temp_32;
+  u32           rc;
+  u8            i;
+  u32           exp_int1, exp_int2;
+  u8            done;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  
+  /* make sure the all ints are cleared before we start */
+  iCom_adapter_ptr->tpr = DIAG_INT_CLEAR_INTS;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8024)); 
+  }
+  else{
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+  }
+  
+  /* save off irq and request irq line */
+  iCom_adapter_ptr->tpr = DIAG_INT_GET_INT_HANDLE;
+  if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+		       SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Interrupt Test. request_irq call failed.\n", /* error message */
+		 DIAG_INT_GET_IRQ_LINE_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f;
+  }
+  else
+      iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /* load all active procs */
+  iCom_adapter_ptr->tpr = DIAG_INT_LOAD_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	diag_port_load(i,
+		       &iCom_adapter_ptr->port_info[i],
+                       iCom_adapter_ptr->base_addr,
+		       topcidiag, 
+		       sizeof(topcidiag));
+
+  /* clear int regs */
+  iCom_adapter_ptr->tpr = DIAG_INT_CLEAR_INT_VARS;
+  iCom_adapter_ptr->diag_int_reset1 = 0;
+  iCom_adapter_ptr->diag_int_reset2 = 0;
+
+  /* unmask all interrupts */
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2)
+      iCom_writel(0,(void*)(iCom_adapter_ptr->base_addr+0x4008));
+  else
+  {
+      iCom_writel(0,(void*)(iCom_adapter_ptr->base_addr+0x8008));
+      iCom_writel(0,(void*)(iCom_adapter_ptr->base_addr+0x8028));
+  }
+      
+  
+
+  /* Start all procs */
+  iCom_adapter_ptr->tpr = DIAG_INT_START_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+    {
+	start_processor(&iCom_adapter_ptr->port_info[i]);
+    }
+  
+  /* Start all tests */
+  iCom_adapter_ptr->tpr = DIAG_INT_START_TESTS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+    {
+	/* 0x06 is the interrupt test */
+	iCom_writeb((0x06),(void*)(iCom_adapter_ptr->base_addr+i*0x2000)); 
+    }
+
+  /* determine what ints to look for */
+  iCom_adapter_ptr->tpr = DIAG_INT_DETERMINE_INTS_TO_CHECK_FOR;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2) {
+    exp_int1 = (u32)0x3FFF3FFF;
+    exp_int2 = 0;
+  }
+  else {
+    exp_int1 = 0x00003FFF;
+    exp_int2 = 0x00003FFF;
+    if(iCom_adapter_ptr->port_info[1].status == ICOM_PORT_ACTIVE)
+      exp_int1 |= 0x3FFF0000;
+    if(iCom_adapter_ptr->port_info[3].status == ICOM_PORT_ACTIVE)
+      exp_int2 |= 0x3FFF0000;
+  }
+
+
+
+  /* wait for all ints to set or for a second to pass */
+  done = 0;
+  temp_32 = 0;
+  iCom_adapter_ptr->tpr = DIAG_INT_WAIT_FOR_INTS;
+  while((!done) && (temp_32 < 100))
+  {
+    if(((iCom_adapter_ptr->diag_int_reset1 & 0x3fff3fff) == exp_int1) &&
+       ((iCom_adapter_ptr->diag_int_reset2 & 0x3fff3fff) == exp_int2))
+      done = 1;
+
+    /**** wait 1 millisecond ****/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/100); /* HZ = 1 second */
+    temp_32++;
+  }
+
+  temp_32 = 0;
+
+  iCom_adapter_ptr->tpr = DIAG_INT_CHECK_FOR_INTS;
+  if(!done) /* failed */ 
+  {
+      /* fail the appropriate port/s */
+      rc = diag_check_ints(iCom_adapter_ptr,
+			   exp_int1,
+			   exp_int2,
+			   "Interrupt Test. Int did not occur.\n",
+			   DIAG_INT_WRONG_INT_FAIL);
+      return -1;
+  }
+
+  
+  /* stop all procs */
+  iCom_adapter_ptr->tpr = DIAG_INT_STOP_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_INT_FREE_INT_HANDLE;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  iCom_adapter_ptr->tpr = DIAG_INT_END;
+return 0; 
+} 
+ 
+/*******************************/
+/* Diagnostic Instruction Test */
+/*******************************/
+static u32 __init diag_inst(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           temp_32;
+  u32           rc;
+  u8            i;
+  u32           exp_int1, exp_int2;
+  u8            done;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  dma_addr_t	dma_addr_pci;
+  int		addr1, addr2, addr3, addr4;
+  
+  /* make sure the all ints are cleared before we start */
+  iCom_adapter_ptr->tpr = DIAG_INST_CLEAR_INTS;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8024)); 
+  }
+  else{
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+  }
+  
+  /* save off irq and request irq line */
+  iCom_adapter_ptr->tpr = DIAG_INST_GET_INT_HANDLE;
+  if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+		       SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: Instruction Test. request_irq call failed.\n", /* error message */
+		 DIAG_INST_GET_IRQ_LINE_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f;
+  }
+  else
+      iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /* load all active procs */
+  iCom_adapter_ptr->tpr = DIAG_INST_LOAD_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	diag_port_load(i,
+		       &iCom_adapter_ptr->port_info[i],
+                       iCom_adapter_ptr->base_addr,
+		       toinstdiag, 
+		       sizeof(toinstdiag));
+
+  /* clear int regs */
+  iCom_adapter_ptr->tpr = DIAG_INST_CLEAR_INT_VARS;
+  iCom_adapter_ptr->diag_int_reset1 = 0;
+  iCom_adapter_ptr->diag_int_reset2 = 0;
+
+  /* get storage to DMA to and from. Need 8 bytes for each port */
+  iCom_adapter_ptr->malloc_addr_1 = (u32 *)kmalloc(32,GFP_KERNEL | GFP_DMA);
+  iCom_adapter_ptr->resources |= HAVE_MALLOC_1; /* indicate we have storage from malloc */
+  dma_addr_pci = pci_map_single(iCom_adapter_ptr->pci_dev,
+				iCom_adapter_ptr->malloc_addr_1,	/* addr to convert */
+				32,		/* size */
+				PCI_DMA_BIDIRECTIONAL);
+  addr1 = (int)dma_addr_pci;
+  addr2 = addr1+8;
+  addr3 = addr1+16;
+  addr4 = addr1+24;
+
+  iCom_writel(addr1,(void*)(iCom_adapter_ptr->base_addr+8)); 
+  iCom_writel(addr2,(void*)(iCom_adapter_ptr->base_addr+8+0x2000)); 
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2){ 
+      iCom_writel(addr3,(void*)(iCom_adapter_ptr->base_addr+8+0x4000)); 
+      iCom_writel(addr4,(void*)(iCom_adapter_ptr->base_addr+8+0x6000));
+  }
+
+  /* start all procs */
+  iCom_adapter_ptr->tpr = DIAG_INST_START_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      start_processor(&iCom_adapter_ptr->port_info[i]);
+  
+  /* Start all tests */
+  iCom_adapter_ptr->tpr = DIAG_INST_START_TESTS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	iCom_writeb((0x01),(void*)(iCom_adapter_ptr->base_addr+i*0x2000)); 
+
+  /* determine what ints to look for */
+  iCom_adapter_ptr->tpr = DIAG_INST_DETERMINE_INTS_TO_CHECK_FOR;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2) {
+    exp_int1 = (u32)0x01000100;
+    exp_int2 = 0;
+  }
+  else {
+      if(iCom_adapter_ptr->subsystem_id == FOUR_PORT_MODEL)
+      {
+	  exp_int1 = 0x01000100;
+	  exp_int2 = 0x01000100;
+      }
+      else /* must be a two port model */
+      {
+	  exp_int1 = 0x00000100;
+	  exp_int2 = 0x00000100;
+      }
+  }
+  
+  /* wait for all ints to set or for a second to pass */
+  done = 0;
+  temp_32 = 0;
+  iCom_adapter_ptr->tpr = DIAG_INST_WAIT_FOR_INTS;
+  while((!done) && (temp_32 < 1000))
+  {
+    if((iCom_adapter_ptr->diag_int_reset1 == exp_int1) &&
+       (iCom_adapter_ptr->diag_int_reset2 == exp_int2))
+      done = 1;
+
+    /**** wait 1 millisecond ****/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/1000); /* HZ = 1 second */
+    temp_32++;
+  }
+
+  temp_32 = 0;
+
+
+  iCom_adapter_ptr->tpr = DIAG_INST_CHECK_FOR_INTS;
+  if(!done) /* failed */
+  {
+      /* fail the appropriate port/s */
+      rc = diag_check_ints(iCom_adapter_ptr,
+			   exp_int1,
+			   exp_int2,
+			   "Instruction Test. Int did not occur.\n",
+			   DIAG_INST_WRONG_INT_FAIL);
+      return -1;
+  }
+  
+  /* stop all procs */
+  iCom_adapter_ptr->tpr = DIAG_INST_STOP_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** check the results data ****/
+   iCom_adapter_ptr->tpr = DIAG_INST_CHECK_RESULTS;
+   for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+       if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+
+       {
+      /* read the results word */
+	   temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+(i*0x2000)+4));
+
+      /* check results word */
+	   if(temp_32 != 0)
+	   {
+	       error_data[0] = temp_32;
+
+	       diag_error(iCom_adapter_ptr,	/* port to fail */
+			  i,  /* IOA_FAILURE indicates an Adapter failure */
+			  "iCom: Instruction Test. Bad results data.\n", /* error message */
+			  DIAG_INST_BAD_RESULTS_DATA_FAIL,		/* tpr */
+			  error_ptr,		/* error data pointer */
+			  1);	/* number of error words. */
+	       return 0x0000000f;
+	   }
+       }
+
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_INST_FREE_INT_HANDLE;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  /**** return storage from malloc ****/
+  kfree(iCom_adapter_ptr->malloc_addr_1);
+  iCom_adapter_ptr->resources &= ~HAVE_MALLOC_1;
+
+  iCom_adapter_ptr->tpr = DIAG_INST_END;
+return 0; 
+} 
+
+/****************************/
+/* Diagnostic Register Test */
+/****************************/
+static u32 __init diag_reg(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           temp_32;
+  u32           rc;
+  u8            i;
+  u32           exp_int1, exp_int2;
+  u8            done;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  u32		itteration;
+  u8		port;
+
+
+  /* make sure the all ints are cleared before we start */
+      iCom_adapter_ptr->tpr = DIAG_REG_CLEAR_INTS+itteration;
+      if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+	  temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+	  iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+	  temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+	  iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8024)); 
+      }
+      else{
+	  temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+	  iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+      }
+
+  /* save off irq and request irq line */
+      iCom_adapter_ptr->tpr = DIAG_REG_GET_INT_HANDLE+itteration;
+      if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+			   SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+	  error_data[0] = rc;
+
+	  diag_error(iCom_adapter_ptr,
+		     IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		     "iCom: Register Test. request_irq call failed.\n", /* error message */
+		     DIAG_REG_GET_IRQ_LINE_FAIL+itteration,		/* tpr */
+		     error_ptr,		/* error data pointer */
+		     1);	/* number of error words. */
+	  return 0x0000000f;
+      }
+      else
+	  iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /* Do this test twice, once for each pico code load */
+  for(itteration=0;itteration<200;itteration+=100)
+  {
+
+  /* load all active procs */
+      iCom_adapter_ptr->tpr = DIAG_REG_LOAD_PROCS+itteration;
+      for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+      {
+	  if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	  {
+	      if(itteration == 0) /* if first time through this code */
+	      {
+		  diag_port_load(i,
+				 &iCom_adapter_ptr->port_info[i],
+				 iCom_adapter_ptr->base_addr,
+				 toregdiag, 
+				 sizeof(toregdiag));
+	      }
+	      else
+	      {
+		  diag_port_load(i,
+				 &iCom_adapter_ptr->port_info[i],
+				 iCom_adapter_ptr->base_addr,
+				 toreg2diag, 
+				 sizeof(toreg2diag));
+	      }
+	  }
+      }
+
+
+  /* clear int regs */
+      iCom_adapter_ptr->tpr = DIAG_REG_CLEAR_INT_VARS+itteration;
+      iCom_adapter_ptr->diag_int_reset1 = 0;
+      iCom_adapter_ptr->diag_int_reset2 = 0;
+
+  /* if the port number is 1 or 3, put 1 in the port field(offset 8) */
+  /* The pico code needs to know if it is the second port of two.    */
+      for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+      {
+	  port = iCom_adapter_ptr->port_info[i].port;
+	  if((port == 1) || (port == 3))
+	      iCom_writel(1,(void*)(iCom_adapter_ptr->base_addr+(port*0x2000)+8));
+      }
+
+  /* start all procs */
+      iCom_adapter_ptr->tpr = DIAG_REG_START_PROCS+itteration;
+      for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+	  if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	      start_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** send the command to start the pico code ****/
+      for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+	  if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	      iCom_writeb(1,(void*)(iCom_adapter_ptr->base_addr+(i*0x2000)));
+
+
+  /**** determine what ints to look for ****/
+   iCom_adapter_ptr->tpr = DIAG_REG_DETERMINE_INTS_TO_CHECK_FOR+itteration;
+   if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2) {
+       exp_int1 = (u32)0x01000100;
+       exp_int2 = 0;
+   }
+  else {
+      if(iCom_adapter_ptr->subsystem_id == FOUR_PORT_MODEL)
+      {
+	  exp_int1 = 0x01000100;
+	  exp_int2 = 0x01000100;
+      }
+      else /* must be a two port model */
+      {
+	  exp_int1 = 0x00000100;
+	  exp_int2 = 0x00000100;
+      }
+   }
+
+  /**** wait for all ints to set or for a second to pass ****/
+   done = 0;
+   temp_32 = 0;
+   iCom_adapter_ptr->tpr = DIAG_REG_WAIT_FOR_INTS+itteration;
+   while((!done) && (temp_32 < 1000))
+   {
+       if((iCom_adapter_ptr->diag_int_reset1 == exp_int1) &&
+	  (iCom_adapter_ptr->diag_int_reset2 == exp_int2))
+	   done = 1;
+
+    /**** wait 1 millisecond ****/
+       current->state = TASK_INTERRUPTIBLE;
+       schedule_timeout(HZ/1000); /* HZ = 1 second */
+       temp_32++;
+   }
+
+   temp_32 = 0;
+
+   iCom_adapter_ptr->tpr = DIAG_REG_CHECK_FOR_INTS+itteration;
+
+   if(!done) /* failed */ 
+   {
+      /* fail the appropriate port/s */
+       rc = diag_check_ints(iCom_adapter_ptr,
+			    exp_int1,
+			    exp_int2,
+			    "Register Test. Int did not occur.\n",
+			    DIAG_REG_WRONG_INT_FAIL);
+       return -1;
+   }
+
+  /**** stop all procs ****/
+   iCom_adapter_ptr->tpr = DIAG_REG_STOP_PROCS+itteration;
+   for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+       if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	   stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** check the results data ****/
+   iCom_adapter_ptr->tpr = DIAG_REG_CHECK_RESULTS+itteration;
+   for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+       if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+
+       {
+      /* read the results word */
+	   temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+(i*0x2000)+4));
+
+      /* check results word */
+	   if(temp_32 != 0)
+	   {
+	       error_data[0] = temp_32;
+
+	       diag_error(iCom_adapter_ptr,	/* port to fail */
+			  i,  /* IOA_FAILURE indicates an Adapter failure */
+			  "iCom: Register Test. Bad results data.\n", /* error message */
+			  DIAG_REG_BAD_RESULTS_DATA_FAIL+itteration,		/* tpr */
+			  error_ptr,		/* error data pointer */
+			  1);	/* number of error words. */
+	       return 0x0000000f;
+	   }
+       }
+  } /* end of "for each pico code load" */
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_REG_FREE_INT_HANDLE+itteration;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  iCom_adapter_ptr->tpr = DIAG_REG_END+itteration;
+
+return 0; 
+} 
+ 
+/***************************************/
+/* Diagnostic RVX Status Register Test */
+/***************************************/
+static u32 __init diag_rvx_sts(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           temp_32;
+  u32           rc;
+  u8            i;
+  u32           exp_int1, exp_int2;
+  u8            done;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  
+  /* make sure the all ints are cleared before we start */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_CLEAR_INTS;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8024)); 
+  }
+  else{
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+  }
+  
+  /* save off irq and request irq line */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_GET_INT_HANDLE;
+  if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+		       SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: RVX Status Register Test. request_irq call failed.\n", /* error message */
+		 DIAG_RVXSTS_GET_IRQ_LINE_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f;
+  }
+  else
+      iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /* load all active procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_LOAD_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	diag_port_load(i,
+		       &iCom_adapter_ptr->port_info[i],
+                       iCom_adapter_ptr->base_addr,
+		       tostsdiag, 
+		       sizeof(tostsdiag));
+
+  /* clear int regs */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_CLEAR_INT_VARS;
+  iCom_adapter_ptr->diag_int_reset1 = 0;
+  iCom_adapter_ptr->diag_int_reset2 = 0;
+
+  /* start all procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_START_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      start_processor(&iCom_adapter_ptr->port_info[i]);
+  
+  /* determine what ints to look for */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_DETERMINE_INTS_TO_CHECK_FOR;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2) {
+    exp_int1 = 0x01000100;
+    exp_int2 = 0;
+  }
+  else
+  {
+      if(iCom_adapter_ptr->subsystem_id == FOUR_PORT_MODEL)
+      {
+	  exp_int1 = 0x01000100;
+	  exp_int2 = 0x01000100;
+      }
+      else /* must be a two port model */
+      {
+	  exp_int1 = 0x00000100;
+	  exp_int2 = 0x00000100;
+      }
+  }
+  
+  /* wait for all ints to set or for a second to pass */
+  done = 0;
+  temp_32 = 0;
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_WAIT_FOR_INTS;
+  while((!done) && (temp_32 < 1000))
+  {
+    if((iCom_adapter_ptr->diag_int_reset1 == exp_int1) &&
+       (iCom_adapter_ptr->diag_int_reset2 == exp_int2))
+      done = 1;
+
+    /**** wait 1 millisecond ****/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/1000); /* HZ = 1 second */
+    temp_32++;
+  }
+
+  temp_32 = 0;
+
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_CHECK_FOR_INTS;
+  if(!done) /* failed */
+  {
+      /* fail the appropriate port/s */
+      rc = diag_check_ints(iCom_adapter_ptr,
+			   exp_int1,
+			   exp_int2,
+			   "Instruction Test. Int did not occur.\n",
+			   DIAG_RVXSTS_WRONG_INT_FAIL);
+      return -1;
+  }
+  
+  /* stop all procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_STOP_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** check the results data ****/
+   iCom_adapter_ptr->tpr = DIAG_RVXSTS_CHECK_RESULTS;
+   for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+       if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+       {
+      /* read the results word */
+	   temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+(i*0x2000)+4));
+
+      /* check results word */
+	   if(temp_32 != 0xAAAAAAAA) /* AAAAAAAA is passing */
+	   {
+	       error_data[0] = temp_32;
+
+	       diag_error(iCom_adapter_ptr,	/* port to fail */
+			  i,  /* IOA_FAILURE indicates an Adapter failure */
+			  "iCom: Instruction Test. Bad results data.\n", /* error message */
+			  DIAG_RVXSTS_BAD_RESULTS_DATA_FAIL,		/* tpr */
+			  error_ptr,		/* error data pointer */
+			  1);	/* number of error words. */
+	       return 0x0000000f;
+	   }
+       }
+
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_FREE_INT_HANDLE;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  iCom_adapter_ptr->tpr = DIAG_RVXSTS_END;
+return 0; 
+}
+
+/*************************************/
+/* Diagnostic RVX Internal Wrap Test */
+/*************************************/
+static u32 __init diag_rvx_internal_wrap(struct iCom_adapter *iCom_adapter_ptr)
+{
+
+  u32           temp_32;
+  u32           rc;
+  u8            i;
+  u32           exp_int1, exp_int2;
+  u8            done;
+  u32		error_data[NUM_ERROR_ENTRIES];
+  u32		*error_ptr = &error_data[0];
+  
+  /* make sure the all ints are cleared before we start */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_CLEAR_INTS;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) == ADAPTER_V2) {
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8004)); 
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x8024));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x8024)); 
+  }
+  else{
+    temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+0x4004));
+    iCom_writel(temp_32,(void*)(iCom_adapter_ptr->base_addr+0x4004)); 
+  }
+  
+  /* save off irq and request irq line */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_GET_INT_HANDLE;
+  if ((rc = request_irq(iCom_adapter_ptr->pci_dev->irq, diag_int_handler, SA_INTERRUPT |
+		       SA_SHIRQ, ICOM_DRIVER_NAME, (void *)iCom_adapter_ptr))) {
+      error_data[0] = rc;
+
+      diag_error(iCom_adapter_ptr,
+		 IOA_FAILURE,  /* IOA_FAILURE indicates an Adapter failure */
+		 "iCom: RVX Internal Wrap Test. request_irq call failed.\n", /* error message */
+		 DIAG_RVXINTWRP_GET_IRQ_LINE_FAIL,		/* tpr */
+		 error_ptr,		/* error data pointer */
+		 1);	/* number of error words. */
+      return 0x0000000f;
+  }
+  else
+      iCom_adapter_ptr->resources |= HAVE_INT_HANDLE;
+
+  /* load all active procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_LOAD_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	diag_port_load(i,
+		       &iCom_adapter_ptr->port_info[i],
+                       iCom_adapter_ptr->base_addr,
+		       towrapdiag, 
+		       sizeof(towrapdiag));
+
+  /* clear int regs */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_CLEAR_INT_VARS;
+  iCom_adapter_ptr->diag_int_reset1 = 0;
+  iCom_adapter_ptr->diag_int_reset2 = 0;
+
+  /* Load the Transmit data */
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+    {
+	iCom_writel(0x11000000,(void*)(iCom_adapter_ptr->base_addr+ 0x8  +(0x2000*i)));
+	iCom_writel(0x22000000,(void*)(iCom_adapter_ptr->base_addr+ 0xc  +(0x2000*i)));
+	iCom_writel(0x33000000,(void*)(iCom_adapter_ptr->base_addr+ 0x10 +(0x2000*i)));
+	iCom_writel(0x44000000,(void*)(iCom_adapter_ptr->base_addr+ 0x14 +(0x2000*i)));
+    }
+
+  /* start all procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_START_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      start_processor(&iCom_adapter_ptr->port_info[i]);
+  
+  /* determine what ints to look for */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_DETERMINE_INTS_TO_CHECK_FOR;
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2) {
+    exp_int1 = 0x01000100;
+    exp_int2 = 0;
+  }
+  else
+  {
+      if(iCom_adapter_ptr->subsystem_id == FOUR_PORT_MODEL)
+      {
+	  exp_int1 = 0x01000100;
+	  exp_int2 = 0x01000100;
+      }
+      else /* must be a two port model */
+      {
+	  exp_int1 = 0x00000100;
+	  exp_int2 = 0x00000100;
+      }
+  }
+  
+  /* wait for all ints to set or for a second to pass */
+  done = 0;
+  temp_32 = 0;
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_WAIT_FOR_INTS;
+  while((!done) && (temp_32 < 1000))
+  {
+    if((iCom_adapter_ptr->diag_int_reset1 == exp_int1) &&
+       (iCom_adapter_ptr->diag_int_reset2 == exp_int2))
+      done = 1;
+
+    /**** wait 1 millisecond ****/
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ/1000); /* HZ = 1 second */
+    temp_32++;
+  }
+
+  temp_32 = 0;
+
+
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_CHECK_FOR_INTS;
+  if(!done) /* failed */
+  {
+      /* fail the appropriate port/s */
+      rc = diag_check_ints(iCom_adapter_ptr,
+			   exp_int1,
+			   exp_int2,
+			   "Instruction Test. Int did not occur.\n",
+			   DIAG_RVXINTWRP_WRONG_INT_FAIL);
+      return -1;
+  }
+  
+  /* stop all procs */
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_STOP_PROCS;
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+    if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+      stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /**** check the results data ****/
+   iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_CHECK_RESULTS;
+   for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+       if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+
+       {
+      /* read the results word */
+	   temp_32 = iCom_readl((void*)(iCom_adapter_ptr->base_addr+(i*0x2000)+4));
+
+      /* check results word */
+	   if(temp_32 != 0xAAAAAAAA) /* AAAAAAAA is passing */
+	   {
+	       error_data[0] = temp_32;
+
+	       diag_error(iCom_adapter_ptr,	/* port to fail */
+			  i,  /* IOA_FAILURE indicates an Adapter failure */
+			  "iCom: Instruction Test. Bad results data.\n", /* error message */
+			  DIAG_RVXINTWRP_BAD_RESULTS_DATA_FAIL,		/* tpr */
+			  error_ptr,		/* error data pointer */
+			  1);	/* number of error words. */
+	       return 0x0000000f;
+	   }
+       }
+
+
+  /**** return int handler ****/
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_FREE_INT_HANDLE;
+  free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+  iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE;
+
+  iCom_adapter_ptr->tpr = DIAG_RVXINTWRP_END;
+return 0; 
+} 
+
+
+/********************************/
+/* Imbedded Modem Internal Wrap */
+/********************************/
+int diag_sm_int_wrap(void* data)
+{
+    struct iCom_port *iCom_port_ptr;
+    u32		error_data[NUM_ERROR_ENTRIES];
+    u32        *error_ptr = &error_data[0];
+    struct iCom_adapter *iCom_adapter_ptr;
+    u8		port;
+    char *att1 = "AT&T1\r\n";
+    char *atc0 = "AT%C0\r\n";
+    char *ok = "OK";
+    char *ath = "+++ATH\r\n";
+    char *connect = "CONNECT";
+    /* 50 U's */
+    char *Us = "UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU"; 
+
+    /* get the port pointer */
+    iCom_port_ptr = (struct iCom_port *)data;
+
+    /* get the adapter pointer */
+    iCom_adapter_ptr = &iCom_adapter_info[iCom_port_ptr->adapter];
+
+    iCom_adapter_ptr->tpr = DIAG_SMINTWRP_START_TPR;                 
+
+    /* get the port number */
+    port = iCom_port_ptr->port;
+
+    /* mask all interrupts */
+    if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2)
+	iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x4008));
+    else
+    {
+	iCom_writel(0xFFFF7FFF,(void*)(iCom_adapter_ptr->base_addr+0x8008));
+	iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x8028));
+    }
+
+    /* turn off compression */  /* send at%c0 */
+
+    /* this is done to init the pico code */
+    mdm_send(iCom_port_ptr,"",0);
+
+    iCom_adapter_ptr->tpr = DIAG_SMINTWRP_DISABLE_COMP_TPR;                 
+    mdm_send(iCom_port_ptr, atc0, strlen(atc0));
+
+    /* Wait for the receive data */
+    iCom_adapter_ptr->tpr = DIAG_SMINTWRP_WAIT_FOR_DATA_TPR;                 
+    if(mdm_rcv(iCom_port_ptr, ok))
+    {
+	/* Put the modem into Analog Loopback mode */  /* send at&t1 */
+	iCom_adapter_ptr->tpr = DIAG_SMINTWRP_SET_LOOPBACK_TPR;                 
+	mdm_send(iCom_port_ptr, att1, strlen(att1));
+
+	/* Wait for the receive data */
+	iCom_adapter_ptr->tpr = DIAG_SMINTWRP_WAIT_FOR_DATA2_TPR;                 
+	if(mdm_rcv(iCom_port_ptr, connect))
+	{
+	    /* send the test data */
+	    mdm_send(iCom_port_ptr, Us, strlen(Us));
+
+	    /* Wait for the receive data */
+	    iCom_adapter_ptr->tpr = DIAG_SMINTWRP_WAIT_FOR_DATA4_TPR;                 
+	    /*if(mdm_rcv(iCom_port_ptr, Us))*/
+	    if(mdm_rcv(iCom_port_ptr, Us))
+	    {
+		/* If there was data received */
+		iCom_adapter_ptr->tpr = DIAG_SMINTWRP_SEND_ATH_TPR;                 
+		mdm_send(iCom_port_ptr, ath, strlen(ath));
+
+		/* Wait for the receive data */
+		iCom_adapter_ptr->tpr = DIAG_SMINTWRP_WAIT_FOR_ATH_OK_TPR;
+		if(mdm_rcv(iCom_port_ptr, ok))
+		{
+                    /* turn off DTR and RTS */
+		    iCom_writeb(0,&iCom_port_ptr->dram->osr);
+		    iCom_writeb(0,&iCom_port_ptr->dram->CmdReg);
+
+		    /* let the line settle */
+		    current->state = TASK_INTERRUPTIBLE;
+		    schedule_timeout(HZ/100); /* HZ = 1 second */
+		}
+		else 
+		{
+		    /* fail because we did not get an OK after ath */
+		    diag_error(iCom_adapter_ptr,
+			       port,  /* IOA_FAILURE indicates an Adapter failure */
+			       "iCom: SM Int Wrap - Imbedded Modem did not receive OK after ath.\n",
+			       DIAG_SMINTWRP_NOT_OK_FAIL_TPR,		/* tpr */
+			       error_ptr,		/* error data pointer */
+			       0);	/* number of error words. */
+
+		    iCom_port_ptr->thread_status = DIAG_SMINTWRP_NOT_OK_FAIL_TPR;
+		    return 0x0000000f;
+		}
+	    }
+	    else
+	    {
+	        /* fail here because we did not receive all U's */
+		diag_error(iCom_adapter_ptr,
+			   port,  /* IOA_FAILURE indicates an Adapter failure */
+			   "iCom: SM Int Wrap - Imbedded Modem did not receive all U's.\n",
+			   DIAG_SMINTWRP_NOT_U_FAIL_TPR,		/* tpr */
+			   error_ptr,		/* error data pointer */
+			   0);	/* number of error words. */
+
+		iCom_port_ptr->thread_status = DIAG_SMINTWRP_NOT_U_FAIL_TPR;
+		return 0x0000000f;
+	    }
+	}
+	else
+	{
+            /* fail here because we did not receive CONNECT */
+	    /* after sending the at&t1 command.             */
+	    diag_error(iCom_adapter_ptr,
+		       port,  /* IOA_FAILURE indicates an Adapter failure */
+		       "iCom: SM Int Wrap - Imbedded Modem did not receive connect after at&t1.\n",
+		       DIAG_SMINTWRP_NOT_CONNECT_FAIL_TPR,		/* tpr */
+		       error_ptr,		/* error data pointer */
+		       0);	/* number of error words. */
+
+	    iCom_port_ptr->thread_status = DIAG_SMINTWRP_NOT_CONNECT_FAIL_TPR;
+	    return 0x0000000f;
+	}
+    }
+    else
+    {
+	/* fail here because we did not receive OK */
+	/* after the at%c0 call.                   */
+	diag_error(iCom_adapter_ptr,
+		   port,  /* IOA_FAILURE indicates an Adapter failure */
+		   "iCom: SM Int Wrap - Imbedded Modem did not receive OK after at%%c0.\n",
+		   DIAG_SMINTWRP_NOT_OK_FAIL1_TPR,		/* tpr */
+		   error_ptr,		/* error data pointer */
+		   0);	/* number of error words. */
+
+        iCom_port_ptr->thread_status = DIAG_SMINTWRP_NOT_OK_FAIL1_TPR;
+	return 0x0000000f;
+    }
+
+    iCom_adapter_ptr->tpr = DIAG_SMINTWRP_END_TPR;
+
+    iCom_port_ptr->thread_status = STATUS_PASS;
+    return 0;
+
+}
+
+
+
+/*********************************/
+/* Four port internal modem test */
+/*********************************/
+static u32 __init diag_concurrent_mdm_int_wrap(struct iCom_adapter *iCom_adapter_ptr)
+{
+    struct iCom_port *iCom_port_ptr;
+    int               port_num;
+    int               task;
+    int               loop;
+    int               still_waiting;
+    int               ports_to_test=0; /* false */
+
+
+    /* Determine what ports need to run the imbedded modem wrap */
+    for (port_num = 0; port_num < iCom_adapter_ptr->numb_ports; port_num++)
+    {
+	iCom_port_ptr = &iCom_adapter_ptr->port_info[port_num];
+
+	/* init status to pass(for non modem ports) */
+	iCom_port_ptr->thread_status = STATUS_PASS; 
+
+        /* If this is a imbedded modem, */
+        /* run the internal wrap test.  */
+        if ((iCom_port_ptr->status == ICOM_PORT_ACTIVE) &&
+            (iCom_port_ptr->passed_diags) &&
+	    (iCom_port_ptr->imbed_modem == ICOM_IMBED_MODEM))
+	{
+            /* change the status to INIT */
+            iCom_port_ptr->thread_status = STATUS_INIT;
+
+            /* start a new thread to test the port */
+            task = kernel_thread(diag_sm_int_wrap, iCom_port_ptr, 0);
+
+            ports_to_test = 1;
+	}
+    }
+
+    /* Wait for the ports to finish */
+    loop = 0;
+    still_waiting = 1;
+    while((loop<20) && (still_waiting) && (ports_to_test))
+    {
+	/* see if they have all completed */
+	for(port_num = still_waiting = 0; port_num < iCom_adapter_ptr->numb_ports; port_num++)
+	{
+	    iCom_port_ptr = &iCom_adapter_ptr->port_info[port_num];
+	    if(iCom_port_ptr->thread_status == STATUS_INIT)
+	    {
+		still_waiting |= 1;
+		continue;
+	    }
+	}
+	loop++;
+
+	current->state = TASK_INTERRUPTIBLE;
+	schedule_timeout(HZ); /* HZ = 1 second */
+    }
+
+    /* see if all the ports passed */
+    for(port_num = 0; port_num < iCom_adapter_ptr->numb_ports; port_num++)
+    {
+	iCom_port_ptr = &iCom_adapter_ptr->port_info[port_num];
+
+	if(iCom_port_ptr->imbed_modem == ICOM_IMBED_MODEM) /* true */
+	{
+	    /* turn off DTR and RTS */
+	    iCom_writeb(0,&iCom_port_ptr->dram->osr);
+	    iCom_writeb(0,&iCom_port_ptr->dram->CmdReg);
+
+	    /* check if passed */
+	    if(iCom_port_ptr->thread_status != STATUS_PASS)
+	    {
+		iCom_port_ptr->passed_diags = 0; /* FAIL */
+	    }
+	}
+    }
+
+    return 0;
+}
+
+	      
+
+  
+
+/**************************/
+/* Main Diagnostic Driver */
+/**************************/
+static u32 __init diag_main(struct iCom_adapter *iCom_adapter_ptr)
+{
+  u32           rc = 0;
+  int		i;
+#ifdef icom_debug
+  struct LocationDataStruct *p_location_data = NULL;
+#endif
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_START;
+#ifdef icom_debug
+  printk("iCom: Diagnostic started. ********\n");
+#ifdef CONFIG_PPC_ISERIES
+  p_location_data = iSeries_GetLocationData(iCom_adapter_ptr->pci_dev);
+
+  if (p_location_data != NULL)
+  {
+      printk("iCom: Frame ID: %d Card Position: %s.\n",p_location_data->FrameId, p_location_data->CardLocation);
+      kfree(p_location_data);
+  }
+#endif
+#endif
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_INIT;
+
+  diag_init(iCom_adapter_ptr);
+
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_BIST;
+  /* run BIST test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic BIST test.\n");
+#endif
+      rc = diag_bist(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic BIST test FAILED.\n");
+      }
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_MEMORY;
+  /* run memory test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic memory.\n");
+#endif
+      rc = diag_memory(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic memory test FAILED.\n");
+      }
+  }
+
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_INTERRUPT;
+  /* run interrupt test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic interrupt test.\n");
+#endif
+      rc = diag_int(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic interrupt test FAILED.\n");
+      }
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_INSTRUCTION;
+  /* run instruction test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic instruction test.\n");
+#endif
+      rc = diag_inst(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic instruction test FAILED.\n");
+      }
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_REGISTER;
+  /* run register test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic register test.\n");
+#endif
+      rc = diag_reg(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic register test FAILED.\n");
+      }
+  }
+
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_RVX_STATUS;
+  /* run RVX status register test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic RVX Status Register test.\n");
+#endif
+      rc = diag_rvx_sts(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic RVX Status register test FAILED.\n");
+      }
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_RVX_INTERNAL_WRAP;
+  /* run RVX internal wrap test */
+  if(rc==0)
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic RVX Internal Wrap test.\n");
+#endif
+      rc = diag_rvx_internal_wrap(iCom_adapter_ptr);
+      if(rc!=0)
+      {
+	  printk("iCom: Diagnostic RVX Internal Wrap test FAILED.\n");
+      }
+  }
+
+  /* make sure all procs are stoped */
+  for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+      if(iCom_adapter_ptr->port_info[i].status == ICOM_PORT_ACTIVE)
+	  stop_processor(&iCom_adapter_ptr->port_info[i]);
+
+  /* return the resources */
+  diag_return_resources(iCom_adapter_ptr);
+
+  /* mask all interrupts */
+  if ((iCom_adapter_ptr->version | ADAPTER_V2) != ADAPTER_V2)
+      iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x4008));
+  else
+  {
+      iCom_writel(0xFFFF7FFF,(void*)(iCom_adapter_ptr->base_addr+0x8008));
+      iCom_writel(0xFFFFFFFF,(void*)(iCom_adapter_ptr->base_addr+0x8028));
+  }
+
+  iCom_adapter_ptr->tpr = DIAG_MAIN_END;
+
+  if(rc==0)
+  {
+#ifdef icom_debug
+     printk("iCom: Diagnostic Completed Successfully ********\n");
+#endif
+
+      /* set the TPR and error data to zero for all ports */
+      for(i=0;i<iCom_adapter_ptr->numb_ports;i++)
+      {
+	  iCom_adapter_ptr->port_info[i].passed_diags = 1; /* true */
+      }
+  }
+
+  else
+  {
+#ifdef icom_debug
+      printk("iCom: Diagnostic FAILED ********\n");
+#endif
+  }
+
+
+  return rc;
+}
+
+static void __init diag_return_resources(struct iCom_adapter *iCom_adapter_ptr)
+{
+  /* make sure all resources are returned before leaving */
+  if(iCom_adapter_ptr->resources & HAVE_INT_HANDLE)
+  {
+      free_irq(iCom_adapter_ptr->pci_dev->irq, (void *)iCom_adapter_ptr);
+      iCom_adapter_ptr->resources &= ~HAVE_INT_HANDLE; /* clear resource bit */
+  }
+
+  if(iCom_adapter_ptr->resources & HAVE_MALLOC_1)
+  {
+      kfree(iCom_adapter_ptr->malloc_addr_1);
+      iCom_adapter_ptr->resources &= ~HAVE_MALLOC_1; /* clear resource bit */
+  }
+
+}
+
+
+/*
+   Module operations
+*/
+int iCom_init(void)
+{
+    int               index, 
+                      index_v2,
+                      index2,
+                      scan_index;
+    struct pci_dev   *dev[MAX_ADAPTERS];
+    unsigned int      irq_number[MAX_ADAPTERS];
+    unsigned long int base_addr[MAX_ADAPTERS];
+    unsigned char     valid_indices[MAX_ADAPTERS];
+#define VALID 1
+#define INVALID 0
+    unsigned int      command_reg;
+    struct iCom_port *iCom_port_ptr;
+    int               retval;
+    int               status;
+    int               port_num;
+    int               adapter_count = 0;
+    int               duplicate;
+    unsigned int      subsystem_id;
+    int               minor_number = 0;
+
+
+    /*
+     * Find base addresses and IRQs for any/all installed cards
+     */
+    for (index=0; index < MAX_ADAPTERS; index++) {
+	valid_indices[index] = INVALID;
+	dev[index] = NULL;
+    }
+
+    /* check for Version 1 Adapters */
+    for (index = 0; index < MAX_ADAPTERS; index++){
+	if (index == 0) {
+	    if (!(dev[index] = pci_find_device(VENDOR_ID, DEVICE_ID, dev[index])))
+		break;
+	}
+	else {
+	    if (!(dev[index] = pci_find_device(VENDOR_ID, DEVICE_ID, dev[index-1])))
+		break;
+	}
+
+	adapter_count++;
+
+	if (pci_enable_device(dev[index])) {
+	    printk("iCom:  Device enable FAILED\n");
+	    continue;
+	}
+
+	if (pci_read_config_dword(dev[index], PCI_COMMAND, &command_reg)) {
+	    printk("iCom:  PCI Config read FAILED\n");
+	    continue;
+	}	
+
+	pci_write_config_dword(dev[index],PCI_COMMAND, command_reg | 0x00000146);
+	pci_write_config_dword(dev[index],0x44, 0x8300830A);
+
+	base_addr[index] = pci_resource_start(dev[index],0);
+	base_addr[index] &= PCI_BASE_ADDRESS_MEM_MASK;
+
+	duplicate = 0;
+	for (index2 = 0; index2 < index; index2++) {
+	    if (base_addr[index] == base_addr[index2])
+		duplicate = 1;
+	}
+	if (duplicate) continue;
+
+	irq_number[index] = dev[index]->irq;
+
+	valid_indices[index] = ADAPTER_V1;
+    }
+
+    /* check for version 2 Adapters */
+    for (index_v2=0; index < MAX_ADAPTERS; index_v2++){
+	if (index_v2 == 0) {
+	    if (!(dev[index] = pci_find_device(VENDOR_ID, DEVICE_ID2, NULL))) {
+		break;
+	    }
+	}
+	else {
+	    if (!(dev[index] = pci_find_device(VENDOR_ID, DEVICE_ID2, dev[index-1]))) {
+		break;
+	    }
+	}
+
+	adapter_count++;
+
+	if (pci_enable_device(dev[index])) {
+	    printk("iCom:  Device enable FAILED\n");
+	    continue;
+	}
+
+	if (pci_read_config_dword(dev[index], PCI_COMMAND, &command_reg)) {
+	    printk("iCom:  PCI Config read FAILED\n");
+	    continue;
+	}	
+
+	pci_write_config_dword(dev[index],PCI_COMMAND, command_reg | 0x00000146);
+	pci_write_config_dword(dev[index],0x44, 0x42004200);
+	pci_write_config_dword(dev[index],0x48, 0x42004200);
+
+	base_addr[index] = pci_resource_start(dev[index],0);	
+	base_addr[index] &= PCI_BASE_ADDRESS_MEM_MASK;
+
+	duplicate = 0;
+	for (index2 = 0; index2 < index; index2++) {
+	    if (base_addr[index] == base_addr[index2])
+		duplicate = 1;
+	}
+	if (duplicate) continue;
+
+	irq_number[index] = dev[index]->irq;
+
+	valid_indices[index++] = ADAPTER_V2;
+    }
+
+    /* allocate memory for control blocks representing each adapter */
+    iCom_adapter_info = (struct iCom_adapter *)
+      kmalloc(adapter_count*sizeof(struct iCom_adapter),GFP_KERNEL);
+
+    if (!iCom_adapter_info) {
+	return -ENOMEM;
+    }
+
+    memset(iCom_adapter_info, 0,adapter_count*sizeof(struct iCom_adapter));
+
+    /* store information just obtained on base_addr and irq */
+    for (index = scan_index = 0; (scan_index < MAX_ADAPTERS) &
+        (index < adapter_count);       scan_index++) {
+
+	if (valid_indices[scan_index]) {
+	    iCom_adapter_info[index].base_addr = base_addr[scan_index];
+	    iCom_adapter_info[index].irq_number = irq_number[scan_index];
+	    iCom_adapter_info[index].pci_dev = dev[scan_index];
+	    iCom_adapter_info[index].version = valid_indices[scan_index];
+	    pci_read_config_dword(dev[scan_index], PCI_SUBSYSTEM_VENDOR_ID, &subsystem_id);
+	    iCom_adapter_info[index].subsystem_id = subsystem_id;
+
+	    if (iCom_adapter_info[index].version == ADAPTER_V1) {
+		iCom_adapter_info[index].numb_ports = 2;
+		iCom_adapter_info[index].port_info[0].port = 0;
+                iCom_adapter_info[index].port_info[0].minor_number = minor_number;
+		iCom_adapter_info[index].port_info[0].status = ICOM_PORT_ACTIVE;
+		iCom_adapter_info[index].port_info[0].imbed_modem = ICOM_UNKNOWN;
+		iCom_adapter_info[index].port_info[1].port = 1;
+                iCom_adapter_info[index].port_info[1].minor_number = minor_number + 1;
+		iCom_adapter_info[index].port_info[1].status = ICOM_PORT_ACTIVE;
+		iCom_adapter_info[index].port_info[1].imbed_modem = ICOM_UNKNOWN;
+	    }
+	    else {
+		if (subsystem_id == FOUR_PORT_MODEL) {
+		    iCom_adapter_info[index].numb_ports = 4;
+
+		    iCom_adapter_info[index].port_info[0].port = 0;
+                    iCom_adapter_info[index].port_info[0].minor_number = minor_number;
+		    iCom_adapter_info[index].port_info[0].status = ICOM_PORT_ACTIVE;
+		    iCom_adapter_info[index].port_info[0].imbed_modem = ICOM_IMBED_MODEM;
+
+		    iCom_adapter_info[index].port_info[1].port = 1;
+                    iCom_adapter_info[index].port_info[1].minor_number = minor_number + 1;
+		    iCom_adapter_info[index].port_info[1].status = ICOM_PORT_ACTIVE;
+		    iCom_adapter_info[index].port_info[1].imbed_modem = ICOM_IMBED_MODEM;
+
+		    iCom_adapter_info[index].port_info[2].port = 2;
+                    iCom_adapter_info[index].port_info[2].minor_number = minor_number + 2;
+		    iCom_adapter_info[index].port_info[2].status = ICOM_PORT_ACTIVE;
+		    iCom_adapter_info[index].port_info[2].imbed_modem = ICOM_IMBED_MODEM;
+
+		    iCom_adapter_info[index].port_info[3].port = 3;
+                    iCom_adapter_info[index].port_info[3].minor_number = minor_number + 3;
+		    iCom_adapter_info[index].port_info[3].status = ICOM_PORT_ACTIVE;
+		    iCom_adapter_info[index].port_info[3].imbed_modem = ICOM_IMBED_MODEM;
+		}
+		else {
+		    iCom_adapter_info[index].numb_ports = 4;
+
+		    iCom_adapter_info[index].port_info[0].port = 0;
+                    iCom_adapter_info[index].port_info[0].minor_number = minor_number;
+		    iCom_adapter_info[index].port_info[0].status = ICOM_PORT_ACTIVE;
+		    if (subsystem_id == V2_ONE_PORT_RVX_ONE_PORT_IMBED_MDM)
+			iCom_adapter_info[index].port_info[0].imbed_modem = ICOM_IMBED_MODEM;
+		    else
+			iCom_adapter_info[index].port_info[0].imbed_modem = ICOM_RVX;
+
+		    iCom_adapter_info[index].port_info[1].status = ICOM_PORT_OFF;
+
+		    iCom_adapter_info[index].port_info[2].port = 2;
+                    iCom_adapter_info[index].port_info[2].minor_number = minor_number + 1;
+		    iCom_adapter_info[index].port_info[2].status = ICOM_PORT_ACTIVE;
+		    iCom_adapter_info[index].port_info[2].imbed_modem = ICOM_RVX;
+
+		    iCom_adapter_info[index].port_info[3].status = ICOM_PORT_OFF;
+		}
+	    }
+            minor_number += 4;
+
+	    if (!request_mem_region(iCom_adapter_info[index].base_addr,
+				    pci_resource_len(iCom_adapter_info[index].pci_dev,0),
+				    "iCom")) {
+		printk("iCom:  request_mem_region FAILED\n");
+	    }
+
+
+	    retval = diag_main(&iCom_adapter_info[index]);
+
+	    if(retval)
+	    {
+		release_mem_region(iCom_adapter_info[index].base_addr,
+				   pci_resource_len(iCom_adapter_info[index].pci_dev,0));
+		continue;	
+	    }
+
+            /* save off irq and request irq line */
+	    if (request_irq(irq_number[scan_index], iCom_interrupt, SA_INTERRUPT |
+			    SA_SHIRQ, ICOM_DRIVER_NAME, (void *)&iCom_adapter_info[index])) {
+		release_mem_region(iCom_adapter_info[index].base_addr,
+				   pci_resource_len(iCom_adapter_info[index].pci_dev,0));
+		printk("iCom:  request_irq FAILED\n");
+		continue;
+	    }
+
+	    for (port_num = 0; port_num < iCom_adapter_info[index].numb_ports; port_num++) {
+		iCom_port_ptr = &iCom_adapter_info[index].port_info[port_num];
+
+		if (iCom_port_ptr->status == ICOM_PORT_ACTIVE) {
+		    /* initialize wait queues */
+		    init_waitqueue_head(&iCom_port_ptr->open_wait);
+		    init_waitqueue_head(&iCom_port_ptr->close_wait);
+		    init_waitqueue_head(&iCom_port_ptr->delta_msr_wait);
+
+		    /* initialize port specific variables */
+		    iCom_port_ptr->tqueue.routine = do_softint;
+		    iCom_port_ptr->tqueue.data = iCom_port_ptr;
+		    if (iCom_adapter_info[index].version == ADAPTER_V1) {
+			iCom_port_ptr->global_reg = (struct iCom_regs *)((char *)iCom_adapter_info[index].base_addr + 0x4000);
+			iCom_port_ptr->int_reg = (unsigned long)iCom_adapter_info[index].base_addr + 0x4004 + 2 - 2 * port_num;
+		    }
+		    else {
+			iCom_port_ptr->global_reg = (struct iCom_regs *)((char *)iCom_adapter_info[index].base_addr + 0x8000);
+			if (iCom_port_ptr->port < 2)
+			    iCom_port_ptr->int_reg = (unsigned long)iCom_adapter_info[index].base_addr + 0x8004 + 2 - 2 * iCom_port_ptr->port;
+			else
+			    iCom_port_ptr->int_reg = (unsigned long)iCom_adapter_info[index].base_addr + 0x8024 + 2 - 2 * (iCom_port_ptr->port - 2);
+		    }
+		    iCom_port_ptr->dram = (struct func_dram*)((char*)iCom_adapter_info[index].base_addr + 0x2000 * iCom_port_ptr->port);
+		    iCom_port_ptr->close_delay = 5*HZ/10;
+		    iCom_port_ptr->closing_wait = 30*HZ;
+		    iCom_port_ptr->adapter = index;
+
+		    /*
+		     * Load and start processor
+		     */
+		    loadCode(iCom_port_ptr);
+
+	   	    /* get port memory */
+		    if ((status = get_port_memory(iCom_port_ptr)) != 0) {
+			printk("iCom:  memory allocation for minor number %d port FAILED\n",iCom_port_ptr->minor_number);
+
+			/* Fail the port, though not technically correct to call the port bad
+			   due to diagnostics the end result is this port should not be accessed
+			 */
+			iCom_port_ptr->passed_diags = 0;
+			continue;
+		    }
+
+		    /* Set Country Code */
+		    iCom_set_code(iCom_port_ptr);
+		}
+	    }
+/*            retval = diag_concurrent_mdm_int_wrap(&iCom_adapter_info[index]);
+	    if(retval) continue;
+*/	    index++;
+	}
+    }
+    active_adapters = index;
+
+    printk("iCom:  Adapter detection complete, %d adapters found with %d valid\n",adapter_count,active_adapters);
+
+    if (active_adapters > 0) {
+
+      /* Initialize the tty_driver structure */
+	memset(&serial_driver, 0, sizeof(struct tty_driver));
+	serial_driver.magic = TTY_DRIVER_MAGIC;
+	serial_driver.driver_name = ICOM_DRIVER_NAME;
+#if defined(CONFIG_DEVFS_FS)
+	serial_driver.name = "ttyA%d";
+#else
+	serial_driver.name = "ttyA";
+#endif
+	serial_driver.major = 243;
+	serial_driver.minor_start = 0;
+	serial_driver.num = NR_PORTS;
+	serial_driver.type = TTY_DRIVER_TYPE_SERIAL;
+	serial_driver.subtype = SERIAL_TYPE_NORMAL;
+	serial_driver.init_termios = tty_std_termios;
+	serial_driver.init_termios.c_cflag = B9600 | CS8 | CREAD | HUPCL | CLOCAL;
+	serial_driver.flags = TTY_DRIVER_REAL_RAW | TTY_DRIVER_NO_DEVFS;
+	serial_driver.refcount = &serial_refcount;
+	serial_driver.table = serial_table;
+	serial_driver.termios = serial_termios;
+	serial_driver.termios_locked = serial_termios_locked;
+
+	serial_driver.open = iCom_open;
+	serial_driver.close = iCom_close;
+	serial_driver.write = iCom_write;
+	serial_driver.put_char = iCom_put_char;
+	serial_driver.flush_chars = iCom_flush_chars;
+	serial_driver.write_room = iCom_write_room;
+	serial_driver.chars_in_buffer = iCom_chars_in_buffer;
+	serial_driver.flush_buffer = iCom_flush_buffer;
+	serial_driver.ioctl = iCom_ioctl;
+	serial_driver.throttle = iCom_throttle;
+	serial_driver.unthrottle = iCom_unthrottle;
+	serial_driver.send_xchar = iCom_send_xchar;
+	serial_driver.set_termios = iCom_set_termios;
+	serial_driver.stop = iCom_stop;
+	serial_driver.start = iCom_start;
+	serial_driver.hangup = iCom_hangup;
+	serial_driver.break_ctl = iCom_break;
+	serial_driver.wait_until_sent = iCom_wait_until_sent;
+	serial_driver.read_proc = iCom_read_proc;
+
+
+	for (index=0; index < active_adapters; index++) {
+	    iCom_adapter_info[index].port_info[0].callout_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[0].normal_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[1].callout_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[1].normal_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[2].callout_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[2].normal_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[3].callout_termios = serial_driver.init_termios;
+	    iCom_adapter_info[index].port_info[3].normal_termios = serial_driver.init_termios;
+	}
+
+	if (tty_register_driver(&serial_driver)) {
+	    for (index=0; index < active_adapters; index++) {
+		free_irq(iCom_adapter_info[index].irq_number, (void *)&iCom_adapter_info[index]);
+	    }
+	    kfree(iCom_adapter_info);
+	    panic("Couldn't register serial driver\n");
+	}
+
+#if defined(CONFIG_DEVFS_FS)
+	for (index = 0; index < active_adapters; index++) {
+	    tty_register_devfs(&serial_driver,
+			       0, index*4 + serial_driver.minor_start);
+	    tty_register_devfs(&serial_driver,
+			       0, index*4 + serial_driver.minor_start + 1);
+
+	    if ((iCom_adapter_info[index].version == ADAPTER_V2) &&
+		(iCom_adapter_info[index].subsystem_id == FOUR_PORT_MODEL)) {
+		tty_register_devfs(&serial_driver,
+				   0, index*4 + serial_driver.minor_start + 2);
+		tty_register_devfs(&serial_driver,
+				   0, index*4 + serial_driver.minor_start + 3);
+	    }
+	}
+#endif
+
+	/* lastly, register unique ioctl */
+	register_ioctl32_conversion(0x4300,NULL);
+
+	return 0;
+    }
+    else {
+	if (adapter_count > 0) {
+	    kfree(iCom_adapter_info);
+	}
+    }
+
+    return -ENODEV;
+}
+
+int init_module(void)
+{
+#ifdef MODULE
+   printk("icom:  Start of module: 0x%lx\n",(unsigned long)THIS_MODULE + (u32)THIS_MODULE->size_of_struct);
+#endif
+   return iCom_init();
+}
+
+void cleanup_module(void) 
+{
+  unsigned long       flags;
+  int                 e1;
+  int                 index;
+  int                 port_num;
+  struct iCom_port   *iCom_port_ptr;
+
+  /* remove registered ioctl */
+  unregister_ioctl32_conversion(0x4300);
+
+  spin_lock_irqsave(&iComlock,flags);
+  if ((e1 = tty_unregister_driver(&serial_driver)))
+    printk("iCom:  failed to unregister serial driver (%d)\n",e1);
+
+#if defined(CONFIG_DEVFS_FS)
+  for (index = 0; index < active_adapters; index++) {
+      tty_unregister_devfs(&serial_driver,
+			   index*4 + serial_driver.minor_start);
+      tty_unregister_devfs(&serial_driver,
+			   index*4 + serial_driver.minor_start + 1);
+
+      if ((iCom_adapter_info[index].version == ADAPTER_V2) &&
+	  (iCom_adapter_info[index].subsystem_id == FOUR_PORT_MODEL)) {
+	  tty_unregister_devfs(&serial_driver,
+			     index*4 + serial_driver.minor_start + 2);
+	  tty_unregister_devfs(&serial_driver,
+			     index*4 + serial_driver.minor_start + 3);
+      }
+  }
+#endif
+
+  for (index=0; index < active_adapters; index++) {
+
+    for (port_num = 0; port_num < iCom_adapter_info[index].numb_ports; port_num++) {
+      iCom_port_ptr = &iCom_adapter_info[index].port_info[port_num];
+
+      if (iCom_port_ptr->status == ICOM_PORT_ACTIVE) {
+
+          /* be sure that DTR and RTS are dropped */
+	  iCom_writeb(0x00,&iCom_port_ptr->dram->osr);
+
+          /* Wait 0.1 Sec for simple Init to complete */
+	  current->state = TASK_INTERRUPTIBLE;
+	  schedule_timeout(HZ/10);
+
+          /* Stop proccessor */
+	  stop_processor(iCom_port_ptr);
+
+	  return_port_memory(iCom_port_ptr);
+      }
+    }
+
+    free_irq(iCom_adapter_info[index].irq_number, (void *)&iCom_adapter_info[index]);
+    release_mem_region(iCom_adapter_info[index].base_addr,
+		       pci_resource_len(iCom_adapter_info[index].pci_dev,0));
+  }
+  spin_unlock_irqrestore(&iComlock,flags);
+
+  kfree(iCom_adapter_info);
+  printk("iCom:  Driver removed\n");
+}
+
+/* the interrupts should be disabled here so that no hardware
+ interrupts should occur, polling will be done to check received
+ data to avoid interrupt level processing */
+static int mdm_rcv(struct iCom_port *iCom_port_ptr, char *exp_str) {
+  int status = 0;
+  int loop_count = 0;
+  char string[256];
+
+
+  /* init string to null */
+  memset(string, 0, 256);
+
+  /* search expected string in received data */
+  while (!status && (loop_count++ < 50))
+  {
+      if(cpu_to_le16(iCom_port_ptr->statStg->rcv[0].flags) & SA_FL_RCV_DONE)
+      {
+	  /* copy received data to string */
+	  strcat(string, (char *)iCom_port_ptr->recv_buf);
+
+	  /* reset the buffer */
+	  memset((char *)iCom_port_ptr->recv_buf, 0, RCV_BUFF_SZ);
+	  iCom_port_ptr->statStg->rcv[0].flags = 0;
+	  iCom_port_ptr->statStg->rcv[0].leLength = 0;
+	  iCom_port_ptr->statStg->rcv[0].WorkingLength = (unsigned short int)cpu_to_le16(RCV_BUFF_SZ);
+      }
+      else if(cpu_to_le16(iCom_port_ptr->statStg->rcv[1].flags) & SA_FL_RCV_DONE)
+      {
+	  /* copy received data to string */
+	  strcat(string, (char *)iCom_port_ptr->recv_buf + 2048);
+
+	  /* reset the buffer */
+	  memset((char *)iCom_port_ptr->recv_buf+2048, 0, RCV_BUFF_SZ);
+	  iCom_port_ptr->statStg->rcv[1].flags = 0;
+	  iCom_port_ptr->statStg->rcv[1].leLength = 0;
+	  iCom_port_ptr->statStg->rcv[1].WorkingLength = (unsigned short int)cpu_to_le16(RCV_BUFF_SZ);
+      }
+
+      /* see if we have a match */
+      if(strstr(string, exp_str))
+      {
+	  status = 1;
+      }
+      else
+      {
+
+          /* wait .1 seconds */
+	  current->state = TASK_INTERRUPTIBLE;
+	  schedule_timeout(HZ/10);
+      }
+  }
+
+  /* clear interrupts */
+  iCom_writew(0x3FFF,(void *)iCom_port_ptr->int_reg);
+
+  return status;
+}
+
+static void mdm_send(struct iCom_port *iCom_port_ptr, char *mdm_cmnd,
+                     int cmnd_length) {
+
+  unsigned char     cmdReg;
+  unsigned long int offset;
+  char	tmp_byte;
+  int index;
+
+  if (0 == cmnd_length) {
+
+    /* turn on DTR and RTS */
+    iCom_writeb(0xC0,&iCom_port_ptr->dram->osr);
+    tmp_byte = iCom_readb(&(iCom_port_ptr->dram->HDLCConfigReg));
+    tmp_byte |= HDLC_HDW_FLOW;
+    iCom_writeb(tmp_byte, &(iCom_port_ptr->dram->HDLCConfigReg));
+
+    /* initialize transmit and receive operations */
+    offset = (unsigned long int)&iCom_port_ptr->statStg->rcv[0] - (unsigned long int)iCom_port_ptr->statStg;
+    iCom_writel(iCom_port_ptr->statStg_pci + offset,&iCom_port_ptr->dram->RcvStatusAddr);
+    iCom_port_ptr->next_rcv = 0;
+    iCom_port_ptr->put_length = 0;
+    *iCom_port_ptr->xmitRestart = 0;
+    iCom_writel(iCom_port_ptr->xmitRestart_pci,&iCom_port_ptr->dram->XmitStatusAddr);
+    iCom_writeb(CMD_XMIT_RCV_ENABLE,&iCom_port_ptr->dram->CmdReg);
+
+    /* wait for CTS */
+    for (index=0; index < 20; index++) {
+	current->state = TASK_INTERRUPTIBLE;
+	schedule_timeout(HZ/100); /* HZ = .01 second */
+	if (iCom_readb(&iCom_port_ptr->dram->isr) & ICOM_CTS) break;
+    }
+    if (index == 20) printk("iCom:  WARNING CTS not up in 200ms for BATs\n");
+    current->state = TASK_INTERRUPTIBLE;
+    schedule_timeout(HZ); /* HZ = 1 second */
+
+    return;
+  }
+ 
+  /* clear target receive buffers 1 and 2 */
+  memset(iCom_port_ptr->recv_buf,0,4096);
+
+  memcpy(iCom_port_ptr->xmit_buf, mdm_cmnd, cmnd_length);
+
+  iCom_port_ptr->statStg->xmit[0].flags = (unsigned short int)cpu_to_le16(SA_FLAGS_READY_TO_XMIT);
+  iCom_port_ptr->statStg->xmit[0].leLength = (unsigned short int)cpu_to_le16(cmnd_length);
+    offset = (unsigned long int)&iCom_port_ptr->statStg->xmit[0] - (unsigned long int)iCom_port_ptr->statStg;
+  *iCom_port_ptr->xmitRestart = cpu_to_le32(iCom_port_ptr->statStg_pci + offset);
+
+  cmdReg = iCom_readb(&iCom_port_ptr->dram->CmdReg);
+  iCom_writeb(cmdReg | CMD_XMIT_RCV_ENABLE,&iCom_port_ptr->dram->CmdReg);
+
+  iCom_writeb(START_XMIT,&iCom_port_ptr->dram->StartXmitCmd);
+  TRACE(iCom_port_ptr,TRACE_WRITE_START,cmnd_length);
+}
+
+static void iCom_set_code(struct iCom_port *iCom_port_ptr) {
+  char mdm_cmnd[15];
+  int index;
+
+  /* check if country code should be set at all */
+  if (strlen(iCom_country_code) == 0) return;
+
+  /* check if cabling present */
+  if (iCom_port_ptr->cable_id != RS232_CABLE) return;
+
+  /* initialize xmit/rcv */
+  mdm_send(iCom_port_ptr,"",0);
+
+  printk("iCom:  Checking for country code on serial adapter %d port %d...",iCom_port_ptr->adapter, iCom_port_ptr->port);
+
+  /* sync up modems (if present) */
+  mdm_send(iCom_port_ptr,"AT\r\n",4);
+  mdm_rcv(iCom_port_ptr,"OK");
+
+  printk(".");
+
+  /* send ATI0 to check for internal modem */
+  mdm_send(iCom_port_ptr,"ATI0\r\n",6);
+
+  /* check returned data for internal modem identification */
+  if (!mdm_rcv(iCom_port_ptr,"SMI")) {
+    /* must not be internal modem - return */
+    printk("\n\tno internal modem on this port\n");
+    return;
+  }
+
+  /* send ATE0S0=0 to turn off command Echo and turn off Auto Answer */
+  mdm_send(iCom_port_ptr,"ATE0S0=0\r\n",10);
+
+  /* wait for OK */
+  if (!mdm_rcv(iCom_port_ptr,"OK")) {
+    /* unable to send command to modem - error */
+    printk("\niCom:  Error, unable to set modem Country Code\n");
+    return;
+  }
+
+  printk(".");
+
+  /* Send new country code AT%T19,0,<new value> */
+  sprintf(mdm_cmnd,"AT%%T19,0,%s\r\n",iCom_country_code);
+  mdm_send(iCom_port_ptr,mdm_cmnd,strlen(mdm_cmnd));
+
+  /* wait for OK */
+  if (!mdm_rcv(iCom_port_ptr,"OK")) {
+    /* unable to set country code */
+    printk("\niCom:  Error, unable to set modem Country Code\n");
+    return;
+  }
+
+  printk(".\n");
+
+  /* send ATE1S0=2 to enable command Echo and auto answer */
+  mdm_send(iCom_port_ptr,"ATE1S0=2\r\n",10);
+
+  /* wait for OK */
+  if (!mdm_rcv(iCom_port_ptr,"OK")) {
+    /* modem state unknown */
+    printk("iCom:  Warning, modem state unknown\n");
+  }
+
+  /* print message that Country Code set appropriately */
+  printk("iCom:  Modem country code for adapter %d port %d has been set to %s\n",iCom_port_ptr->adapter, iCom_port_ptr->port,iCom_country_code);
+
+  /* disable xmitter/recvr */
+  iCom_writeb(CMD_RCV_DISABLE,&iCom_port_ptr->dram->CmdReg);
+  for (index = 0; index < 10; index++) {
+      /* Wait 0.1 Sec for receive operations to complete*/
+      current->state = TASK_INTERRUPTIBLE;
+      schedule_timeout(HZ/10);
+
+      if (iCom_readb(&iCom_port_ptr->dram->PrevCmdReg) == 0x00) {
+	  break;
+      }
+  }
+
+  /* clear interrupts */
+  iCom_writew(0x3FFF,(void *)iCom_port_ptr->int_reg);
+}
+
+#ifdef ICOM_TRACE
+void TRACE(struct iCom_port *iCom_port_ptr, u32 trace_pt,
+	   u32 trace_data) {
+
+  u32 *tp_start, *tp_end, **tp_next;
+
+  if (trace_pt == TRACE_GET_MEM) {
+    if (iCom_port_ptr->trace_blk != 0) return;
+    iCom_port_ptr->trace_blk = kmalloc(TRACE_BLK_SZ,GFP_KERNEL);
+    memset(iCom_port_ptr->trace_blk, 0,TRACE_BLK_SZ);
+    iCom_port_ptr->trace_blk[0] = (unsigned long)iCom_port_ptr->trace_blk + 3*sizeof(unsigned long);
+    iCom_port_ptr->trace_blk[1] = (unsigned long)iCom_port_ptr->trace_blk + TRACE_BLK_SZ;
+    iCom_port_ptr->trace_blk[2] = iCom_port_ptr->trace_blk[0];
+  }
+  if (iCom_port_ptr->trace_blk == 0) return;
+
+  if (trace_pt == TRACE_RET_MEM) {
+    kfree(iCom_port_ptr->trace_blk);
+    iCom_port_ptr->trace_blk = 0;
+    return;
+  }
+
+  tp_start  = (u32 *)iCom_port_ptr->trace_blk[0];
+  tp_end    = (u32 *)iCom_port_ptr->trace_blk[1];
+  tp_next   = (u32 **)&iCom_port_ptr->trace_blk[2];
+
+  if (trace_data != 0) {
+    **tp_next = trace_data;
+    *tp_next = *tp_next + 1;
+    if (*tp_next == tp_end) *tp_next = tp_start;
+    **tp_next = TRACE_WITH_DATA | trace_pt;
+  }
+  else
+    **tp_next = trace_pt;
+  
+  *tp_next = *tp_next + 1;
+  if (*tp_next == tp_end) *tp_next = tp_start;
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/icom.h linuxppc64_2_4/drivers/char/icom.h
--- linux-2.4.19/drivers/char/icom.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/char/icom.h	Tue Aug  6 10:36:54 2002
@@ -0,0 +1,995 @@
+/*
+ * iCom.h
+ *
+ * Copyright (C) 2001 Michael Anderson, IBM Corporation
+ *
+ * Serial device driver include file.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ */
+#define TRACE_BLK_SZ         1024
+#define TRACE_WITH_DATA      0x80000000
+#define TRACE_TIME           0x40000000
+#define TRACE_GET_MEM        0x20000000
+#define TRACE_RET_MEM        0x10000000
+#define TRACE_GET_PORT_MEM   0x00000001
+#define TRACE_FOD_ADDR       0x00000005
+#define TRACE_FOD_XBUFF      0x00000006
+#define TRACE_FID_ADDR       0x00000007
+#define TRACE_FID_RBUFF      0x00000008
+#define TRACE_RET_PORT_MEM   0x00000100
+#define TRACE_LOAD_MEM       0x00000200
+#define TRACE_CHANGE_SPEED   0x00000300
+#define TRACE_PARENB         0x00000301
+#define TRACE_PARODD         0x00000302
+#define TRACE_XR_ENAB        0x00000303
+#define TRACE_STARTUP        0x00000400
+#define TRACE_CABLE_ID       0x00000401
+#define TRACE_SHUTDOWN       0x00000500
+#define TRACE_DEVICE_NUMB    0x00000600
+#define TRACE_STARTUP_ERROR  0x00000601
+#define TRACE_CLOSE          0x00000700
+#define TRACE_CLOSE_HANGUP   0x00000701
+#define TRACE_OPEN_ACTIVE    0x00000702
+#define TRACE_WRITE          0x00000800
+#define TRACE_WRITE_FULL     0x00000801
+#define TRACE_WRITE_NODATA   0x00000802
+#define TRACE_WRITE_START    0x00000803
+#define TRACE_PUT_CHAR       0x00000900
+#define TRACE_PUT_FULL       0x00000901
+#define TRACE_FLUSH_CHAR     0x00000a00
+#define TRACE_START_FLUSH    0x00000a01
+#define TRACE_WRITE_ROOM     0x00000b00
+#define TRACE_CHARS_IN_BUFF  0x00000c00
+#define TRACE_CHARS_REMAIN   0x00000c01
+#define TRACE_GET_MODEM      0x00000d00
+#define TRACE_SET_MODEM      0x00000e00
+#define TRACE_RAISE_RTS      0x00000e01
+#define TRACE_RAISE_DTR      0x00000e02
+#define TRACE_LOWER_RTS      0x00000e03
+#define TRACE_LOWER_DTR      0x00000e04
+#define TRACE_GET_SERIAL     0x00000f00
+#define TRACE_SET_SERIAL     0x00001000
+#define TRACE_SET_LSR        0x00001100
+#define TRACE_IOCTL          0x00001200
+#define TRACE_IOCTL_IGNORE   0x00001201
+#define TRACE_SEND_XCHAR     0x00001300
+#define TRACE_QUICK_WRITE    0x00001301
+#define TRACE_THROTTLE       0x00001400
+#define TRACE_UNTHROTTLE     0x00001500
+#define TRACE_SET_TERMIOS    0x00001600
+#define TRACE_STOP           0x00001700
+#define TRACE_START          0x00001800
+#define TRACE_HANGUP         0x00001900
+#define TRACE_BREAK          0x00001a00
+#define TRACE_WAIT_UNTIL_SENT  0x00001b00
+#define TRACE_FLUSH_BUFFER     0x00001c00
+#define TRACE_CHECK_MODEM      0x00001d00
+#define TRACE_CTS_UP           0x00001d01
+#define TRACE_CTS_DOWN         0x00001d02
+#define TRACE_INTERRUPT        0x00001e00
+#define TRACE_XMIT_COMPLETE    0x00001e01
+#define TRACE_RCV_COMPLETE     0x00001e02
+#define TRACE_FID_STATUS       0x00001e03
+#define TRACE_RCV_COUNT        0x00001e04
+#define TRACE_REAL_COUNT       0x00001e05
+#define TRACE_BREAK_DET        0x00001e06
+#define TRACE_IGNORE_CHAR      0x00001e07
+#define TRACE_PARITY_ERROR     0x00001e08
+#define TRACE_XMIT_DISABLED    0x00001e09
+#define TRACE_WAKEUP           0x00001f00
+#define TRACE_CLEAR_INTERRUPTS 0x0000ff00
+#define TRACE_START_PROC_A     0x0000ff01
+#define TRACE_START_PROC_B     0x0000ff02
+#define TRACE_STOP_PROC_A      0x0000ff03
+#define TRACE_STOP_PROC_B      0x0000ff04
+#define TRACE_RAISE_DTR_RTS    0x0000ff05
+#define TRACE_START_PROC_C     0x0000ff06
+#define TRACE_START_PROC_D     0x0000ff07
+#define TRACE_STOP_PROC_C      0x0000ff08
+#define TRACE_STOP_PROC_D      0x0000ff09
+#define TRACE_ENABLE_INTERRUPTS_PA 0x0000ff0a
+#define TRACE_ENABLE_INTERRUPTS_PB 0x0000ff0b
+#define TRACE_ENABLE_INTERRUPTS_PC 0x0000ff0c
+#define TRACE_ENABLE_INTERRUPTS_PD 0x0000ff0d
+#define TRACE_DIS_INTERRUPTS_PA 0x0000ff0e
+#define TRACE_DIS_INTERRUPTS_PB 0x0000ff0f
+#define TRACE_DIS_INTERRUPTS_PC 0x0000ff10
+#define TRACE_DIS_INTERRUPTS_PD 0x0000ff11
+#define TRACE_DROP_DTR_RTS   0x0000ff12
+
+#ifndef TRACE_ONLY
+
+#define DIAG_MAIN_START				0xCD700100
+#define DIAG_MAIN_INIT				0xCD700102
+#define DIAG_MAIN_BIST				0xCD700104
+#define DIAG_MAIN_MEMORY			0xCD700106
+#define DIAG_MAIN_INTERRUPT			0xCD700108
+#define DIAG_MAIN_INSTRUCTION			0xCD70010A
+#define DIAG_MAIN_REGISTER			0xCD70010C
+#define DIAG_MAIN_RVX_STATUS			0xCD70010E
+#define DIAG_MAIN_RVX_INTERNAL_WRAP		0xCD700110
+#define DIAG_MAIN_END				0xCD7001FF
+
+#define DIAG_INIT_ERROR_DATA			0xCD700200
+#define DIAG_INIT_SET_GLOBAL_REGS		0xCD700210
+#define DIAG_INIT_END				0xCD700220
+
+#define DIAG_INTHANDLE_START			0xCD700300
+#define DIAG_INTHANDLE_CHECK_FOR_INTS		0xCD700310
+#define DIAG_INTHANDLE_END			0xCD700320
+
+#define DIAG_LOAD_STOP_PROC			0xCD700400
+#define DIAG_LOAD_ZERO_DRAM			0xCD700410
+#define DIAG_LOAD_LOAD_CODE			0xCD700420
+#define DIAG_LOAD_DONE				0xCD700430
+
+#define DIAG_BIST_CLEAR_INTS			0xCD700500
+#define DIAG_BIST_SAVE_PCI_CMD_REG		0xCD700510
+#define DIAG_BIST_READ_PCI_CMD_FAIL		0xCD700520
+#define DIAG_BIST_SAVE_CMD_REG			0xCD700530
+#define DIAG_BIST_READ_CMD_REG_FAIL		0xCD700540
+#define DIAG_BIST_SAVE_BAR0_REG			0xCD700550
+#define DIAG_BIST_READ_BAR0_FAIL		0xCD700560
+#define DIAG_BIST_HOOK_INT_LINE			0xCD700570
+#define DIAG_BIST_HOOK_INT_HANDLER_FAIL		0xCD700580
+#define DIAG_BIST_START_BIST			0xCD700590
+#define DIAG_BIST_START_BIST_FAIL		0xCD7005A0
+#define DIAG_BIST_WAIT_FOR_INT			0xCD7005B0
+#define DIAG_BIST_CHECK_INT_REGS		0xCD7005C0
+#define DIAG_BIST_NO_BIST_INT_FAIL		0xCD7005D0
+#define DIAG_CHECK_READ_BIST_REG		0xCD7005E0
+#define DIAG_BIST_READ_CONFIG_OFFSET_0C_FAIL	0xCD7005F0
+#define DIAG_BIST_CHECK_MISR_AND_PRPG		0xCD700600
+#define DIAG_BIST_INCORRECT_MISR_OR_PRPG	0xCD700610
+#define DIAG_BIST_CHECK_BAR0			0xCD700620
+#define DIAG_BIST_READ_BAR0_FAIL		0xCD700630
+#define DIAG_BIST_CHECK_BAR0_RESET_VALUE	0xCD700644
+#define DIAG_BIST_BAR0_RESET_FAIL		0xCD700652
+#define DIAG_BIST_RESTORE_BAR0_REG		0xCD700664
+#define DIAG_BIST_WRITE_BAR0_FAIL		0xCD700670
+#define DIAG_BIST_RESTORE_PCI_CMD_REG		0xCD700680
+#define DIAG_BIST_WRITE_PCI_COMMAND_FAIL	0xCD700690
+#define DIAG_BIST_RETURN_INT_HANDLE		0xCD7006A0
+
+#define DIAG_MEM_FOR_EVERY_PORT			0xCD700700
+#define DIAG_MEM_STOP_PROC			0xCD700710
+#define DIAG_MEM_INVALID_PORT_SELECT		0xCD700720
+#define DIAG_MEM_CALL_MEM_TEST			0xCD700730
+
+#define DIAG_MEM_WRITE_ADDR_TAG			0xCD700800
+#define DIAG_MEM_CHECK_ADDR_TAG			0xCD700810
+#define DIAG_MEM_ADDR_TAG_TEST_FAILED		0xCD700820
+#define DIAG_MEM_WRITE_AA			0xCD700830
+#define DIAG_MEM_CHECK_AA			0xCD700840
+#define DIAG_MEM_AA_TEST_FAIL			0xCD700850
+#define DIAG_MEM_CHECK_55			0xCD700860
+#define DIAG_MEM_55_TEST_FAIL			0xCD700870
+#define DIAG_MEM_CHECK_00			0xCD700880
+#define DIAG_MEM_00_TEST_FAIL			0xCD700890
+
+#define DIAG_INT_CLEAR_INTS			0xCD700900
+#define DIAG_INT_GET_INT_HANDLE			0xCD700910
+#define DIAG_INT_GET_IRQ_LINE_FAIL		0xCD700920
+#define DIAG_INT_LOAD_PROCS			0xCD700930
+#define DIAG_INT_CLEAR_INT_VARS			0xCD700940
+#define DIAG_INT_START_PROCS			0xCD700950
+#define DIAG_INT_START_TESTS			0xCD700954
+#define DIAG_INT_DETERMINE_INTS_TO_CHECK_FOR	0xCD700960
+#define DIAG_INT_WAIT_FOR_INTS			0xCD700970
+#define DIAG_INT_CHECK_FOR_INTS			0xCD700980
+#define DIAG_INT_WRONG_INT_FAIL			0xCD700990
+#define DIAG_INT_STOP_PROCS			0xCD7009A0
+#define DIAG_INT_FREE_INT_HANDLE		0xCD7009B0
+#define DIAG_INT_END				0xCD7009C0
+
+#define DIAG_INST_CLEAR_INTS			0xCD700A00
+#define DIAG_INST_GET_INT_HANDLE		0xCD700A10
+#define DIAG_INST_GET_IRQ_LINE_FAIL		0xCD700A20
+#define DIAG_INST_LOAD_PROCS			0xCD700A30
+#define DIAG_INST_CLEAR_INT_VARS		0xCD700A40
+#define DIAG_INST_START_PROCS			0xCD700A50
+#define DIAG_INST_START_TESTS			0xCD700A54
+#define DIAG_INST_DETERMINE_INTS_TO_CHECK_FOR	0xCD700A60
+#define DIAG_INST_WAIT_FOR_INTS			0xCD700A70
+#define DIAG_INST_CHECK_FOR_INTS		0xCD700A80
+#define DIAG_INST_WRONG_INT_FAIL		0xCD700A90
+#define DIAG_INST_STOP_PROCS			0xCD700AA0
+#define DIAG_INST_CHECK_RESULTS			0xCD700AB0
+#define DIAG_INST_BAD_RESULTS_DATA_FAIL		0xCD000AC0
+#define DIAG_INST_FREE_INT_HANDLE		0xCD700AD0
+#define DIAG_INST_END				0xCD700AE0
+
+#define DIAG_REG_CLEAR_INTS			0xCD700B00
+#define DIAG_REG_GET_INT_HANDLE			0xCD700B10
+#define DIAG_REG_GET_IRQ_LINE_FAIL		0xCD700B20
+#define DIAG_REG_LOAD_PROCS			0xCD700B30
+#define DIAG_REG_CLEAR_INT_VARS			0xCD700B40
+#define DIAG_REG_START_PROCS			0xCD700B50
+#define DIAG_REG_DETERMINE_INTS_TO_CHECK_FOR	0xCD700B60
+#define DIAG_REG_WAIT_FOR_INTS			0xCD700B70
+#define DIAG_REG_CHECK_FOR_INTS			0xCD700B80
+#define DIAG_REG_WRONG_INT_FAIL			0xCD700B90
+#define DIAG_REG_STOP_PROCS			0xCD700BA0
+#define DIAG_REG_CHECK_RESULTS			0xCD700BB0
+#define DIAG_REG_BAD_RESULTS_DATA_FAIL		0xCD000BC0
+#define DIAG_REG_FREE_INT_HANDLE		0xCD700BD0
+#define DIAG_REG_END				0xCD700BE0
+
+/* The following REG2 TPR's are for the second pass. */
+/* It is the same test as REG but with different pico code */
+#define DIAG_REG2_CLEAR_INTS			0xCD700C00
+#define DIAG_REG2_GET_INT_HANDLE		0xCD700C10
+#define DIAG_REG2_GET_IRQ_LINE_FAIL		0xCD700C20
+#define DIAG_REG2_LOAD_PROCS			0xCD700C30
+#define DIAG_REG2_CLEAR_INT_VARS		0xCD700C40
+#define DIAG_REG2_START_PROCS			0xCD700C50
+#define DIAG_REG2_DETERMINE_INTS_TO_CHECK_FOR	0xCD700C60
+#define DIAG_REG2_WAIT_FOR_INTS			0xCD700C70
+#define DIAG_REG2_CHECK_FOR_INTS		0xCD700C80
+#define DIAG_REG2_WRONG_INT_FAIL		0xCD700C90
+#define DIAG_REG2_STOP_PROCS			0xCD700CA0
+#define DIAG_REG2_CHECK_RESULTS			0xCD700CB0
+#define DIAG_REG2_BAD_RESULTS_DATA_FAIL		0xCD000CC0
+#define DIAG_REG2_FREE_INT_HANDLE		0xCD700CD0
+#define DIAG_REG2_END				0xCD700CE0
+
+#define DIAG_RVXSTS_CLEAR_INTS			0xCD700D00
+#define DIAG_RVXSTS_GET_INT_HANDLE		0xCD700D10
+#define DIAG_RVXSTS_GET_IRQ_LINE_FAIL		0xCD700D20
+#define DIAG_RVXSTS_LOAD_PROCS			0xCD700D30
+#define DIAG_RVXSTS_CLEAR_INT_VARS		0xCD700D40
+#define DIAG_RVXSTS_START_PROCS			0xCD700D50
+#define DIAG_RVXSTS_DETERMINE_INTS_TO_CHECK_FOR	0xCD700D60
+#define DIAG_RVXSTS_WAIT_FOR_INTS		0xCD700D70
+#define DIAG_RVXSTS_CHECK_FOR_INTS		0xCD700D80
+#define DIAG_RVXSTS_WRONG_INT_FAIL		0xCD700D90
+#define DIAG_RVXSTS_STOP_PROCS			0xCD700DA0
+#define DIAG_RVXSTS_CHECK_RESULTS		0xCD700DB0
+#define DIAG_RVXSTS_BAD_RESULTS_DATA_FAIL	0xCD000DC0
+#define DIAG_RVXSTS_FREE_INT_HANDLE		0xCD700DD0
+#define DIAG_RVXSTS_END				0xCD700DE0
+
+#define DIAG_RVXINTWRP_CLEAR_INTS		0xCD700E00
+#define DIAG_RVXINTWRP_GET_INT_HANDLE		0xCD700E10
+#define DIAG_RVXINTWRP_GET_IRQ_LINE_FAIL	0xCD700E20
+#define DIAG_RVXINTWRP_LOAD_PROCS		0xCD700E30
+#define DIAG_RVXINTWRP_CLEAR_INT_VARS		0xCD700E40
+#define DIAG_RVXINTWRP_START_PROCS		0xCD700E50
+#define DIAG_RVXINTWRP_DETERMINE_INTS_TO_CHECK_FOR 0xCD700E60
+#define DIAG_RVXINTWRP_WAIT_FOR_INTS		0xCD700E70
+#define DIAG_RVXINTWRP_CHECK_FOR_INTS		0xCD700E80
+#define DIAG_RVXINTWRP_WRONG_INT_FAIL		0xCD700E90
+#define DIAG_RVXINTWRP_STOP_PROCS		0xCD700EA0
+#define DIAG_RVXINTWRP_CHECK_RESULTS		0xCD700EB0
+#define DIAG_RVXINTWRP_BAD_RESULTS_DATA_FAIL	0xCD000EC0
+#define DIAG_RVXINTWRP_FREE_INT_HANDLE		0xCD700ED0
+#define DIAG_RVXINTWRP_END			0xCD700EE0
+
+#define DIAG_SMINTWRP_START_TPR			0xCD700F00
+#define DIAG_SMINTWRP_DISABLE_COMP_TPR          0xCD700F10
+#define DIAG_SMINTWRP_WAIT_FOR_DATA_TPR         0xCD700F20
+#define DIAG_SMINTWRP_SET_LOOPBACK_TPR          0xCD700F30
+#define DIAG_SMINTWRP_WAIT_FOR_DATA2_TPR        0xCD700F30
+#define DIAG_SMINTWRP_SEND_U_TPR                0xCD700F40
+#define DIAG_SMINTWRP_WAIT_FOR_DATA4_TPR        0xCD700F50
+#define DIAG_SMINTWRP_SEND_ATH_TPR              0xCD700F60
+#define DIAG_SMINTWRP_WAIT_FOR_ATH_OK_TPR       0xCD700F70
+#define DIAG_SMINTWRP_NOT_OK_FAIL_TPR           0xCD700F80
+#define DIAG_SMINTWRP_NOT_U_FAIL_TPR            0xCD700F90
+#define DIAG_SMINTWRP_NOT_CONNECT_FAIL_TPR      0xCD700FA0
+#define DIAG_SMINTWRP_NOT_OK_FAIL1_TPR          0xCD700FB0
+#define DIAG_SMINTWRP_END_TPR                   0xCD700FC0
+ 
+static unsigned char callSetup[1968] = 
+          {0xBD,0xD9,0x23,0x00,0xDD,0xDD,0x18,0x05,0x23,0x80,0x3E,0x7F,0x23,0x00,0x3E,0x7C,
+           0x23,0x10,0x3E,0xB7,0x3E,0xB5,0x23,0x20,0x3E,0xB6,0x3E,0xB4,0xA2,0x0A,0x86,0x0A,
+           0xAE,0x0A,0xBE,0x0A,0x23,0x80,0x3E,0x01,0x23,0x2A,0x3E,0x02,0x3D,0xDA,0x23,0xFF,
+           0x3E,0x03,0x22,0x86,0xD9,0xDD,0x10,0x1D,0x22,0x08,0x2F,0xF0,0xD9,0xF1,0x18,0x56,
+           0xD9,0xDD,0x18,0x33,0x23,0x00,0x3E,0x06,0x3E,0x46,0x3E,0x86,0x3E,0xC6,0x21,0xF0,
+           0x37,0xFA,0x2F,0xFA,0x3E,0x08,0x23,0x00,0x3E,0x07,0x3E,0x87,0x3E,0xC7,0x23,0xFF,
+           0x3E,0x09,0x3E,0x47,0x00,0x3A,0x23,0x00,0x3E,0x06,0x3E,0x08,0x23,0x13,0x3E,0x07,
+           0x23,0x01,0x3E,0x09,0xA2,0xFC,0xA2,0xF6,0xCD,0xF0,0x10,0x3F,0x82,0xFC,0xD9,0xDD,
+           0x18,0x4F,0x23,0xFF,0x3E,0x87,0x68,0x70,0x23,0x14,0x68,0x62,0x3F,0x20,0x23,0x00,
+           0x3E,0x87,0x68,0x70,0x23,0x14,0x68,0x62,0x3F,0x20,0x22,0x86,0x00,0x54,0x68,0x70,
+           0x23,0x14,0x68,0x62,0x3F,0x20,0x22,0x08,0x2F,0xF0,0x3D,0xF5,0x3B,0xB0,0x10,0x6E,
+           0x23,0x00,0x3E,0x02,0x23,0x01,0x3E,0x7C,0x23,0x14,0x3E,0x03,0x23,0x00,0x3E,0x7C,
+           0x23,0x80,0xDD,0xF2,0x1B,0xA3,0x68,0x69,0x3B,0x00,0x10,0x61,0x23,0x09,0x3E,0x02,
+           0x3D,0xDA,0x23,0x08,0xDD,0xDD,0x18,0x6D,0x23,0x88,0x3E,0x7F,0x21,0xDA,0xC9,0xF0,
+           0x10,0x72,0x33,0x04,0x3E,0x02,0x3D,0xDA,0xD9,0xF1,0x10,0x79,0x23,0x00,0x3D,0xF1,
+           0x00,0xA1,0x21,0xF5,0x2F,0xF0,0x3B,0xE0,0x10,0x7F,0x23,0x22,0x00,0x98,0x3B,0x60,
+           0x10,0x83,0x23,0x22,0x00,0x98,0x3B,0xC0,0x10,0x87,0x23,0xEE,0x00,0x98,0x3B,0xB0,
+           0x10,0x8B,0x23,0x44,0x00,0x98,0x3B,0x20,0x10,0x8F,0x23,0xC4,0x00,0x98,0x3B,0x30,
+           0x10,0x93,0x23,0x22,0x00,0x98,0x3B,0xF0,0x1B,0xA7,0x00,0xBE,0xD9,0xDD,0x18,0x99,
+           0x3E,0x46,0x23,0x11,0x68,0x69,0x3B,0x00,0x10,0x9A,0x22,0x06,0x37,0xFF,0x3D,0xEF,
+           0x81,0xF5,0x0A,0xE0,0x08,0xC4,0x08,0xE3,0x0A,0x5B,0x0B,0x5D,0xDD,0xF3,0x10,0xA1,
+           0xBD,0xF3,0x9D,0xD9,0x21,0xDE,0x3B,0x00,0x18,0xB5,0x5D,0x00,0x4B,0xC8,0x41,0xE0,
+           0x22,0x7F,0x33,0x40,0x3E,0x7F,0x21,0xDE,0x4D,0xC8,0x58,0x00,0x4B,0xC8,0x41,0xFC,
+           0x22,0x7F,0x2F,0xBF,0x3E,0x7F,0x21,0xF7,0x4D,0xC8,0x00,0xBD,0x23,0xDD,0x3D,0xF4,
+           0x0B,0xC6,0x96,0xF4,0x9E,0xF6,0x00,0xC3,0xDD,0xE6,0x68,0x6E,0xC5,0xE6,0x68,0x6F,
+           0xC1,0xE6,0x18,0xD4,0x81,0xE6,0x68,0x70,0x43,0xC8,0x45,0xCA,0x21,0xE4,0x68,0x62,
+           0x21,0xE5,0x68,0x74,0x47,0xD4,0x68,0x6D,0x41,0xD2,0x43,0xC8,0x50,0x00,0x47,0xC8,
+           0x41,0xC8,0x68,0x71,0x68,0x6E,0xC1,0xE6,0x68,0x6E,0x85,0xE6,0x8A,0xF4,0xC1,0xE6,
+           0x68,0x6F,0xA5,0xE6,0x68,0x6D,0x21,0xE7,0x2F,0x0F,0x3D,0xCA,0x2F,0x0C,0x3B,0x0C,
+           0x68,0x6E,0x21,0xE7,0xDE,0xF0,0x1A,0x4F,0xDA,0xF0,0x19,0x0C,0xA2,0x0A,0x99,0xE7,
+           0x23,0x00,0x3D,0xD6,0x3D,0xE8,0x21,0xCA,0x3B,0x0F,0x10,0xFE,0x23,0x81,0x3E,0x01,
+           0x21,0xDA,0x3E,0x02,0x23,0x7E,0x3E,0x03,0x92,0xF1,0x01,0x0A,0x23,0x40,0x3E,0x01,
+           0x21,0xC6,0x3E,0x02,0x21,0xC7,0x3E,0x03,0x21,0xCA,0x3B,0x0C,0x23,0x00,0x11,0x09,
+           0x23,0x01,0x3D,0xD7,0x82,0x0A,0x68,0x6D,0x21,0xCA,0x3B,0x0F,0x19,0x4A,0x21,0xE7,
+           0xDE,0xF0,0x1A,0x4F,0xD6,0xF0,0x11,0x30,0x68,0x70,0x43,0xC8,0x21,0xCA,0xC2,0xF0,
+           0x21,0xC9,0x68,0x64,0xDD,0xD7,0x11,0x26,0x3D,0xC8,0x21,0xCA,0xC6,0xF0,0x21,0xC8,
+           0x68,0x64,0x3D,0xC8,0x21,0xCA,0xCA,0xF0,0x21,0xC8,0x68,0x64,0x68,0x6B,0x29,0xD8,
+           0x3B,0x7F,0xDA,0xF1,0x21,0xE7,0x2F,0x0F,0x3D,0xCA,0x11,0x30,0x91,0xE7,0x02,0x14,
+           0xDE,0x04,0x68,0x6E,0x68,0x70,0x43,0xC8,0x21,0xCA,0xC2,0xF0,0x21,0xC9,0x68,0x64,
+           0x3D,0xD8,0x21,0xCA,0x3D,0xCB,0xDD,0xD7,0x11,0x47,0x21,0xCA,0xC6,0xF0,0x21,0xD8,
+           0x68,0x64,0x3D,0xD8,0x21,0xCA,0xCA,0xF0,0x21,0xD8,0x68,0x64,0x3D,0xD8,0x21,0xE7,
+           0x2F,0x0F,0x3D,0xCA,0xDE,0x04,0x68,0x6E,0x22,0x00,0xCA,0x0A,0x11,0x56,0xA2,0x0A,
+           0x99,0xE8,0x21,0xCA,0x3B,0x0F,0x1A,0x56,0x82,0x0A,0x68,0x6D,0x3D,0xC8,0x22,0x04,
+           0x3D,0xC9,0x21,0xCA,0x3B,0x0F,0x11,0x66,0x21,0xC9,0x2F,0x38,0x3B,0x00,0x11,0x63,
+           0x21,0xC8,0x3E,0xB0,0x01,0xFA,0xD5,0xE7,0x68,0x6E,0x02,0x14,0x21,0xC9,0xD6,0xF0,
+           0x11,0x6A,0x95,0xE8,0xCE,0xF0,0x11,0x6D,0x9D,0xE8,0x21,0xD7,0x3B,0x00,0x19,0xF9,
+           0xDE,0xF0,0x11,0x7A,0xDA,0xF0,0x19,0xBC,0x21,0xC8,0x3B,0x54,0x11,0xFA,0x23,0xC2,
+           0x3D,0xD7,0x01,0xFA,0x3B,0x01,0x11,0x80,0x21,0xC8,0x3B,0x52,0x11,0xF7,0x01,0xF3,
+           0x3B,0x02,0x11,0x86,0x21,0xC8,0x3B,0x4F,0x11,0xF7,0x01,0xF3,0x3B,0x03,0x11,0x8C,
+           0x21,0xC8,0x3B,0x42,0x11,0xF7,0x01,0xF3,0x3B,0x04,0x11,0x92,0x21,0xC8,0x3B,0x47,
+           0x11,0xF7,0x01,0xF3,0x3B,0x05,0x11,0x98,0x21,0xC8,0x3B,0x20,0x11,0xF7,0x01,0xF3,
+           0x3B,0x06,0x11,0x9E,0x21,0xC8,0x3B,0x54,0x11,0xF7,0x01,0xF3,0x3B,0x07,0x11,0xA4,
+           0x21,0xC8,0x3B,0x4F,0x11,0xF7,0x01,0xF3,0x3B,0x08,0x11,0xAA,0x21,0xC8,0x3B,0x4E,
+           0x11,0xF7,0x01,0xF3,0x3B,0x09,0x11,0xF7,0x21,0xC8,0x3B,0x59,0x11,0xF7,0x23,0x80,
+           0x3D,0xD7,0x21,0xCB,0xC6,0xF0,0x21,0xD8,0x68,0x64,0x3D,0xD8,0x21,0xCB,0xCA,0xF0,
+           0x21,0xD8,0x68,0x64,0x3D,0xD8,0x01,0xFA,0x2F,0x3F,0x3B,0x02,0x11,0xC3,0x21,0xC8,
+           0x3B,0x4F,0x11,0xF0,0x01,0xF3,0x3B,0x03,0x11,0xC9,0x21,0xC8,0x3B,0x4E,0x11,0xF0,
+           0x01,0xF3,0x3B,0x04,0x11,0xCF,0x21,0xC8,0x3B,0x59,0x11,0xF0,0x01,0xF3,0x3B,0x05,
+           0x11,0xD5,0x21,0xC8,0x3B,0x20,0x11,0xF0,0x01,0xF3,0x3B,0x06,0x11,0xDB,0x21,0xC8,
+           0x3B,0x52,0x11,0xF0,0x01,0xF3,0x3B,0x07,0x11,0xE1,0x21,0xC8,0x3B,0x4F,0x11,0xF0,
+           0x01,0xF3,0x3B,0x08,0x11,0xE7,0x21,0xC8,0x3B,0x42,0x11,0xF0,0x01,0xF3,0x3B,0x09,
+           0x11,0xF0,0x21,0xC8,0x3B,0x47,0x11,0xF0,0x21,0xE8,0x33,0x01,0x3D,0xE8,0x01,0xFA,
+           0x23,0x80,0x3D,0xD7,0x01,0x74,0x21,0xD7,0x68,0x68,0x3D,0xD7,0x01,0xF9,0x23,0x00,
+           0x3D,0xD7,0x95,0xE7,0xD1,0xE8,0x1A,0x23,0x45,0xDE,0xDE,0xF1,0x12,0x01,0x91,0xE8,
+           0x02,0x16,0x51,0xCC,0x21,0xD6,0x68,0x62,0x21,0xC8,0x68,0x67,0x45,0xDE,0x23,0x01,
+           0x68,0x63,0x47,0xDE,0x21,0xD6,0x68,0x68,0x3D,0xD6,0x21,0xE8,0x2F,0x07,0x3B,0x01,
+           0x1A,0x14,0x21,0xD6,0x3B,0x08,0x12,0x23,0xD1,0xE8,0x1A,0x23,0x21,0xD6,0x3B,0x00,
+           0x1A,0x23,0x41,0xE0,0x57,0xCC,0xDD,0xF2,0x1B,0xA3,0xC2,0xF1,0x12,0x1B,0x68,0x62,
+           0x43,0xE0,0x23,0x00,0x3D,0xD6,0x21,0xCA,0x3B,0x0F,0x12,0x48,0x21,0xC9,0x2F,0x38,
+           0x3B,0x00,0x12,0x2C,0x95,0xE7,0x68,0x6D,0x3B,0x10,0x12,0x43,0x21,0xC8,0x3B,0x7E,
+           0x12,0x40,0x22,0x74,0x3B,0x47,0x12,0x3D,0x22,0x75,0x3B,0x0F,0x12,0x3D,0x21,0xE7,
+           0xDE,0xF0,0x1A,0x4F,0xD6,0xF0,0x68,0x6E,0x02,0x56,0x23,0x04,0x3D,0xE8,0x02,0x56,
+           0x23,0x03,0x3D,0xE8,0x02,0x56,0xCE,0xF0,0x68,0x6F,0x23,0x02,0x3D,0xE8,0x02,0x56,
+           0x21,0xE8,0x2F,0x07,0x3B,0x01,0x1A,0x56,0xD1,0xE7,0x68,0x6E,0x02,0x56,0xA2,0x0A,
+           0xCE,0xF4,0x68,0x6F,0xDD,0xE7,0x68,0x6E,0x23,0x07,0x3D,0xE8,0xA2,0x0A,0x23,0x00,
+           0x3D,0xE7,0x8E,0xF4,0x68,0x6D,0x21,0xE7,0x2F,0x0F,0x3D,0xCA,0x2F,0x0C,0x3D,0xCB,
+           0x3B,0x08,0x68,0x6E,0x21,0xE7,0xDE,0xF0,0x1A,0xD4,0xDA,0xF0,0x1A,0x80,0x99,0xE7,
+           0x23,0x00,0x3D,0xD6,0x3D,0xE8,0x21,0xCA,0x3B,0x0B,0x12,0x77,0x23,0x81,0x3E,0x01,
+           0x21,0xDA,0x3E,0x02,0x23,0x7E,0x3E,0x03,0x86,0x0A,0xAE,0x0A,0x68,0x6D,0x23,0x40,
+           0x3E,0x01,0x21,0xC6,0x3E,0x02,0x21,0xC7,0x3E,0x03,0x86,0x0A,0xAE,0x0A,0x68,0x6D,
+           0x21,0xD6,0x3B,0x00,0x12,0xA4,0x45,0xDE,0xDE,0xF1,0x12,0x8E,0x21,0xCA,0x3B,0x0B,
+           0x12,0xDC,0xD1,0xE7,0x12,0xC0,0xC6,0x04,0x68,0x6E,0x02,0xDC,0x21,0xDF,0x3B,0x00,
+           0x12,0x95,0x21,0xDE,0x3B,0x08,0xDA,0xF1,0x12,0x96,0x23,0x08,0x41,0xE0,0x55,0xCC,
+           0xDD,0xF2,0x1B,0xA3,0xC2,0xF1,0x12,0x98,0x68,0x62,0x43,0xE0,0x45,0xDE,0x68,0x63,
+           0x47,0xDE,0x3D,0xD6,0x23,0x00,0x3D,0xD8,0xC2,0x04,0x68,0x6E,0xD5,0xE7,0x1A,0xB1,
+           0x21,0xCA,0x3B,0x0B,0x12,0xB1,0x23,0x7E,0x68,0x6A,0x3E,0x00,0x95,0xE7,0x96,0xF1,
+           0xAE,0x0A,0x51,0xCC,0x21,0xD8,0x68,0x62,0x68,0x68,0x3D,0xD8,0x68,0x66,0x68,0x6B,
+           0x3E,0x00,0x3E,0xB1,0xCE,0x0A,0x1A,0xD0,0x21,0xD6,0x68,0x69,0x3D,0xD6,0x02,0x80,
+           0x22,0x05,0x2F,0x0F,0x3B,0x0C,0xDA,0xF1,0x68,0x6F,0x22,0x78,0x68,0x6B,0x3E,0x00,
+           0x22,0x79,0x3E,0x00,0x23,0x7E,0x68,0x6A,0x3E,0x00,0x91,0xE7,0xCE,0x0A,0x68,0x6E,
+           0x86,0x0A,0xAE,0x0A,0x99,0xE8,0x02,0xDC,0x86,0x0A,0xAE,0x0A,0xCE,0xF4,0x68,0x6F,
+           0xDD,0xE7,0x68,0x6E,0x23,0x07,0x3D,0xE8,0x23,0x00,0x3D,0xE7,0x8E,0xF4,0x68,0x6D,
+           0x22,0x86,0xD9,0xDD,0x12,0xE4,0x22,0x08,0x2F,0xF0,0x3B,0xB0,0x1B,0x13,0x22,0x86,
+           0xD9,0xDD,0x12,0xEB,0x22,0x08,0x2F,0xF0,0x3B,0xF0,0x1B,0xA7,0xD9,0xDD,0x1A,0xF5,
+           0x21,0xF0,0x37,0xFA,0x2F,0xFA,0x3E,0x08,0x02,0xFE,0xDD,0xF0,0x68,0x6C,0xF2,0x06,
+           0xD9,0xF0,0x68,0x6C,0xE6,0x06,0xC5,0xF0,0x68,0x6C,0xE2,0x06,0xDD,0xF2,0x1B,0xA3,
+           0xDD,0xF1,0x13,0x05,0xDD,0xD9,0x1B,0xAD,0xBD,0xF1,0x22,0x06,0x37,0xFF,0x39,0xEF,
+           0x13,0x0A,0x68,0x6D,0x3D,0xC8,0x35,0xEF,0x2D,0xEE,0x3B,0x00,0x21,0xC8,0x3D,0xEF,
+           0x68,0x6F,0x92,0xF4,0x68,0x6D,0x22,0x86,0xD9,0xDD,0x13,0x17,0x22,0x08,0x2F,0xF0,
+           0x3B,0xF0,0x1B,0xA7,0xD9,0xDD,0x1B,0x20,0xDD,0xF0,0x68,0x6C,0xFE,0x08,0x03,0x23,
+           0xDD,0xF0,0x68,0x6C,0xF2,0x06,0xDD,0xE9,0x13,0x2C,0xD9,0xE9,0x1B,0x2C,0x22,0xBC,
+           0x3B,0x10,0x13,0x2D,0x99,0xE9,0x82,0xF4,0x3E,0xBC,0xDD,0xEA,0x13,0x36,0xD9,0xEA,
+           0x1B,0x36,0x22,0xBD,0x3B,0x10,0x13,0x37,0x99,0xEA,0x82,0xF4,0x3E,0xBD,0xDD,0xEB,
+           0x13,0x40,0xD9,0xEB,0x1B,0x40,0x22,0xBE,0x3B,0x10,0x13,0x41,0x99,0xEB,0x82,0xF4,
+           0x3E,0xBE,0xDD,0xEC,0x13,0x4A,0xD9,0xEC,0x1B,0x4A,0x22,0xBF,0x3B,0x10,0x13,0x4B,
+           0x99,0xEC,0x82,0xF4,0x3E,0xBF,0xDD,0xF2,0x1B,0xA3,0xDD,0xF1,0x13,0x52,0xDD,0xD9,
+           0x1B,0xAD,0xBD,0xF1,0x22,0x06,0x37,0xFF,0x2F,0x80,0x39,0xEF,0x13,0x58,0x68,0x6D,
+           0x3D,0xEF,0xDD,0xEE,0x68,0x6E,0x92,0xF4,0x68,0x6D,0x21,0xE7,0x2F,0x0F,0x3B,0x04,
+           0x68,0x6E,0x21,0xE7,0xDE,0xF0,0x1B,0x83,0xDA,0xF0,0x1B,0x78,0x68,0x70,0x43,0xC8,
+           0x21,0xCA,0xC2,0xF0,0x21,0xC9,0x68,0x64,0x3D,0xD8,0x23,0x81,0x3E,0x01,0x21,0xDA,
+           0x33,0x20,0x3E,0x02,0x23,0x80,0x3E,0x01,0x23,0x00,0x3E,0xF5,0x99,0xE7,0x68,0x6D,
+           0x68,0x70,0x43,0xC8,0x21,0xCA,0xC2,0xF0,0x21,0xC9,0x68,0x64,0x68,0x6B,0x29,0xD8,
+           0x3B,0x3B,0xDA,0xF1,0x13,0x8E,0xCE,0xF4,0x68,0x6F,0xDD,0xE7,0x68,0x6E,0x21,0xDA,
+           0x2F,0xDF,0x3E,0x02,0x68,0x6A,0xFE,0x08,0xBD,0xF0,0x03,0x9F,0xD6,0xF0,0x1B,0x9B,
+           0x22,0xF5,0x3B,0x08,0xDA,0xF1,0x68,0x6E,0x21,0xDA,0x2F,0xDF,0x3E,0x02,0x68,0x6A,
+           0xFE,0x08,0xBD,0xF0,0x95,0xE7,0x22,0xF5,0x3B,0x22,0xDA,0xF1,0x68,0x6E,0x23,0x00,
+           0x3D,0xE7,0x8E,0xF4,0x68,0x6D,0x0B,0xC6,0xBD,0xF2,0x9E,0xF6,0x03,0xA4,0x23,0xCC,
+           0x3D,0xF4,0x0B,0xC6,0x96,0xF4,0x9E,0xF6,0x03,0xAC,0xBD,0xD9,0xA2,0x0A,0x86,0x0A,
+           0xAE,0x0A,0xBD,0xF1,0x0B,0x5D,0xD9,0xF1,0x1B,0xBD,0x0A,0xE0,0xDD,0xF3,0x13,0xB1,
+           0x23,0x00,0x3D,0xF1,0xBD,0xF3,0x9D,0xD9,0x00,0x00,0x58,0x00,0x4B,0xC8,0x41,0xF8,
+           0x22,0x7F,0x2F,0xBF,0x3E,0x7F,0x21,0xF6,0x4D,0xC8,0x03,0xC5,0xA2,0x0A,0x86,0x0A,
+           0xAE,0x0A,0xD9,0xDD,0x1B,0xCD,0x23,0x00,0x3E,0x46,0x23,0x00,0x3D,0xF5,0x3D,0xF0,
+           0x3D,0xEF,0x23,0x12,0x3E,0x06,0x23,0xC0,0x3E,0x08,0x68,0x6D,0x00,0x00,0x00,0x00};
+
+static unsigned char resRVdce[192] = 
+          {0x22,0x86,0xD9,0xDD,0x15,0x04,0x22,0x08,0x2F,0xF0,0x3B,0xF0,0x1D,0x30,0xD9,0xDD,
+           0x1D,0x0E,0x21,0xF0,0x37,0xFA,0x2F,0xFA,0x3E,0x08,0x05,0x17,0xDD,0xF0,0x68,0x6C,
+           0xF2,0x06,0xD9,0xF0,0x68,0x6C,0xE6,0x06,0xC5,0xF0,0x68,0x6C,0xE2,0x06,0xDD,0xF2,
+           0x1D,0x2C,0xDD,0xF1,0x15,0x1E,0xDD,0xD9,0x1D,0x36,0xBD,0xF1,0x22,0x06,0x37,0xFF,
+           0x39,0xEF,0x15,0x23,0x68,0x6D,0x3D,0xC8,0x35,0xEF,0x2D,0xEE,0x3B,0x00,0x21,0xC8,
+           0x3D,0xEF,0x68,0x6F,0x92,0xF4,0x68,0x6D,0x0D,0x4F,0xBD,0xF2,0x9E,0xF6,0x05,0x2D,
+           0x23,0xCC,0x3D,0xF4,0x0D,0x4F,0x96,0xF4,0x9E,0xF6,0x05,0x35,0xBD,0xD9,0xA2,0x0A,
+           0x86,0x0A,0xAE,0x0A,0xBD,0xF1,0x0D,0x5F,0xD9,0xF1,0x1D,0x46,0x0D,0x00,0xDD,0xF3,
+           0x15,0x3A,0x23,0x00,0x3D,0xF1,0xBD,0xF3,0x9D,0xD9,0x00,0x00,0x58,0x00,0x4B,0xC8,
+           0x41,0xF8,0x22,0x7F,0x2F,0xBF,0x3E,0x7F,0x21,0xF6,0x4D,0xC8,0x05,0x4E,0xA2,0x0A,
+           0x86,0x0A,0xAE,0x0A,0xD9,0xDD,0x1D,0x56,0x23,0x00,0x3E,0x46,0x23,0x00,0x3D,0xF5,
+           0x3D,0xF0,0x3D,0xEF,0x23,0x12,0x3E,0x06,0x23,0xC0,0x3E,0x08,0x68,0x6D,0x68,0x6D};
+
+static unsigned char funcLoad[1760] = 
+          {0x81,0xDD,0xA1,0xC5,0x23,0x40,0x3E,0x01,0x21,0xC6,0x3E,0x02,0x21,0xC7,0x3E,0x03,
+           0x23,0x10,0x3E,0xB7,0x3E,0xB5,0x23,0x20,0x3E,0xB6,0x3E,0xB4,0xCD,0xBD,0x18,0x13,
+           0x23,0x7E,0x3D,0x9F,0x00,0x15,0x23,0xC0,0x3D,0x9F,0x50,0x00,0x43,0x84,0x23,0x00,
+           0x3D,0x98,0x0B,0x20,0x09,0xB0,0xC1,0xC5,0x18,0x01,0x49,0x8C,0x68,0x6D,0x21,0xBF,
+           0x3B,0x00,0x18,0x27,0xC2,0x04,0x10,0x34,0x3E,0x00,0x23,0x00,0x3D,0xBF,0xC5,0xC5,
+           0x18,0x34,0xC9,0xC5,0x10,0x32,0xC2,0x04,0x10,0x34,0x23,0x00,0x68,0x6A,0x3E,0x00,
+           0x3E,0x00,0x00,0x34,0x49,0x8E,0x68,0x6D,0x21,0xC5,0x39,0xC4,0x13,0x3C,0xC2,0xF1,
+           0x10,0x5B,0xD5,0x98,0x18,0x46,0x95,0x98,0x45,0x8A,0xDE,0xF1,0x10,0x51,0x21,0x42,
+           0x39,0x40,0x10,0x56,0x45,0x88,0xDE,0xF1,0x10,0x57,0x00,0x5B,0xB5,0x98,0x45,0x88,
+           0xDE,0xF1,0x10,0x57,0x45,0x8A,0xDE,0xF1,0x10,0x51,0x21,0x42,0x39,0x40,0x10,0x56,
+           0x00,0x5B,0x49,0x8A,0x45,0x84,0x47,0x8A,0x68,0x6D,0x00,0x5B,0x00,0xDB,0x49,0x88,
+           0x45,0x84,0x47,0x88,0x68,0x6D,0x0D,0x00,0x00,0x1B,0x00,0x1F,0xDE,0x04,0x10,0x1F,
+           0x78,0x62,0x67,0x8C,0xD9,0x97,0x10,0x66,0xB9,0x97,0x00,0x99,0xDE,0x04,0x10,0xB8,
+           0x68,0x70,0x43,0x80,0x45,0x82,0x21,0xB7,0x68,0x62,0x47,0x82,0x22,0x00,0x3D,0x90,
+           0xCA,0x0A,0x18,0x99,0x22,0x04,0x2F,0x28,0x3D,0x91,0x3B,0x00,0x18,0x80,0xD6,0xF0,
+           0x10,0x7C,0x95,0xA4,0x8D,0xA4,0x00,0x80,0xCE,0xF0,0x10,0x80,0x9D,0xA4,0x8D,0xA4,
+           0x45,0x70,0x21,0x90,0x68,0x67,0x68,0x60,0x47,0x70,0x21,0x99,0x68,0x68,0x3D,0x99,
+           0x53,0x74,0x18,0xD9,0xCD,0xA4,0xAD,0xA4,0x18,0x99,0x3B,0x20,0x68,0x6E,0x21,0x70,
+           0x3B,0x60,0xDA,0xF1,0x10,0x97,0x23,0x00,0x3D,0x70,0x39,0x74,0x18,0xD9,0x23,0x00,
+           0x00,0xBC,0x78,0x5E,0x67,0x8C,0x21,0x70,0x3B,0x60,0xDA,0xF1,0x10,0xA3,0x23,0x00,
+           0x3D,0x70,0x39,0x74,0x18,0xA7,0xCA,0x0A,0x18,0xA7,0xC1,0x97,0x10,0xAC,0xAA,0x0A,
+           0xA1,0x97,0xA2,0x0A,0x23,0x50,0x00,0xB5,0xDD,0xA4,0x10,0xB0,0x23,0x90,0x00,0xB5,
+           0xD5,0xA4,0x10,0xB4,0x23,0x11,0x00,0xB5,0x23,0x10,0xBD,0xA4,0xB5,0xA4,0x00,0xBC,
+           0x41,0x80,0x68,0x71,0x68,0x6E,0x00,0x99,0x45,0x40,0x68,0x67,0x68,0x60,0x21,0x99,
+           0x68,0x67,0x23,0x00,0x3D,0x99,0x21,0x40,0x68,0x6B,0x27,0x02,0x3B,0x40,0x10,0xC9,
+           0x23,0x00,0x39,0x42,0x18,0xCD,0x3D,0x40,0x68,0x6D,0x45,0x40,0x3D,0x40,0x23,0x50,
+           0x68,0x67,0x68,0x60,0x68,0x66,0x33,0x80,0x68,0x67,0xA2,0x0A,0x78,0x5E,0x67,0x8C,
+           0x00,0x1F,0x81,0x97,0x00,0x99,0x45,0x42,0x68,0x66,0x3D,0x94,0x68,0x60,0x68,0x66,
+           0x2F,0x3F,0x3D,0x95,0x3B,0x00,0x19,0x20,0xDD,0x97,0x19,0x00,0xD9,0x97,0x10,0xEA,
+           0x9D,0x97,0x01,0x00,0x45,0x50,0x43,0x90,0x50,0x00,0x21,0x95,0x68,0x62,0x53,0x90,
+           0xDA,0xF1,0x19,0x5B,0x45,0x50,0x68,0x63,0x47,0x50,0x41,0x4C,0x21,0x95,0x4F,0x74,
+           0x68,0x62,0x43,0x4C,0x45,0x54,0x68,0x62,0x47,0x54,0x79,0x04,0x67,0x8A,0x00,0x5B,
+           0x21,0x95,0x45,0x54,0x68,0x62,0x47,0x54,0x21,0x74,0x68,0x6B,0x25,0x95,0x3B,0x60,
+           0xDA,0xF1,0x11,0x0B,0x23,0x00,0x3D,0x74,0x21,0x94,0xDA,0xF0,0x11,0x10,0x82,0x0A,
+           0x21,0x94,0xD2,0xF0,0x19,0x20,0x45,0x50,0xDE,0xF1,0x11,0x88,0xDD,0x97,0x19,0x88,
+           0x41,0x48,0xDE,0xF1,0x19,0x1E,0x23,0x0C,0x55,0x48,0x01,0x88,0x99,0x97,0x01,0x88,
+           0xCD,0x94,0x11,0x24,0x8E,0xF2,0x01,0x88,0x41,0x44,0xDE,0xF1,0x19,0x45,0x45,0x42,
+           0x68,0x60,0x68,0x66,0xDE,0xF0,0x11,0x2D,0x82,0x0A,0x21,0x94,0xDD,0x97,0x11,0x32,
+           0x33,0x02,0xBD,0x97,0xB9,0x97,0x3D,0x56,0x41,0xB0,0x23,0x10,0x68,0x62,0x23,0x04,
+           0x57,0x54,0x79,0x3C,0x67,0x8A,0x00,0x5B,0x92,0xF2,0x23,0x00,0x3D,0x54,0x3D,0x55,
+           0x41,0x44,0x43,0xB0,0x23,0x10,0x55,0x44,0x01,0x88,0x41,0xB0,0x23,0x10,0x55,0x44,
+           0x79,0x4B,0x67,0x8A,0x00,0x5B,0x41,0x44,0xDE,0xF1,0x11,0x27,0x68,0x70,0x23,0x0E,
+           0x68,0x62,0x43,0x78,0x79,0x55,0x67,0x8A,0x00,0x5B,0x41,0x78,0x68,0x71,0x19,0x45,
+           0x79,0x55,0x67,0x8A,0x00,0x5B,0x21,0x95,0x68,0x6B,0x29,0x50,0x3D,0x95,0x41,0x4C,
+           0x21,0x50,0x4F,0x74,0x45,0x54,0x68,0x62,0x47,0x54,0x79,0x68,0x67,0x8A,0x00,0x5B,
+           0x21,0x74,0x68,0x6B,0x25,0x50,0x3D,0x74,0x41,0x48,0xDE,0xF1,0x11,0x77,0x9D,0x97,
+           0x21,0x95,0x45,0x54,0x68,0x62,0x47,0x54,0x50,0x00,0x47,0x50,0x01,0x04,0x23,0x0C,
+           0x55,0x48,0x79,0x7C,0x67,0x8A,0x00,0x5B,0x45,0x50,0x43,0x90,0x50,0x00,0x21,0x95,
+           0x68,0x62,0x53,0x90,0xDA,0xF1,0x19,0x5B,0x45,0x50,0x68,0x63,0x47,0x50,0x00,0xF5,
+           0x21,0x42,0x68,0x6B,0x27,0x02,0x3B,0x40,0x11,0x8E,0x23,0x00,0x3D,0x42,0x00,0x5B,
+           0x4B,0x92,0x09,0xB0,0x41,0xB0,0xDE,0xF1,0x11,0x9B,0x79,0x99,0x67,0x8A,0x49,0x92,
+           0x68,0x6D,0x4B,0x92,0x01,0x92,0x23,0x10,0x55,0x44,0x79,0xA1,0x67,0x8A,0x49,0x92,
+           0x68,0x6D,0x82,0x0A,0x78,0x5E,0x67,0x8C,0x50,0x00,0x43,0x70,0x43,0x74,0x23,0x00,
+           0x3D,0xB5,0x3D,0x99,0x3D,0x54,0x3D,0x55,0x3D,0x97,0x00,0x55,0x81,0x98,0x01,0xB1,
+           0xA1,0x98,0xA2,0x0A,0x51,0x00,0x47,0x40,0x47,0x42,0x45,0x84,0x47,0x8A,0x78,0x5D,
+           0x67,0x8C,0xC1,0x98,0x68,0x6E,0x8A,0xF2,0x68,0x6D,0x00,0x34,0x96,0xF2,0xB1,0x98,
+           0xDD,0xBC,0x10,0x34,0xBD,0xBC,0x79,0xC8,0x67,0x8E,0x7A,0x5F,0x67,0x88,0x00,0x34,
+           0x00,0x34,0xC2,0x04,0x10,0x34,0x4B,0x92,0x0A,0x4F,0x49,0x92,0xC1,0x9D,0x11,0xD1,
+           0x00,0x34,0x79,0xD4,0x67,0x8E,0x68,0x6D,0xC2,0x04,0x10,0x34,0x4B,0x92,0x0A,0x4F,
+           0x49,0x92,0xC1,0x9D,0x11,0xDC,0x00,0x34,0x45,0x6E,0x68,0x66,0x68,0x6B,0x3E,0x00,
+           0x21,0x6E,0x68,0x68,0x3B,0x00,0x11,0xE5,0x23,0x80,0x3D,0x6E,0x68,0x6B,0x29,0x6C,
+           0x11,0xEC,0xC9,0x98,0x19,0xF9,0x01,0xEE,0xC9,0x98,0x19,0xF3,0x3B,0x00,0x68,0x6E,
+           0x7A,0x10,0x67,0x8E,0x68,0x6D,0x3B,0x1F,0xDA,0xF1,0x19,0xF9,0x3B,0x00,0x19,0xFF,
+           0x68,0x6D,0x45,0x88,0xDE,0xF1,0x68,0x6E,0x7A,0x82,0x67,0x88,0x68,0x6D,0x7A,0x02,
+           0x67,0x8E,0x68,0x6D,0xC2,0x04,0x10,0x34,0x4B,0x92,0x0A,0x4F,0x49,0x92,0xC1,0x9D,
+           0x12,0x0A,0x00,0x34,0x21,0x6C,0x39,0x6E,0x18,0x34,0x79,0xD4,0x67,0x8E,0x01,0xDC,
+           0xC2,0x04,0x10,0x34,0x4B,0x92,0x0A,0x4F,0x49,0x92,0xC1,0x9D,0x12,0x18,0x00,0x34,
+           0xBD,0x98,0x91,0x98,0xAD,0x98,0x7A,0x3F,0x67,0x8E,0x7A,0x20,0x67,0x88,0x68,0x6D,
+           0x41,0x58,0xDE,0xF1,0x12,0x29,0x41,0x7C,0x23,0x14,0x55,0x58,0x7A,0x29,0x67,0x88,
+           0x00,0x5B,0x23,0x80,0x3D,0x6A,0x41,0x7C,0x23,0x12,0x68,0x62,0x23,0x01,0x57,0x6A,
+           0x7A,0x33,0x67,0x88,0x00,0x5B,0x8D,0x98,0x41,0x58,0xDE,0xF1,0x12,0x39,0x9D,0x98,
+           0x68,0x6D,0x96,0xF2,0xBD,0xBC,0xB1,0x98,0x79,0xC8,0x67,0x8E,0x02,0x6D,0xDD,0x98,
+           0x10,0x34,0xDD,0xBC,0x19,0xBE,0xD5,0x6B,0x1A,0x4A,0xD1,0x6B,0x1A,0x48,0x91,0x6B,
+           0xC6,0x04,0x10,0x34,0x96,0xF2,0xB1,0x98,0x79,0xC0,0x67,0x8E,0x00,0x34,0xC1,0xBD,
+           0x12,0x56,0x22,0x06,0xDA,0xF0,0x1A,0x57,0xBE,0x0A,0xA1,0x9D,0x68,0x6D,0x81,0x9D,
+           0x22,0x05,0x2F,0x0F,0x3B,0x01,0xDA,0xF1,0x68,0x6E,0x9E,0x0A,0x68,0x6D,0x41,0xB8,
+           0x23,0x04,0x55,0x7C,0x7A,0x65,0x67,0x88,0x00,0x5B,0x41,0x7C,0xDE,0xF1,0x12,0x6D,
+           0x23,0x88,0x3D,0xBE,0x79,0xC0,0x67,0x8E,0x00,0x5B,0x23,0x14,0x55,0x58,0x43,0x7C,
+           0x7A,0x74,0x67,0x88,0x99,0xB6,0x00,0x5B,0x41,0x58,0xDE,0xF1,0x12,0x7D,0x41,0xB8,
+           0x23,0x04,0x57,0x84,0x7A,0x7D,0x67,0x88,0x00,0x5B,0x99,0x98,0x89,0x98,0x50,0x80,
+           0x47,0x6C,0x47,0x6E,0x45,0x68,0xDE,0xF1,0x12,0xA7,0x23,0x80,0xDD,0x6B,0x13,0x19,
+           0x23,0x10,0x3D,0x6A,0x41,0x7C,0x23,0x12,0x68,0x62,0x23,0x01,0x57,0x6A,0x7A,0x92,
+           0x67,0x88,0x00,0x5B,0x41,0x58,0x23,0x82,0xDE,0xF1,0x1B,0x19,0x23,0x14,0x55,0x58,
+           0x43,0x7C,0x7A,0x9C,0x67,0x88,0x00,0x5B,0xDD,0x6B,0x1A,0xA7,0x41,0x58,0xDE,0xF1,
+           0x12,0xA7,0x41,0xB8,0x23,0x04,0x57,0x84,0x7A,0xA7,0x67,0x88,0x00,0x5B,0x45,0x66,
+           0xDE,0xF1,0x1A,0xC2,0x65,0x66,0x63,0x90,0x45,0x64,0x53,0x90,0xDA,0xF1,0x1A,0xBB,
+           0x68,0x5D,0x67,0x66,0x41,0x5C,0x23,0x84,0xDE,0xF1,0x1B,0x19,0x23,0x0A,0x55,0x5C,
+           0x7A,0xAA,0x67,0x88,0x00,0x5B,0x68,0x7D,0x47,0x64,0x41,0x60,0x68,0x7C,0x43,0x60,
+           0x70,0x00,0x67,0x66,0x45,0x64,0xDE,0xF1,0x12,0xCE,0x41,0x5C,0x23,0x86,0xDE,0xF1,
+           0x1B,0x19,0x23,0x0A,0x55,0x5C,0x7A,0xCE,0x67,0x88,0x00,0x5B,0x21,0x6E,0x68,0x6B,
+           0x29,0x6C,0x1A,0xD5,0x3B,0x00,0x1A,0xD5,0x02,0xD8,0x23,0x00,0x68,0x6B,0x29,0x6C,
+           0x3D,0x96,0x45,0x68,0x43,0x90,0x70,0x00,0x68,0x42,0x73,0x90,0xDA,0xF1,0x12,0xE7,
+           0x45,0x64,0x53,0x90,0xDA,0xF1,0x21,0x64,0x12,0xED,0x21,0x68,0x02,0xED,0x63,0x90,
+           0x45,0x64,0x53,0x90,0xDA,0xF1,0x1A,0xEE,0x21,0x64,0x3D,0x96,0x21,0x96,0x41,0x60,
+           0x4D,0x6C,0x68,0x62,0x43,0x60,0x45,0x64,0x68,0x63,0x47,0x64,0x45,0x68,0x68,0x63,
+           0x47,0x68,0x7A,0xFC,0x67,0x88,0x00,0x5B,0x21,0x6C,0x68,0x6B,0x25,0x96,0x3B,0x00,
+           0x13,0x02,0x23,0x80,0x3D,0x6C,0x45,0x68,0xDE,0xF1,0x13,0x0C,0xDD,0x6B,0x1B,0x0C,
+           0xA9,0x98,0xD9,0x98,0x10,0x5B,0x03,0x11,0xD9,0x98,0x10,0x5B,0x21,0x6C,0x3B,0x80,
+           0x12,0x82,0xB9,0x98,0x79,0xC9,0x67,0x8E,0xC2,0x04,0x10,0x5B,0x49,0x8E,0x68,0x6D,
+           0x03,0x14,0x3D,0xBE,0x0B,0x20,0x09,0xB0,0x9E,0xF4,0x00,0x1B,0x81,0x98,0x03,0x21,
+           0xA1,0x98,0x86,0x0A,0xAE,0x0A,0xC5,0x98,0x13,0x28,0x22,0x01,0x2F,0xFC,0x3E,0x01,
+           0x45,0x84,0x47,0x88,0x79,0xBD,0x67,0x8E,0xC1,0x98,0x68,0x6E,0x86,0xF2,0x68,0x6D,
+           0x79,0xC0,0x67,0x8E,0xB1,0x98,0xC5,0x98,0x13,0x5F,0x22,0x01,0x2F,0xFD,0xD5,0xBD,
+           0x13,0x3A,0x33,0x03,0x3E,0x01,0x03,0x5F,0x3D,0x90,0x35,0xC4,0x3D,0x91,0xD1,0x90,
+           0x1B,0x62,0xD5,0x90,0x13,0x4A,0xD5,0x91,0x10,0x37,0x09,0xB0,0x0B,0x20,0x82,0xF2,
+           0x95,0xC4,0x00,0x37,0xDD,0x91,0x13,0x51,0xDD,0x90,0x13,0x50,0x09,0x90,0x03,0x51,
+           0x09,0xAE,0xD9,0x91,0x13,0x5F,0xD9,0x90,0x1B,0x30,0xD1,0x98,0x13,0x5E,0xCD,0x98,
+           0x1B,0x5C,0x21,0x90,0x33,0x40,0x03,0x60,0x96,0xF2,0xB1,0x98,0x0B,0x1E,0x21,0x90,
+           0x3D,0xC4,0x00,0x37,0x91,0xC4,0x58,0x00,0x4B,0xC8,0x41,0xFC,0x22,0x7F,0x2F,0xBF,
+           0x3E,0x7F,0x21,0xF7,0x4D,0xC8,0x03,0x6B,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00};
+
+static unsigned char topcidiag[192] = 
+          {0x23,0x01,0x3C,0x04,0x23,0x80,0x3E,0x7F,0x23,0x00,0x3E,0x7C,0x20,0x00,0x3B,0x00,
+           0x18,0x06,0x3B,0x03,0x10,0x0F,0x08,0x24,0x23,0x00,0x3C,0x00,0x00,0x06,0x3B,0x04,
+           0x10,0x15,0x08,0x2B,0x23,0x00,0x3C,0x00,0x00,0x06,0x3B,0x06,0x10,0x1B,0x08,0x40,
+           0x23,0x00,0x3C,0x00,0x00,0x06,0x3B,0x07,0x10,0x21,0x08,0x47,0x23,0x00,0x3C,0x00,
+           0x00,0x06,0x20,0x00,0x3C,0x07,0x00,0x57,0x40,0x08,0x23,0x04,0x56,0x0C,0x3F,0x80,
+           0x23,0x02,0x3C,0x04,0x00,0x57,0x23,0x03,0x3C,0x04,0x23,0x80,0x3E,0x7F,0x40,0x08,
+           0x23,0x04,0x56,0x0C,0x3F,0x80,0x23,0x00,0x3E,0x7F,0x23,0x04,0x56,0x0C,0x3F,0x80,
+           0x23,0x00,0x3C,0x00,0x23,0x80,0x3E,0x7F,0x82,0xF2,0x00,0x06,0x23,0x04,0x3C,0x04,
+           0x23,0xFF,0x3E,0xF2,0x3E,0xF4,0x23,0x00,0x3C,0x00,0x3C,0x04,0x00,0x06,0x23,0x05,
+           0x3C,0x04,0x40,0x10,0x20,0x18,0x54,0x48,0x3F,0x80,0x40,0x14,0x20,0x18,0x56,0x48,
+           0x3F,0x80,0x20,0x18,0x3C,0x04,0x82,0xF2,0x23,0x00,0x3C,0x00,0x00,0x06,0x82,0xF4,
+           0x00,0x58,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00};
+ 
+static unsigned char toinstdiag[1776] = 
+          {0x23,0x01,0x3C,0x04,0x20,0x00,0x3B,0x00,0x18,0x02,0x3B,0x01,0x10,0x0B,0x08,0x0E,
+           0x23,0x00,0x3C,0x00,0x00,0x02,0x20,0x00,0x3C,0x07,0x03,0x75,0x23,0x80,0x3E,0x7F,
+           0x23,0x02,0x3C,0x04,0x23,0x55,0x3B,0x55,0x18,0x19,0x3C,0x07,0x23,0x55,0x3C,0x06,
+           0x03,0x75,0x23,0xAA,0x3B,0xAA,0x18,0x20,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x75,
+           0x23,0x03,0x3C,0x04,0x68,0x6A,0x3F,0x00,0x18,0x26,0x03,0x75,0x68,0x6B,0x3F,0x00,
+           0x10,0x2A,0x03,0x75,0x23,0x04,0x3C,0x04,0x23,0x10,0x3C,0x68,0x24,0x68,0x3B,0x20,
+           0x18,0x35,0x3C,0x07,0x23,0x20,0x3C,0x06,0x03,0x75,0x23,0x05,0x3C,0x04,0x23,0x20,
+           0x68,0x6B,0x27,0x05,0x3B,0x25,0x18,0x40,0x3C,0x07,0x23,0x25,0x3C,0x06,0x03,0x75,
+           0x23,0x06,0x3C,0x04,0x23,0x25,0x68,0x6B,0x2B,0x05,0x3B,0x20,0x18,0x4B,0x3C,0x07,
+           0x23,0x20,0x3C,0x06,0x03,0x75,0x23,0x07,0x3C,0x04,0x23,0x20,0x68,0x6B,0x28,0x68,
+           0x3B,0x10,0x18,0x56,0x3C,0x07,0x23,0x10,0x3C,0x06,0x03,0x75,0x23,0x08,0x3C,0x04,
+           0x23,0x55,0x2C,0x68,0x3B,0x10,0x18,0x60,0x3C,0x07,0x23,0x10,0x3C,0x06,0x03,0x75,
+           0x23,0x09,0x3C,0x04,0x23,0xFF,0x2F,0xAA,0x3B,0xAA,0x18,0x6A,0x3C,0x07,0x23,0xAA,
+           0x3C,0x06,0x03,0x75,0x23,0x10,0x3C,0x04,0x23,0x55,0x3C,0x68,0x23,0xCC,0x30,0x68,
+           0x3B,0xDD,0x18,0x76,0x3C,0x07,0x23,0xDD,0x3C,0x06,0x03,0x75,0x23,0x11,0x3C,0x04,
+           0x23,0x55,0x33,0x33,0x3B,0x77,0x18,0x80,0x3C,0x07,0x23,0x77,0x3C,0x06,0x03,0x75,
+           0x23,0x12,0x3C,0x04,0x23,0x55,0x3C,0x68,0x23,0xCC,0x34,0x68,0x3B,0x99,0x18,0x8C,
+           0x3C,0x07,0x23,0x99,0x3C,0x06,0x03,0x75,0x23,0x13,0x3C,0x04,0x23,0x55,0x37,0x33,
+           0x3B,0x66,0x18,0x96,0x3C,0x07,0x23,0x66,0x3C,0x06,0x03,0x75,0x23,0x14,0x3C,0x04,
+           0x23,0x55,0x68,0x6B,0x68,0x64,0x3C,0x68,0x18,0xA1,0x3C,0x07,0x23,0x2A,0x3C,0x06,
+           0x03,0x75,0x23,0x15,0x3C,0x04,0x20,0x68,0x3B,0x2A,0x18,0xAA,0x3C,0x07,0x23,0x2A,
+           0x3C,0x06,0x03,0x75,0x23,0x16,0x3C,0x04,0x23,0xAA,0x68,0x6B,0x68,0x65,0x3C,0x68,
+           0x18,0xB5,0x3C,0x07,0x23,0x54,0x3C,0x06,0x03,0x75,0x23,0x17,0x3C,0x04,0x20,0x68,
+           0x3B,0x54,0x18,0xBE,0x3C,0x07,0x23,0x2A,0x3C,0x06,0x03,0x75,0x23,0x18,0x3C,0x04,
+           0x23,0x55,0x3C,0x68,0x23,0xAA,0x20,0x68,0x3B,0x55,0x18,0xCA,0x3C,0x07,0x23,0x55,
+           0x3C,0x06,0x03,0x75,0x23,0x19,0x3C,0x04,0x23,0x55,0x68,0x69,0x3B,0x54,0x18,0xD4,
+           0x3C,0x07,0x23,0x54,0x3C,0x06,0x03,0x75,0x23,0x20,0x3C,0x04,0x23,0x54,0x68,0x68,
+           0x3B,0x55,0x18,0xDE,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x23,0x21,0x3C,0x04,
+           0x3C,0x68,0x68,0x6B,0x38,0x68,0x18,0xE8,0x3C,0x07,0x20,0x68,0x3C,0x06,0x03,0x75,
+           0x23,0x22,0x3C,0x04,0x68,0x6B,0x68,0x6C,0x3F,0x00,0x18,0xEF,0x03,0x75,0x23,0x23,
+           0x3C,0x04,0x68,0x6C,0x3F,0x00,0x10,0xF5,0x03,0x75,0x23,0x24,0x3C,0x04,0x08,0xFF,
+           0x33,0x50,0x3B,0x55,0x11,0x02,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x33,0x05,
+           0x68,0x6D,0x00,0xFB,0x23,0x25,0x3C,0x04,0x23,0x00,0x68,0x6B,0x09,0x0E,0x33,0x50,
+           0x3B,0x55,0x19,0x11,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x33,0x05,0x68,0x6E,
+           0x01,0x0A,0x23,0x26,0x3C,0x04,0x23,0x00,0x68,0x6A,0x09,0x1D,0x33,0x50,0x3B,0x55,
+           0x19,0x20,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x33,0x05,0x68,0x6F,0x01,0x19,
+           0x23,0x27,0x3C,0x04,0x23,0x55,0x3C,0x68,0xA0,0x68,0x20,0x68,0x3B,0x54,0x19,0x2C,
+           0x3C,0x07,0x23,0x54,0x3C,0x06,0x03,0x75,0x23,0x28,0x3C,0x04,0x80,0x68,0x20,0x68,
+           0x3B,0x55,0x19,0x36,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x23,0x29,0x3C,0x04,
+           0xC0,0x68,0x19,0x3B,0x03,0x75,0x23,0x30,0x3C,0x04,0xC4,0x68,0x11,0x40,0x03,0x75,
+           0x23,0x31,0x3C,0x04,0x23,0x55,0x3C,0x68,0x68,0x6A,0xE4,0x68,0x20,0x68,0x3B,0x57,
+           0x19,0x4D,0x3C,0x07,0x23,0x57,0x3C,0x06,0x03,0x75,0x23,0x32,0x3C,0x04,0x68,0x6B,
+           0xE4,0x68,0x20,0x68,0x3B,0x55,0x19,0x58,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,
+           0x23,0x33,0x3C,0x04,0x23,0x5A,0x68,0x73,0x3B,0xA5,0x19,0x62,0x3C,0x07,0x23,0xA5,
+           0x3C,0x06,0x03,0x75,0x23,0x34,0x3C,0x04,0x23,0x07,0x3C,0x49,0x23,0xFF,0x3C,0x48,
+           0x48,0x48,0x4A,0x4A,0x20,0x4A,0x38,0x48,0x19,0x71,0x3C,0x07,0x20,0x48,0x3C,0x06,
+           0x03,0x75,0x23,0x35,0x3C,0x04,0x20,0x4B,0x2F,0x07,0x38,0x49,0x19,0x7B,0x3C,0x07,
+           0x20,0x49,0x3C,0x06,0x03,0x75,0x23,0x36,0x3C,0x04,0x23,0x02,0x3C,0x49,0x23,0x55,
+           0x3C,0x48,0x48,0x48,0x4A,0x4A,0x20,0x4A,0x38,0x48,0x19,0x8A,0x3C,0x07,0x20,0x48,
+           0x3C,0x06,0x03,0x75,0x23,0x37,0x3C,0x04,0x20,0x4B,0x2F,0x07,0x38,0x49,0x19,0x94,
+           0x3C,0x07,0x20,0x49,0x3C,0x06,0x03,0x75,0x23,0x38,0x3C,0x04,0x23,0x05,0x3C,0x49,
+           0x23,0xAA,0x3C,0x48,0x48,0x48,0x4A,0x4A,0x20,0x4A,0x38,0x48,0x19,0xA3,0x3C,0x07,
+           0x20,0x48,0x3C,0x06,0x03,0x75,0x23,0x39,0x3C,0x04,0x20,0x4B,0x2F,0x07,0x38,0x49,
+           0x19,0xAD,0x3C,0x07,0x20,0x49,0x3C,0x06,0x03,0x75,0x23,0x40,0x3C,0x04,0x23,0x00,
+           0x3C,0x49,0x3C,0x48,0x48,0x48,0x4A,0x4A,0x20,0x4A,0x38,0x48,0x19,0xBB,0x3C,0x07,
+           0x20,0x48,0x3C,0x06,0x03,0x75,0x23,0x41,0x3C,0x04,0x20,0x4B,0x2F,0x07,0x38,0x49,
+           0x19,0xC5,0x3C,0x07,0x20,0x49,0x3C,0x06,0x03,0x75,0x23,0x2A,0x3C,0x04,0x23,0x55,
+           0x68,0x72,0x4A,0x4A,0x20,0x4A,0x3B,0x55,0x19,0xD1,0x3C,0x07,0x23,0x55,0x3C,0x06,
+           0x03,0x75,0x23,0x43,0x3C,0x04,0x20,0x4B,0x2F,0x07,0x3B,0x00,0x19,0xDB,0x3C,0x07,
+           0x23,0x00,0x3C,0x06,0x03,0x75,0x23,0x44,0x3C,0x04,0x23,0x00,0x59,0xE1,0x68,0x6D,
+           0x03,0x75,0x23,0x45,0x3C,0x04,0x23,0x00,0x3C,0x4F,0x23,0x55,0x3C,0x4E,0x23,0xAA,
+           0x3C,0x4D,0x23,0xFF,0x3C,0x4C,0x40,0x4C,0x42,0x50,0x0A,0x5B,0x23,0x46,0x3C,0x04,
+           0x23,0x55,0x3C,0x4F,0x23,0xAA,0x3C,0x4E,0x23,0xFF,0x3C,0x4D,0x23,0x00,0x3C,0x4C,
+           0x40,0x4C,0x42,0x50,0x0A,0x5B,0x23,0x47,0x3C,0x04,0x23,0xAA,0x3C,0x4F,0x23,0xFF,
+           0x3C,0x4E,0x23,0x00,0x3C,0x4D,0x23,0x55,0x3C,0x4C,0x40,0x4C,0x42,0x50,0x0A,0x5B,
+           0x23,0x48,0x3C,0x04,0x23,0xFF,0x3C,0x4F,0x23,0x00,0x3C,0x4E,0x23,0x55,0x3C,0x4D,
+           0x23,0xAA,0x3C,0x4C,0x40,0x4C,0x42,0x50,0x0A,0x5B,0x23,0x49,0x3C,0x04,0x68,0x60,
+           0x42,0x50,0x20,0x4C,0x68,0x68,0x3C,0x4C,0x0A,0x5B,0x23,0x50,0x3C,0x04,0x68,0x61,
+           0x42,0x50,0x20,0x4C,0x68,0x69,0x3C,0x4C,0x0A,0x5B,0x23,0x51,0x3C,0x04,0x23,0x10,
+           0x68,0x62,0x42,0x50,0x20,0x4C,0x68,0x6B,0x27,0x10,0x3C,0x4C,0x0A,0x5B,0x23,0x52,
+           0x3C,0x04,0x23,0x10,0x68,0x74,0x42,0x50,0x20,0x4D,0x68,0x6B,0x27,0x10,0x3C,0x4D,
+           0x0A,0x5B,0x23,0x53,0x3C,0x04,0x23,0x10,0x68,0x63,0x42,0x50,0x20,0x4C,0x68,0x6B,
+           0x2B,0x10,0x3C,0x4C,0x0A,0x5B,0x23,0x54,0x3C,0x04,0x23,0x55,0x3C,0x4D,0x23,0xAA,
+           0x3C,0x4C,0x44,0x4C,0x42,0x50,0x23,0x00,0x3C,0x4E,0x3C,0x4F,0x0A,0x5B,0x23,0x55,
+           0x3C,0x04,0x23,0xFF,0x3C,0x4E,0x3C,0x4F,0x3C,0x50,0x3C,0x51,0x3C,0x52,0x3C,0x53,
+           0x46,0x50,0x0A,0x5B,0x02,0x7F,0x20,0x53,0x38,0x4F,0x1A,0x64,0x3C,0x07,0x20,0x4F,
+           0x3C,0x06,0x23,0x04,0x3C,0x05,0x03,0x75,0x20,0x52,0x38,0x4E,0x1A,0x6D,0x3C,0x07,
+           0x20,0x4E,0x3C,0x06,0x23,0x03,0x3C,0x05,0x03,0x75,0x20,0x51,0x38,0x4D,0x1A,0x76,
+           0x3C,0x07,0x20,0x4D,0x3C,0x06,0x23,0x02,0x3C,0x05,0x03,0x75,0x20,0x50,0x38,0x4C,
+           0x68,0x6F,0x3C,0x07,0x20,0x4C,0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,0x23,0x56,
+           0x3C,0x04,0x23,0x55,0x3C,0x68,0x50,0x68,0x23,0x00,0x68,0x66,0x3B,0x55,0x1A,0x8C,
+           0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x75,0x23,0x57,0x3C,0x04,0x23,0xAA,0x68,0x67,
+           0x20,0x68,0x3B,0xAA,0x1A,0x97,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x75,0x23,0x58,
+           0x3C,0x04,0x23,0x11,0x3C,0x4F,0x23,0x22,0x3C,0x4E,0x23,0x33,0x3C,0x4D,0x23,0x44,
+           0x3C,0x4C,0x40,0x4C,0x52,0x4C,0x23,0x59,0x3C,0x04,0x1A,0xA7,0x03,0x75,0x23,0x60,
+           0x3C,0x04,0x68,0x61,0x52,0x4C,0x23,0x61,0x3C,0x04,0xDA,0xF1,0x12,0xB0,0x03,0x75,
+           0x23,0x62,0x3C,0x04,0x68,0x60,0x68,0x60,0x52,0x4C,0x23,0x63,0x3C,0x04,0x12,0xB9,
+           0x03,0x75,0x23,0x64,0x3C,0x04,0x68,0x70,0x23,0x08,0x68,0x62,0x68,0x71,0x12,0xC1,
+           0x02,0xC2,0x23,0xAA,0x3B,0xAA,0x1A,0xC8,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x75,
+           0x23,0x65,0x3C,0x04,0x3F,0x20,0x68,0x71,0x1A,0xCE,0x03,0x75,0x23,0x66,0x3C,0x04,
+           0x0B,0x0B,0x40,0x08,0x23,0x08,0x56,0x54,0x82,0xF3,0xC2,0xF3,0x1A,0xD8,0x03,0x75,
+           0x23,0x67,0x3C,0x04,0x3F,0x80,0xC2,0xF3,0x12,0xDE,0x03,0x75,0x23,0x68,0x3C,0x04,
+           0x40,0x08,0x23,0x08,0x54,0x5C,0x86,0xF3,0xC6,0xF3,0x1A,0xE7,0x03,0x75,0x23,0x69,
+           0x3C,0x04,0x3F,0x80,0xC6,0xF3,0x12,0xED,0x03,0x75,0x23,0x70,0x3C,0x04,0x0B,0x25,
+           0x23,0x71,0x3C,0x04,0x0B,0x0B,0x50,0x54,0x46,0x64,0x50,0x5C,0x46,0x66,0x40,0x08,
+           0x23,0x08,0x4E,0x64,0xC2,0xF1,0x12,0xFD,0x03,0x75,0x23,0x72,0x3C,0x04,0x3F,0x80,
+           0xC2,0xF1,0x1B,0x03,0x03,0x75,0x23,0x73,0x3C,0x04,0x40,0x08,0x23,0x08,0x4C,0x66,
+           0x3F,0x80,0x0B,0x25,0x03,0x6D,0x23,0x11,0x3C,0x54,0x23,0x22,0x3C,0x55,0x23,0x33,
+           0x3C,0x56,0x23,0x44,0x3C,0x57,0x23,0x55,0x3C,0x58,0x23,0x66,0x3C,0x59,0x23,0x77,
+           0x3C,0x5A,0x23,0x88,0x3C,0x5B,0x23,0x00,0x3C,0x5C,0x3C,0x5D,0x3C,0x5E,0x3C,0x5F,
+           0x3C,0x60,0x3C,0x61,0x3C,0x62,0x3C,0x63,0x68,0x6D,0x20,0x5C,0x38,0x54,0x1B,0x2E,
+           0x3C,0x07,0x20,0x54,0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,0x20,0x5D,0x38,0x55,
+           0x1B,0x37,0x3C,0x07,0x20,0x55,0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,0x20,0x5E,
+           0x38,0x56,0x1B,0x40,0x3C,0x07,0x20,0x56,0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,
+           0x20,0x5F,0x38,0x57,0x1B,0x49,0x3C,0x07,0x20,0x57,0x3C,0x06,0x23,0x01,0x3C,0x05,
+           0x03,0x75,0x20,0x60,0x38,0x58,0x1B,0x52,0x3C,0x07,0x20,0x58,0x3C,0x06,0x23,0x01,
+           0x3C,0x05,0x03,0x75,0x20,0x61,0x38,0x59,0x1B,0x5B,0x3C,0x07,0x20,0x59,0x3C,0x06,
+           0x23,0x01,0x3C,0x05,0x03,0x75,0x20,0x62,0x38,0x5A,0x1B,0x64,0x3C,0x07,0x20,0x5A,
+           0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,0x20,0x63,0x38,0x5B,0x68,0x6F,0x3C,0x07,
+           0x20,0x5B,0x3C,0x06,0x23,0x01,0x3C,0x05,0x03,0x75,0x23,0x00,0x3C,0x04,0x3C,0x07,
+           0x3C,0x06,0x3C,0x05,0x82,0xF2,0x58,0x08,0x68,0x6D,0x82,0xF4,0x03,0x76,0x00,0x00};
+
+static unsigned char toregdiag[1728] = 
+          {0x23,0x01,0x3C,0x04,0xD2,0xFC,0x10,0x08,0xC1,0xFE,0x10,0x08,0x8E,0xFC,0x81,0xFF,
+           0x20,0x00,0x3B,0x00,0x18,0x08,0x3B,0x01,0x10,0x11,0x08,0x14,0x23,0x00,0x3C,0x00,
+           0x00,0x08,0x20,0x00,0x3C,0x07,0x03,0x5E,0x23,0x02,0x3C,0x04,0x23,0xAA,0xC2,0xF0,
+           0x10,0x1C,0x23,0x01,0x3C,0x05,0x03,0x5E,0xC6,0xF0,0x18,0x25,0x23,0x02,0x3C,0x05,
+           0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xCA,0xF0,0x10,0x2E,0x23,0x03,
+           0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0xCE,0xF0,0x18,0x37,
+           0x23,0x04,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xD2,0xF0,
+           0x10,0x40,0x23,0x05,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,
+           0xD6,0xF0,0x18,0x49,0x23,0x06,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,
+           0x03,0x5E,0xDA,0xF0,0x10,0x52,0x23,0x07,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,
+           0x3C,0x07,0x03,0x5E,0xDE,0xF0,0x18,0x5B,0x23,0x08,0x3C,0x05,0x23,0x01,0x3C,0x06,
+           0x23,0x00,0x3C,0x07,0x03,0x5E,0x23,0x03,0x3C,0x04,0x23,0x55,0xC2,0xF0,0x18,0x67,
+           0x23,0x01,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xC6,0xF0,
+           0x10,0x70,0x23,0x02,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,
+           0xCA,0xF0,0x18,0x79,0x23,0x03,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,
+           0x03,0x5E,0xCE,0xF0,0x10,0x82,0x23,0x04,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,
+           0x3C,0x07,0x03,0x5E,0xD2,0xF0,0x18,0x8B,0x23,0x05,0x3C,0x05,0x23,0x01,0x3C,0x06,
+           0x23,0x00,0x3C,0x07,0x03,0x5E,0xD6,0xF0,0x10,0x94,0x23,0x06,0x3C,0x05,0x23,0x00,
+           0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0xDA,0xF0,0x18,0x9D,0x23,0x07,0x3C,0x05,
+           0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xDE,0xF0,0x10,0xA6,0x23,0x08,
+           0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0x23,0x04,0x3C,0x04,
+           0x23,0x05,0x3B,0x04,0xDA,0xF1,0x18,0xAD,0x03,0x5E,0x23,0x05,0x3C,0x04,0x23,0x05,
+           0x3B,0x05,0xDA,0xF1,0x10,0xB4,0x03,0x5E,0x23,0x06,0x3C,0x04,0x23,0x05,0x3B,0x06,
+           0xDA,0xF1,0x10,0xBB,0x03,0x5E,0x23,0x07,0x3C,0x04,0x23,0x04,0x3C,0x4C,0x23,0x05,
+           0x38,0x4C,0xDA,0xF1,0x18,0xC4,0x03,0x5E,0x23,0x08,0x3C,0x04,0x23,0x05,0x3C,0x4C,
+           0x38,0x4C,0xDA,0xF1,0x10,0xCC,0x03,0x5E,0x23,0x09,0x3C,0x04,0x23,0x06,0x3C,0x4C,
+           0x23,0x05,0x38,0x4C,0xDA,0xF1,0x10,0xD5,0x03,0x5E,0x23,0x10,0x3C,0x04,0x23,0x00,
+           0x3E,0x7C,0x22,0x7C,0x3B,0x00,0x18,0xE4,0x3C,0x4C,0x23,0x00,0x3E,0x7C,0x20,0x4C,
+           0x3C,0x07,0x23,0x00,0x3C,0x06,0x03,0x5E,0x23,0x11,0x3C,0x04,0x23,0x01,0x3E,0x7C,
+           0x22,0x7C,0x3B,0x01,0x18,0xF3,0x3C,0x4C,0x23,0x00,0x3E,0x7C,0x20,0x4C,0x3C,0x07,
+           0x23,0x01,0x3C,0x06,0x03,0x5E,0x23,0x12,0x3C,0x04,0x23,0x02,0x3E,0x7C,0x22,0x7C,
+           0x3B,0x02,0x19,0x02,0x3C,0x4C,0x23,0x00,0x3E,0x7C,0x20,0x4C,0x3C,0x07,0x23,0x02,
+           0x3C,0x06,0x03,0x5E,0x23,0x13,0x3C,0x04,0x23,0x03,0x3E,0x7C,0x22,0x7C,0x3B,0x03,
+           0x19,0x11,0x3C,0x4C,0x23,0x00,0x3E,0x7C,0x20,0x4C,0x3C,0x07,0x23,0x03,0x3C,0x06,
+           0x03,0x5E,0x23,0x00,0x3E,0x7C,0x23,0x14,0x3C,0x04,0x22,0x7F,0x3C,0x4C,0x23,0x00,
+           0x3E,0x7F,0x20,0x08,0x3B,0x00,0x11,0x23,0x22,0x7F,0x3B,0x00,0x19,0x2A,0x3C,0x07,
+           0x23,0x00,0x3C,0x06,0x03,0x5E,0x22,0x7F,0x3B,0x04,0x19,0x2A,0x3C,0x07,0x23,0x00,
+           0x3C,0x06,0x03,0x5E,0x23,0x15,0x3C,0x04,0x23,0x51,0x3E,0x7F,0x20,0x08,0x3B,0x00,
+           0x11,0x38,0x22,0x7F,0x3B,0x51,0x19,0x3F,0x3C,0x07,0x23,0x51,0x3C,0x06,0x03,0x5E,
+           0x22,0x7F,0x3B,0x55,0x19,0x3F,0x3C,0x07,0x23,0x55,0x3C,0x06,0x03,0x5E,0x23,0x16,
+           0x3C,0x04,0x23,0xAA,0x3E,0x7F,0x20,0x08,0x3B,0x00,0x11,0x4D,0x22,0x7F,0x3B,0xAA,
+           0x19,0x54,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x5E,0x22,0x7F,0x3B,0xAE,0x19,0x54,
+           0x3C,0x07,0x23,0xAE,0x3C,0x06,0x03,0x5E,0x23,0x17,0x3C,0x04,0x23,0xFB,0x3E,0x7F,
+           0x20,0x08,0x3B,0x00,0x11,0x62,0x22,0x7F,0x3B,0xFB,0x19,0x69,0x3C,0x07,0x23,0xFB,
+           0x3C,0x06,0x03,0x5E,0x22,0x7F,0x3B,0xFF,0x19,0x69,0x3C,0x07,0x23,0xFF,0x3C,0x06,
+           0x03,0x5E,0x20,0x4C,0x3E,0x7F,0x23,0x18,0x3C,0x04,0x23,0xFF,0x3E,0xF5,0x22,0xF5,
+           0x3B,0xFF,0x19,0x76,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x03,0x5E,0x23,0x19,0x3C,0x04,
+           0x23,0x00,0x3E,0xF5,0x22,0xF5,0x3B,0x10,0xDA,0xF1,0x11,0x82,0x3C,0x07,0x23,0xFF,
+           0x3C,0x06,0x03,0x5E,0x23,0x20,0x3C,0x04,0x23,0x00,0x3C,0x4D,0x23,0x20,0x3E,0x01,
+           0x23,0x14,0x3E,0x03,0x22,0xF5,0x3B,0x01,0xDA,0xF1,0x19,0x97,0x20,0x4D,0x68,0x68,
+           0x3C,0x4D,0x3B,0x20,0xDA,0xF1,0x11,0x8A,0x22,0xF5,0x3C,0x05,0x03,0x5E,0x23,0x21,
+           0x3C,0x04,0x23,0x00,0x3E,0x01,0x22,0x01,0x3B,0x00,0x19,0xA2,0x3C,0x07,0x23,0x00,
+           0x3C,0x06,0x03,0x5E,0x23,0x22,0x3C,0x04,0x23,0x15,0x3E,0x01,0x22,0x01,0x3B,0x15,
+           0x19,0xAD,0x3C,0x07,0x23,0x15,0x3C,0x06,0x03,0x5E,0x23,0x23,0x3C,0x04,0x23,0x0A,
+           0x3E,0x01,0x22,0x01,0x3B,0x0A,0x19,0xB8,0x3C,0x07,0x23,0x0A,0x3C,0x06,0x03,0x5E,
+           0x23,0x24,0x3C,0x04,0x23,0x1F,0x3E,0x01,0x22,0x01,0x3B,0x1F,0x19,0xC3,0x3C,0x07,
+           0x23,0x1F,0x3C,0x06,0x03,0x5E,0x23,0x00,0x3E,0x01,0x23,0x25,0x3C,0x04,0x23,0x00,
+           0x3E,0x02,0x22,0x02,0x3B,0x00,0x19,0xD0,0x3C,0x07,0x23,0x00,0x3C,0x06,0x03,0x5E,
+           0x23,0x26,0x23,0x55,0x3E,0x02,0x22,0x02,0x3B,0x55,0x19,0xDA,0x3C,0x07,0x23,0x55,
+           0x3C,0x06,0x03,0x5E,0x23,0x27,0x3C,0x04,0x23,0xAA,0x3E,0x02,0x22,0x02,0x3B,0xAA,
+           0x19,0xE5,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x5E,0x23,0x28,0x3C,0x04,0x23,0xFF,
+           0x3E,0x02,0x22,0x02,0x3B,0xFF,0x19,0xF0,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x03,0x5E,
+           0x23,0x00,0x3E,0x02,0x23,0x29,0x3C,0x04,0x23,0x00,0x3E,0x03,0x22,0x03,0x3B,0x00,
+           0x19,0xFD,0x3C,0x07,0x23,0x00,0x3C,0x06,0x03,0x5E,0x23,0x30,0x3C,0x04,0x23,0x15,
+           0x3E,0x03,0x22,0x03,0x3B,0x15,0x1A,0x08,0x3C,0x07,0x23,0x15,0x3C,0x06,0x03,0x5E,
+           0x23,0x31,0x3C,0x04,0x23,0x0A,0x3E,0x03,0x22,0x03,0x3B,0x0A,0x1A,0x13,0x3C,0x07,
+           0x23,0x0A,0x3C,0x06,0x03,0x5E,0x23,0x32,0x3C,0x04,0x23,0x1F,0x3E,0x03,0x22,0x03,
+           0x3B,0x1F,0x1A,0x1E,0x3C,0x07,0x23,0x1F,0x3C,0x06,0x03,0x5E,0x23,0x00,0x3E,0x03,
+           0x23,0x33,0x3C,0x04,0x23,0xFF,0x3E,0x7D,0x22,0x7D,0x3B,0xFF,0x1A,0x2B,0x3C,0x07,
+           0x23,0xFF,0x3C,0x06,0x03,0x5E,0x23,0x34,0x3C,0x04,0x23,0xAA,0x3E,0x7D,0x22,0x7D,
+           0x3B,0xAA,0x1A,0x36,0x3C,0x07,0x23,0xAA,0x3C,0x06,0x03,0x5E,0x23,0x35,0x3C,0x04,
+           0x23,0x55,0x3E,0x7D,0x22,0x7D,0x3B,0x55,0x1A,0x41,0x3C,0x07,0x23,0x55,0x3C,0x06,
+           0x03,0x5E,0x23,0x36,0x3C,0x04,0x23,0x00,0x3E,0x7D,0x22,0x7D,0x3B,0x00,0x1A,0x4C,
+           0x3C,0x07,0x23,0x00,0x3C,0x06,0x03,0x5E,0x23,0x37,0x3C,0x04,0x23,0xAA,0x3E,0x7D,
+           0xC2,0x7D,0x12,0x55,0x23,0x01,0x3C,0x05,0x03,0x5E,0xC6,0x7D,0x1A,0x5E,0x23,0x02,
+           0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xCA,0x7D,0x12,0x67,
+           0x23,0x03,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0xCE,0x7D,
+           0x1A,0x70,0x23,0x04,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,
+           0xD2,0x7D,0x12,0x79,0x23,0x05,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,
+           0x03,0x5E,0xD6,0x7D,0x1A,0x82,0x23,0x06,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,
+           0x3C,0x07,0x03,0x5E,0xDA,0x7D,0x12,0x8B,0x23,0x07,0x3C,0x05,0x23,0x00,0x3C,0x06,
+           0x23,0x01,0x3C,0x07,0x03,0x5E,0xDE,0x7D,0x1A,0x94,0x23,0x08,0x3C,0x05,0x23,0x01,
+           0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0x23,0x38,0x3C,0x04,0x23,0x55,0x3E,0x7D,
+           0xC2,0x7D,0x1A,0xA1,0x23,0x01,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,
+           0x03,0x5E,0xC6,0x7D,0x12,0xAA,0x23,0x02,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,
+           0x3C,0x07,0x03,0x5E,0xCA,0x7D,0x1A,0xB3,0x23,0x03,0x3C,0x05,0x23,0x01,0x3C,0x06,
+           0x23,0x00,0x3C,0x07,0x03,0x5E,0xCE,0x7D,0x12,0xBC,0x23,0x04,0x3C,0x05,0x23,0x00,
+           0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0xD2,0x7D,0x1A,0xC5,0x23,0x05,0x3C,0x05,
+           0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xD6,0x7D,0x12,0xCE,0x23,0x06,
+           0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0xDA,0x7D,0x1A,0xD7,
+           0x23,0x07,0x3C,0x05,0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xDE,0x7D,
+           0x12,0xE0,0x23,0x08,0x3C,0x05,0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,
+           0x23,0x39,0x3C,0x04,0x23,0xAA,0xCA,0xF1,0x1A,0xEA,0x23,0x01,0x3C,0x06,0x23,0x00,
+           0x3C,0x07,0x03,0x5E,0x23,0xAB,0xCA,0xF1,0x12,0xF2,0x23,0x00,0x3C,0x06,0x23,0x01,
+           0x3C,0x07,0x03,0x5E,0x23,0x40,0x3C,0x04,0x23,0xAA,0x9A,0xF1,0xCE,0xF1,0x1A,0xFD,
+           0x23,0x01,0x3C,0x06,0x23,0x00,0x3C,0x07,0x03,0x5E,0xCE,0xF1,0x13,0x04,0x23,0x00,
+           0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0x23,0x41,0x3C,0x04,0x23,0x00,0x3C,0x48,
+           0x3C,0x49,0x3C,0x4A,0x3C,0x4B,0x40,0x48,0xDE,0xF1,0x1B,0x13,0x23,0x01,0x3C,0x06,
+           0x23,0x00,0x3C,0x07,0x03,0x5E,0x23,0x42,0x3C,0x04,0x23,0x11,0x3C,0x4B,0x23,0x22,
+           0x3C,0x4A,0x23,0x33,0x3C,0x49,0x23,0x44,0x3C,0x48,0x40,0x48,0xDE,0xF1,0x13,0x25,
+           0x23,0x00,0x3C,0x06,0x23,0x01,0x3C,0x07,0x03,0x5E,0x23,0x43,0x3C,0x04,0x22,0x73,
+           0x3B,0x11,0x1B,0x2E,0x3C,0x07,0x23,0x11,0x3C,0x06,0x03,0x5E,0x23,0x44,0x3C,0x04,
+           0x22,0x72,0x3B,0x22,0x1B,0x37,0x3C,0x07,0x23,0x22,0x3C,0x06,0x03,0x5E,0x23,0x45,
+           0x3C,0x04,0x22,0x71,0x3B,0x33,0x1B,0x40,0x3C,0x07,0x23,0x33,0x3C,0x06,0x03,0x5E,
+           0x23,0x46,0x3C,0x04,0x22,0x70,0x3B,0x44,0x1B,0x49,0x3C,0x07,0x23,0x44,0x3C,0x06,
+           0x03,0x5E,0x23,0x47,0x3C,0x04,0x8A,0x0A,0xCA,0x0A,0x1B,0x4F,0x03,0x5E,0x23,0x48,
+           0x3C,0x04,0xAA,0x0A,0xCA,0x0A,0x13,0x55,0x03,0x5E,0x23,0x00,0x3E,0x7C,0x3C,0x04,
+           0x3C,0x07,0x3C,0x06,0x3C,0x05,0x82,0xF2,0x58,0x0E,0x68,0x6D,0x82,0xF4,0x03,0x5F};
+
+static unsigned char toreg2diag[672] = 
+          {0x23,0x01,0x3C,0x04,0x20,0x00,0x3B,0x00,0x18,0x02,0x3B,0x01,0x10,0x0B,0x08,0x0E,
+           0x23,0x00,0x3C,0x00,0x00,0x02,0x20,0x00,0x3C,0x07,0x01,0x4E,0x23,0x02,0x3C,0x04,
+           0x23,0x10,0x3E,0xB7,0x3E,0xB5,0x23,0x20,0x3E,0xB6,0x3E,0xB4,0xB6,0xF1,0x3F,0x00,
+           0x22,0x78,0x3B,0xFF,0x18,0x1F,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x03,
+           0x3C,0x04,0x22,0x79,0x3B,0xFF,0x18,0x28,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x01,0x4E,
+           0x23,0x04,0x3C,0x04,0x96,0xF1,0x3F,0x00,0x22,0x78,0x3B,0x00,0x18,0x33,0x3C,0x07,
+           0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x05,0x3C,0x04,0x22,0x79,0x3B,0x00,0x18,0x3C,
+           0x3C,0x07,0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x06,0x3C,0x04,0x23,0x11,0x3E,0xB1,
+           0x23,0x22,0x3E,0xB1,0x23,0x33,0x3E,0xB1,0x23,0x44,0x3E,0xB1,0x23,0x55,0x3E,0xB1,
+           0x23,0x66,0x3E,0xB1,0x23,0x77,0x3E,0xB1,0x23,0x88,0x3E,0xB1,0x3F,0x00,0x22,0x78,
+           0x3B,0x4A,0x18,0x56,0x3C,0x07,0x23,0x4A,0x3C,0x06,0x01,0x4E,0x23,0x07,0x3C,0x04,
+           0x22,0x79,0x3B,0xF7,0x18,0x5F,0x3C,0x07,0x23,0xF7,0x3C,0x06,0x01,0x4E,0x23,0x08,
+           0x3C,0x04,0xB2,0xF1,0x3F,0x00,0x22,0x74,0x3B,0xFF,0x18,0x6A,0x3C,0x07,0x23,0xFF,
+           0x3C,0x06,0x01,0x4E,0x23,0x09,0x3C,0x04,0x22,0x75,0x3B,0xFF,0x18,0x73,0x3C,0x07,
+           0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x10,0x3C,0x04,0x92,0xF1,0x3F,0x00,0x22,0x74,
+           0x3B,0x00,0x18,0x7E,0x3C,0x07,0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x11,0x3C,0x04,
+           0x22,0x75,0x3B,0x00,0x18,0x87,0x3C,0x07,0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x12,
+           0x3C,0x04,0x23,0x11,0x3E,0xB0,0x23,0x22,0x3E,0xB0,0x23,0x33,0x3E,0xB0,0x23,0x44,
+           0x3E,0xB0,0x23,0x55,0x3E,0xB0,0x23,0x66,0x3E,0xB0,0x23,0x77,0x3E,0xB0,0x23,0x88,
+           0x3E,0xB0,0x3F,0x00,0x22,0x74,0x3B,0x4A,0x18,0xA1,0x3C,0x07,0x23,0x4A,0x3C,0x06,
+           0x01,0x4E,0x23,0x13,0x3C,0x04,0x22,0x75,0x3B,0xF7,0x18,0xAA,0x3C,0x07,0x23,0xF7,
+           0x3C,0x06,0x01,0x4E,0x23,0x14,0x3C,0x04,0x23,0x80,0x3E,0xB7,0x3E,0xB5,0x23,0x04,
+           0x3E,0xB6,0x3E,0xB4,0xB6,0xF1,0x3F,0x00,0x22,0x78,0x3B,0xFF,0x18,0xBB,0x3C,0x07,
+           0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x15,0x3C,0x04,0x22,0x79,0x3B,0xFF,0x18,0xC4,
+           0x3C,0x07,0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x16,0x3C,0x04,0x96,0xF1,0x3F,0x00,
+           0x22,0x78,0x3B,0x00,0x18,0xCF,0x3C,0x07,0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x17,
+           0x3C,0x04,0x22,0x79,0x3B,0x00,0x18,0xD8,0x3C,0x07,0x23,0x00,0x3C,0x06,0x01,0x4E,
+           0x23,0x18,0x3C,0x04,0x23,0x11,0x3E,0xB1,0x23,0x22,0x3E,0xB1,0x23,0x33,0x3E,0xB1,
+           0x23,0x44,0x3E,0xB1,0x23,0x55,0x3E,0xB1,0x23,0x66,0x3E,0xB1,0x23,0x77,0x3E,0xB1,
+           0x23,0x88,0x3E,0xB1,0x3F,0x00,0x22,0x78,0x3B,0x5E,0x18,0xF2,0x3C,0x07,0x23,0x5E,
+           0x3C,0x06,0x01,0x4E,0x23,0x19,0x3C,0x04,0x22,0x79,0x3B,0xBF,0x18,0xFB,0x3C,0x07,
+           0x23,0xBF,0x3C,0x06,0x01,0x4E,0x23,0x20,0x3C,0x04,0xB2,0xF1,0x3F,0x00,0x22,0x74,
+           0x3B,0xFF,0x19,0x06,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x21,0x3C,0x04,
+           0x22,0x75,0x3B,0xFF,0x19,0x0F,0x3C,0x07,0x23,0xFF,0x3C,0x06,0x01,0x4E,0x23,0x22,
+           0x3C,0x04,0x92,0xF1,0x3F,0x00,0x22,0x74,0x3B,0x00,0x19,0x1A,0x3C,0x07,0x23,0x00,
+           0x3C,0x06,0x01,0x4E,0x23,0x23,0x3C,0x04,0x22,0x75,0x3B,0x00,0x19,0x23,0x3C,0x07,
+           0x23,0x00,0x3C,0x06,0x01,0x4E,0x23,0x24,0x3C,0x04,0x23,0x11,0x3E,0xB0,0x23,0x22,
+           0x3E,0xB0,0x23,0x33,0x3E,0xB0,0x23,0x44,0x3E,0xB0,0x23,0x55,0x3E,0xB0,0x23,0x66,
+           0x3E,0xB0,0x23,0x77,0x3E,0xB0,0x23,0x88,0x3E,0xB0,0x3F,0x00,0x22,0x74,0x3B,0x5E,
+           0x19,0x3D,0x3C,0x07,0x23,0x5E,0x3C,0x06,0x01,0x4E,0x23,0x25,0x3C,0x04,0x22,0x75,
+           0x3B,0xBF,0x19,0x46,0x3C,0x07,0x23,0xBF,0x3C,0x06,0x01,0x4E,0x23,0x00,0x3C,0x04,
+           0x3C,0x07,0x3C,0x06,0x3C,0x05,0x82,0xF2,0x58,0x08,0x68,0x6D,0x82,0xF4,0x01,0x4F};
+
+static unsigned char tostsdiag[1232] = 
+          {0x23,0x01,0x3C,0x04,0x23,0x00,0x3E,0x7C,0x23,0x14,0x3E,0x01,0x23,0x80,0x3E,0x02,
+           0x23,0x1D,0x3E,0x03,0x23,0x14,0x3E,0x43,0x23,0x7C,0x3E,0x07,0xA2,0x0A,0x23,0x50,
+           0x68,0x69,0x3B,0x00,0x10,0x10,0x23,0x02,0x3C,0x04,0x22,0x04,0x3B,0x03,0x18,0x1C,
+           0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0x65,0x23,0x03,0x3C,0x04,0xA2,0x0A,0x23,0x50,
+           0x68,0x69,0x3B,0x00,0x10,0x20,0x82,0x0A,0x0A,0x5C,0x68,0x6A,0x23,0x01,0x3E,0x00,
+           0x3F,0x10,0x22,0x00,0xCE,0x04,0x18,0x31,0x22,0x04,0x3C,0x07,0x23,0x8B,0x3C,0x06,
+           0x02,0x65,0x23,0x04,0x3C,0x04,0xA2,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,0x10,0x35,
+           0x82,0x0A,0x23,0x05,0x3C,0x04,0xAA,0x0A,0xCA,0x0A,0x10,0x41,0x22,0x04,0x3C,0x07,
+           0x02,0x65,0x23,0x06,0x3C,0x04,0x0A,0x22,0x23,0x07,0x3C,0x04,0x22,0x04,0x2F,0x07,
+           0x3B,0x00,0x18,0x4E,0x3C,0x07,0x23,0x00,0x3C,0x06,0x02,0x65,0x23,0x08,0x3C,0x04,
+           0xCA,0x0A,0x10,0x50,0x22,0x04,0x3B,0xC1,0x18,0x59,0x3C,0x07,0x23,0xC3,0x3C,0x06,
+           0x02,0x65,0x23,0x09,0x3C,0x04,0x86,0x0A,0xA2,0x0A,0x23,0x10,0x3C,0x04,0x23,0x34,
+           0x3E,0x01,0x23,0x80,0x3E,0x02,0x23,0x1D,0x3E,0x03,0x23,0x00,0x3E,0x7C,0x23,0x7C,
+           0x3E,0x07,0x23,0x11,0x3C,0x04,0x22,0x04,0x3B,0x01,0x18,0x72,0x3C,0x07,0x23,0x03,
+           0x3C,0x06,0x02,0x65,0x23,0x12,0x3C,0x04,0x82,0x0A,0x0A,0x5C,0x68,0x6A,0x20,0x08,
+           0x3E,0x00,0x3F,0x10,0x22,0x00,0xCE,0x04,0x18,0x82,0x22,0x04,0x3C,0x07,0x23,0x8B,
+           0x3C,0x06,0x02,0x65,0x23,0x14,0x3C,0x04,0xA2,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,
+           0x10,0x86,0x82,0x0A,0x23,0x15,0x3C,0x04,0xAA,0x0A,0xCA,0x0A,0x10,0x92,0x22,0x04,
+           0x3C,0x07,0x02,0x65,0x23,0x16,0x3C,0x04,0x0A,0x22,0x68,0x6A,0x23,0x02,0x3E,0x00,
+           0x23,0x17,0x3C,0x04,0x22,0x04,0x2F,0x07,0x3B,0x00,0x18,0xA2,0x3C,0x07,0x23,0x00,
+           0x3C,0x06,0x02,0x65,0x23,0x18,0x3C,0x04,0xCA,0x0A,0x10,0xA4,0xC6,0x04,0x10,0xA6,
+           0x22,0x04,0x3B,0xC3,0x18,0xAF,0x3C,0x07,0x23,0xC3,0x3C,0x06,0x02,0x65,0x23,0x19,
+           0x3C,0x04,0x86,0x0A,0xA2,0x0A,0x23,0x20,0x3C,0x04,0x23,0x8C,0x3E,0x01,0x23,0xA0,
+           0x3E,0x02,0x23,0x7E,0x3E,0x03,0x23,0x00,0x3E,0x7C,0x23,0x7C,0x3E,0x07,0x23,0x21,
+           0x3C,0x04,0x82,0x0A,0x23,0x0A,0x68,0x69,0x3B,0x00,0x10,0xC3,0x23,0x22,0x3C,0x04,
+           0xAA,0x0A,0xCA,0x0A,0x10,0xCE,0x22,0x04,0x3C,0x07,0x02,0x65,0x23,0x23,0x3C,0x04,
+           0x22,0x04,0x2F,0x03,0x3B,0x03,0x18,0xD8,0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0x65,
+           0x23,0x7E,0x68,0x6A,0x3E,0x00,0x68,0x6B,0x0A,0x19,0x0A,0x19,0x0A,0x19,0x0A,0x19,
+           0x23,0x7E,0x68,0x6A,0x3E,0x00,0x23,0x24,0x3C,0x04,0x22,0x04,0x2F,0x03,0x3B,0x00,
+           0x18,0xED,0x3C,0x07,0x23,0x00,0x3C,0x06,0x02,0x65,0x23,0x25,0x3C,0x04,0xDA,0x04,
+           0x18,0xEF,0xDA,0x04,0x18,0xF1,0x01,0x02,0x23,0x26,0x3C,0x04,0xCA,0x0A,0x10,0xF6,
+           0xC6,0x04,0x10,0xF8,0x22,0x04,0x2F,0xBB,0x3B,0x83,0x19,0x02,0x3C,0x07,0x23,0x83,
+           0x3C,0x06,0x02,0x65,0x23,0x27,0x3C,0x04,0xA2,0x0A,0x86,0x0A,0x23,0x30,0x68,0x69,
+           0x3B,0x00,0x11,0x07,0x82,0x0A,0x23,0x28,0x3C,0x04,0x23,0x7E,0x68,0x6A,0x3E,0x00,
+           0x68,0x6B,0x3E,0x00,0x68,0x6A,0x3E,0x00,0x3F,0x10,0x22,0x00,0x3F,0x10,0x22,0x00,
+           0xD2,0x04,0x12,0x65,0x23,0x29,0x3C,0x04,0x23,0xFF,0x68,0x6A,0x3E,0x00,0x3E,0x00,
+           0x3F,0x10,0x22,0x00,0xD6,0x04,0x12,0x65,0x3F,0x10,0x22,0x00,0x23,0x2A,0x3C,0x04,
+           0xCE,0x04,0x12,0x65,0x23,0x2B,0x3C,0x04,0x3F,0x10,0x22,0x00,0xCE,0x04,0x11,0x2E,
+           0x23,0x2C,0x3C,0x04,0x86,0x0A,0xA2,0x0A,0x23,0x30,0x3C,0x04,0x23,0x60,0x3E,0x01,
+           0x23,0x80,0x3E,0x02,0x23,0x32,0x3E,0x03,0x23,0x00,0x3E,0x7C,0x23,0x7C,0x3E,0x07,
+           0x23,0x31,0x3C,0x04,0xAA,0x0A,0xCA,0x0A,0x11,0x48,0x22,0x04,0x3C,0x07,0x02,0x65,
+           0x23,0x32,0x3C,0x04,0x82,0x0A,0x23,0xAA,0x68,0x6A,0x3E,0x00,0x3E,0x00,0x23,0x32,
+           0x3E,0x00,0x3E,0x00,0x0A,0x19,0x0A,0x19,0x0A,0x19,0x0A,0x19,0x23,0x33,0x3C,0x04,
+           0x23,0x00,0x3E,0xF5,0x22,0xF5,0x3B,0x02,0x11,0x5A,0x22,0x04,0x2F,0x03,0x3B,0x00,
+           0x19,0x65,0x3C,0x07,0x23,0x00,0x3C,0x06,0x02,0x65,0x23,0x34,0x3C,0x04,0xDA,0x04,
+           0x19,0x67,0xDA,0x04,0x19,0x69,0x23,0x35,0x3C,0x04,0xCA,0x0A,0x11,0x6D,0xC6,0x04,
+           0x11,0x6F,0x22,0x04,0x2F,0x83,0x3B,0x83,0x19,0x79,0x3C,0x07,0x23,0x83,0x3C,0x06,
+           0x02,0x65,0x23,0x36,0x3C,0x04,0xA2,0x0A,0x86,0x0A,0x40,0x08,0x42,0x48,0x42,0x88,
+           0x40,0x0C,0x42,0x4C,0x42,0x8C,0x40,0x10,0x42,0x50,0x42,0x90,0x40,0x14,0x42,0x54,
+           0x42,0x94,0x23,0x40,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x23,0x40,0x3E,0x01,0x23,0xA6,
+           0x3E,0x02,0x23,0x14,0x3E,0x03,0xA2,0x0A,0x86,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,
+           0x11,0x96,0x82,0x0A,0x23,0x7C,0x3E,0x07,0x23,0x41,0x68,0x6B,0x24,0xD3,0x3C,0x04,
+           0x22,0x04,0x2F,0xAB,0x3B,0x03,0x19,0xA8,0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0x65,
+           0x23,0x42,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x23,0x30,0x68,0x69,0x3B,0x00,0x11,0xAD,
+           0x23,0x00,0x68,0x6A,0x3E,0x00,0x68,0x6B,0x3E,0x00,0xDE,0x04,0x11,0xB5,0x22,0x00,
+           0xD6,0x04,0x19,0xBB,0x02,0x65,0x23,0x43,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x3B,0x43,
+           0x19,0xD1,0xA2,0x0A,0x86,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,0x11,0xC4,0x82,0x0A,
+           0x23,0x80,0x68,0x6A,0x3E,0x00,0xDE,0x04,0x11,0xCB,0x22,0x00,0xCE,0x04,0x19,0xD1,
+           0x02,0x65,0xA2,0x0A,0x86,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,0x11,0xD4,0x82,0x0A,
+           0x23,0x44,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x0A,0x19,0x0A,0x19,0x0A,0x19,0x0A,0x19,
+           0x23,0x45,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x23,0x00,0x3E,0xF5,0x22,0xF5,0x3B,0x02,
+           0x11,0xE6,0x22,0x04,0x2F,0x03,0x3B,0x00,0x19,0xF1,0x3C,0x07,0x23,0x00,0x3C,0x06,
+           0x02,0x65,0x23,0x46,0x68,0x6B,0x24,0xD3,0x3C,0x04,0xDA,0x04,0x19,0xF5,0xDA,0x04,
+           0x19,0xF7,0x23,0x47,0x68,0x6B,0x24,0xD3,0x3C,0x04,0xCA,0x0A,0x11,0xFD,0xC6,0x04,
+           0x11,0xFF,0x22,0x04,0x2F,0xBB,0x3B,0x83,0x1A,0x09,0x3C,0x07,0x23,0x83,0x3C,0x06,
+           0x02,0x65,0xA2,0x0A,0x86,0x0A,0x23,0x30,0x68,0x69,0x3B,0x00,0x12,0x0C,0x82,0x0A,
+           0x23,0xAA,0x3C,0x04,0x3C,0x07,0x3C,0x06,0x3C,0x05,0x23,0x00,0x3E,0x7C,0x82,0xF2,
+           0x02,0x66,0x20,0x08,0x3E,0x00,0x20,0x0C,0x3E,0x00,0x20,0x10,0x3E,0x00,0x20,0x14,
+           0x3E,0x00,0x68,0x6D,0x4A,0xD1,0x0A,0x5C,0x23,0x01,0x3C,0xD0,0x20,0x08,0x0A,0x4A,
+           0x68,0x64,0x3C,0xC4,0x20,0x0C,0x0A,0x4A,0x68,0x64,0x3C,0xC5,0x20,0x10,0x0A,0x4A,
+           0x68,0x64,0x3C,0xC6,0x20,0x14,0x0A,0x4A,0x68,0x64,0x3C,0xC7,0x20,0x08,0x0A,0x4A,
+           0x68,0x64,0x3C,0xC4,0x20,0x0C,0x0A,0x4A,0x68,0x64,0x3C,0xC5,0x20,0x10,0x0A,0x4A,
+           0x68,0x64,0x3C,0xC6,0x23,0x07,0x3C,0xD0,0x20,0x14,0x0A,0x4A,0x68,0x64,0x3C,0xC7,
+           0x48,0xD1,0x68,0x6D,0x68,0x6A,0x68,0x65,0xFA,0x7D,0xCA,0xF1,0xFE,0x7D,0x68,0x6B,
+           0x3E,0x00,0xDA,0x7D,0x20,0xD0,0x2F,0x07,0x68,0x65,0xDE,0x7D,0xCE,0xF1,0x12,0x59,
+           0x33,0x10,0x68,0x6B,0x3E,0x00,0x68,0x6D,0x23,0x40,0x3E,0x06,0x3E,0x08,0x68,0x6A,
+           0x23,0x55,0x3E,0x00,0x23,0x1D,0x3E,0x00,0x68,0x6D,0x82,0xF4,0x02,0x66,0x00,0x00};
+
+static unsigned char towrapdiag[1376] = 
+          {0x23,0x01,0x3C,0x04,0x23,0x14,0x3E,0x01,0x23,0x80,0x3E,0x02,0x23,0x1D,0x3E,0x03,
+           0x23,0x00,0x3E,0x7C,0x23,0x7C,0x3E,0x07,0x23,0x14,0x3E,0x43,0xA2,0x0A,0x23,0x50,
+           0x68,0x69,0x3B,0x00,0x10,0x10,0x23,0x02,0x3C,0x04,0x22,0x04,0x3B,0x03,0x18,0x1C,
+           0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0xA8,0x23,0x03,0x3C,0x04,0x82,0x0A,0x0A,0x1F,
+           0x23,0x04,0x3C,0x04,0x0A,0x37,0x23,0x05,0x3C,0x04,0x23,0x00,0x3E,0xF5,0x22,0xF5,
+           0x3B,0x02,0x10,0x27,0x22,0x04,0x3B,0x53,0x18,0x31,0x3C,0x07,0x23,0x53,0x3C,0x06,
+           0x02,0xA8,0x23,0x04,0x68,0x69,0x3B,0x00,0x10,0x32,0x3E,0x06,0x3E,0x08,0xA2,0x0A,
+           0x23,0x06,0x3C,0x04,0x0A,0x50,0x23,0x07,0x3C,0x04,0x0A,0x6D,0x23,0x08,0x3C,0x04,
+           0x22,0x04,0x3B,0x03,0x18,0x47,0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0xA8,0x23,0x10,
+           0x3C,0x04,0x23,0x34,0x3E,0x01,0x23,0x80,0x3E,0x02,0x23,0x1D,0x3E,0x03,0x23,0x00,
+           0x3E,0x7C,0x3C,0xC0,0x3C,0xC1,0x3C,0xC2,0x3C,0xC3,0x3C,0xC8,0x3C,0xC9,0x3C,0xCA,
+           0x3C,0xCB,0x3C,0xC4,0x3C,0xC5,0x3C,0xC6,0x3C,0xC7,0x23,0x7C,0x3E,0x07,0x23,0x12,
+           0x3C,0x04,0x22,0x04,0x3B,0x03,0x18,0x68,0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0xA8,
+           0x23,0x13,0x3C,0x04,0x82,0x0A,0x0A,0x1F,0x68,0x6A,0x23,0x02,0x3E,0x00,0x23,0x14,
+           0x3C,0x04,0x0A,0x37,0x3F,0x10,0x22,0x00,0x3C,0xCC,0x23,0x15,0x3C,0x04,0xC6,0x04,
+           0x10,0x77,0x22,0x04,0x3B,0x5B,0x18,0x80,0x3C,0x07,0x23,0x5B,0x3C,0x06,0x02,0xA8,
+           0x3E,0x06,0x3E,0x08,0xA2,0x0A,0x23,0x16,0x3C,0x04,0x0A,0x50,0x23,0x17,0x3C,0x04,
+           0x20,0xC4,0x2F,0x03,0x3C,0xC4,0x20,0xC5,0x2F,0x03,0x3C,0xC5,0x20,0xC6,0x2F,0x03,
+           0x3C,0xC6,0x20,0xC7,0x2F,0x03,0x3C,0xC7,0x20,0xC8,0x2F,0x03,0x3C,0xC8,0x20,0xC9,
+           0x2F,0x03,0x3C,0xC9,0x20,0xCA,0x2F,0x03,0x3C,0xCA,0x20,0xCB,0x2F,0x03,0x3C,0xCB,
+           0x0A,0x6D,0x23,0x18,0x3C,0x04,0x20,0xCC,0x3B,0x00,0x18,0xAA,0x3C,0x07,0x23,0x00,
+           0x3C,0x06,0x02,0x8A,0x23,0x19,0x3C,0x04,0x22,0x04,0x3B,0x03,0x18,0xB3,0x3C,0x07,
+           0x23,0x03,0x3C,0x06,0x02,0xA8,0x23,0x20,0x3C,0x04,0x23,0x8C,0x3E,0x01,0x23,0xA0,
+           0x3E,0x02,0x23,0x7E,0x3E,0x03,0x23,0x00,0x3E,0x7C,0x3C,0xC0,0x3C,0xC1,0x3C,0xC2,
+           0x3C,0xC3,0x3C,0xC8,0x3C,0xC9,0x3C,0xCA,0x3C,0xCB,0x3C,0xC4,0x3C,0xC5,0x3C,0xC6,
+           0x3C,0xC7,0x23,0x10,0x3E,0xB7,0x3E,0xB5,0x23,0x20,0x3E,0xB6,0x3E,0xB4,0x23,0x7C,
+           0x3E,0x07,0x23,0x21,0x3C,0x04,0x82,0x0A,0x23,0x0A,0x68,0x69,0x3B,0x00,0x10,0xD5,
+           0x23,0x7E,0x68,0x6A,0x3E,0x00,0x68,0x6B,0x0A,0x05,0x96,0xF1,0x92,0xF1,0x20,0x08,
+           0x3E,0xB1,0x20,0x0C,0x3E,0xB1,0x20,0x10,0x3E,0xB1,0x20,0x14,0x3E,0xB1,0x68,0x6B,
+           0x22,0x78,0x3E,0x00,0x22,0x79,0x3E,0x00,0x23,0x7E,0x68,0x6A,0x3E,0x00,0x23,0x22,
+           0x3C,0x04,0x3F,0x10,0x22,0x00,0x1A,0xA8,0x3C,0xC0,0x3E,0xB0,0x3F,0x10,0x22,0x00,
+           0x1A,0xA8,0x3C,0xC1,0x3E,0xB0,0x3F,0x10,0x22,0x00,0x1A,0xA8,0x3C,0xC2,0x3E,0xB0,
+           0x3F,0x10,0x22,0x00,0x1A,0xA8,0x3C,0xC3,0x3E,0xB0,0x3F,0x10,0x22,0x00,0x1A,0xA8,
+           0x3C,0xCD,0x3E,0xB0,0x3F,0x10,0x22,0x00,0x1A,0xA8,0x3C,0xCE,0x3E,0xB0,0x3F,0x10,
+           0x22,0x00,0x12,0xA8,0x3C,0xCF,0x23,0x23,0x3C,0x04,0xD2,0x04,0x12,0xA8,0xA2,0x0A,
+           0x23,0x24,0x3C,0x04,0x0A,0x50,0x23,0x25,0x3C,0x04,0x22,0x74,0x3B,0x47,0x19,0x24,
+           0x3C,0x07,0x23,0x47,0x3C,0x06,0x02,0x8A,0x22,0x75,0x3B,0x0F,0x19,0x2B,0x3C,0x07,
+           0x23,0x0F,0x3C,0x06,0x02,0x8A,0x23,0x26,0x3C,0x04,0x20,0xCF,0x3B,0x7E,0x19,0x34,
+           0x3C,0x07,0x23,0x7E,0x3C,0x06,0x02,0x8A,0x23,0x27,0x3C,0x04,0x23,0x00,0x3E,0xF5,
+           0x22,0xF5,0x3B,0x02,0x11,0x38,0x22,0x04,0x2F,0xFB,0x3B,0x03,0x19,0x43,0x3C,0x07,
+           0x23,0x03,0x3C,0x06,0x02,0xA8,0x23,0x30,0x3C,0x04,0x23,0x60,0x3E,0x01,0x23,0x80,
+           0x3E,0x02,0x23,0x32,0x3E,0x03,0x23,0x00,0x3E,0x7C,0x3C,0xC0,0x3C,0xC1,0x3C,0xC2,
+           0x3C,0xC3,0x3C,0xC8,0x3C,0xC9,0x3C,0xCA,0x3C,0xCB,0x3C,0xC4,0x3C,0xC5,0x3C,0xC6,
+           0x3C,0xC7,0x23,0x80,0x3E,0xB7,0x3E,0xB5,0x23,0x04,0x3E,0xB6,0x3E,0xB4,0x23,0x7C,
+           0x3E,0x07,0x23,0x31,0x3C,0x04,0x82,0x0A,0x23,0xAA,0x3E,0x00,0x3E,0x00,0x23,0x32,
+           0x3E,0x00,0x3E,0x00,0x0A,0x05,0xB6,0xF1,0xB2,0xF1,0x20,0x08,0x3E,0xB1,0x20,0x0C,
+           0x3E,0xB1,0x20,0x10,0x3E,0xB1,0x20,0x14,0x3E,0xB1,0x68,0x6B,0x22,0x78,0x3E,0x00,
+           0x22,0x79,0x3E,0x00,0x23,0x32,0x3C,0x04,0x0A,0x0E,0x20,0xC0,0x3E,0xB0,0x20,0xC1,
+           0x3E,0xB0,0x20,0xC2,0x3E,0xB0,0x20,0xC3,0x3E,0xB0,0x3F,0x10,0x22,0x00,0x3C,0xCD,
+           0x3E,0xB0,0x3F,0x10,0x22,0x00,0x3C,0xCE,0x3E,0xB0,0xA2,0x0A,0x23,0x33,0x3C,0x04,
+           0x0A,0x50,0x23,0x34,0x3C,0x04,0x22,0x74,0x37,0xFF,0x3B,0x01,0x19,0x9B,0x3C,0x07,
+           0x23,0x01,0x3C,0x06,0x02,0x8A,0x22,0x75,0x37,0xFF,0x3B,0xB0,0x19,0xA3,0x3C,0x07,
+           0x23,0xB0,0x3C,0x06,0x02,0x8A,0x23,0x35,0x3C,0x04,0x23,0x00,0x3E,0xF5,0x22,0xF5,
+           0x3B,0x02,0x11,0xA7,0x22,0x04,0x2F,0xC3,0x3B,0x03,0x19,0xB2,0x3C,0x07,0x23,0x03,
+           0x3C,0x06,0x02,0xA8,0x40,0x08,0x42,0x48,0x42,0x88,0x40,0x0C,0x42,0x4C,0x42,0x8C,
+           0x40,0x10,0x42,0x50,0x42,0x90,0x40,0x14,0x42,0x54,0x42,0x94,0x23,0x40,0x68,0x6B,
+           0x24,0xD3,0x3C,0x04,0x23,0x40,0x3E,0x01,0x23,0xA0,0x3E,0x02,0x23,0x34,0x3E,0x03,
+           0x23,0x0F,0x68,0x69,0x3B,0x00,0x11,0xC9,0x82,0x0A,0x3C,0xC0,0x3C,0xC1,0x3C,0xC2,
+           0x3C,0xC3,0x3C,0xC8,0x3C,0xC9,0x3C,0xCA,0x3C,0xCB,0x3C,0xC4,0x3C,0xC5,0x3C,0xC6,
+           0x3C,0xC7,0x23,0x7C,0x3E,0x07,0x23,0x41,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x0A,0x05,
+           0x23,0x42,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x0A,0x0E,0xA2,0x0A,0x23,0x43,0x68,0x6B,
+           0x24,0xD3,0x3C,0x04,0x0A,0x50,0x23,0x44,0x68,0x6B,0x24,0xD3,0x3C,0x04,0x23,0x00,
+           0x3E,0xF5,0x22,0xF5,0x3B,0x02,0x11,0xF1,0x22,0x04,0x2F,0x2B,0x3B,0x03,0x19,0xFC,
+           0x3C,0x07,0x23,0x03,0x3C,0x06,0x02,0xA8,0x23,0xAA,0x3C,0x04,0x3C,0x07,0x3C,0x06,
+           0x3C,0x05,0x23,0x00,0x3E,0x7C,0x82,0xF2,0x02,0xA9,0x20,0x08,0x3E,0x00,0x20,0x0C,
+           0x3E,0x00,0x20,0x10,0x3E,0x00,0x20,0x14,0x3E,0x00,0x68,0x6D,0xDE,0x04,0x12,0x0E,
+           0x22,0x00,0x3C,0xC0,0xDE,0x04,0x12,0x12,0x22,0x00,0x3C,0xC1,0xDE,0x04,0x12,0x16,
+           0x22,0x00,0x3C,0xC2,0xDE,0x04,0x12,0x1A,0x22,0x00,0x3C,0xC3,0x68,0x6D,0x4A,0xD1,
+           0x0A,0x9F,0x23,0x01,0x3C,0xD0,0x20,0x08,0x0A,0x8D,0x68,0x64,0x3C,0xC4,0x20,0x0C,
+           0x0A,0x8D,0x68,0x64,0x3C,0xC5,0x20,0x10,0x0A,0x8D,0x68,0x64,0x3C,0xC6,0x23,0x07,
+           0x3C,0xD0,0x20,0x14,0x0A,0x8D,0x68,0x64,0x3C,0xC7,0x48,0xD1,0x68,0x6D,0x3F,0x10,
+           0x22,0x00,0x3C,0xC0,0x3F,0x10,0x22,0x00,0x3C,0xC8,0x3F,0x10,0x22,0x00,0x3C,0xC1,
+           0x3F,0x10,0x22,0x00,0x3C,0xC9,0x3F,0x10,0x22,0x00,0x3C,0xC2,0x3F,0x10,0x22,0x00,
+           0x3C,0xCA,0x3F,0x10,0x22,0x00,0x3C,0xC3,0x3F,0x10,0x22,0x00,0x3C,0xCB,0x68,0x6D,
+           0x20,0x08,0x38,0xC0,0x1A,0x57,0x3C,0x06,0x20,0xC0,0x3C,0x07,0x02,0x8A,0x20,0x0C,
+           0x38,0xC1,0x1A,0x5E,0x3C,0x06,0x20,0xC1,0x3C,0x07,0x02,0x8A,0x20,0x10,0x38,0xC2,
+           0x1A,0x65,0x3C,0x06,0x20,0xC2,0x3C,0x07,0x02,0x8A,0x20,0x14,0x38,0xC3,0x1A,0x6C,
+           0x3C,0x06,0x20,0xC3,0x3C,0x07,0x02,0x8A,0x68,0x6D,0x20,0xC4,0x38,0xC8,0x1A,0x74,
+           0x3C,0x06,0x20,0xC8,0x3C,0x07,0x02,0x8A,0x20,0xC5,0x38,0xC9,0x1A,0x7B,0x3C,0x06,
+           0x20,0xC9,0x3C,0x07,0x02,0x8A,0x20,0xC6,0x38,0xCA,0x1A,0x82,0x3C,0x06,0x20,0xCA,
+           0x3C,0x07,0x02,0x8A,0x20,0xC7,0x38,0xCB,0x1A,0x89,0x3C,0x06,0x20,0xCB,0x3C,0x07,
+           0x02,0x8A,0x68,0x6D,0x22,0x04,0x3C,0x05,0x02,0xA8,0x68,0x6A,0x68,0x65,0xFA,0x7D,
+           0xCA,0xF1,0xFE,0x7D,0x68,0x6B,0x3E,0x00,0xDA,0x7D,0x20,0xD0,0x2F,0x07,0x68,0x65,
+           0xDE,0x7D,0xCE,0xF1,0x12,0x9C,0x33,0x10,0x68,0x6B,0x3E,0x00,0x68,0x6D,0x23,0x40,
+           0x3E,0x06,0x3E,0x08,0x68,0x6A,0x23,0x55,0x3E,0x00,0x23,0x1D,0x3E,0x00,0x68,0x6D,
+           0x82,0xF4,0x02,0xA9,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00};
+
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/keyboard.c linuxppc64_2_4/drivers/char/keyboard.c
--- linux-2.4.19/drivers/char/keyboard.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/char/keyboard.c	Wed May 29 08:30:50 2002
@@ -42,6 +42,9 @@
 #include <linux/kbd_ll.h>
 #include <linux/sysrq.h>
 #include <linux/pm.h>
+#ifdef	CONFIG_KDB
+#include <linux/kdb.h>
+#endif	/* CONFIG_KDB */
 
 #define SIZE(x) (sizeof(x)/sizeof((x)[0]))
 
@@ -238,6 +241,13 @@
 		    up_flag = kbd_unexpected_up(keycode);
 	} else
 		rep = test_and_set_bit(keycode, key_down);
+
+#ifdef	CONFIG_KDB
+	if (!up_flag && (keycode == E1_PAUSE) && kdb_on) {
+		kdb(KDB_REASON_KEYBOARD, 0, kbd_pt_regs);
+		return;
+	}
+#endif	/* CONFIG_KDB */
 
 #ifdef CONFIG_MAGIC_SYSRQ		/* Handle the SysRq Hack */
 	if (keycode == SYSRQ_KEY) {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/serial.c linuxppc64_2_4/drivers/char/serial.c
--- linux-2.4.19/drivers/char/serial.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/char/serial.c	Mon Jul 22 09:56:11 2002
@@ -223,6 +223,30 @@
 #include <linux/sysrq.h>
 #endif
 
+#ifdef	CONFIG_KDB
+#include <linux/kdb.h>
+/*
+ * kdb_serial_line records the serial line number of the first serial console.
+ * NOTE: The kernel ignores characters on the serial line unless a user space
+ * program has opened the line first.  To enter kdb before user space has opened
+ * the serial line, you can use the 'kdb=early' flag to lilo and set the
+ * appropriate breakpoints.
+ *
+ * kdb_serial_str[] is the sequence that the user must enter on the serial
+ * console to invoke kdb.  It can be a single character such as "\001"
+ * (control-A) or multiple characters such as "\eKdB".  NOTE: All except the
+ * last character are passed through to the application reading from the serial
+ * console.
+ *
+ * I tried to make the sequence a CONFIG_ option but most of CML1 cannot cope
+ * with '\' in strings, CML2 should be able to do it.  KAO.
+ */
+
+static int  kdb_serial_line = -1;
+/* static char kdb_serial_str[] = "\001"; */
+static char kdb_serial_str[] = "startKDB";
+static char *kdb_serial_ptr = kdb_serial_str;
+#endif	/* CONFIG_KDB */
 /*
  * All of the compatibilty code so we can compile serial.c against
  * older kernels is hidden in serial_compat.h
@@ -577,6 +601,18 @@
 				return;		// if TTY_DONT_FLIP is set
 		}
 		ch = serial_inp(info, UART_RX);
+#ifdef	CONFIG_KDB
+		if ((info->line == kdb_serial_line) && kdb_on) {
+		    if (ch == *kdb_serial_ptr) {
+			if (!(*++kdb_serial_ptr)) {
+			    kdb(KDB_REASON_KEYBOARD, 0, (kdb_eframe_t)regs);
+			    kdb_serial_ptr = kdb_serial_str;
+			    break;
+			}
+		    } else
+			kdb_serial_ptr = kdb_serial_str;
+		}
+#endif	/* CONFIG_KDB */
 		*tty->flip.char_buf_ptr = ch;
 		icount->rx++;
 		
@@ -5987,6 +6023,17 @@
 	 */
 	if (serial_in(info, UART_LSR) == 0xff)
 		return -1;
+
+#ifdef	CONFIG_KDB
+	/*
+	 * Remember the line number of the first serial
+	 * console.  We'll make this the kdb serial console too.
+	 */
+	if (kdb_serial_line == -1) {
+		kdb_serial_line = co->index;
+		kdb_port = state->port;
+	}
+#endif	/* CONFIG_KDB */
 
 	return 0;
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/char/tty_io.c linuxppc64_2_4/drivers/char/tty_io.c
--- linux-2.4.19/drivers/char/tty_io.c	Fri Aug  2 19:39:43 2002
+++ linuxppc64_2_4/drivers/char/tty_io.c	Mon Aug 19 09:15:52 2002
@@ -162,6 +162,9 @@
 extern void tx3912_rs_init(void);
 extern void txx927_console_init(void);
 extern void sb1250_serial_console_init(void);
+extern void viocons_init(void);
+extern int viocons_init2(void);
+extern int hvc_console_init(void);
 
 #ifndef MIN
 #define MIN(a,b)	((a) < (b) ? (a) : (b))
@@ -2184,6 +2187,11 @@
 	 * set up the console device so that later boot sequences can 
 	 * inform about problems etc..
 	 */
+
+#ifdef CONFIG_VIOCONS
+       viocons_init();
+#endif
+
 #ifdef CONFIG_VT
 	con_init();
 #endif
@@ -2245,6 +2253,9 @@
 #ifdef CONFIG_SERIAL_TX3912_CONSOLE
 	tx3912_console_init();
 #endif
+#ifdef CONFIG_HVC_CONSOLE
+        hvc_console_init();
+#endif
 #ifdef CONFIG_TXX927_SERIAL_CONSOLE
 	txx927_console_init();
 #endif
@@ -2301,6 +2312,10 @@
 	/* console calls tty_register_driver() before kmalloc() works.
 	 * Thus, we can't devfs_register() then.  Do so now, instead. 
 	 */
+#ifdef CONFIG_VIOCONS
+        viocons_init2();
+#endif
+
 #ifdef CONFIG_VT
 	con_init_devfs();
 #endif
@@ -2389,5 +2404,8 @@
 #endif
 #ifdef CONFIG_A2232
 	a2232board_init();
+#endif
+#ifdef CONFIG_ICOM
+        iCom_init();
 #endif
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/Makefile linuxppc64_2_4/drivers/dump/Makefile
--- linux-2.4.19/drivers/dump/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/Makefile	Mon Jul  8 14:13:07 2002
@@ -0,0 +1,40 @@
+#
+# Makefile for the dump device drivers.
+#
+# 12 June 2000, Christoph Hellwig <schch@pe.tu-clausthal.de>
+# Rewritten by Matt D. Robinson (yakker@sourceforge.net) for
+# the dump directory.
+#
+
+O_TARGET	:=	dumpdrv.o
+export-objs	:=	dump_base.o
+
+list-multi	:=	dump.o
+dump-objs	:=	dump_base.o
+
+# get the base dump module and compression modules out of the way
+obj-$(CONFIG_DUMP_COMPRESS_RLE)		+= dump_rle.o
+obj-$(CONFIG_DUMP_COMPRESS_GZIP)	+= dump_gzip.o
+obj-$(CONFIG_DUMP)			+= dump.o
+
+# now deal with each individual architecture.
+ifeq ($(ARCH),i386)
+	dump-objs		+= dump_i386.o
+endif
+
+ifeq ($(ARCH),alpha)
+	dump-objs		+= dump_alpha.o
+endif
+
+ifeq ($(ARCH),ia64)
+	dump-objs		+= dump_ia64.o
+endif
+
+ifeq ($(ARCH),ppc64)
+	dump-objs		+= dump_ppc64.o
+endif
+
+include $(TOPDIR)/Rules.make
+
+dump.o:	$(dump-objs)
+	$(LD) -r -o $@ $(dump-objs)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_base.c linuxppc64_2_4/drivers/dump/dump_base.c
--- linux-2.4.19/drivers/dump/dump_base.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_base.c	Mon Aug 19 09:15:52 2002
@@ -0,0 +1,2077 @@
+/*
+ * Standard kernel functions for Linux crash dumps.
+ *
+ * Created by: Matt Robinson (yakker@sourceforge.net)
+ * Contributions from SGI, IBM, HP, MCL, and others.
+ *
+ * Copyright (C) 1999 - 2002 Silicon Graphics, Inc. All rights reserved.
+ * Copyright (C) 2000 - 2002 TurboLinux, Inc.  All rights reserved.
+ * Copyright (C) 2001 - 2002 Matt D. Robinson.  All rights reserved.
+ * Copyright (C) 2002 Free Software Foundation, Inc. All rights reserved.
+ *
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/*
+ * -----------------------------------------------------------------------
+ *
+ * DUMP HISTORY
+ *
+ * This dump code goes back to SGI's first attempts at dumping system
+ * memory on SGI systems running IRIX.  A few developers at SGI needed
+ * a way to take this system dump and analyze it, and created 'icrash',
+ * or IRIX Crash.  The mechanism (the dumps and 'icrash') were used
+ * by support people to generate crash reports when a system failure
+ * occurred.  This was vital for large system configurations that
+ * couldn't apply patch after patch after fix just to hope that the
+ * problems would go away.  So the system memory, along with the crash
+ * dump analyzer, allowed support people to quickly figure out what the
+ * problem was on the system with the crash dump.
+ *
+ * In comes Linux.  SGI started moving towards the open source community,
+ * and upon doing so, SGI wanted to take its support utilities into Linux
+ * with the hopes that they would end up the in kernel and user space to
+ * be used by SGI's customers buying SGI Linux systems.  One of the first
+ * few products to be open sourced by SGI was LKCD, or Linux Kernel Crash
+ * Dumps.  LKCD comprises of a patch to the kernel to enable system
+ * dumping, along with 'lcrash', or Linux Crash, to analyze the system
+ * memory dump.  A few additional system scripts and kernel modifications
+ * are also included to make the dump mechanism and dump data easier to
+ * process and use.
+ *
+ * As soon as LKCD was released into the open source community, a number
+ * of larger companies started to take advantage of it.  Today, there are
+ * many commmunity members that contribute to LKCD, and it continues to
+ * flourish and grow as an open source project.
+ *
+ * -----------------------------------------------------------------------
+ *
+ * SYSTEM DUMP LAYOUT
+ * 
+ * System dumps are currently the combination of a dump header and a set
+ * of data pages which contain the system memory.  The layout of the dump
+ * (for full dumps) is as follows:
+ *
+ *             +-----------------------------+
+ *             |     generic dump header     |
+ *             +-----------------------------+
+ *             |   architecture dump header  |
+ *             +-----------------------------+
+ *             |         page header         |
+ *             +-----------------------------+
+ *             |          page data          |
+ *             +-----------------------------+
+ *             |         page header         |
+ *             +-----------------------------+
+ *             |          page data          |
+ *             +-----------------------------+
+ *             |              |              |
+ *             |              |              |
+ *             |              |              |
+ *             |              |              |
+ *             |              V              |
+ *             +-----------------------------+
+ *             |        PAGE_END header      |
+ *             +-----------------------------+
+ *
+ * There are two dump headers, the first which is architecture
+ * independent, and the other which is architecture dependent.  This
+ * allows different architectures to dump different data structures
+ * which are specific to their chipset, CPU, etc.
+ *
+ * After the dump headers come a succession of dump page headers along
+ * with dump pages.  The page header contains information about the page
+ * size, any flags associated with the page (whether it's compressed or
+ * not), and the address of the page.  After the page header is the page
+ * data, which is either compressed (or not).  Each page of data is
+ * dumped in succession, until the final dump header (PAGE_END) is
+ * placed at the end of the dump, assuming the dump device isn't out
+ * of space.
+ *
+ * This mechanism allows for multiple compression types, different
+ * types of data structures, different page ordering, etc., etc., etc.
+ * It's a very straightforward mechanism for dumping system memory.
+ * -----------------------------------------------------------------------
+ *
+ * DUMP IMPLEMENTATION
+ *
+ * Dumps are implemented using a "start at the top and work your way
+ * to the bottom" method.  The starting location of kernel memory is
+ * determined, and each successive page is passed through to the
+ * dump_add_page() function, which determines whether to compress the
+ * page, throw it out, add the page header, etc.  This mechanism is
+ * going to change over time as non-disruptive dumps are created, so
+ * it is best to read through the code (it is commented pretty well),
+ * and let the developers know if it isn't clear enough.  We believe
+ * in well-documented, well-commented code.
+ *
+ * -----------------------------------------------------------------------
+ *
+ * DUMP TUNABLES
+ *
+ * This is the list of system tunables (via /proc) that are available
+ * for Linux systems.  All the read, write, etc., functions are listed
+ * here.  Currently, there are a few different tunables for dumps:
+ *
+ * dump_device (used to be dumpdev):
+ *     The device for dumping the memory pages out to.  This is almost
+ *     always the primary swap partition for disruptive dumps.
+ *
+ * dump_compress (used to be dump_compress_pages):
+ *     This is the flag which indicates which compression mechanism
+ *     to use.  This is a BITMASK, not an index (0,1,2,4,8,16,etc.).
+ *     This is the current set of values:
+ *
+ *     0: DUMP_COMPRESS_NONE -- Don't compress any pages.
+ *     1: DUMP_COMPRESS_RLE  -- This uses RLE compression.
+ *     2: DUMP_COMPRESS_GZIP -- This uses GZIP compression.
+ *
+ * dump_level:
+ *     The amount of effort the dump module should make to save
+ *     information for post crash analysis.  This value is now
+ *     a BITMASK value, not an index:
+ *
+ *     0:   Do nothing, no dumping. (DUMP_LEVEL_NONE)
+ *
+ *     1:   Print out the dump information to the dump header, and
+ *          write it out to the dump_device. (DUMP_LEVEL_HEADER)
+ *
+ *     2:   Write out the dump header and all kernel memory pages.
+ *          (DUMP_LEVEL_KERN)
+ *
+ *     4:   Write out the dump header and all kernel and user
+ *          memory pages.  (DUMP_LEVEL_USED)
+ *
+ *     8:   Write out the dump header and all conventional/cached 
+ *	    memory (RAM) pages in the system (kernel, user, free).  
+ *	    (DUMP_LEVEL_ALL_RAM)
+ *
+ *    16:   Write out everything, including non-conventional memory
+ *	    like firmware, proms, I/O registers, uncached memory.
+ *	    (DUMP_LEVEL_ALL)
+ *
+ *     The dump_level will default to 1.
+ *
+ *      REMIND: How about we change these to 1, 3, 7, 15, 31.
+ *	        If we reserve the bits for individual passes
+ *		it's easer to test things like non-conventional
+ *		memory dump on systems with limited disk space.
+ *		If would be more consistant with dump_level being 
+ *		a bitmask. We might also consider changeing the name 
+ *		to 'dump_passes' to make it more clear that the bitmask 
+ *		is not a index.
+ * 
+ *
+ * dump_flags:
+ *     These are the flags to use when talking about dumps.  There
+ *     are lots of possibilities.  This is a BITMASK value, not an index.
+ * 
+ *     1:  Try to keep the system running _after_ we are done
+ *         dumping -- for non-disruptive dumps.  (DUMP_FLAGS_NONDISRUPT)
+ *
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * -----------------------------------------------------------------------
+ *                      H E A D E R   F I L E S
+ * -----------------------------------------------------------------------
+ */
+
+/* header files */
+#include <asm/system.h>
+#include <linux/config.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/devfs_fs_kernel.h>
+#include <linux/delay.h>
+#include <linux/reboot.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <asm/uaccess.h>
+#include <linux/dump.h>
+#include <linux/smp_lock.h>
+#include <linux/mm.h>
+#include <linux/proc_fs.h>
+#include <linux/file.h>
+#include <linux/sysctl.h>
+#include <linux/iobuf.h>
+#include <linux/mman.h>
+#include <linux/init.h>
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/utsname.h>
+#include <linux/highmem.h>
+#include <linux/sysrq.h>
+#include <asm/hardirq.h>
+#include <linux/version.h>
+#include <asm/system.h>
+
+extern void __dump_progress_add_page(void);
+
+/*
+ * -----------------------------------------------------------------------
+ *                       D E F I N I T I O N S
+ * -----------------------------------------------------------------------
+ */
+#define DUMP_MODULE_NAME "dump"
+
+#if DUMP_DEBUG
+#define DUMP_PREFIX __func__
+#else
+#define DUMP_PREFIX DUMP_MODULE_NAME
+#endif
+
+
+#define DUMP_PRINTN(args...)  {						\
+	printk("\n");							\
+	DUMP_PRINTF(args);						\
+}
+#define DUMP_PRINTF(args...)  {						\
+	printk("%s: ", DUMP_PREFIX);					\
+	printk (args);							\
+}
+#define DUMP_PRINT(args...) {						\
+	printk(args);							\
+}
+
+#ifndef KERNEL_VERSION
+#define KERNEL_VERSION(a,b,c) (((a) << 16) | ((b) << 8) | (c))
+#endif
+
+/* 
+ * Handle printing of 64-bit values  
+ *
+ * NOTE: on ia64 %llx is recomend for ia32. 
+ *       on RedHat 7.2 	%llx work in user space but not in the kernel.
+ *	 Perhaps this is dependent on the kernel version.
+ */
+#if BITS_PER_LONG == 64
+#define PU64X "%lx"
+#else
+#define PU64X "%Lx"
+#endif
+
+
+/*
+ * -----------------------------------------------------------------------
+ *                         V A R I A B L E S
+ * -----------------------------------------------------------------------
+ */
+
+/* Dump tunables */
+kdev_t dump_device;                /* the actual kdev_t device number      */
+int dump_level;                    /* the current dump level               */
+int dump_compress;                 /* whether to try to compress each page */
+int dump_flags;                    /* whether to try to compress each page */
+
+/* Other global fields */
+char dumpdev_name[PATH_MAX];       /* the name of the dump device          */
+void *dump_page_buf;               /* dump page buffer for memcpy()!       */
+void *dump_page_buf_0;             /* dump page buffer returned by kmalloc */
+int dump_blk_size;                 /* sector size for dump_device          */
+int dump_blk_shift;                /* shift need to convert to sector size */
+dump_header_t dump_header;         /* the primary dump header              */
+dump_header_asm_t dump_header_asm; /* the arch-specific dump header        */
+struct kiobuf *dump_iobuf;         /* kiobuf for raw I/O to disk           */
+loff_t dump_fpos;                  /* the offset in the output device      */
+int dump_mbanks;		   /* number of  physical memory banks     */
+dump_mbank_t dump_mbank[MAXCHUNKS];/* describes layout of physical memory  */
+long dump_unreserved_mem = 0;      /* Save Pages even if it isn't reserved */
+long dump_unreferenced_mem = 0;    /* Save Pages even if page_count == 0   */
+long dump_nonconventional_mem = 0;  /* Save non-conventional mem (firmware) */
+
+static int dump_compress_none(char *old, int oldsize, char *new, int newsize);
+
+static dump_compress_t dump_none_compression = {
+	compress_type:	DUMP_COMPRESS_NONE,
+	compress_func:	dump_compress_none,
+};
+
+/* our device operations and functions */
+static int dump_open(struct inode *i, struct file *f);
+static int dump_release(struct inode *i, struct file *f);
+static int dump_ioctl(struct inode *i, struct file *f,
+	unsigned int cmd, unsigned long arg);
+
+static struct file_operations dump_fops = {
+	open:		dump_open,
+	release:	dump_release,
+	ioctl:		dump_ioctl,
+};
+
+/* function pointers and prototypes */
+int (*dump_compress_func)(char *old, int oldsize, char *new, int newsize);
+
+/* proc entries */
+static struct proc_dir_entry *dump_root;    /* /proc/sys/dump root dir     */
+static struct proc_dir_entry *dump_dd;      /* dump_device tunable         */
+static struct proc_dir_entry *dump_cp;      /* dump_compress tunable       */
+static struct proc_dir_entry *dump_l;       /* dump_level tunable          */
+static struct proc_dir_entry *dump_f;       /* dump_flags tunable          */
+
+/* static variables                                                        */
+static int dump_okay = FALSE;      	   /* can we dump out to disk?     */
+static char dpcpage[DUMP_DPC_PAGE_SIZE];  /* buffer used for compression   */
+static unsigned long dump_save_flags;     /* save_flags()/restore_flags()  */
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,4,4))
+static unsigned long dump_blocks[KIO_MAX_SECTORS];
+#endif
+
+/* for dumping from interrupt context (Fixme)  */
+static int saved_irq_count; 	/* remember the current irq nesting level   */
+static int saved_bh_count;	/* remember if we were in soft irq context  */
+
+/* used for dump compressors */
+static struct list_head dump_compress_list = LIST_HEAD_INIT(dump_compress_list);
+
+/* external variables                                                      */
+extern volatile int dumping_cpu;   /* cpu on which dump is progressing     */
+extern volatile int dump_in_progress;         /* note when we're dumping   */
+extern int panic_timeout;          /* time before reboot                   */
+extern int *blk_size[];            /* device block size calculations       */
+extern struct new_utsname system_utsname;     /* system information        */
+
+#if !defined(irq_count) && !defined(bh_count)
+extern irq_cpustat_t irq_stat[];
+#endif
+
+/* external functions                                                      */
+extern void si_meminfo(struct sysinfo *);
+extern void *kmalloc(size_t, int);
+extern void kfree(const void *);
+extern unsigned long simple_strtoul(const char *,char **,unsigned int);
+
+/* lkcd info structure -- this is used by lcrash for basic system data     */
+lkcdinfo_t lkcdinfo = {
+	0,
+	(sizeof(void *) * 8),
+#if defined(__LITTLE_ENDIAN) 
+	__LITTLE_ENDIAN,
+#else
+	__BIG_ENDIAN,
+#endif
+	0,
+	PAGE_SHIFT,
+	PAGE_SIZE,
+	PAGE_MASK,
+	PAGE_OFFSET,
+	0
+};
+
+#if DUMP_DEBUG
+void dump_bp(void) {}
+#endif
+
+
+/*
+ * -----------------------------------------------------------------------
+ *            / P R O C   T U N A B L E   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_read_proc()
+ * Func: Read the proc data for dump tunables.
+ */
+static int
+dump_read_proc(char *page, char **start, off_t off,
+	int count, int *eof, void *data)
+{
+	int len;
+	char *out = page;
+	struct proc_dir_entry *p = (struct proc_dir_entry *)data;
+
+
+	if (0 == strcmp(p->name, DUMP_LEVEL_NAME)) {
+		out += sprintf(out, "%d\n", dump_level);
+		len = out - page;
+	} else if (0 == strcmp(p->name, DUMP_FLAGS_NAME)) {
+		out += sprintf(out, "%d\n", dump_flags);
+		len = out - page;
+	} else if (0 == strcmp(p->name, DUMP_COMPRESS_NAME)) {
+		out += sprintf(out, "%d\n", dump_compress);
+		len = out - page;
+	} else if (0 == strcmp(p->name, DUMP_DEVICE_NAME)) {
+#if 0
+		len = strlen(dumpdev_name);
+		memcpy((void *)page, (const void *)dumpdev_name, len);
+#else
+		out += sprintf(out, "%s\n", dumpdev_name);
+		len = out - page;
+#endif
+	} else {
+		return (0);
+	}
+	len -= off;
+	if (len < count) {
+		*eof = 1;
+		if (len <= 0) return 0;
+	} else {
+		len = count;
+	}
+	*start = page + off;
+	return (len);
+}
+
+/*
+ * -----------------------------------------------------------------------
+ *              C O M P R E S S I O N   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_compress_none()
+ * Func: Don't do any compression, period.
+ */
+static int
+dump_compress_none(char *old, int oldsize, char *new, int newsize)
+{
+	/* just return the old size */
+	return (oldsize);
+}
+
+/*
+ * -----------------------------------------------------------------------
+ *                  U T I L I T Y   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_kernel_write()
+ *
+ * Func: Write out kernel information, check the device limitations,
+ *       block sizes, etc.  The thing I don't like about this function
+ *       is that most of it is a duplicate of what rw_raw_dev() does,
+ *       but sct doesn't want rw_raw_dev() to do kernel kiobuf stuff, and
+ *       brw_kiovec() isn't a good place to do device checks, since we'd
+ *       have to check all this stuff anyway, and device block calculations
+ *       are ugly in there for each call to the function.  So we do it all
+ *       here, call brw_kiovec() on our own, and return the results.
+ *
+ * 	 Writes DUMP_BUFFER_SIZE bytes in page buffer
+ *
+ * Returns: number of bytes written or -ERRNO. 
+ *	    At EOF it returns 0.
+ */
+static ssize_t
+dump_kernel_write(int *eof)
+{
+	int		err = 0, iosize, i;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,4,4))
+	unsigned long	*b = dump_blocks;
+#else
+	unsigned long	*b = dump_iobuf->blocks;
+#endif
+	unsigned long blocknr, blocks, limit;
+	
+	/* check the device size to make sure we are in-line */
+	if (blk_size[MAJOR(dump_device)]) {
+		limit = (((loff_t) blk_size[MAJOR(dump_device)][MINOR(dump_device)])
+				<< BLOCK_SIZE_BITS) >> dump_blk_shift;
+	} else {
+		limit = INT_MAX;
+	}
+
+	/* not aligned? */
+	if (dump_fpos & (dump_blk_size - 1)) {
+		DUMP_PRINTN("invalid buffer alignment!");
+		return (-EINVAL);
+	}
+
+	/* beyond the end of the device -- not really an error, per se */
+	if ((dump_fpos >> dump_blk_shift) > limit) {
+		DUMP_PRINTN("no space left on device!");
+		DUMP_BP();
+		return (0);
+	}
+
+	/* 
+	 * If near the end of the dump device we set *eof (if ptr provided) to
+	 * let the caller know that it's near the EOF on the dump device.
+	 */
+	if ( ((dump_fpos + (4 * DUMP_BUFFER_SIZE)) >> dump_blk_shift) > limit) {
+		if(eof != NULL) {
+			*eof = 1;
+		}
+		DUMP_BP();
+	}
+	/* 
+	 * If near the end of the dump device we set *eof (if ptr provided) to
+	 * let the caller know that it's near the EOF on the dump device.
+	 */
+	else if ( ((dump_fpos + (2 * DUMP_BUFFER_SIZE)) >> dump_blk_shift) > limit) {
+		if(eof != NULL) {
+			*eof = 1;
+		}
+		DUMP_BP();
+	}
+
+	/* reset the block number values */
+	blocknr = dump_fpos >> dump_blk_shift;
+
+	/* calculate the number of blocks */
+	blocks = DUMP_BUFFER_SIZE >> dump_blk_shift;
+	if (blocks > (KIO_MAX_SECTORS >> (dump_blk_shift - 9))) {
+		blocks = KIO_MAX_SECTORS >> (dump_blk_shift - 9);
+	}
+
+	/* don't write more blocks than our max limit */
+	if (blocks > limit - blocknr) {
+		blocks = limit - blocknr;
+	}
+
+	/* make sure we have blocks to write! */
+	if (!blocks) {
+		DUMP_PRINTN("no blocks left on dump device!");
+		return (0);
+	}
+
+	/* map the kernel kiobuf based on the iosize */
+	iosize = blocks << dump_blk_shift;
+
+	/* map the block numbers */
+	for (i = 0; i < blocks; i++) {
+		b[i] = blocknr++;
+	}
+
+	/* write out the data to disk */
+	err = brw_kiovec(WRITE, 1, &dump_iobuf, dump_device, b, dump_blk_size);
+
+	/* make sure our I/O size matches return from brw_kiovec() */
+	if (err < 0) {
+		DUMP_PRINTN("brw_kiovec() returned %d!", err);
+		return (-EFAULT);
+	}
+
+	if (err != DUMP_BUFFER_SIZE) {
+		DUMP_PRINTN("brw_kiovec() did not write out DUMP_BUFFER_SIZE!");
+		return (-EFAULT);
+	}
+
+	/* err is the transferred bytes -- return those */
+	dump_fpos += err;
+	return (err);
+}
+
+/*
+ * Name: dump_add_end_marker()
+ * Func: add last dump page marker to the end of the dump buffer.
+ */
+static void
+dump_add_end_marker(unsigned long *toffset, int flags)
+{
+	dump_page_t dp;
+
+	/* set the end marker */
+	dp.dp_address = dp.dp_size = 0x0;
+	dp.dp_flags = flags;
+
+#if DUMP_DEBUG >= 6
+	dp.dp_byte_offset = dump_header.dh_num_bytes + DUMP_BUFFER_SIZE + PAGE_SIZE;
+	dp.dp_page_index = dump_header.dh_num_dump_pages;
+#endif
+
+	/* clear the buffer and copy the page header */
+	memcpy((void *)(dump_page_buf + *toffset),
+		(const void *)&dp, sizeof(dump_page_t));
+	*toffset += sizeof(dump_page_t);
+
+#if DUMP_DEBUG >= 6
+	dump_header.dh_num_bytes += sizeof(dump_page_t);
+#endif
+	dump_header.dh_num_dump_pages++;
+
+	return;
+}
+
+
+long dump_add_page_debug = 0;
+long dump_add_page_test_pattern = 0;
+
+/*
+ * Name: dump_add_page()
+ * Func: Add a hardware page to the dump buffer.
+ *
+ * NB: page is a DUMP_PAGE_SIZE page, 
+ *     not a system page (PAGE_SIZE).
+ * 
+ * REMIND:
+ *	We should likely move some of this to
+ *	arch specific code and/or clean it up.
+ */
+static int
+dump_add_page(unsigned long page_index, unsigned long *toffset, int pass)
+{
+#if defined(CONFIG_X86) || defined(CONFIG_ALPHA)
+	extern int page_is_ram(unsigned long);
+#endif
+	unsigned long size;
+	dump_page_t dp;
+	void *vaddr;
+	u64 paddr = (((u64)page_index) << DUMP_PAGE_SHIFT);
+	struct page *p;
+
+#if !defined(CONFIG_DISCONTIGMEM) && !defined(CONFIG_IA64)
+	p = (struct page *) &(mem_map[page_index]);
+
+	/*
+	 * If the address is in highmem, map it before copying to
+	 * the dump buffer.  In essence, the dump buffer is the
+	 * bounce buffer.
+	 */
+#ifdef CONFIG_HIGHMEM
+	if (PageHighMem(p)) {
+		/*
+		 * Since this can be executed from IRQ context,
+		 * reentrance on the same CPU must be avoided:
+		 */
+		vaddr = kmap_atomic(p, KM_USER0);
+	}
+	else
+#endif
+#if defined(CONFIG_X86) || defined(CONFIG_ALPHA)
+	if (!page_is_ram(page_index)) {
+		return (1);
+	}
+	else
+#endif
+	/* low memory */
+	vaddr = page_address(p);
+
+#else /*  CONFIG_DISCONTIGMEM || CONFIG_IA64 */
+	vaddr = __va(paddr);
+	p = virt_to_page(vaddr);
+#endif
+	dp.dp_address = paddr;
+	dp.dp_flags = DUMP_DH_RAW;
+
+
+#if DUMP_DEBUG >= 6
+	/*
+	 * Helpfull when looking at hexdump of /dev/vdump.
+	 * 
+	 *                                               Dump Header         Swap Header
+	 */
+	dp.dp_byte_offset = dump_header.dh_num_bytes + DUMP_BUFFER_SIZE + DUMP_HEADER_OFFSET;
+	dp.dp_page_index = dump_header.dh_num_dump_pages;
+
+	switch(pass) {
+	    case 1:	break;
+	    case 2:	return(1);					/* Already Dumped */
+	    case 3:	return(1);					/* Already Dumped */
+	    case 4:	break;
+	}
+#else
+	/*
+ 	 * Selective dump:
+	 *  	Some systems, have huge memories, NUMA for example, where
+	 *	a dump of all of memory isn't likely to fit on a swap partition.
+	 *	This is a simple 1st attempt at ordering the dump so the most 
+	 *	important pages are dumped first.
+	 *
+	 *	pass1: Reserved Pages
+	 *	pass2: Page count > 0
+	 *	pass3: the rest of conventional memory
+	 *	pass4: non-conventional memory,
+	 */
+	switch (pass) {
+	case 1:
+		if (PageReserved(p) == 0) {
+			return(1); 				/* Skip on this pass */
+		}
+		break;
+
+	case 2: 
+		if( PageReserved(p) || page_count(p) == 0) {
+			return(1);				/* Skip on this pass */
+		}
+		break;
+
+	case 3:
+		if(  PageReserved(p) ||  page_count(p) > 0) {
+			return(1);
+		}
+		break;
+
+	case 4:
+		break;
+
+	}
+#endif
+
+#if !defined(CONFIG_DISCONTIGMEM) && !defined(CONFIG_IA64)
+	if( !kern_addr_valid(dp.dp_address) ) {
+		/* dump of I/O memory not supported yet */
+		printk(KERN_ALERT "dump_add_page: !kern_addr_valid(dp.dp_address: " PU64X "\n", 
+								   dp.dp_address);
+		return(1);
+	}
+#endif
+
+#if DUMP_DEBUG
+	if(dump_add_page_debug) {
+		printk(KERN_ALERT "dump_add_page(page:%lx, *toffset:%lx):\n",  
+				                 page,     *toffset);
+	}
+#endif
+
+	/*
+	 * Don't compress the page if any part of it overlaps
+	 * with the current task_struct or current stack.
+	 */
+	if ((((unsigned long)vaddr < (unsigned long)current + THREAD_SIZE)  &&
+		((unsigned long)vaddr + DUMP_PAGE_SIZE >
+		(unsigned long)current))) {
+			size = DUMP_PAGE_SIZE;
+	}
+	else {
+		size = dump_compress_func((char *)vaddr, DUMP_PAGE_SIZE,
+			(char *)dpcpage, DUMP_DPC_PAGE_SIZE);
+		if (!size) {
+			/* dump compression failed -- default to raw mode */
+			size = DUMP_PAGE_SIZE;
+		}
+		/* set the compressed flag if the page did compress */
+		if (size < DUMP_PAGE_SIZE) {
+			dp.dp_flags = DUMP_DH_COMPRESSED;
+		}
+	}
+
+	/* set the universal size */
+	dp.dp_size = size;
+
+	/* copy the page header */
+	memcpy((void *)(dump_page_buf + *toffset), (const void *)&dp,
+		sizeof(dump_page_t));
+
+	*toffset += sizeof(dump_page_t);
+
+	if (dp.dp_flags & DUMP_DH_COMPRESSED) {
+		/* copy the compressed page */
+		memcpy((void *)(dump_page_buf + *toffset),
+			(const void *)dpcpage, size);
+	} else {
+		/* copy directly from memory */
+		DUMP_memcpy((void *)(dump_page_buf + *toffset),
+			(const void *)vaddr, size);
+	}
+
+#if DUMP_DEBUG
+	/*
+	 * Write a Test Pattern:
+	 *	Page data is full of bytes with the page number.
+	 */	
+	if (dump_add_page_test_pattern) {
+		unsigned char num_pages = (char) (dump_header.dh_num_dump_pages & 0Xff);
+		char *cp = (dump_page_buf + *toffset);
+		int i;
+
+		for( i = 0; i < size; i++) {
+			*cp++ = num_pages;	
+		}
+	}
+#endif
+
+#ifdef CONFIG_HIGHMEM
+	if (PageHighMem(p)) {
+		/*
+		 * Since this can be executed from IRQ context,
+		 * reentrance on the same CPU must be avoided:
+		 */
+		kunmap_atomic(vaddr, KM_USER0);
+	}
+#endif
+	*toffset += size;
+
+#if DUMP_DEBUG >= 6
+	if (dump_add_page_debug) {
+		printk(KERN_ALERT "dump_add_page: *toffset:%lx += size:%lx:\n",  
+						  *toffset,       size);
+	}
+	dump_header.dh_num_bytes += (size + sizeof(dump_page_t));
+#endif
+
+	dump_header.dh_num_dump_pages++;
+
+	/* Notify the arch code for possible progress indicators. */
+	__dump_progress_add_page();
+
+	return (0);
+}
+
+
+/*
+ * Name: dump_silence_system()
+ * Func: Silence the system, or make CPUs spin, etc.  The intention
+ *       here is to get things to a "quiet" state, so we can dump.
+ */
+void
+dump_silence_system(void)
+{
+	unsigned int stage = 0;
+	int cpu = smp_processor_id();
+
+	if (in_interrupt()) {
+		printk(KERN_ALERT "Dumping from interrupt handler !\n");
+		printk(KERN_ALERT "Uncertain scenario - but will try my best\n");
+		/* 
+		 * Must be an unrelated interrupt, not in the middle of io ! 
+		 * If we've panic'ed in the middle of io we should take 
+		 * another approach 
+		 */
+	}
+	/* see if there's something to do before we re-enable interrupts */
+	(void)__dump_silence_system(stage);
+
+	/* we set this to FALSE so we don't ever re-enter this code! */
+	dump_okay = FALSE;
+	dumping_cpu = cpu;
+	dump_in_progress = TRUE;
+	save_flags(dump_save_flags);
+
+	/* -------------------------------------------------- */
+	/* Kludge - dump from interrupt context is unreliable (Fixme)
+	 *
+	 * We do this so that softirqs initiated for dump i/o 
+	 * get processed and we don't hang while waiting for i/o
+	 * to complete or in any irq synchronization attempt.
+	 *
+	 * This is not quite legal of course, as it has the side 
+	 * effect of making all interrupts & softirqs triggered 
+	 * while dump is in progress complete before currently 
+	 * pending softirqs and the currently executing interrupt 
+	 * code. 
+	 */
+	/*
+	 * We use irq_stat directly instead of using local_irq_count
+	 * and local_bh_count as they can't be used as Lvalues on
+	 * uniprocessor machines. 
+	 */
+#ifdef __powerpc64__
+	saved_irq_count = local_irq_count(cpu);
+	saved_bh_count =  local_bh_count(cpu);
+	local_irq_count(cpu) = 0;
+	local_bh_count(cpu) = 0;
+#else
+#if defined(irq_count) && defined(bh_count)
+	saved_irq_count = irq_count(cpu);
+	saved_bh_count =  bh_count(cpu);
+	irq_count(cpu) = 0;
+	bh_count(cpu) = 0;
+#else
+	saved_irq_count = irq_stat[cpu].__local_irq_count;
+	saved_bh_count = irq_stat[cpu].__local_bh_count;
+	irq_stat[cpu].__local_irq_count = 0;
+	irq_stat[cpu].__local_bh_count = 0;
+#endif
+#endif
+	/* -----------------------------------------------------*/
+
+	sti(); /* enable interrupts just in case ... */
+
+	/* now increment the stage and do stuff after interrupts are enabled */
+	stage++;
+	(void)__dump_silence_system(stage);
+
+	/* time to leave */
+	return;
+}
+
+/*
+ * Name: dump_resume_system()
+ * Func: Resume the system state.
+ */
+void
+dump_resume_system(void)
+{
+	unsigned int stage = 0;
+
+	/* let the architectures return their proper system state */
+	(void)__dump_resume_system(stage);
+
+	/* restore flags and other dump state fields */
+	restore_flags(dump_save_flags);
+
+#ifdef __powerpc64__
+	local_irq_count(dumping_cpu) =  saved_irq_count;
+	local_bh_count(dumping_cpu) = saved_bh_count;
+#else
+#if defined(irq_count) && defined(bh_count)
+	irq_count(dumping_cpu) =  saved_irq_count;
+	bh_count(dumping_cpu) = saved_bh_count;
+#else
+	irq_stat[dumping_cpu].__local_irq_count = saved_irq_count;
+	irq_stat[dumping_cpu].__local_bh_count = saved_bh_count;
+#endif
+#endif
+
+	dump_in_progress = FALSE;
+	dump_okay = TRUE;
+
+	/* reboot the system if this isn't a disrupted dump */
+	if ((panic_timeout > 0) && (!(dump_flags & DUMP_FLAGS_NONDISRUPT))) {
+		DUMP_PRINTF("Dump: Rebooting in %d seconds ...", panic_timeout);
+		mdelay(panic_timeout * 1000);
+		machine_restart(NULL);
+	}
+
+	return;
+}
+
+static void
+dump_speedo(void)
+{
+	static int i = 0;
+
+	switch (++i%4) {
+	case 0:
+		printk("|\b");
+		break;
+	case 1:
+		printk("\\\b");
+		break;
+	case 2:
+		printk("-\b");
+		break;
+	case 3:
+		printk("/\b");
+		break;
+	}
+}
+
+/*
+ * -----------------------------------------------------------------------
+ *                     H E A D E R   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_configure_header()
+ * Func: Update the header with the appropriate information.
+ */
+static int
+dump_configure_header(char *panic_str, struct pt_regs *regs)
+{
+	/* make sure the dump header isn't TOO big */
+	if ((sizeof(dump_header_t) +
+		sizeof(dump_header_asm_t)) > DUMP_BUFFER_SIZE) {
+			DUMP_PRINTN("dump_configure_header(): combined "
+				"headers larger than DUMP_BUFFER_SIZE!");
+			return (0);
+	}
+
+	/* configure dump header values */
+	dump_header.dh_magic_number = DUMP_MAGIC_NUMBER;
+	dump_header.dh_version = DUMP_VERSION_NUMBER;
+	dump_header.dh_memory_start = PAGE_OFFSET;
+	dump_header.dh_memory_end = DUMP_MAGIC_NUMBER;
+	dump_header.dh_header_size = sizeof(dump_header_t);
+	dump_header.dh_dump_page_size = DUMP_PAGE_SIZE;
+	dump_header.dh_dump_level = dump_level;
+	dump_header.dh_current_task = current;
+	dump_header.dh_dump_compress = dump_compress;
+	dump_header.dh_dump_flags = dump_flags;
+	dump_header.dh_dump_device = dump_device;
+
+#if DUMP_DEBUG >= 6
+	dump_header.dh_num_bytes = 0;
+#endif
+	dump_header.dh_num_dump_pages = 0;
+	do_gettimeofday(&dump_header.dh_time);
+
+#ifndef UTSNAME_ENTRY_SZ
+#define UTSNAME_ENTRY_SZ 65
+#endif
+	memcpy((void *)&(dump_header.dh_utsname_sysname), 
+		(const void *)&(system_utsname.sysname), UTSNAME_ENTRY_SZ);
+	memcpy((void *)&(dump_header.dh_utsname_nodename), 
+		(const void *)&(system_utsname.nodename), UTSNAME_ENTRY_SZ);
+	memcpy((void *)&(dump_header.dh_utsname_release), 
+		(const void *)&(system_utsname.release), UTSNAME_ENTRY_SZ);
+	memcpy((void *)&(dump_header.dh_utsname_version), 
+		(const void *)&(system_utsname.version), UTSNAME_ENTRY_SZ);
+	memcpy((void *)&(dump_header.dh_utsname_machine), 
+		(const void *)&(system_utsname.machine), UTSNAME_ENTRY_SZ);
+	memcpy((void *)&(dump_header.dh_utsname_domainname), 
+		(const void *)&(system_utsname.domainname), UTSNAME_ENTRY_SZ);
+
+	if (panic_str) {
+		memcpy((void *)&(dump_header.dh_panic_string),
+			(const void *)panic_str, DUMP_PANIC_LEN);
+	}
+
+        dump_header_asm.dha_magic_number = DUMP_ASM_MAGIC_NUMBER;
+        dump_header_asm.dha_version = DUMP_ASM_VERSION_NUMBER;
+        dump_header_asm.dha_header_size = sizeof(dump_header_asm_t);
+
+	/* copy the registers if they are valid */
+	if (regs) {
+		memcpy((void *)&(dump_header_asm.dha_regs),
+			(const void *)regs, sizeof(struct pt_regs));
+	}
+
+	/* configure architecture-specific dump header values */
+	if (!__dump_configure_header(regs)) {
+		return (0);
+	}
+	return (1);
+}
+
+/*
+ * Name: dump_write_header()
+ * Func: Write out the dump header.
+ *
+ * Returns:
+ *	-1: failed
+ *	 0: wrote o bytes (truncated)
+ *	+1: wrote header
+ */
+static int
+dump_write_header(void)
+{
+	int state;
+	loff_t toffset;
+
+	/* clear the dump page buffer */
+	memset(dump_page_buf, 0, DUMP_BUFFER_SIZE);
+
+	/* copy the dump header directly into the dump page buffer */
+	memcpy(dump_page_buf, (const void *)&dump_header,
+		sizeof(dump_header_t));
+
+	memcpy((void *)(dump_page_buf + sizeof(dump_header_t)),
+		(const void *)&dump_header_asm, sizeof(dump_header_asm_t));
+
+	/* save our file pointer */
+	toffset = dump_fpos;
+
+	/* 
+	 * ALWAYS write out the dump header at DUMP_HEADER_OFFSET, 
+	 * this is after the longest swap header possibly written by mkswap;
+	 * likely the largest PAGE_SIZE supported by the archecture.
+	 */
+	dump_fpos = DUMP_HEADER_OFFSET;
+
+	/* do the real write here */
+	state = dump_kernel_write(NULL);
+	dump_fpos = toffset;
+	if (state < 0) {
+		DUMP_PRINTF("dump_kernel_write() failed!\n");
+		return (-1);
+	} else if (!state) {
+		/* wrote zero bytes - failed */
+		return (0);
+	}
+	/* Wrote the header - success */
+	return (1);
+}
+
+/*
+ * -----------------------------------------------------------------------
+ *                  E X E C U T E   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_execute_memdump()
+ * Func: Perform the actual memory dump.  This walks through the
+ *       memory pages and dumps the data to disk (using other functions).
+ *
+ * Returns:
+ *	-1: failed
+ *	 0: dump truncated	
+ *	 1: success
+ */
+static int
+dump_execute_memdump(void)
+{
+	int counter = 0, n_bytes = 0, i;
+	unsigned long buf_loc;
+	u64 mem_loc;
+	int dump_truncated = 0;
+	int pass = 0;
+
+	DUMP_PRINTN("Compression value is 0x%x, Writing dump header ", dump_compress);
+
+	/* update the header to disk the first time */
+	n_bytes = dump_write_header();
+	DUMP_PRINT("\n");
+
+	if (n_bytes < 0) {
+		DUMP_PRINTF("Initial dump header update failed!\n");
+		return (-1);
+	} else if (n_bytes == 0) {
+		return (0);
+	}
+
+	/* if we only want the header, return */
+	if (dump_level & DUMP_LEVEL_HEADER) {
+		return (1);
+	}
+
+	/* set beginning offset to keep compats right with swap devices */
+	dump_fpos = DUMP_HEADER_OFFSET + DUMP_BUFFER_SIZE;
+
+	/* clear the dump page buffer */
+	memset(dump_page_buf, 0, DUMP_BUFFER_SIZE);
+
+	/* set the buffer location */
+	buf_loc = 0;
+
+	for (pass = 1; pass <= 4; pass++) {
+
+		if( (pass == 2) && dump_unreserved_mem == 0) {
+			 continue;
+		}
+		if( (pass == 3) && dump_unreferenced_mem == 0) {
+			 continue;
+		}
+		if (pass == 4) { 
+			if (dump_nonconventional_mem == 0) break;
+			if (dump_mbanks == 1) break;				/* Contigious Mem Sys have 1 mbank */
+		} 
+		/* Algorithm suggested by Jack Steiner <steiner@sgi.com */
+		switch(pass) {
+			case 1:	DUMP_PRINTN("Pass 1: Saving Reserved Pages: "); 				break;
+			case 2:	DUMP_PRINTN("Pass 2: Saving Remaining Referenced Pages: "); 			break;
+			case 3:	DUMP_PRINTN("Pass 3: Saving Remaining Unreferenced Pages: ");			break;
+			case 4:	DUMP_PRINTN("Pass 4: Saving Unconventional Memory: ");				break;
+		}
+		for (i = 0; i < dump_mbanks; i++) {
+		  u64 mem_bank_start = dump_mbank[ i ].start;
+		  u64 mem_bank_end = dump_mbank[ i ].end;
+		  int type = dump_mbank[ i ].type;
+
+		  if ( (type != DUMP_MBANK_TYPE_CONVENTIONAL_MEMORY) && (pass != 4) ) continue;
+		  if ( (type == DUMP_MBANK_TYPE_CONVENTIONAL_MEMORY) && (pass == 4) ) continue;
+	
+		  DUMP_PRINTN("Memory Bank[%d]: " PU64X " ... " PU64X ": ", i, mem_bank_start, mem_bank_end);
+		  for (mem_loc = mem_bank_start; mem_loc < mem_bank_end; mem_loc += DUMP_PAGE_SIZE) {
+	
+			/* add the page (if it's real RAM) */
+			if (dump_add_page((mem_loc >> DUMP_PAGE_SHIFT), 
+						&buf_loc, pass)) {
+				/* didn't add a page to buffer */
+				continue;
+			}
+	
+			/* see if we've filled the buffer */
+			if (buf_loc >= DUMP_BUFFER_SIZE) {
+				int near_eof = 0;
+	
+				/* write out the dump page buffer */
+				n_bytes = dump_kernel_write(&near_eof);
+				if (n_bytes < 0) {
+					DUMP_PRINTN("Write of dump pages failed!");
+					return(-1);
+				} else if (n_bytes == 0) {
+					DUMP_PRINTN("EOF on Write of dump pages; dump terminated!");
+					return(0);
+				}
+				if( near_eof ) {
+					DUMP_PRINTN("Near EOF on Write of dump pages; truncating dump,");
+					dump_truncated = 1;
+				}
+	
+				/* bump the counter for writing out the header */
+				counter++;
+	
+				/*
+				 * Update the header every once in a while -- this
+				 * _must_ be done before we write the overflow end of
+				 * the dump_page_buf into the top of dump_page_buf,
+				 * as dump_write_header() uses dump_page_buf to
+				 * write the default/asm dump headers.  After we are
+				 * done updating the header, _then_ we can move the
+				 * leftover dump data into the top of dump_page_buf.
+				 *
+				 * NOTE: a period is printed to indicate that the header
+				 *	 was updated. Speedo shows page outactivity.
+				 */
+				if ((counter & 0x3f) == 0) {
+					n_bytes = dump_write_header();
+					if (n_bytes < 0) {
+						DUMP_PRINTN("Dump header update failed!");
+						return (-1);
+					} else if (n_bytes == 0) {
+						DUMP_PRINTN("Dump header update failed; bizzare");
+						return (0);
+					}
+					DUMP_PRINT(".");
+				} else {
+					if ((counter & 0x07) == 0) {
+						dump_speedo();
+					}
+				}
+	
+				/* clear the dump page buffer */
+				memset(dump_page_buf, 0, DUMP_BUFFER_SIZE);
+	
+				/* adjust leftover data back to the top of the page */
+				if (buf_loc > DUMP_BUFFER_SIZE) {
+	        			/* copy the dump page buffer remnants */
+	        			memcpy((void *)dump_page_buf,
+	                			(const void *)(dump_page_buf +
+							DUMP_BUFFER_SIZE),
+	                			buf_loc - DUMP_BUFFER_SIZE);
+	
+	        			/* set the new buffer location */
+	        			buf_loc -= DUMP_BUFFER_SIZE;
+				} else {
+					/* reset the buffer location counter */
+					buf_loc = 0;
+				}
+			}
+			if (dump_truncated) {
+				break;
+			}
+		    }
+		    DUMP_PRINT(" ");	
+		    if (dump_truncated) {
+			break;
+		    }
+		}
+		if (dump_truncated) {
+			break;
+		}
+		DUMP_PRINT("\n");
+	}
+
+	/* 
+	 * we have written out most of the dump pages, a few may still be in the
+	 * page buffer and it's possible we had to truncate the dump because we
+	 * got very close to the end of the dump partition. Now we add a EOF
+	 * marker to the page out buffer and flush out the remaining pages.
+	 *
+	 * We need to write the DUMP_DH_END even for truncated data because
+	 * the reader won't get an EOF if he reads the data in smaller chunks
+	 * than the DUMP_BUFFER_SIZE it's written in.
+	 */
+	if (dump_truncated) {
+		dump_add_end_marker(&buf_loc, (DUMP_DH_END | DUMP_DH_TRUNCATED));
+	} else {
+		dump_add_end_marker(&buf_loc, DUMP_DH_END);
+	}
+	n_bytes = dump_kernel_write(NULL);
+
+	if (n_bytes < 0) {
+                DUMP_PRINTN("Final write of last of page buffer and DUMP_DH_END failed!");
+                return (-1);
+        } else if (n_bytes == 0) {
+		DUMP_PRINTN("Hit EOF writing DUMP_DH_END; bad luck\n");
+                return (0);
+        }
+
+	/*
+	 * Writing out DUMP_DH_END may have pushed us past the end of the first
+	 * part of the page buffer, if it did we have to write out one last 
+	 * DUMP_BUFFER with the spill over page.
+	 */
+	if (buf_loc > DUMP_BUFFER_SIZE) {
+		/* clear the first part of the buffer */
+		memset(dump_page_buf, 0, DUMP_BUFFER_SIZE);
+
+		/*
+		 * Copy the dump page buffer remnants in the second 
+		 * part of the buffer to the first part.
+		 */
+		memcpy((void *)dump_page_buf,
+			(const void *)(dump_page_buf + DUMP_BUFFER_SIZE),
+			buf_loc - DUMP_BUFFER_SIZE);
+
+		n_bytes = dump_kernel_write(NULL);
+		if (n_bytes < 0) {
+			DUMP_PRINTN("Final write of spillover page failed!");
+			return (-1);
+		} else if (n_bytes == 0) {
+			DUMP_PRINTN("Hit EOF writing spill over page with DUMP_DH_END; very bad luck!\n");
+			return (0);
+		}
+	}
+	if (dump_truncated) {
+		return(0);
+	}
+	/* success */
+	return (1);
+}
+
+/*
+ * Name: dump_execute()
+ * Func: Execute the dumping process.  This makes sure all the appropriate
+ *       fields are updated correctly, and calls dump_execute_memdump(),
+ *       which does the real work.
+ * 
+ * if( dump_flags & DUMP_FLAGS_NONDISRUPT ) {
+ *    Returns:
+ *      -1: failed
+ *       0: dump truncated
+ *       1: success
+ * }
+ */
+int
+dump_execute(char *panic_str, struct pt_regs *regs)
+{
+	int state = 1;
+
+	/* make sure we can dump */
+	if (dump_okay == FALSE) {
+		return(-1);
+	}
+	printk(KERN_ALERT "dump_execute starting\n");
+
+	if(!dump_configure_header(panic_str, regs)) {
+		DUMP_PRINTN("dump header could not be configured!");
+		return(-1);
+	}
+
+	/* silence the system */
+	dump_silence_system();
+
+	/* tell interested parties that a dump is happening */
+	notifier_call_chain(&dump_notifier_list, DUMP_BEGIN, &dump_device);
+
+	/* bail out if we're not going to do any dumping */
+	if (dump_level != DUMP_LEVEL_NONE) {
+		/* inform users of what we are about to do */
+		DUMP_PRINTN("Dumping to device 0x%x [%s] on CPU %d ...",
+			dump_device, bdevname(dump_device),
+			smp_processor_id());
+
+		/* start walking through the page tables */
+		state = dump_execute_memdump();
+
+		DUMP_PRINT("\n");
+
+		/* update header to disk for the last time */
+		if (dump_write_header() < 0) {
+			DUMP_PRINTF("Final dump header update failed!\n");
+		}
+
+		if (state < 0) {
+			DUMP_PRINTF("Dump Failed!\n");
+		} else if (state == 0) {
+			DUMP_PRINTF("Dump Truncated (likely out of space).\n");
+		} else {
+			DUMP_PRINTF("Dump Complete; %d dump pages saved.\n", dump_header.dh_num_dump_pages);
+		}
+	}
+
+	/* tell interested parties that a dump has completed */
+	notifier_call_chain(&dump_notifier_list, DUMP_END, &dump_device);
+
+	/* put the system state back */
+	dump_resume_system();
+
+	return (state);
+}
+
+/*
+ * Name: dump_register_compression()
+ * Func: Register a dump compression mechanism.
+ */
+void
+dump_register_compression(dump_compress_t *item)
+{
+	/* let's make sure our list is valid */
+	if (!item) {
+		return;
+	}
+
+	/* add our item */
+	list_add(&(item->list), &dump_compress_list);
+
+	/* print information to callers */
+	DUMP_PRINTF("Registering dump compression type 0x%x\n", item->compress_type);
+}
+
+/*
+ * Name: dump_unregister_compression()
+ * Func: Remove a dump compression mechanism, and re-assign the dump
+ *       compression pointer if necessary.
+ */
+void
+dump_unregister_compression(int compression_type)
+{
+	struct list_head *tmp;
+	dump_compress_t *dc;
+
+	/* let's make sure our list is valid */
+	if (compression_type == DUMP_COMPRESS_NONE) {
+		DUMP_PRINTN("Compression list is invalid!");
+		return;
+	}
+
+	/* try to remove the compression item */
+	list_for_each(tmp, &dump_compress_list) {
+		dc = list_entry(tmp, dump_compress_t, list);
+		if (dc->compress_type == compression_type) {
+			list_del(&(dc->list));
+			DUMP_PRINTN("De-registering dump compression type 0x%x\n",
+				compression_type);
+			return;
+		}
+	}
+}
+
+/*
+ * Name: dump_compress_init()
+ * Func: Initialize (or re-initialize) compression scheme.
+ */
+static int
+dump_compress_init(int compression_type)
+{
+	struct list_head *tmp;
+	dump_compress_t *dc;
+
+	/* try to remove the compression item */
+	list_for_each(tmp, &dump_compress_list) {
+		dc = list_entry(tmp, dump_compress_t, list);
+		if (dc->compress_type == compression_type) {
+			dump_compress_func = dc->compress_func;
+			dump_compress = compression_type;
+
+			DUMP_PRINTF("dump_compress = %d\n", dump_compress);
+			DUMP_BP();
+			return (0);
+		}
+	}
+
+	/* 
+	 * nothing on the list -- return ENODATA to indicate an error 
+	 *
+	 * NB: 
+	 *	EAGAIN: reports "Resource temporarily unavailable" which
+	 *		isn't very enlightening.
+	 */
+	DUMP_PRINTF("compression_type:%d not found\n", compression_type);
+	DUMP_BP();
+
+	return (-ENODATA);
+}
+
+void dump_iobuf_end_io(struct kiobuf *iobuf)
+{
+	/* No wakeup needed since we've stopped scheduling */
+	return;
+}
+/*
+ * Name: dump_open_kdev()
+ * Func: Try to open the kdev_t argument as the real dump device.
+ *       This is where all the work is done for setting up the dump
+ *       device.  It's assumed at this point that by passing in the
+ *       dump device's major/minor number, we can open it up, check
+ *       it out, and use it for whatever purposes.
+ */
+static int
+dump_open_kdev(kdev_t tmp_dump_device)
+{
+	int i;
+	unsigned long a;
+	unsigned long dump_page_addr;
+
+	/* make sure this is a valid block device */
+	if (!tmp_dump_device) {
+		return (-EINVAL);
+	}
+
+	/* we'd better have a name here, or this isn't valid ... */
+	if (!get_blkfops(MAJOR(tmp_dump_device))) {
+		return (-ENODEV);
+	}
+
+	/* if this is the second call to this function, clean up ... */
+	if ((dump_okay == TRUE) && (dump_page_buf_0 != (void *)NULL)) {
+		kfree((const void *)dump_page_buf_0);
+	}
+
+	/* 
+	 * Allocate buffer to be used for copying pages (only once ...) 
+	 *
+	 * An extra:
+	 *	 2 * DUMP_PAGE_SIZE:
+	 * 		is needed for the page overflow of last page, it's page header
+	 * 		and the EOF page header.
+	 *
+	 * and an  extra:
+	 *	1 * PAGE_SIZE:
+	 *		is needed for rounding up the start of the dump_page_buf to
+	 *		a system page (PAGE_SIZE) boundry.
+	 * 
+	 */
+	dump_page_buf = dump_page_buf_0 = (void *)kmalloc(
+						DUMP_BUFFER_SIZE + (DUMP_PAGE_SIZE * 2) + PAGE_SIZE,
+						GFP_KERNEL);
+
+
+	if (dump_page_buf_0 == (void *)0) {
+		DUMP_PRINTF("Cannot kmalloc() dump page buffer!\n");
+		dump_okay = FALSE;
+		return (-ENOMEM);
+	}
+
+	/*
+	 * Allocate our kiobuf -- we used to do this in dump_execute(),
+	 * but since the memory allocation schemes and the kiobuf code
+	 * keeps changing out underneath us, we'll do it when we open
+	 * up the dump device.
+	 */
+	dump_iobuf = (struct kiobuf *)NULL;
+
+	/* allocate a new one for use */
+	if (alloc_kiovec(1, &dump_iobuf)) {
+		DUMP_PRINTF("alloc_kiovec() call failed!\n");
+		dump_okay = FALSE;
+		return (-ENOMEM);
+	}
+
+	/* align the dump page addresses */
+	dump_page_addr = (unsigned long) dump_page_buf;
+	if (dump_page_addr % PAGE_SIZE) {
+		dump_page_buf = (void *) PAGE_ALIGN(dump_page_addr);
+	}
+
+	a = (unsigned long) dump_page_buf;
+	/* get the base address, and copy the number of total pages */
+	for (i = 0; i < (DUMP_BUFFER_SIZE >> PAGE_SHIFT); i++, a += PAGE_SIZE) {
+		dump_iobuf->maplist[i] = (struct page *)virt_to_page(a);
+	}
+	dump_iobuf->locked = 1;
+	dump_iobuf->offset = 0;
+	dump_iobuf->length = DUMP_BUFFER_SIZE;
+	dump_iobuf->nr_pages = (DUMP_BUFFER_SIZE >> PAGE_SHIFT);
+	dump_iobuf->end_io = dump_iobuf_end_io;
+
+	/* assign the new dump file structure */
+	dump_device = tmp_dump_device;
+	memcpy(dumpdev_name, bdevname(dump_device), PATH_MAX);
+
+	/* set the sector size information */
+	dump_blk_size = PAGE_SIZE;
+	set_blocksize(dump_device, dump_blk_size);
+
+	/* set the sector bits */
+	i = dump_blk_size;
+	for (dump_blk_shift = 0; !(i & 1);) {
+		i >>= 1, dump_blk_shift++;
+	}
+	 DUMP_PRINTF("dump_blk_shift:%d,  PAGE_SHIFT:%d\n", 
+		      dump_blk_shift,     PAGE_SHIFT);
+
+	/* set the dump okay flag to 1, the dump device is valid */
+	dump_okay = TRUE;
+
+	DUMP_PRINTN("dump device 0x%x opened; Ready to take a save a core dump\n", dump_device);
+
+	/* after opening the block device, return */
+	return (0);
+}
+
+/*
+ * Name: dump_release()
+ * Func: Release the dump device -- it's never necessary to call
+ *       this function, but it's here regardless.
+ */
+static int
+dump_release(struct inode *i, struct file *f)
+{
+	return (0);
+}
+
+/*
+ * Name: dump_ioctl()
+ * Func: Allow all dump tunables through a standard ioctl() mechanism.
+ *       This is far better than before, where we'd go through /proc,
+ *       because now this will work for multiple OS and architectures.
+ */
+static int
+dump_ioctl(struct inode *i, struct file *f,
+	unsigned int cmd, unsigned long arg)
+{
+	/* check capabilities */
+	if (!capable(CAP_SYS_ADMIN)) {
+		return (-EPERM);
+	}
+
+	/*
+	 * This is the main mechanism for controlling get/set data
+	 * for various dump device parameters.  The real trick here
+	 * is setting the dump device (DIOSDUMPDEV).  That's what
+	 * triggers everything else.
+	 */
+	switch (cmd) {
+		/* set dump_device */
+		case DIOSDUMPDEV:
+			/* check flags */
+			if (!(f->f_flags & O_RDWR)) {
+				return (-EPERM);
+			}
+			__dump_open();
+			return (dump_open_kdev((kdev_t)arg));
+
+		/* get dump_device */
+		case DIOGDUMPDEV:
+			return (put_user((long)dump_device, (long *)arg));
+
+		/* set dump_level */
+		case DIOSDUMPLEVEL:
+			/* check flags */
+			if (!(f->f_flags & O_RDWR)) {
+				return (-EPERM);
+			}
+			/* make sure we have a positive value */
+			if (arg < 0) {
+				return (-EINVAL);
+			}
+			dump_level = (int)arg;
+
+			/*
+			 * REMIND: Still in development:
+			 *
+			 *	We will consider reserved pages a initial proxy for kernel pages.
+			 */
+			if (dump_level > DUMP_LEVEL_KERN) {
+				dump_unreserved_mem = 1;
+			} else {
+				dump_unreserved_mem = 0;
+			}
+
+			/*
+			 * REMIND: Still in development:
+			 *
+			 * 	Using refcount > 1 as a proxy for kernel & user pages.
+			 */
+			if (dump_level > DUMP_LEVEL_USED) {
+				dump_unreferenced_mem = 1;
+			} else {
+				dump_unreferenced_mem = 0;
+			}
+
+			/*
+			 *  REMIND: Still in development:
+			 *
+			 *	This may not be 100% stable on all ia64
+			 *	memory banks. Accessing uncached memory
+			 *	with cached accesses can cause subsequent
+			 *	bus errors when the cache is flushed.
+			 */
+			if( dump_level > DUMP_LEVEL_ALL_RAM ) {
+				dump_nonconventional_mem = 1;
+			} else {
+				dump_nonconventional_mem = 0;
+			}
+			break;
+
+		/* get dump_level */
+		case DIOGDUMPLEVEL:
+			return (put_user((long)dump_level, (long *)arg));
+
+		/* set dump_flags */
+		case DIOSDUMPFLAGS:
+			/* check flags */
+			if (!(f->f_flags & O_RDWR)) {
+				return (-EPERM);
+			}
+			/* make sure we have a positive value */
+			if (arg < 0) {
+				return (-EINVAL);
+			}
+			dump_flags = (int)arg;
+			break;
+
+		/* get dump_flags */
+		case DIOGDUMPFLAGS:
+			return (put_user((long)dump_flags, (long *)arg));
+
+		/* set the dump_compress status */
+		case DIOSDUMPCOMPRESS:
+			/* check flags */
+			if (!(f->f_flags & O_RDWR)) {
+				return (-EPERM);
+			}
+			return (dump_compress_init((int)arg));
+
+		/* get the dump_compress status */
+		case DIOGDUMPCOMPRESS:
+			return (put_user((long)dump_compress, (long *)arg));
+
+#if DUMP_DEBUG
+		case DIODUMPTEST:
+		/*
+		 * The lkcd_config cmd has a -t option available for testing dump
+		 * when DUMP_DEBUG is defined in dump.h. The -t option is followed
+		 * by a string that is parsed here.
+		 *
+		 * A string compare is used instead of more DUMP constants to make 
+		 * it easy to add new options quickley while debugging the dump driver.
+		 */
+		if (dump_okay) {
+			int saved_dump_flags = dump_flags;
+			char *bogus_pointer = 0;
+			char bogus_data;
+			char buffer[128];
+			int dumps_wanted = 0;
+			int stop_wanted = 0;
+			int tlb_flush_wanted = 0;
+			char *dump_test = "test";
+			int success;
+	
+			unlock_kernel();
+			if( ((caddr_t) arg) == NULL ) {
+				DUMP_PRINTF("arg == NULL; default test is to do nothing");
+				return (0);
+			} else {
+				/*
+				 * These functions are suppose to be dynamic and configurable
+				 * for your current needs; no sweet if you check in whats
+				 * currently usefull for you. Perhaps it should be checked in empty
+				 * while no new platforms are being worked on.
+				 */
+				if (strncpy_from_user(&buffer[0], (char *)arg, sizeof(buffer) - 1) < 0) {
+					DUMP_PRINTF("strncpy_from_user() failed\n");
+		                        return( -EFAULT );
+		                }
+				if (strncmp(&buffer[0], "panic", sizeof(buffer)) == 0) {
+					dump_flags |= DUMP_FLAGS_NONDISRUPT;
+					panic("test panic and dump");
+					/* NOTREACHED */
+				}
+				if (strncmp(&buffer[0], "dump", sizeof(buffer)) == 0) {		/* dump */
+					dumps_wanted = 1;
+					dump_test = "test dump";
+				}
+#ifdef CONFIG_SMP
+				if (strncmp(&buffer[0], "stop", sizeof(buffer)) == 0) {
+					dumps_wanted = 1;
+					stop_wanted = 1;
+					dump_test = "test dump with cpu's stopped";
+				}
+				if (strncmp(&buffer[0], "flush", sizeof(buffer)) == 0) {	/* OK */
+					dumps_wanted = 1;
+					tlb_flush_wanted = 1;
+					dump_test = "test dump with cpu TLB's flushed";
+				}
+#endif
+				if (strncmp(&buffer[0], "trap", sizeof(buffer)) == 0) {		/* OK */
+					*(int *)0 = 0;
+					DUMP_PRINTF("*(int *)0 = 0; should have cause a trap and died");
+					bogus_data =  *bogus_pointer;
+					DUMP_PRINTF("*bogus_pointer; should have taken a trap and died");
+					BUG();
+					return( -EFAULT );
+				}
+				if (strncmp(&buffer[0], "forever", sizeof(buffer)) == 0) {	/* OK */
+					dumps_wanted = 0XEFFF;
+					dump_test = "test dump forever";
+				}
+			}
+			if (dumps_wanted) {
+				int dumps = 0;
+
+				dump_flags |= DUMP_FLAGS_NONDISRUPT;
+				/*
+				 * REMIND: How to break loop on intr/<ctrl-Z> ?
+				 */
+				do {
+
+#if defined(CONFIG_SMP) && defined(CONFIG_IA64_SGI_SN1)
+					if( stop_wanted ) {
+						smp_send_stop();
+					}
+					if( tlb_flush_wanted ) {
+						extern void smp_send_flush_tlb(void);
+
+						smp_send_flush_tlb();
+					}
+#endif
+					success = dump_execute(dump_test, NULL);
+					dumps++;
+					DUMP_PRINTF("Test '%s': success:%d, dumps:%d\n", dump_test, success, dumps);
+				} while (success && (dumps < dumps_wanted));
+
+				DUMP_PRINTF("success:%d\n", success);
+				dump_flags = saved_dump_flags;
+
+				if( dumps >= dumps_wanted ) {
+					return ( 0 );
+				} else {
+					return ( -EINTR );
+				}
+			}
+			DUMP_PRINTF("Test '%s' not known.\n", &buffer[0]);
+			return( -ENOSYS );
+		} else {
+			/* doing a panic when dump_okay == 0 is almost always a waste of time */
+			DUMP_PRINTF("dump_okay == 0 (Not ready to dump memory).\n");
+		}
+		break;
+#endif /* DUMP_DEBUG */
+	}
+	return (0);
+}
+
+/*
+ * Name: dump_open()
+ * Func: Open the dump device for use when the system crashes.
+ */
+static int
+dump_open(struct inode *i, struct file *f)
+{
+	/* opening a device is straightforward -- nothing to do here */
+	return (0);
+}
+
+/*
+ * -----------------------------------------------------------------------
+ *                     I N I T   F U N C T I O N S
+ * -----------------------------------------------------------------------
+ */
+
+/*
+ * Name: dump_init_proc_entry()
+ * Func: Create a dump proc entry based on the /proc dump root.
+ *       Returns 1 on failure, 0 on success.
+ */
+int
+dump_init_proc_entry(char *name, struct proc_dir_entry *dirent)
+{
+	if (!(dirent = create_proc_entry(name,
+		S_IFREG|S_IRUGO|S_IWUSR, dump_root))) {
+			DUMP_PRINT("unable to initialize "
+				"/proc/%s/%s!\n", DUMP_ROOT_NAME, name);
+		return (1);
+	}
+	dirent->data = (void *)dirent;
+	dirent->read_proc = &dump_read_proc;
+	dirent->write_proc = NULL;
+	dirent->size = 16;
+	return (0);
+}
+
+/*
+ * Name: dump_proc_init()
+ * Func: Initialize the /proc interfaces for dumping.
+ * 
+ * Typically:
+ *	/proc/sys/dump/
+ *		dump_compress  dump_device  dump_flags  dump_level
+ */
+int
+dump_proc_init(void)
+{
+	/* create the proc entries for the various tunables */
+	dump_root = create_proc_entry(DUMP_ROOT_NAME, S_IFDIR, 0);
+	if (dump_root) {
+		dump_root->owner = THIS_MODULE;
+	} else {
+		DUMP_PRINTF("unable to initialize /proc/%s!\n", DUMP_ROOT_NAME);
+		return (-EBUSY);
+	}
+
+	if (dump_init_proc_entry(DUMP_DEVICE_NAME, dump_dd)) {
+		remove_proc_entry(DUMP_ROOT_NAME, 0);
+		return (-EBUSY);
+	}
+	if (dump_init_proc_entry(DUMP_LEVEL_NAME, dump_l)) {
+		remove_proc_entry(DUMP_ROOT_NAME, 0);
+		remove_proc_entry(DUMP_DEVICE_NAME, dump_root);
+		return (-EBUSY);
+	}
+	if (dump_init_proc_entry(DUMP_FLAGS_NAME, dump_f)) {
+		remove_proc_entry(DUMP_ROOT_NAME, 0);
+		remove_proc_entry(DUMP_DEVICE_NAME, dump_root);
+		remove_proc_entry(DUMP_LEVEL_NAME, dump_root);
+		return (-EBUSY);
+	}
+	if (dump_init_proc_entry(DUMP_COMPRESS_NAME, dump_cp)) {
+		remove_proc_entry(DUMP_ROOT_NAME, 0);
+		remove_proc_entry(DUMP_DEVICE_NAME, dump_root);
+		remove_proc_entry(DUMP_LEVEL_NAME, dump_root);
+		remove_proc_entry(DUMP_FLAGS_NAME, dump_root);
+		return (-EBUSY);
+	}
+	return (0);
+}
+
+/*
+ * Name: dump_proc_cleanup()
+ * Func: Cleanup the /proc interfaces for dumping.
+ */
+void
+dump_proc_cleanup(void)
+{
+	/* remove the proc entries */
+	remove_proc_entry(DUMP_DEVICE_NAME, dump_root);
+	remove_proc_entry(DUMP_LEVEL_NAME, dump_root);
+	remove_proc_entry(DUMP_FLAGS_NAME, dump_root);
+	remove_proc_entry(DUMP_COMPRESS_NAME, dump_root);
+	remove_proc_entry(DUMP_ROOT_NAME, 0);
+	return;
+}
+
+static void sysrq_handle_crashdump(int key, struct pt_regs *pt_regs,
+		struct kbd_struct *kbd, struct tty_struct *tty) {
+	printk(KERN_ALERT "sysrq_handle_crashdump\n");
+	dump("sysrq", pt_regs);
+}
+static struct sysrq_key_op sysrq_crashdump_op = {
+	handler:	sysrq_handle_crashdump,
+	help_msg:	"Crash",
+	action_msg:	"Start a Crash Dump (If Configured)",
+};
+
+static void sysrq_handle_dumpregs(int key, struct pt_regs *pt_regs,
+		struct kbd_struct *kbd, struct tty_struct *tty) {
+#if defined(CONFIG_X86) && defined(CONFIG_SMP)
+	extern void (*dump_trace_ptr)(struct pt_regs *);
+	printk("Show state of all cpus\n");
+	if (dump_trace_ptr) {
+		dump_trace_ptr(pt_regs);
+	} else {
+		printk("Load dump module/configure first\n");
+	}
+#endif
+}
+static struct sysrq_key_op sysrq_dumpregs_op = {
+	handler:	sysrq_handle_dumpregs,
+	help_msg:	"Dumpregisters",
+	action_msg:	"Dump CPU Registers (If Configured)"
+};
+
+
+/*
+ * Name: dump_init()
+ * Func: Initialize the dump process.  This will set up any architecture
+ *       dependent code.  The big key is we need the memory offsets before
+ *       the page table is initialized, because the base memory offset
+ *       is changed after paging_init() is called.
+ */
+int
+dump_init(void)
+{
+	struct sysinfo info;
+	int i; 
+
+	/* try to initialize /proc interfaces */
+	if (dump_proc_init() < 0) {
+		DUMP_PRINTF("dump_proc_init failed!; dump not initialized\n");
+		return (-EBUSY);
+	}
+
+	/* try to create our dump device */
+	if (devfs_register_chrdev(DUMP_MAJOR, "dump", &dump_fops)) {
+		DUMP_PRINTF("cannot register dump character device!\n");
+		return (-EBUSY);
+	}
+
+	/* initialize the dump headers to zero */
+	memset(&dump_header, 0, sizeof(dump_header));
+	memset(&dump_header_asm, 0, sizeof(dump_header_asm));
+
+#if    !defined(CONFIG_DISCONTIGMEM) && !defined(CONFIG_IA64)
+	/* 
+	 * CONFIG_DISCONTIGMEM and CONFIG_IA64 systems are responsible 
+	 * for initializing dump_mbank[] in __dump_init().
+	 */
+	dump_mbanks = 1;
+	dump_mbank[ 0 ].start = 0;
+	dump_mbank[ 0 ].end  = (((u64) max_mapnr) << PAGE_SHIFT) - 1;
+	dump_mbank[ 0 ].type = DUMP_MBANK_TYPE_CONVENTIONAL_MEMORY;
+#endif
+
+	/* 
+	 * initialize the dump device at the arch level.
+ 	 *
+	 * if defined(CONFIG_DISCONTIGMEM) {
+	 *       __dump_init() must set up dump_mbank[].
+	 * }
+	 */
+	__dump_init((u64)PAGE_OFFSET);
+
+	/* initialize the dump page buffer */
+	dump_page_buf = (void *)0;
+
+	/* set the dump function pointer for dump execution */
+	dump_function_ptr = dump_execute;
+
+	/* set the dump_compression_list structure up */
+	dump_compress = DUMP_COMPRESS_NONE;
+	dump_compress_func = dump_compress_none;
+	dump_register_compression(&dump_none_compression);
+
+	/* initialize the dump flags, dump level and dump_compress fields */
+	dump_flags = DUMP_FLAGS_NONE;
+	dump_level = DUMP_LEVEL_ALL;
+
+	/* grab the total memory size now (not if/when we crash) */
+	si_meminfo(&info);
+
+	/* set the memory size */
+	dump_header.dh_memory_size = (u64)info.totalram;
+
+	for (i = 0; i < dump_mbanks; i++) {
+		DUMP_PRINTF("mbank[%d]: type:%d, phys_addr: " PU64X " ... " PU64X "\n",
+			i,
+			dump_mbank[i].type,
+			dump_mbank[i].start,
+			dump_mbank[i].end);
+
+		if (dump_mbank[i].start % DUMP_PAGE_SIZE) {
+			DUMP_PRINTF("oops, start is not DUMP_PAGE_SIZE:%x aligned!\n", (int) DUMP_PAGE_SIZE);
+		}
+		if ((dump_mbank[i].end + 1) % DUMP_PAGE_SIZE) {
+			DUMP_PRINTF("oops, end is not DUMP_PAGE_SIZE:%x aligned!\n", (int) DUMP_PAGE_SIZE);
+		}
+	}
+	__sysrq_put_key_op('c', &sysrq_crashdump_op);
+	__sysrq_put_key_op('d', &sysrq_dumpregs_op);
+
+	DUMP_PRINTF("Crash dump driver initialized.\n");
+	return (0);
+}
+
+void
+dump_cleanup(void)
+{
+	/* get rid of the allocated dump page buffer */
+	if (dump_page_buf_0) {
+		kfree((const void *)dump_page_buf_0);
+	}
+
+	/* arch-specific cleanup routine */
+	__dump_cleanup();
+
+	/* remove the proc entries */
+	dump_proc_cleanup();
+
+	/* try to create our dump device */
+	if (devfs_unregister_chrdev(DUMP_MAJOR, "dump")) {
+		DUMP_PRINT("cannot unregister dump character device!\n");
+	}
+
+	/* reset the dump function pointer */
+	dump_function_ptr = NULL;
+
+	return;
+}
+
+EXPORT_SYMBOL(dump_register_compression);
+EXPORT_SYMBOL(dump_unregister_compression);
+
+#ifdef MODULE
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,4,16))
+#if !defined(MODULE_LICENSE)
+#define MODULE_LICENSE(str)
+#endif
+#endif
+
+MODULE_AUTHOR("Matt D. Robinson <yakker@sourceforge.net>");
+MODULE_DESCRIPTION("Linux Kernel Crash Dump (LKCD) driver");
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,16))
+MODULE_LICENSE("GPL");
+#endif
+module_init(dump_init);
+module_exit(dump_cleanup);
+#endif /* MODULE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_gzip.c linuxppc64_2_4/drivers/dump/dump_gzip.c
--- linux-2.4.19/drivers/dump/dump_gzip.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_gzip.c	Mon Aug 19 09:15:52 2002
@@ -0,0 +1,137 @@
+/*
+ * GZIP Compression functions for kernel crash dumps.
+ *
+ * Created by: Matt Robinson (yakker@sourceforge.net)
+ * Copyright 2001 Matt D. Robinson.  All rights reserved.
+ *
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/* header files */
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/dump.h>
+#include "dump_zlib.c"
+
+/*
+ * -----------------------------------------------------------------------
+ *                       D E F I N I T I O N S
+ * -----------------------------------------------------------------------
+ */
+#define DUMP_MODULE_NAME "dump_gzip"
+#define DUMP_PRINTN(format, args...) \
+	printk("%s: " format , DUMP_MODULE_NAME , ## args);
+#define DUMP_PRINT(format, args...) \
+	printk(format , ## args);
+
+/*
+ * Name: dump_compress_gzip_alloc()
+ * Func: The kmalloc function for gzip dump compression.
+ */
+static void *
+dump_compress_gzip_alloc(void *arg, unsigned int items, unsigned int size)
+{
+	return ((void *)kmalloc((size * items), GFP_ATOMIC));
+}
+
+/*
+ * Name: dump_compress_gzip_free()
+ * Func: The kfree function for gzip dump compression.
+ */
+static void
+dump_compress_gzip_free(void *arg, void *ptr)
+{
+	kfree(ptr);
+	return;
+}
+
+/*
+ * Name: dump_compress_gzip()
+ * Func: Compress a DUMP_PAGE_SIZE page using gzip-style algorithms (the.
+ *       deflate functions similar to what's used in PPP).
+ */
+static int
+dump_compress_gzip(char *old, int oldsize, char *new, int newsize)
+{
+	/* error code and dump stream */
+	int err;
+	z_stream dump_stream;
+
+	/* setup the alloc/free functions */
+	dump_stream.zalloc = dump_compress_gzip_alloc;
+	dump_stream.zfree = dump_compress_gzip_free;
+	dump_stream.opaque = (void *)0;
+
+	if ((err = deflateInit(&dump_stream, Z_BEST_COMPRESSION)) != Z_OK) {
+		/* fall back to RLE compression */
+		DUMP_PRINT("dump_compress_gzip(): deflateInit() "
+			"failed (%d)!\n", err);
+		return (0);
+	}
+
+	/* use old (page of memory) and size (DUMP_PAGE_SIZE) as in-streams */
+	dump_stream.next_in = old;
+	dump_stream.avail_in = oldsize;
+
+	/* out streams are new (dpcpage) and new size (DUMP_DPC_PAGE_SIZE) */
+	dump_stream.next_out = new;
+	dump_stream.avail_out = newsize;
+
+	/* deflate the page -- check for error */
+	err = deflate(&dump_stream, Z_FINISH);
+	if (err != Z_STREAM_END) {
+		/* zero is return code here */
+		(void)deflateEnd(&dump_stream);
+		DUMP_PRINT("dump_compress_gzip(): deflate() failed (%d)!\n",
+			err);
+		return (0);
+	}
+
+	/* let's end the deflated compression stream */
+	if ((err = deflateEnd(&dump_stream)) != Z_OK) {
+		DUMP_PRINT("dump_compress_gzip(): deflateEnd() "
+			"failed (%d)!\n", err);
+	}
+
+	/* return the compressed byte total (if it's smaller) */
+	if (dump_stream.total_out >= oldsize) {
+		return (oldsize);
+	}
+	return (dump_stream.total_out);
+}
+
+/* setup the gzip compression functionality */
+static dump_compress_t dump_gzip_compression = {
+	compress_type:	DUMP_COMPRESS_GZIP,
+	compress_func:	dump_compress_gzip,
+};
+
+/*
+ * Name: dump_compress_gzip_init()
+ * Func: Initialize gzip as a compression mechanism.
+ */
+int __init
+dump_compress_gzip_init(void)
+{
+	dump_register_compression(&dump_gzip_compression);
+	return (0);
+}
+
+/*
+ * Name: dump_compress_gzip_cleanup()
+ * Func: Remove gzip as a compression mechanism.
+ */
+void __init
+dump_compress_gzip_cleanup(void)
+{
+	dump_unregister_compression(DUMP_COMPRESS_GZIP);
+}
+
+/* module initialization */
+module_init(dump_compress_gzip_init);
+module_exit(dump_compress_gzip_cleanup);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_ppc64.c linuxppc64_2_4/drivers/dump/dump_ppc64.c
--- linux-2.4.19/drivers/dump/dump_ppc64.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_ppc64.c	Tue Aug  6 09:43:10 2002
@@ -0,0 +1,290 @@
+/*
+ * Architecture specific (alpha) functions for Linux crash dumps.
+ *
+ * 2.3 kernel modifications by: Matt D. Robinson (yakker@turbolinux.com)
+ * Copyright 2000 TurboLinux, Inc.  All rights reserved.
+ * 2.4 modifications by Matt D. Robinson (yakker@aparity.com)
+ * Copyright 2001 Matt D. Robinson.  All rights reserved.
+ * Port to ppc64 by Todd Inglett <tinglett@vnet.ibm.com>
+ * Copyright 2002 International Business Machines
+ * 
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/*
+ * The hooks for dumping the kernel virtual memory to disk are in this
+ * file.  Any time a modification is made to the virtual memory mechanism,
+ * these routines must be changed to use the new mechanisms.
+ */
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/dump.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <asm/hardirq.h>
+#include <linux/irq.h>
+#include <asm/machdep.h>
+#if defined(CONFIG_KDB) && !defined(CONFIG_DUMP_MODULE)
+#include <linux/kdb.h>
+#endif
+
+extern unsigned long irq_affinity[];
+extern dump_header_t dump_header;
+
+static unsigned long saved_affinity[NR_IRQS];
+
+static unsigned int last_percent;
+
+static void
+save_this_cpu_state(int cpu, struct pt_regs *regs, struct task_struct *tsk)
+{
+	printk(KERN_ALERT "dump cpu(%d) saving state for task 0x%p\n", cpu, tsk);
+	dump_header_asm.dha_smp_regs[cpu] = *regs;
+	dump_header_asm.dha_smp_current_task[cpu] = tsk;
+
+	if (dump_header_asm.dha_stack[cpu]) {
+		memcpy(dump_header_asm.dha_stack[cpu], tsk, THREAD_SIZE);
+	}
+}
+
+static int dump_expect_ipi[NR_CPUS];
+static atomic_t waiting_for_dump_ipi;
+
+static int
+dump_ipi_handler(struct pt_regs *regs) 
+{
+	int cpu = smp_processor_id();
+
+	if (!dump_expect_ipi[cpu])
+		return 0;
+	
+	save_this_cpu_state(cpu, regs, current);
+
+	dump_expect_ipi[cpu] = 0;
+	atomic_dec(&waiting_for_dump_ipi);
+	return 1;
+}
+
+/* save registers on other processors
+ * If the other cpus don't respond we simply do not get their states.
+ */
+void 
+save_other_cpu_states(void)
+{
+	int i;
+
+	if (smp_num_cpus > 1) {
+		atomic_set(&waiting_for_dump_ipi, smp_num_cpus-1);
+		for (i = 0; i < NR_CPUS; i++)
+			dump_expect_ipi[i] = 1;
+
+		printk(KERN_ALERT "sending IPI to other cpus...\n");
+		dump_send_ipi(dump_ipi_handler);
+		i = 5000;	/* wait max of 5 seconds */
+		while (atomic_read(&waiting_for_dump_ipi) && (--i > 0)) {
+			barrier();
+			mdelay(1);
+		}
+		printk(KERN_ALERT "done waiting: %d remain\n",
+		       atomic_read(&waiting_for_dump_ipi));
+		dump_send_ipi(NULL);	/* clear handler */
+	}
+}
+
+/*
+ * Name: __dump_configure_header()
+ * Func: Configure the dump header with all proper values.
+ */
+int
+__dump_configure_header(struct pt_regs *regs)
+{
+	int cpu = smp_processor_id();
+
+	printk(KERN_ALERT "smp_num_cpus=%d\n", smp_num_cpus);
+	printk(KERN_ALERT "dumping_cpu=%d\n", cpu);
+	dump_header_asm.dha_smp_num_cpus = smp_num_cpus;
+	dump_header_asm.dha_dumping_cpu = cpu;
+
+	save_this_cpu_state(cpu, regs, current);
+
+	save_other_cpu_states();
+
+	return (1);
+}
+
+/*
+ * Name: __dump_write_header()
+ * Func: Update the header information with all architecture specific
+ *       information.
+ */
+int
+__dump_write_header(char *dpage)
+{
+	memcpy((void *)(dpage + sizeof(dump_header_t)),
+		(const void *)&dump_header_asm, sizeof(dump_header_asm_t));
+
+	return (1);
+}
+
+#if defined(CONFIG_KDB) && !defined(CONFIG_DUMP_MODULE)
+int
+kdb_sysdump(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	kdb_printf("Dumping to disk...\n");
+	dump("dump from kdb", regs);
+	kdb_printf("Dump Complete\n");
+	return 0;
+}
+#endif
+
+/*
+ * Name: __dump_init()
+ * Func: Initialize the dumping routine process.  This is in case
+ *       it's necessary in the future.
+ */
+void
+__dump_init(uint64_t local_memory_start)
+{
+
+#if defined(FIXME) && defined(CONFIG_KDB) && !defined(CONFIG_DUMP_MODULE)
+	/* This won't currently work because interrupts are off in kdb
+	 * and the dump process doesn't understand how to recover.
+	 */
+	/* ToDo: add a command to query/set dump configuration */
+	kdb_register_repeat("sysdump", kdb_sysdump, "", "use lkcd to dump the system to disk (if configured)", 0, KDB_REPEAT_NONE);
+#endif
+	/* return */
+	return;
+}
+
+/*
+ * Name: __dump_open()
+ * Func: Open the dump device (architecture specific).  This is in
+ *       case it's necessary in the future.
+ */
+void
+__dump_open(void)
+{
+}
+
+
+/*
+ * Name: __dump_cleanup()
+ * Func: Free any architecture specific data structures. This is called
+ *       when the dump module is being removed.
+ */
+void
+__dump_cleanup(void)
+{
+}
+
+/*
+ * Non dumping cpus will spin here. If a cpu is handling an irq when ipi is
+ * received, we let go of it here while making sure that it hits schedule
+ * on the way up and make it spin there instead.
+ */
+static void
+__dump_spin(void *arg)
+{
+	if (in_irq()) {
+		current->need_resched = 1;
+	} else {
+		while (dump_in_progress) ;
+	}
+}
+
+
+/*
+ * Routine to save the old irq affinities and change affinities of all irqs to
+ * the dumping cpu.
+ * 
+ * NB: Need to be expanded to multiple nodes.
+ */
+static void
+__dump_set_irq_affinity(void)
+{
+	int i;
+	int cpu = smp_processor_id();
+
+	memcpy(saved_affinity, irq_affinity, NR_IRQS * sizeof(unsigned long));
+
+	for (i = 0; i < NR_IRQS; i++) {
+		if (irq_desc[i].handler == NULL) {
+			continue;
+		}
+		irq_affinity[i] = 1UL << cpu;
+		if (irq_desc[i].handler->set_affinity != NULL) {
+			irq_desc[i].handler->set_affinity(i, irq_affinity[i]);
+		}
+	}
+}
+
+/*
+ * Restore old irq affinities.
+ */
+static void
+__dump_reset_irq_affinity(void)
+{
+	int i;
+
+	memcpy(irq_affinity, saved_affinity, NR_IRQS * sizeof(unsigned long));
+
+	for (i = 0; i < NR_IRQS; i++) {
+		if (irq_desc[i].handler == NULL) {
+			continue;
+		}
+		if (irq_desc[i].handler->set_affinity != NULL) {
+			irq_desc[i].handler->set_affinity(i, saved_affinity[i]);
+		}
+	}
+}
+
+/*
+ * Name: __dump_silence_system()
+ * Func: Do an architecture-specific silencing of the system.
+ */
+unsigned int
+__dump_silence_system(unsigned int stage)
+{
+	ppc64_dump_msg(0x1, "Dump Start");
+	last_percent = 0;
+	if (stage) {	/* Do this after interrupts are enabled */
+		/* read the comments above ... */
+		__dump_set_irq_affinity();
+		synchronize_irq();
+		smp_call_function(__dump_spin, NULL, 0, 0);
+	}
+	return 0;
+}
+
+/*
+ * Name: __dump_resume_system()
+ * Func: Resume the system state in an architecture-specific way.
+ */
+unsigned int
+__dump_resume_system(unsigned int stage)
+{
+	ppc64_dump_msg(0x3, "Dump Done");
+	/* put the irq affinity tables back */
+	__dump_reset_irq_affinity();
+	return 0;
+}
+
+/* Cheap progress hack.  It estimates pages to write and
+ * assumes all pages will go -- so it may get way off.
+ */
+void
+__dump_progress_add_page(void)
+{
+	unsigned long total_pages = nr_free_pages() + nr_inactive_pages + nr_active_pages;
+	unsigned int percent = (dump_header.dh_num_dump_pages * 100) / total_pages;
+	char buf[30];
+
+	if (percent > last_percent && percent <= 100) {
+		sprintf(buf, "Dump %3d%%     ", percent);
+		ppc64_dump_msg(0x2, buf);
+		last_percent = percent;
+	}
+
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_rle.c linuxppc64_2_4/drivers/dump/dump_rle.c
--- linux-2.4.19/drivers/dump/dump_rle.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_rle.c	Mon Jul  8 14:13:07 2002
@@ -0,0 +1,170 @@
+/*
+ * RLE Compression functions for kernel crash dumps.
+ *
+ * Created by: Matt Robinson (yakker@sourceforge.net)
+ * Copyright 2001 Matt D. Robinson.  All rights reserved.
+ *
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/* header files */
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/init.h>
+#include <linux/dump.h>
+
+/*
+ * Name: dump_compress_rle()
+ * Func: Compress a DUMP_PAGE_SIZE (hardware) page down to something more reasonable,
+ *       if possible.  This is the same routine we use in IRIX.
+ */
+static int
+dump_compress_rle(char *old, int oldsize, char *new, int newsize)
+{
+	int ri, wi, count = 0;
+	u_char value = 0, cur_byte;
+
+	/*
+	 * If the block should happen to "compress" to larger than the
+	 * buffer size, allocate a larger one and change cur_buf_size.
+	 */
+
+	wi = ri = 0;
+
+	while (ri < oldsize) {
+		if (!ri) {
+			cur_byte = value = old[ri];
+			count = 0;
+		} else {
+			if (count == 255) {
+				if (wi + 3 > oldsize) {
+					return oldsize;
+				}
+				new[wi++] = 0;
+				new[wi++] = count;
+				new[wi++] = value;
+				value = cur_byte = old[ri];
+				count = 0;
+			} else { 
+				if ((cur_byte = old[ri]) == value) {
+					count++;
+				} else {
+					if (count > 1) {
+						if (wi + 3 > oldsize) {
+							return oldsize;
+						}
+						new[wi++] = 0;
+						new[wi++] = count;
+						new[wi++] = value;
+					} else if (count == 1) {
+						if (value == 0) {
+							if (wi + 3 > oldsize) {
+								return oldsize;
+							}
+							new[wi++] = 0;
+							new[wi++] = 1;
+							new[wi++] = 0;
+						} else {
+							if (wi + 2 > oldsize) {
+								return oldsize;
+							}
+							new[wi++] = value;
+							new[wi++] = value;
+						}
+					} else { /* count == 0 */
+						if (value == 0) {
+							if (wi + 2 > oldsize) {
+								return oldsize;
+							}
+							new[wi++] = value;
+							new[wi++] = value;
+						} else {
+							if (wi + 1 > oldsize) {
+								return oldsize;
+							}
+							new[wi++] = value;
+						}
+					} /* if count > 1 */
+
+					value = cur_byte;
+					count = 0;
+
+				} /* if byte == value */
+
+			} /* if count == 255 */
+
+		} /* if ri == 0 */
+		ri++;
+
+	}
+	if (count > 1) {
+		if (wi + 3 > oldsize) {
+			return oldsize;
+		}
+		new[wi++] = 0;
+		new[wi++] = count;
+		new[wi++] = value;
+	} else if (count == 1) {
+		if (value == 0) {
+			if (wi + 3 > oldsize)
+				return oldsize;
+			new[wi++] = 0;
+			new[wi++] = 1;
+			new[wi++] = 0;
+		} else {
+			if (wi + 2 > oldsize)
+				return oldsize;
+			new[wi++] = value;
+			new[wi++] = value;
+		}
+	} else { /* count == 0 */
+		if (value == 0) {
+			if (wi + 2 > oldsize)
+				return oldsize;
+			new[wi++] = value;
+			new[wi++] = value;
+		} else {
+			if (wi + 1 > oldsize)
+				return oldsize;
+			new[wi++] = value;
+		}
+	} /* if count > 1 */
+
+	value = cur_byte;
+	count = 0;
+	return (wi);
+}
+
+/* setup the rle compression functionality */
+static dump_compress_t dump_rle_compression = {
+	compress_type:	DUMP_COMPRESS_RLE,
+	compress_func:	dump_compress_rle,
+};
+
+/*
+ * Name: dump_compress_rle_init()
+ * Func: Initialize rle compression for dumping.
+ */
+int __init
+dump_compress_rle_init(void)
+{
+	dump_register_compression(&dump_rle_compression);
+	return (0);
+}
+
+/*
+ * Name: dump_compress_rle_cleanup()
+ * Func: Remove rle compression for dumping.
+ */
+void
+dump_compress_rle_cleanup(void)
+{
+	dump_unregister_compression(DUMP_COMPRESS_RLE);
+}
+
+/* module initialization */
+module_init(dump_compress_rle_init);
+module_exit(dump_compress_rle_cleanup);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_zlib.c linuxppc64_2_4/drivers/dump/dump_zlib.c
--- linux-2.4.19/drivers/dump/dump_zlib.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_zlib.c	Mon Jul  8 14:13:07 2002
@@ -0,0 +1,5374 @@
+/*
+ * This file is derived from various .h and .c files from the zlib-1.0.4
+ * distribution by Jean-loup Gailly and Mark Adler, with some additions
+ * by Paul Mackerras to aid in implementing Deflate compression and
+ * decompression for PPP packets.  See zlib.h for conditions of
+ * distribution and use.
+ *
+ * Changes that have been made include:
+ * - added Z_PACKET_FLUSH (see zlib.h for details)
+ * - added inflateIncomp and deflateOutputPending
+ * - allow strm->next_out to be NULL, meaning discard the output
+ *
+ * $Id: dump_zlib.c,v 1.1 2002/07/08 19:13:07 tinglett Exp $
+ */
+
+/* 
+ *  ==FILEVERSION 971210==
+ *
+ * This marker is used by the Linux installation script to determine
+ * whether an up-to-date version of this file is already installed.
+ */
+
+#define NO_DUMMY_DECL
+#define NO_ZCFUNCS
+#define MY_ZCALLOC
+
+#if defined(__FreeBSD__) && (defined(KERNEL) || defined(_KERNEL))
+#define inflate	inflate_ppp	/* FreeBSD already has an inflate :-( */
+#endif
+
+
+/* +++ zutil.h */
+/* zutil.h -- internal interface and configuration of the compression library
+ * Copyright (C) 1995-1996 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* From: zutil.h,v 1.16 1996/07/24 13:41:13 me Exp $ */
+
+#ifndef _Z_UTIL_H
+#define _Z_UTIL_H
+
+#include "dump_zlib.h"
+
+#if defined(KERNEL) || defined(_KERNEL)
+/* Assume this is a *BSD or SVR4 kernel */
+#include <sys/types.h>
+#include <sys/time.h>
+#include <sys/systm.h>
+#  define HAVE_MEMCPY
+#  define memcpy(d, s, n)	bcopy((s), (d), (n))
+#  define memset(d, v, n)	bzero((d), (n))
+#  define memcmp		bcmp
+
+#else
+#if defined(__KERNEL__)
+/* Assume this is a Linux kernel */
+#include <linux/string.h>
+#define HAVE_MEMCPY
+
+#else /* not kernel */
+
+#if defined(MSDOS)||defined(VMS)||defined(CRAY)||defined(WIN32)||defined(RISCOS)
+#   include <stddef.h>
+#   include <errno.h>
+#else
+    extern int errno;
+#endif
+#ifdef STDC
+#  include <string.h>
+#  include <stdlib.h>
+#endif
+#endif /* __KERNEL__ */
+#endif /* _KERNEL || KERNEL */
+
+#ifndef local
+#  define local static
+#endif
+/* compile with -Dlocal if your debugger can't find static symbols */
+
+typedef unsigned char  uch;
+typedef uch FAR uchf;
+typedef unsigned short ush;
+typedef ush FAR ushf;
+typedef unsigned long  ulg;
+
+extern const char *z_errmsg[10]; /* indexed by 2-zlib_error */
+/* (size given to avoid silly warnings with Visual C++) */
+
+#define ERR_MSG(err) z_errmsg[Z_NEED_DICT-(err)]
+
+#define ERR_RETURN(strm,err) \
+  return (strm->msg = (char*)ERR_MSG(err), (err))
+/* To be used only when the state is known to be valid */
+
+        /* common constants */
+
+#ifndef DEF_WBITS
+#  define DEF_WBITS MAX_WBITS
+#endif
+/* default windowBits for decompression. MAX_WBITS is for compression only */
+
+#if MAX_MEM_LEVEL >= 8
+#  define DEF_MEM_LEVEL 8
+#else
+#  define DEF_MEM_LEVEL  MAX_MEM_LEVEL
+#endif
+/* default memLevel */
+
+#define STORED_BLOCK 0
+#define STATIC_TREES 1
+#define DYN_TREES    2
+/* The three kinds of block type */
+
+#define MIN_MATCH  3
+#define MAX_MATCH  258
+/* The minimum and maximum match lengths */
+
+#define PRESET_DICT 0x20 /* preset dictionary flag in zlib header */
+
+        /* target dependencies */
+
+#ifdef MSDOS
+#  define OS_CODE  0x00
+#  ifdef __TURBOC__
+#    include <alloc.h>
+#  else /* MSC or DJGPP */
+#    include <malloc.h>
+#  endif
+#endif
+
+#ifdef OS2
+#  define OS_CODE  0x06
+#endif
+
+#ifdef WIN32 /* Window 95 & Windows NT */
+#  define OS_CODE  0x0b
+#endif
+
+#if defined(VAXC) || defined(VMS)
+#  define OS_CODE  0x02
+#  define FOPEN(name, mode) \
+     fopen((name), (mode), "mbc=60", "ctx=stm", "rfm=fix", "mrs=512")
+#endif
+
+#ifdef AMIGA
+#  define OS_CODE  0x01
+#endif
+
+#if defined(ATARI) || defined(atarist)
+#  define OS_CODE  0x05
+#endif
+
+#ifdef MACOS
+#  define OS_CODE  0x07
+#endif
+
+#ifdef __50SERIES /* Prime/PRIMOS */
+#  define OS_CODE  0x0F
+#endif
+
+#ifdef TOPS20
+#  define OS_CODE  0x0a
+#endif
+
+#if defined(_BEOS_) || defined(RISCOS)
+#  define fdopen(fd,mode) NULL /* No fdopen() */
+#endif
+
+        /* Common defaults */
+
+#ifndef OS_CODE
+#  define OS_CODE  0x03  /* assume Unix */
+#endif
+
+#ifndef FOPEN
+#  define FOPEN(name, mode) fopen((name), (mode))
+#endif
+
+         /* functions */
+
+#ifdef HAVE_STRERROR
+   extern char *strerror OF((int));
+#  define zstrerror(errnum) strerror(errnum)
+#else
+#  define zstrerror(errnum) ""
+#endif
+
+#if defined(pyr)
+#  define NO_MEMCPY
+#endif
+#if (defined(M_I86SM) || defined(M_I86MM)) && !defined(_MSC_VER)
+ /* Use our own functions for small and medium model with MSC <= 5.0.
+  * You may have to use the same strategy for Borland C (untested).
+  */
+#  define NO_MEMCPY
+#endif
+#if defined(STDC) && !defined(HAVE_MEMCPY) && !defined(NO_MEMCPY)
+#  define HAVE_MEMCPY
+#endif
+#ifdef HAVE_MEMCPY
+#  ifdef SMALL_MEDIUM /* MSDOS small or medium model */
+#    define zmemcpy _fmemcpy
+#    define zmemcmp _fmemcmp
+#    define zmemzero(dest, len) _fmemset(dest, 0, len)
+#  else
+#    define zmemcpy memcpy
+#    define zmemcmp memcmp
+#    define zmemzero(dest, len) memset(dest, 0, len)
+#  endif
+#else
+   extern void zmemcpy  OF((Bytef* dest, Bytef* source, uInt len));
+   extern int  zmemcmp  OF((Bytef* s1,   Bytef* s2, uInt len));
+   extern void zmemzero OF((Bytef* dest, uInt len));
+#endif
+
+/* Diagnostic functions */
+#ifdef DEBUG_ZLIB
+#  include <stdio.h>
+#  ifndef verbose
+#    define verbose 0
+#  endif
+   extern void z_error    OF((char *m));
+#  define Assert(cond,msg) {if(!(cond)) z_error(msg);}
+#  define Trace(x) fprintf x
+#  define Tracev(x) {if (verbose) fprintf x ;}
+#  define Tracevv(x) {if (verbose>1) fprintf x ;}
+#  define Tracec(c,x) {if (verbose && (c)) fprintf x ;}
+#  define Tracecv(c,x) {if (verbose>1 && (c)) fprintf x ;}
+#else
+#  define Assert(cond,msg)
+#  define Trace(x)
+#  define Tracev(x)
+#  define Tracevv(x)
+#  define Tracec(c,x)
+#  define Tracecv(c,x)
+#endif
+
+
+typedef uLong (*check_func) OF((uLong check, const Bytef *buf, uInt len));
+
+voidpf zcalloc OF((voidpf opaque, unsigned items, unsigned size));
+void   zcfree  OF((voidpf opaque, voidpf ptr));
+
+#define ZALLOC(strm, items, size) \
+           (*((strm)->zalloc))((strm)->opaque, (items), (size))
+#define ZFREE(strm, addr)  (*((strm)->zfree))((strm)->opaque, (voidpf)(addr))
+#define TRY_FREE(s, p) {if (p) ZFREE(s, p);}
+
+#endif /* _Z_UTIL_H */
+/* --- zutil.h */
+
+/* +++ deflate.h */
+/* deflate.h -- internal compression state
+ * Copyright (C) 1995-1996 Jean-loup Gailly
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* From: deflate.h,v 1.10 1996/07/02 12:41:00 me Exp $ */
+
+#ifndef _DEFLATE_H
+#define _DEFLATE_H
+
+/* #include "zutil.h" */
+
+/* ===========================================================================
+ * Internal compression state.
+ */
+
+#define LENGTH_CODES 29
+/* number of length codes, not counting the special END_BLOCK code */
+
+#define LITERALS  256
+/* number of literal bytes 0..255 */
+
+#define L_CODES (LITERALS+1+LENGTH_CODES)
+/* number of Literal or Length codes, including the END_BLOCK code */
+
+#define D_CODES   30
+/* number of distance codes */
+
+#define BL_CODES  19
+/* number of codes used to transfer the bit lengths */
+
+#define HEAP_SIZE (2*L_CODES+1)
+/* maximum heap size */
+
+#define MAX_BITS 15
+/* All codes must not exceed MAX_BITS bits */
+
+#define INIT_STATE    42
+#define BUSY_STATE   113
+#define FINISH_STATE 666
+/* Stream status */
+
+
+/* Data structure describing a single value and its code string. */
+typedef struct ct_data_s {
+    union {
+        ush  freq;       /* frequency count */
+        ush  code;       /* bit string */
+    } fc;
+    union {
+        ush  dad;        /* father node in Huffman tree */
+        ush  len;        /* length of bit string */
+    } dl;
+} FAR ct_data;
+
+#define Freq fc.freq
+#define Code fc.code
+#define Dad  dl.dad
+#define Len  dl.len
+
+typedef struct static_tree_desc_s  static_tree_desc;
+
+typedef struct tree_desc_s {
+    ct_data *dyn_tree;           /* the dynamic tree */
+    int     max_code;            /* largest code with non zero frequency */
+    static_tree_desc *stat_desc; /* the corresponding static tree */
+} FAR tree_desc;
+
+typedef ush Pos;
+typedef Pos FAR Posf;
+typedef unsigned IPos;
+
+/* A Pos is an index in the character window. We use short instead of int to
+ * save space in the various tables. IPos is used only for parameter passing.
+ */
+
+typedef struct deflate_state {
+    z_streamp strm;      /* pointer back to this zlib stream */
+    int   status;        /* as the name implies */
+    Bytef *pending_buf;  /* output still pending */
+    ulg   pending_buf_size; /* size of pending_buf */
+    Bytef *pending_out;  /* next pending byte to output to the stream */
+    int   pending;       /* nb of bytes in the pending buffer */
+    int   noheader;      /* suppress zlib header and adler32 */
+    Byte  data_type;     /* UNKNOWN, BINARY or ASCII */
+    Byte  method;        /* STORED (for zip only) or DEFLATED */
+    int   last_flush;    /* value of flush param for previous deflate call */
+
+                /* used by deflate.c: */
+
+    uInt  w_size;        /* LZ77 window size (32K by default) */
+    uInt  w_bits;        /* log2(w_size)  (8..16) */
+    uInt  w_mask;        /* w_size - 1 */
+
+    Bytef *window;
+    /* Sliding window. Input bytes are read into the second half of the window,
+     * and move to the first half later to keep a dictionary of at least wSize
+     * bytes. With this organization, matches are limited to a distance of
+     * wSize-MAX_MATCH bytes, but this ensures that IO is always
+     * performed with a length multiple of the block size. Also, it limits
+     * the window size to 64K, which is quite useful on MSDOS.
+     * To do: use the user input buffer as sliding window.
+     */
+
+    ulg window_size;
+    /* Actual size of window: 2*wSize, except when the user input buffer
+     * is directly used as sliding window.
+     */
+
+    Posf *prev;
+    /* Link to older string with same hash index. To limit the size of this
+     * array to 64K, this link is maintained only for the last 32K strings.
+     * An index in this array is thus a window index modulo 32K.
+     */
+
+    Posf *head; /* Heads of the hash chains or NIL. */
+
+    uInt  ins_h;          /* hash index of string to be inserted */
+    uInt  hash_size;      /* number of elements in hash table */
+    uInt  hash_bits;      /* log2(hash_size) */
+    uInt  hash_mask;      /* hash_size-1 */
+
+    uInt  hash_shift;
+    /* Number of bits by which ins_h must be shifted at each input
+     * step. It must be such that after MIN_MATCH steps, the oldest
+     * byte no longer takes part in the hash key, that is:
+     *   hash_shift * MIN_MATCH >= hash_bits
+     */
+
+    long block_start;
+    /* Window position at the beginning of the current output block. Gets
+     * negative when the window is moved backwards.
+     */
+
+    uInt match_length;           /* length of best match */
+    IPos prev_match;             /* previous match */
+    int match_available;         /* set if previous match exists */
+    uInt strstart;               /* start of string to insert */
+    uInt match_start;            /* start of matching string */
+    uInt lookahead;              /* number of valid bytes ahead in window */
+
+    uInt prev_length;
+    /* Length of the best match at previous step. Matches not greater than this
+     * are discarded. This is used in the lazy match evaluation.
+     */
+
+    uInt max_chain_length;
+    /* To speed up deflation, hash chains are never searched beyond this
+     * length.  A higher limit improves compression ratio but degrades the
+     * speed.
+     */
+
+    uInt max_lazy_match;
+    /* Attempt to find a better match only when the current match is strictly
+     * smaller than this value. This mechanism is used only for compression
+     * levels >= 4.
+     */
+#   define max_insert_length  max_lazy_match
+    /* Insert new strings in the hash table only if the match length is not
+     * greater than this length. This saves time but degrades compression.
+     * max_insert_length is used only for compression levels <= 3.
+     */
+
+    int level;    /* compression level (1..9) */
+    int strategy; /* favor or force Huffman coding*/
+
+    uInt good_match;
+    /* Use a faster search when the previous match is longer than this */
+
+    int nice_match; /* Stop searching when current match exceeds this */
+
+                /* used by trees.c: */
+    /* Didn't use ct_data typedef below to suppress compiler warning */
+    struct ct_data_s dyn_ltree[HEAP_SIZE];   /* literal and length tree */
+    struct ct_data_s dyn_dtree[2*D_CODES+1]; /* distance tree */
+    struct ct_data_s bl_tree[2*BL_CODES+1];  /* Huffman tree for bit lengths */
+
+    struct tree_desc_s l_desc;               /* desc. for literal tree */
+    struct tree_desc_s d_desc;               /* desc. for distance tree */
+    struct tree_desc_s bl_desc;              /* desc. for bit length tree */
+
+    ush bl_count[MAX_BITS+1];
+    /* number of codes at each bit length for an optimal tree */
+
+    int heap[2*L_CODES+1];      /* heap used to build the Huffman trees */
+    int heap_len;               /* number of elements in the heap */
+    int heap_max;               /* element of largest frequency */
+    /* The sons of heap[n] are heap[2*n] and heap[2*n+1]. heap[0] is not used.
+     * The same heap array is used to build all trees.
+     */
+
+    uch depth[2*L_CODES+1];
+    /* Depth of each subtree used as tie breaker for trees of equal frequency
+     */
+
+    uchf *l_buf;          /* buffer for literals or lengths */
+
+    uInt  lit_bufsize;
+    /* Size of match buffer for literals/lengths.  There are 4 reasons for
+     * limiting lit_bufsize to 64K:
+     *   - frequencies can be kept in 16 bit counters
+     *   - if compression is not successful for the first block, all input
+     *     data is still in the window so we can still emit a stored block even
+     *     when input comes from standard input.  (This can also be done for
+     *     all blocks if lit_bufsize is not greater than 32K.)
+     *   - if compression is not successful for a file smaller than 64K, we can
+     *     even emit a stored file instead of a stored block (saving 5 bytes).
+     *     This is applicable only for zip (not gzip or zlib).
+     *   - creating new Huffman trees less frequently may not provide fast
+     *     adaptation to changes in the input data statistics. (Take for
+     *     example a binary file with poorly compressible code followed by
+     *     a highly compressible string table.) Smaller buffer sizes give
+     *     fast adaptation but have of course the overhead of transmitting
+     *     trees more frequently.
+     *   - I can't count above 4
+     */
+
+    uInt last_lit;      /* running index in l_buf */
+
+    ushf *d_buf;
+    /* Buffer for distances. To simplify the code, d_buf and l_buf have
+     * the same number of elements. To use different lengths, an extra flag
+     * array would be necessary.
+     */
+
+    ulg opt_len;        /* bit length of current block with optimal trees */
+    ulg static_len;     /* bit length of current block with static trees */
+    ulg compressed_len; /* total bit length of compressed file */
+    uInt matches;       /* number of string matches in current block */
+    int last_eob_len;   /* bit length of EOB code for last block */
+
+#ifdef DEBUG_ZLIB
+    ulg bits_sent;      /* bit length of the compressed data */
+#endif
+
+    ush bi_buf;
+    /* Output buffer. bits are inserted starting at the bottom (least
+     * significant bits).
+     */
+    int bi_valid;
+    /* Number of valid bits in bi_buf.  All bits above the last valid bit
+     * are always zero.
+     */
+
+} FAR deflate_state;
+
+/* Output a byte on the stream.
+ * IN assertion: there is enough room in pending_buf.
+ */
+#define put_byte(s, c) {s->pending_buf[s->pending++] = (c);}
+
+
+#define MIN_LOOKAHEAD (MAX_MATCH+MIN_MATCH+1)
+/* Minimum amount of lookahead, except at the end of the input file.
+ * See deflate.c for comments about the MIN_MATCH+1.
+ */
+
+#define MAX_DIST(s)  ((s)->w_size-MIN_LOOKAHEAD)
+/* In order to simplify the code, particularly on 16 bit machines, match
+ * distances are limited to MAX_DIST instead of WSIZE.
+ */
+
+        /* in trees.c */
+void _tr_init         OF((deflate_state *s));
+int  _tr_tally        OF((deflate_state *s, unsigned dist, unsigned lc));
+ulg  _tr_flush_block  OF((deflate_state *s, charf *buf, ulg stored_len,
+			  int eof));
+void _tr_align        OF((deflate_state *s));
+void _tr_stored_block OF((deflate_state *s, charf *buf, ulg stored_len,
+                          int eof));
+void _tr_stored_type_only OF((deflate_state *));
+
+#endif
+/* --- deflate.h */
+
+/* +++ deflate.c */
+/* deflate.c -- compress data using the deflation algorithm
+ * Copyright (C) 1995-1996 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/*
+ *  ALGORITHM
+ *
+ *      The "deflation" process depends on being able to identify portions
+ *      of the input text which are identical to earlier input (within a
+ *      sliding window trailing behind the input currently being processed).
+ *
+ *      The most straightforward technique turns out to be the fastest for
+ *      most input files: try all possible matches and select the longest.
+ *      The key feature of this algorithm is that insertions into the string
+ *      dictionary are very simple and thus fast, and deletions are avoided
+ *      completely. Insertions are performed at each input character, whereas
+ *      string matches are performed only when the previous match ends. So it
+ *      is preferable to spend more time in matches to allow very fast string
+ *      insertions and avoid deletions. The matching algorithm for small
+ *      strings is inspired from that of Rabin & Karp. A brute force approach
+ *      is used to find longer strings when a small match has been found.
+ *      A similar algorithm is used in comic (by Jan-Mark Wams) and freeze
+ *      (by Leonid Broukhis).
+ *         A previous version of this file used a more sophisticated algorithm
+ *      (by Fiala and Greene) which is guaranteed to run in linear amortized
+ *      time, but has a larger average cost, uses more memory and is patented.
+ *      However the F&G algorithm may be faster for some highly redundant
+ *      files if the parameter max_chain_length (described below) is too large.
+ *
+ *  ACKNOWLEDGEMENTS
+ *
+ *      The idea of lazy evaluation of matches is due to Jan-Mark Wams, and
+ *      I found it in 'freeze' written by Leonid Broukhis.
+ *      Thanks to many people for bug reports and testing.
+ *
+ *  REFERENCES
+ *
+ *      Deutsch, L.P.,"DEFLATE Compressed Data Format Specification".
+ *      Available in ftp://ds.internic.net/rfc/rfc1951.txt
+ *
+ *      A description of the Rabin and Karp algorithm is given in the book
+ *         "Algorithms" by R. Sedgewick, Addison-Wesley, p252.
+ *
+ *      Fiala,E.R., and Greene,D.H.
+ *         Data Compression with Finite Windows, Comm.ACM, 32,4 (1989) 490-595
+ *
+ */
+
+/* From: deflate.c,v 1.15 1996/07/24 13:40:58 me Exp $ */
+
+/* #include "deflate.h" */
+
+char deflate_copyright[] = " deflate 1.0.4 Copyright 1995-1996 Jean-loup Gailly ";
+/*
+  If you use the zlib library in a product, an acknowledgment is welcome
+  in the documentation of your product. If for some reason you cannot
+  include such an acknowledgment, I would appreciate that you keep this
+  copyright string in the executable of your product.
+ */
+
+/* ===========================================================================
+ *  Function prototypes.
+ */
+typedef enum {
+    need_more,      /* block not completed, need more input or more output */
+    block_done,     /* block flush performed */
+    finish_started, /* finish started, need only more output at next deflate */
+    finish_done     /* finish done, accept no more input or output */
+} block_state;
+
+typedef block_state (*compress_func) OF((deflate_state *s, int flush));
+/* Compression function. Returns the block state after the call. */
+
+local void fill_window    OF((deflate_state *s));
+local block_state deflate_stored OF((deflate_state *s, int flush));
+local block_state deflate_fast   OF((deflate_state *s, int flush));
+local block_state deflate_slow   OF((deflate_state *s, int flush));
+local void lm_init        OF((deflate_state *s));
+local void putShortMSB    OF((deflate_state *s, uInt b));
+local void flush_pending  OF((z_streamp strm));
+local int read_buf        OF((z_streamp strm, charf *buf, unsigned size));
+#ifdef ASMV
+      void match_init OF((void)); /* asm code initialization */
+      uInt longest_match  OF((deflate_state *s, IPos cur_match));
+#else
+local uInt longest_match  OF((deflate_state *s, IPos cur_match));
+#endif
+
+#ifdef DEBUG_ZLIB
+local  void check_match OF((deflate_state *s, IPos start, IPos match,
+                            int length));
+#endif
+
+/* ===========================================================================
+ * Local data
+ */
+
+#define NIL 0
+/* Tail of hash chains */
+
+#ifndef TOO_FAR
+#  define TOO_FAR 4096
+#endif
+/* Matches of length 3 are discarded if their distance exceeds TOO_FAR */
+
+#define MIN_LOOKAHEAD (MAX_MATCH+MIN_MATCH+1)
+/* Minimum amount of lookahead, except at the end of the input file.
+ * See deflate.c for comments about the MIN_MATCH+1.
+ */
+
+/* Values for max_lazy_match, good_match and max_chain_length, depending on
+ * the desired pack level (0..9). The values given below have been tuned to
+ * exclude worst case performance for pathological files. Better values may be
+ * found for specific files.
+ */
+typedef struct config_s {
+   ush good_length; /* reduce lazy search above this match length */
+   ush max_lazy;    /* do not perform lazy search above this match length */
+   ush nice_length; /* quit search above this match length */
+   ush max_chain;
+   compress_func func;
+} config;
+
+local config configuration_table[10] = {
+/*      good lazy nice chain */
+/* 0 */ {0,    0,  0,    0, deflate_stored},  /* store only */
+/* 1 */ {4,    4,  8,    4, deflate_fast}, /* maximum speed, no lazy matches */
+/* 2 */ {4,    5, 16,    8, deflate_fast},
+/* 3 */ {4,    6, 32,   32, deflate_fast},
+
+/* 4 */ {4,    4, 16,   16, deflate_slow},  /* lazy matches */
+/* 5 */ {8,   16, 32,   32, deflate_slow},
+/* 6 */ {8,   16, 128, 128, deflate_slow},
+/* 7 */ {8,   32, 128, 256, deflate_slow},
+/* 8 */ {32, 128, 258, 1024, deflate_slow},
+/* 9 */ {32, 258, 258, 4096, deflate_slow}}; /* maximum compression */
+
+/* Note: the deflate() code requires max_lazy >= MIN_MATCH and max_chain >= 4
+ * For deflate_fast() (levels <= 3) good is ignored and lazy has a different
+ * meaning.
+ */
+
+#define EQUAL 0
+/* result of memcmp for equal strings */
+
+#ifndef NO_DUMMY_DECL
+struct static_tree_desc_s {int dummy;}; /* for buggy compilers */
+#endif
+
+/* ===========================================================================
+ * Update a hash value with the given input byte
+ * IN  assertion: all calls to UPDATE_HASH are made with consecutive
+ *    input characters, so that a running hash key can be computed from the
+ *    previous key instead of complete recalculation each time.
+ */
+#define UPDATE_HASH(s,h,c) (h = (((h)<<s->hash_shift) ^ (c)) & s->hash_mask)
+
+
+/* ===========================================================================
+ * Insert string str in the dictionary and set match_head to the previous head
+ * of the hash chain (the most recent string with same hash key). Return
+ * the previous length of the hash chain.
+ * IN  assertion: all calls to INSERT_STRING are made with consecutive
+ *    input characters and the first MIN_MATCH bytes of str are valid
+ *    (except for the last MIN_MATCH-1 bytes of the input file).
+ */
+#define INSERT_STRING(s, str, match_head) \
+   (UPDATE_HASH(s, s->ins_h, s->window[(str) + (MIN_MATCH-1)]), \
+    s->prev[(str) & s->w_mask] = match_head = s->head[s->ins_h], \
+    s->head[s->ins_h] = (Pos)(str))
+
+/* ===========================================================================
+ * Initialize the hash table (avoiding 64K overflow for 16 bit systems).
+ * prev[] will be initialized on the fly.
+ */
+#define CLEAR_HASH(s) \
+    s->head[s->hash_size-1] = NIL; \
+    zmemzero((charf *)s->head, (unsigned)(s->hash_size-1)*sizeof(*s->head));
+
+/* ========================================================================= */
+int deflateInit_(strm, level, version, stream_size)
+    z_streamp strm;
+    int level;
+    const char *version;
+    int stream_size;
+{
+    return deflateInit2_(strm, level, Z_DEFLATED, MAX_WBITS, DEF_MEM_LEVEL,
+			 Z_DEFAULT_STRATEGY, version, stream_size);
+    /* To do: ignore strm->next_in if we use it as window */
+}
+
+/* ========================================================================= */
+int deflateInit2_(strm, level, method, windowBits, memLevel, strategy,
+		  version, stream_size)
+    z_streamp strm;
+    int  level;
+    int  method;
+    int  windowBits;
+    int  memLevel;
+    int  strategy;
+    const char *version;
+    int stream_size;
+{
+    deflate_state *s;
+    int noheader = 0;
+    static char* my_version = ZLIB_VERSION;
+
+    ushf *overlay;
+    /* We overlay pending_buf and d_buf+l_buf. This works since the average
+     * output size for (length,distance) codes is <= 24 bits.
+     */
+
+    if (version == Z_NULL || version[0] != my_version[0] ||
+        stream_size != sizeof(z_stream)) {
+	return Z_VERSION_ERROR;
+    }
+    if (strm == Z_NULL) return Z_STREAM_ERROR;
+
+    strm->msg = Z_NULL;
+#ifndef NO_ZCFUNCS
+    if (strm->zalloc == Z_NULL) {
+	strm->zalloc = zcalloc;
+	strm->opaque = (voidpf)0;
+    }
+    if (strm->zfree == Z_NULL) strm->zfree = zcfree;
+#endif
+
+    if (level == Z_DEFAULT_COMPRESSION) level = 6;
+
+    if (windowBits < 0) { /* undocumented feature: suppress zlib header */
+        noheader = 1;
+        windowBits = -windowBits;
+    }
+    if (memLevel < 1 || memLevel > MAX_MEM_LEVEL || method != Z_DEFLATED ||
+        windowBits < 8 || windowBits > 15 || level < 0 || level > 9 ||
+	strategy < 0 || strategy > Z_HUFFMAN_ONLY) {
+        return Z_STREAM_ERROR;
+    }
+    s = (deflate_state *) ZALLOC(strm, 1, sizeof(deflate_state));
+    if (s == Z_NULL) return Z_MEM_ERROR;
+    strm->state = (struct internal_state FAR *)s;
+    s->strm = strm;
+
+    s->noheader = noheader;
+    s->w_bits = windowBits;
+    s->w_size = 1 << s->w_bits;
+    s->w_mask = s->w_size - 1;
+
+    s->hash_bits = memLevel + 7;
+    s->hash_size = 1 << s->hash_bits;
+    s->hash_mask = s->hash_size - 1;
+    s->hash_shift =  ((s->hash_bits+MIN_MATCH-1)/MIN_MATCH);
+
+    s->window = (Bytef *) ZALLOC(strm, s->w_size, 2*sizeof(Byte));
+    s->prev   = (Posf *)  ZALLOC(strm, s->w_size, sizeof(Pos));
+    s->head   = (Posf *)  ZALLOC(strm, s->hash_size, sizeof(Pos));
+
+    s->lit_bufsize = 1 << (memLevel + 6); /* 16K elements by default */
+
+    overlay = (ushf *) ZALLOC(strm, s->lit_bufsize, sizeof(ush)+2);
+    s->pending_buf = (uchf *) overlay;
+    s->pending_buf_size = (ulg)s->lit_bufsize * (sizeof(ush)+2L);
+
+    if (s->window == Z_NULL || s->prev == Z_NULL || s->head == Z_NULL ||
+        s->pending_buf == Z_NULL) {
+        strm->msg = (char*)ERR_MSG(Z_MEM_ERROR);
+        deflateEnd (strm);
+        return Z_MEM_ERROR;
+    }
+    s->d_buf = overlay + s->lit_bufsize/sizeof(ush);
+    s->l_buf = s->pending_buf + (1+sizeof(ush))*s->lit_bufsize;
+
+    s->level = level;
+    s->strategy = strategy;
+    s->method = (Byte)method;
+
+    return deflateReset(strm);
+}
+
+/* ========================================================================= */
+int deflateSetDictionary (strm, dictionary, dictLength)
+    z_streamp strm;
+    const Bytef *dictionary;
+    uInt  dictLength;
+{
+    deflate_state *s;
+    uInt length = dictLength;
+    uInt n;
+    IPos hash_head = 0;
+
+    if (strm == Z_NULL || strm->state == Z_NULL || dictionary == Z_NULL)
+	return Z_STREAM_ERROR;
+
+    s = (deflate_state *) strm->state;
+    if (s->status != INIT_STATE) return Z_STREAM_ERROR;
+
+    strm->adler = adler32(strm->adler, dictionary, dictLength);
+
+    if (length < MIN_MATCH) return Z_OK;
+    if (length > MAX_DIST(s)) {
+	length = MAX_DIST(s);
+#ifndef USE_DICT_HEAD
+	dictionary += dictLength - length; /* use the tail of the dictionary */
+#endif
+    }
+    zmemcpy((charf *)s->window, dictionary, length);
+    s->strstart = length;
+    s->block_start = (long)length;
+
+    /* Insert all strings in the hash table (except for the last two bytes).
+     * s->lookahead stays null, so s->ins_h will be recomputed at the next
+     * call of fill_window.
+     */
+    s->ins_h = s->window[0];
+    UPDATE_HASH(s, s->ins_h, s->window[1]);
+    for (n = 0; n <= length - MIN_MATCH; n++) {
+	INSERT_STRING(s, n, hash_head);
+    }
+    if (hash_head) hash_head = 0;  /* to make compiler happy */
+    return Z_OK;
+}
+
+/* ========================================================================= */
+int deflateReset (strm)
+    z_streamp strm;
+{
+    deflate_state *s;
+    
+    if (strm == Z_NULL || strm->state == Z_NULL ||
+        strm->zalloc == Z_NULL || strm->zfree == Z_NULL) return Z_STREAM_ERROR;
+
+    strm->total_in = strm->total_out = 0;
+    strm->msg = Z_NULL; /* use zfree if we ever allocate msg dynamically */
+    strm->data_type = Z_UNKNOWN;
+
+    s = (deflate_state *)strm->state;
+    s->pending = 0;
+    s->pending_out = s->pending_buf;
+
+    if (s->noheader < 0) {
+        s->noheader = 0; /* was set to -1 by deflate(..., Z_FINISH); */
+    }
+    s->status = s->noheader ? BUSY_STATE : INIT_STATE;
+    strm->adler = 1;
+    s->last_flush = Z_NO_FLUSH;
+
+    _tr_init(s);
+    lm_init(s);
+
+    return Z_OK;
+}
+
+/* ========================================================================= */
+int deflateParams(strm, level, strategy)
+    z_streamp strm;
+    int level;
+    int strategy;
+{
+    deflate_state *s;
+    compress_func func;
+    int err = Z_OK;
+
+    if (strm == Z_NULL || strm->state == Z_NULL) return Z_STREAM_ERROR;
+    s = (deflate_state *) strm->state;
+
+    if (level == Z_DEFAULT_COMPRESSION) {
+	level = 6;
+    }
+    if (level < 0 || level > 9 || strategy < 0 || strategy > Z_HUFFMAN_ONLY) {
+	return Z_STREAM_ERROR;
+    }
+    func = configuration_table[s->level].func;
+
+    if (func != configuration_table[level].func && strm->total_in != 0) {
+	/* Flush the last buffer: */
+	err = deflate(strm, Z_PARTIAL_FLUSH);
+    }
+    if (s->level != level) {
+	s->level = level;
+	s->max_lazy_match   = configuration_table[level].max_lazy;
+	s->good_match       = configuration_table[level].good_length;
+	s->nice_match       = configuration_table[level].nice_length;
+	s->max_chain_length = configuration_table[level].max_chain;
+    }
+    s->strategy = strategy;
+    return err;
+}
+
+/* =========================================================================
+ * Put a short in the pending buffer. The 16-bit value is put in MSB order.
+ * IN assertion: the stream state is correct and there is enough room in
+ * pending_buf.
+ */
+local void putShortMSB (s, b)
+    deflate_state *s;
+    uInt b;
+{
+    put_byte(s, (Byte)(b >> 8));
+    put_byte(s, (Byte)(b & 0xff));
+}   
+
+/* =========================================================================
+ * Flush as much pending output as possible. All deflate() output goes
+ * through this function so some applications may wish to modify it
+ * to avoid allocating a large strm->next_out buffer and copying into it.
+ * (See also read_buf()).
+ */
+local void flush_pending(strm)
+    z_streamp strm;
+{
+    deflate_state *s = (deflate_state *) strm->state;
+    unsigned len = s->pending;
+
+    if (len > strm->avail_out) len = strm->avail_out;
+    if (len == 0) return;
+
+    if (strm->next_out != Z_NULL) {
+	zmemcpy(strm->next_out, s->pending_out, len);
+	strm->next_out += len;
+    }
+    s->pending_out += len;
+    strm->total_out += len;
+    strm->avail_out  -= len;
+    s->pending -= len;
+    if (s->pending == 0) {
+        s->pending_out = s->pending_buf;
+    }
+}
+
+/* ========================================================================= */
+int deflate (strm, flush)
+    z_streamp strm;
+    int flush;
+{
+    int old_flush; /* value of flush param for previous deflate call */
+    deflate_state *s;
+
+    if (strm == Z_NULL || strm->state == Z_NULL ||
+	flush > Z_FINISH || flush < 0) {
+        return Z_STREAM_ERROR;
+    }
+    s = (deflate_state *) strm->state;
+
+    if ((strm->next_in == Z_NULL && strm->avail_in != 0) ||
+	(s->status == FINISH_STATE && flush != Z_FINISH)) {
+        ERR_RETURN(strm, Z_STREAM_ERROR);
+    }
+    if (strm->avail_out == 0) ERR_RETURN(strm, Z_BUF_ERROR);
+
+    s->strm = strm; /* just in case */
+    old_flush = s->last_flush;
+    s->last_flush = flush;
+
+    /* Write the zlib header */
+    if (s->status == INIT_STATE) {
+
+        uInt header = (Z_DEFLATED + ((s->w_bits-8)<<4)) << 8;
+        uInt level_flags = (s->level-1) >> 1;
+
+        if (level_flags > 3) level_flags = 3;
+        header |= (level_flags << 6);
+	if (s->strstart != 0) header |= PRESET_DICT;
+        header += 31 - (header % 31);
+
+        s->status = BUSY_STATE;
+        putShortMSB(s, header);
+
+	/* Save the adler32 of the preset dictionary: */
+	if (s->strstart != 0) {
+	    putShortMSB(s, (uInt)(strm->adler >> 16));
+	    putShortMSB(s, (uInt)(strm->adler & 0xffff));
+	}
+	strm->adler = 1L;
+    }
+
+    /* Flush as much pending output as possible */
+    if (s->pending != 0) {
+        flush_pending(strm);
+        if (strm->avail_out == 0) {
+	    /* Since avail_out is 0, deflate will be called again with
+	     * more output space, but possibly with both pending and
+	     * avail_in equal to zero. There won't be anything to do,
+	     * but this is not an error situation so make sure we
+	     * return OK instead of BUF_ERROR at next call of deflate:
+             */
+	    s->last_flush = -1;
+	    return Z_OK;
+	}
+
+    /* Make sure there is something to do and avoid duplicate consecutive
+     * flushes. For repeated and useless calls with Z_FINISH, we keep
+     * returning Z_STREAM_END instead of Z_BUFF_ERROR.
+     */
+    } else if (strm->avail_in == 0 && flush <= old_flush &&
+	       flush != Z_FINISH) {
+        ERR_RETURN(strm, Z_BUF_ERROR);
+    }
+
+    /* User must not provide more input after the first FINISH: */
+    if (s->status == FINISH_STATE && strm->avail_in != 0) {
+        ERR_RETURN(strm, Z_BUF_ERROR);
+    }
+
+    /* Start a new block or continue the current one.
+     */
+    if (strm->avail_in != 0 || s->lookahead != 0 ||
+        (flush != Z_NO_FLUSH && s->status != FINISH_STATE)) {
+        block_state bstate;
+
+	bstate = (*(configuration_table[s->level].func))(s, flush);
+
+        if (bstate == finish_started || bstate == finish_done) {
+            s->status = FINISH_STATE;
+        }
+        if (bstate == need_more || bstate == finish_started) {
+	    if (strm->avail_out == 0) {
+	        s->last_flush = -1; /* avoid BUF_ERROR next call, see above */
+	    }
+	    return Z_OK;
+	    /* If flush != Z_NO_FLUSH && avail_out == 0, the next call
+	     * of deflate should use the same flush parameter to make sure
+	     * that the flush is complete. So we don't have to output an
+	     * empty block here, this will be done at next call. This also
+	     * ensures that for a very small output buffer, we emit at most
+	     * one empty block.
+	     */
+	}
+        if (bstate == block_done) {
+            if (flush == Z_PARTIAL_FLUSH) {
+                _tr_align(s);
+	    } else if (flush == Z_PACKET_FLUSH) {
+		/* Output just the 3-bit `stored' block type value,
+		   but not a zero length. */
+		_tr_stored_type_only(s);
+            } else { /* FULL_FLUSH or SYNC_FLUSH */
+                _tr_stored_block(s, (char*)0, 0L, 0);
+                /* For a full flush, this empty block will be recognized
+                 * as a special marker by inflate_sync().
+                 */
+                if (flush == Z_FULL_FLUSH) {
+                    CLEAR_HASH(s);             /* forget history */
+                }
+            }
+            flush_pending(strm);
+	    if (strm->avail_out == 0) {
+	      s->last_flush = -1; /* avoid BUF_ERROR at next call, see above */
+	      return Z_OK;
+	    }
+        }
+    }
+    Assert(strm->avail_out > 0, "bug2");
+
+//    if (flush != Z_FINISH) return Z_OK;
+    if (flush != Z_FINISH) {
+    	return Z_OK;
+    }
+    if (s->noheader) return Z_STREAM_END;
+
+    /* Write the zlib trailer (adler32) */
+    putShortMSB(s, (uInt)(strm->adler >> 16));
+    putShortMSB(s, (uInt)(strm->adler & 0xffff));
+    flush_pending(strm);
+    /* If avail_out is zero, the application will call deflate again
+     * to flush the rest.
+     */
+    s->noheader = -1; /* write the trailer only once! */
+    return s->pending != 0 ? Z_OK : Z_STREAM_END;
+}
+
+/* ========================================================================= */
+int deflateEnd (strm)
+    z_streamp strm;
+{
+    int status;
+    deflate_state *s;
+
+    if (strm == Z_NULL || strm->state == Z_NULL) return Z_STREAM_ERROR;
+    s = (deflate_state *) strm->state;
+
+    status = s->status;
+    if (status != INIT_STATE && status != BUSY_STATE &&
+	status != FINISH_STATE) {
+      return Z_STREAM_ERROR;
+    }
+
+    /* Deallocate in reverse order of allocations: */
+    TRY_FREE(strm, s->pending_buf);
+    TRY_FREE(strm, s->head);
+    TRY_FREE(strm, s->prev);
+    TRY_FREE(strm, s->window);
+
+    ZFREE(strm, s);
+    strm->state = Z_NULL;
+
+    return status == BUSY_STATE ? Z_DATA_ERROR : Z_OK;
+}
+
+/* =========================================================================
+ * Copy the source state to the destination state.
+ */
+int deflateCopy (dest, source)
+    z_streamp dest;
+    z_streamp source;
+{
+    deflate_state *ds;
+    deflate_state *ss;
+    ushf *overlay;
+
+    if (source == Z_NULL || dest == Z_NULL || source->state == Z_NULL)
+        return Z_STREAM_ERROR;
+    ss = (deflate_state *) source->state;
+
+    *dest = *source;
+
+    ds = (deflate_state *) ZALLOC(dest, 1, sizeof(deflate_state));
+    if (ds == Z_NULL) return Z_MEM_ERROR;
+    dest->state = (struct internal_state FAR *) ds;
+    *ds = *ss;
+    ds->strm = dest;
+
+    ds->window = (Bytef *) ZALLOC(dest, ds->w_size, 2*sizeof(Byte));
+    ds->prev   = (Posf *)  ZALLOC(dest, ds->w_size, sizeof(Pos));
+    ds->head   = (Posf *)  ZALLOC(dest, ds->hash_size, sizeof(Pos));
+    overlay = (ushf *) ZALLOC(dest, ds->lit_bufsize, sizeof(ush)+2);
+    ds->pending_buf = (uchf *) overlay;
+
+    if (ds->window == Z_NULL || ds->prev == Z_NULL || ds->head == Z_NULL ||
+        ds->pending_buf == Z_NULL) {
+        deflateEnd (dest);
+        return Z_MEM_ERROR;
+    }
+    /* ??? following zmemcpy doesn't work for 16-bit MSDOS */
+    zmemcpy(ds->window, ss->window, ds->w_size * 2 * sizeof(Byte));
+    zmemcpy(ds->prev, ss->prev, ds->w_size * sizeof(Pos));
+    zmemcpy(ds->head, ss->head, ds->hash_size * sizeof(Pos));
+    zmemcpy(ds->pending_buf, ss->pending_buf, (uInt)ds->pending_buf_size);
+
+    ds->pending_out = ds->pending_buf + (ss->pending_out - ss->pending_buf);
+    ds->d_buf = overlay + ds->lit_bufsize/sizeof(ush);
+    ds->l_buf = ds->pending_buf + (1+sizeof(ush))*ds->lit_bufsize;
+
+    ds->l_desc.dyn_tree = ds->dyn_ltree;
+    ds->d_desc.dyn_tree = ds->dyn_dtree;
+    ds->bl_desc.dyn_tree = ds->bl_tree;
+
+    return Z_OK;
+}
+
+/* ===========================================================================
+ * Return the number of bytes of output which are immediately available
+ * for output from the decompressor.
+ */
+int deflateOutputPending (strm)
+    z_streamp strm;
+{
+    if (strm == Z_NULL || strm->state == Z_NULL) return 0;
+    
+    return ((deflate_state *)(strm->state))->pending;
+}
+
+/* ===========================================================================
+ * Read a new buffer from the current input stream, update the adler32
+ * and total number of bytes read.  All deflate() input goes through
+ * this function so some applications may wish to modify it to avoid
+ * allocating a large strm->next_in buffer and copying from it.
+ * (See also flush_pending()).
+ */
+local int read_buf(strm, buf, size)
+    z_streamp strm;
+    charf *buf;
+    unsigned size;
+{
+    unsigned len = strm->avail_in;
+
+    if (len > size) len = size;
+    if (len == 0) return 0;
+
+    strm->avail_in  -= len;
+
+    if (!((deflate_state *)(strm->state))->noheader) {
+        strm->adler = adler32(strm->adler, strm->next_in, len);
+    }
+    zmemcpy(buf, strm->next_in, len);
+    strm->next_in  += len;
+    strm->total_in += len;
+
+    return (int)len;
+}
+
+/* ===========================================================================
+ * Initialize the "longest match" routines for a new zlib stream
+ */
+local void lm_init (s)
+    deflate_state *s;
+{
+    s->window_size = (ulg)2L*s->w_size;
+
+    CLEAR_HASH(s);
+
+    /* Set the default configuration parameters:
+     */
+    s->max_lazy_match   = configuration_table[s->level].max_lazy;
+    s->good_match       = configuration_table[s->level].good_length;
+    s->nice_match       = configuration_table[s->level].nice_length;
+    s->max_chain_length = configuration_table[s->level].max_chain;
+
+    s->strstart = 0;
+    s->block_start = 0L;
+    s->lookahead = 0;
+    s->match_length = s->prev_length = MIN_MATCH-1;
+    s->match_available = 0;
+    s->ins_h = 0;
+#ifdef ASMV
+    match_init(); /* initialize the asm code */
+#endif
+}
+
+/* ===========================================================================
+ * Set match_start to the longest match starting at the given string and
+ * return its length. Matches shorter or equal to prev_length are discarded,
+ * in which case the result is equal to prev_length and match_start is
+ * garbage.
+ * IN assertions: cur_match is the head of the hash chain for the current
+ *   string (strstart) and its distance is <= MAX_DIST, and prev_length >= 1
+ * OUT assertion: the match length is not greater than s->lookahead.
+ */
+#ifndef ASMV
+/* For 80x86 and 680x0, an optimized version will be provided in match.asm or
+ * match.S. The code will be functionally equivalent.
+ */
+local uInt longest_match(s, cur_match)
+    deflate_state *s;
+    IPos cur_match;                             /* current match */
+{
+    unsigned chain_length = s->max_chain_length;/* max hash chain length */
+    register Bytef *scan = s->window + s->strstart; /* current string */
+    register Bytef *match;                       /* matched string */
+    register int len;                           /* length of current match */
+    int best_len = s->prev_length;              /* best match length so far */
+    int nice_match = s->nice_match;             /* stop if match long enough */
+    IPos limit = s->strstart > (IPos)MAX_DIST(s) ?
+        s->strstart - (IPos)MAX_DIST(s) : NIL;
+    /* Stop when cur_match becomes <= limit. To simplify the code,
+     * we prevent matches with the string of window index 0.
+     */
+    Posf *prev = s->prev;
+    uInt wmask = s->w_mask;
+
+#ifdef UNALIGNED_OK
+    /* Compare two bytes at a time. Note: this is not always beneficial.
+     * Try with and without -DUNALIGNED_OK to check.
+     */
+    register Bytef *strend = s->window + s->strstart + MAX_MATCH - 1;
+    register ush scan_start = *(ushf*)scan;
+    register ush scan_end   = *(ushf*)(scan+best_len-1);
+#else
+    register Bytef *strend = s->window + s->strstart + MAX_MATCH;
+    register Byte scan_end1  = scan[best_len-1];
+    register Byte scan_end   = scan[best_len];
+#endif
+
+    /* The code is optimized for HASH_BITS >= 8 and MAX_MATCH-2 multiple of 16.
+     * It is easy to get rid of this optimization if necessary.
+     */
+    Assert(s->hash_bits >= 8 && MAX_MATCH == 258, "Code too clever");
+
+    /* Do not waste too much time if we already have a good match: */
+    if (s->prev_length >= s->good_match) {
+        chain_length >>= 2;
+    }
+    /* Do not look for matches beyond the end of the input. This is necessary
+     * to make deflate deterministic.
+     */
+    if ((uInt)nice_match > s->lookahead) nice_match = s->lookahead;
+
+    Assert((ulg)s->strstart <= s->window_size-MIN_LOOKAHEAD, "need lookahead");
+
+    do {
+        Assert(cur_match < s->strstart, "no future");
+        match = s->window + cur_match;
+
+        /* Skip to next match if the match length cannot increase
+         * or if the match length is less than 2:
+         */
+#if (defined(UNALIGNED_OK) && MAX_MATCH == 258)
+        /* This code assumes sizeof(unsigned short) == 2. Do not use
+         * UNALIGNED_OK if your compiler uses a different size.
+         */
+        if (*(ushf*)(match+best_len-1) != scan_end ||
+            *(ushf*)match != scan_start) continue;
+
+        /* It is not necessary to compare scan[2] and match[2] since they are
+         * always equal when the other bytes match, given that the hash keys
+         * are equal and that HASH_BITS >= 8. Compare 2 bytes at a time at
+         * strstart+3, +5, ... up to strstart+257. We check for insufficient
+         * lookahead only every 4th comparison; the 128th check will be made
+         * at strstart+257. If MAX_MATCH-2 is not a multiple of 8, it is
+         * necessary to put more guard bytes at the end of the window, or
+         * to check more often for insufficient lookahead.
+         */
+        Assert(scan[2] == match[2], "scan[2]?");
+        scan++, match++;
+        do {
+        } while (*(ushf*)(scan+=2) == *(ushf*)(match+=2) &&
+                 *(ushf*)(scan+=2) == *(ushf*)(match+=2) &&
+                 *(ushf*)(scan+=2) == *(ushf*)(match+=2) &&
+                 *(ushf*)(scan+=2) == *(ushf*)(match+=2) &&
+                 scan < strend);
+        /* The funny "do {}" generates better code on most compilers */
+
+        /* Here, scan <= window+strstart+257 */
+        Assert(scan <= s->window+(unsigned)(s->window_size-1), "wild scan");
+        if (*scan == *match) scan++;
+
+        len = (MAX_MATCH - 1) - (int)(strend-scan);
+        scan = strend - (MAX_MATCH-1);
+
+#else /* UNALIGNED_OK */
+
+        if (match[best_len]   != scan_end  ||
+            match[best_len-1] != scan_end1 ||
+            *match            != *scan     ||
+            *++match          != scan[1])      continue;
+
+        /* The check at best_len-1 can be removed because it will be made
+         * again later. (This heuristic is not always a win.)
+         * It is not necessary to compare scan[2] and match[2] since they
+         * are always equal when the other bytes match, given that
+         * the hash keys are equal and that HASH_BITS >= 8.
+         */
+        scan += 2, match++;
+        Assert(*scan == *match, "match[2]?");
+
+        /* We check for insufficient lookahead only every 8th comparison;
+         * the 256th check will be made at strstart+258.
+         */
+        do {
+        } while (*++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 scan < strend);
+
+        Assert(scan <= s->window+(unsigned)(s->window_size-1), "wild scan");
+
+        len = MAX_MATCH - (int)(strend - scan);
+        scan = strend - MAX_MATCH;
+
+#endif /* UNALIGNED_OK */
+
+        if (len > best_len) {
+            s->match_start = cur_match;
+            best_len = len;
+            if (len >= nice_match) break;
+#ifdef UNALIGNED_OK
+            scan_end = *(ushf*)(scan+best_len-1);
+#else
+            scan_end1  = scan[best_len-1];
+            scan_end   = scan[best_len];
+#endif
+        }
+    } while ((cur_match = prev[cur_match & wmask]) > limit
+             && --chain_length != 0);
+
+    if ((uInt)best_len <= s->lookahead) return best_len;
+    return s->lookahead;
+}
+#endif /* ASMV */
+
+#ifdef DEBUG_ZLIB
+/* ===========================================================================
+ * Check that the match at match_start is indeed a match.
+ */
+local void check_match(s, start, match, length)
+    deflate_state *s;
+    IPos start, match;
+    int length;
+{
+    /* check that the match is indeed a match */
+    if (zmemcmp((charf *)s->window + match,
+                (charf *)s->window + start, length) != EQUAL) {
+        fprintf(stderr, " start %u, match %u, length %d\n",
+		start, match, length);
+        do {
+	    fprintf(stderr, "%c%c", s->window[match++], s->window[start++]);
+	} while (--length != 0);
+        z_error("invalid match");
+    }
+    if (z_verbose > 1) {
+        fprintf(stderr,"\\[%d,%d]", start-match, length);
+        do { putc(s->window[start++], stderr); } while (--length != 0);
+    }
+}
+#else
+#  define check_match(s, start, match, length)
+#endif
+
+/* ===========================================================================
+ * Fill the window when the lookahead becomes insufficient.
+ * Updates strstart and lookahead.
+ *
+ * IN assertion: lookahead < MIN_LOOKAHEAD
+ * OUT assertions: strstart <= window_size-MIN_LOOKAHEAD
+ *    At least one byte has been read, or avail_in == 0; reads are
+ *    performed for at least two bytes (required for the zip translate_eol
+ *    option -- not supported here).
+ */
+local void fill_window(s)
+    deflate_state *s;
+{
+    register unsigned n, m;
+    register Posf *p;
+    unsigned more;    /* Amount of free space at the end of the window. */
+    uInt wsize = s->w_size;
+
+    do {
+        more = (unsigned)(s->window_size -(ulg)s->lookahead -(ulg)s->strstart);
+
+        /* Deal with !@#$% 64K limit: */
+        if (more == 0 && s->strstart == 0 && s->lookahead == 0) {
+            more = wsize;
+
+        } else if (more == (unsigned)(-1)) {
+            /* Very unlikely, but possible on 16 bit machine if strstart == 0
+             * and lookahead == 1 (input done one byte at time)
+             */
+            more--;
+
+        /* If the window is almost full and there is insufficient lookahead,
+         * move the upper half to the lower one to make room in the upper half.
+         */
+        } else if (s->strstart >= wsize+MAX_DIST(s)) {
+
+            zmemcpy((charf *)s->window, (charf *)s->window+wsize,
+                   (unsigned)wsize);
+            s->match_start -= wsize;
+            s->strstart    -= wsize; /* we now have strstart >= MAX_DIST */
+            s->block_start -= (long) wsize;
+
+            /* Slide the hash table (could be avoided with 32 bit values
+               at the expense of memory usage). We slide even when level == 0
+               to keep the hash table consistent if we switch back to level > 0
+               later. (Using level 0 permanently is not an optimal usage of
+               zlib, so we don't care about this pathological case.)
+             */
+            n = s->hash_size;
+            p = &s->head[n];
+            do {
+                m = *--p;
+                *p = (Pos)(m >= wsize ? m-wsize : NIL);
+            } while (--n);
+
+            n = wsize;
+            p = &s->prev[n];
+            do {
+                m = *--p;
+                *p = (Pos)(m >= wsize ? m-wsize : NIL);
+                /* If n is not on any hash chain, prev[n] is garbage but
+                 * its value will never be used.
+                 */
+            } while (--n);
+            more += wsize;
+        }
+        if (s->strm->avail_in == 0) return;
+
+        /* If there was no sliding:
+         *    strstart <= WSIZE+MAX_DIST-1 && lookahead <= MIN_LOOKAHEAD - 1 &&
+         *    more == window_size - lookahead - strstart
+         * => more >= window_size - (MIN_LOOKAHEAD-1 + WSIZE + MAX_DIST-1)
+         * => more >= window_size - 2*WSIZE + 2
+         * In the BIG_MEM or MMAP case (not yet supported),
+         *   window_size == input_size + MIN_LOOKAHEAD  &&
+         *   strstart + s->lookahead <= input_size => more >= MIN_LOOKAHEAD.
+         * Otherwise, window_size == 2*WSIZE so more >= 2.
+         * If there was sliding, more >= WSIZE. So in all cases, more >= 2.
+         */
+        Assert(more >= 2, "more < 2");
+
+        n = read_buf(s->strm, (charf *)s->window + s->strstart + s->lookahead,
+                     more);
+        s->lookahead += n;
+
+        /* Initialize the hash value now that we have some input: */
+        if (s->lookahead >= MIN_MATCH) {
+            s->ins_h = s->window[s->strstart];
+            UPDATE_HASH(s, s->ins_h, s->window[s->strstart+1]);
+#if MIN_MATCH != 3
+            Call UPDATE_HASH() MIN_MATCH-3 more times
+#endif
+        }
+        /* If the whole input has less than MIN_MATCH bytes, ins_h is garbage,
+         * but this is not important since only literal bytes will be emitted.
+         */
+
+    } while (s->lookahead < MIN_LOOKAHEAD && s->strm->avail_in != 0);
+}
+
+/* ===========================================================================
+ * Flush the current block, with given end-of-file flag.
+ * IN assertion: strstart is set to the end of the current match.
+ */
+#define FLUSH_BLOCK_ONLY(s, eof) { \
+   _tr_flush_block(s, (s->block_start >= 0L ? \
+                   (charf *)&s->window[(unsigned)s->block_start] : \
+                   (charf *)Z_NULL), \
+		(ulg)((long)s->strstart - s->block_start), \
+		(eof)); \
+   s->block_start = s->strstart; \
+   flush_pending(s->strm); \
+   Tracev((stderr,"[FLUSH]")); \
+}
+
+/* Same but force premature exit if necessary. */
+#define FLUSH_BLOCK(s, eof) { \
+   FLUSH_BLOCK_ONLY(s, eof); \
+   if (s->strm->avail_out == 0) return (eof) ? finish_started : need_more; \
+}
+
+/* ===========================================================================
+ * Copy without compression as much as possible from the input stream, return
+ * the current block state.
+ * This function does not insert new strings in the dictionary since
+ * uncompressible data is probably not useful. This function is used
+ * only for the level=0 compression option.
+ * NOTE: this function should be optimized to avoid extra copying from
+ * window to pending_buf.
+ */
+local block_state deflate_stored(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    /* Stored blocks are limited to 0xffff bytes, pending_buf is limited
+     * to pending_buf_size, and each stored block has a 5 byte header:
+     */
+    ulg max_block_size = 0xffff;
+    ulg max_start;
+
+    if (max_block_size > s->pending_buf_size - 5) {
+        max_block_size = s->pending_buf_size - 5;
+    }
+
+    /* Copy as much as possible from input to output: */
+    for (;;) {
+        /* Fill the window as much as possible: */
+        if (s->lookahead <= 1) {
+
+            Assert(s->strstart < s->w_size+MAX_DIST(s) ||
+		   s->block_start >= (long)s->w_size, "slide too late");
+
+            fill_window(s);
+            if (s->lookahead == 0 && flush == Z_NO_FLUSH) return need_more;
+
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+	Assert(s->block_start >= 0L, "block gone");
+
+	s->strstart += s->lookahead;
+	s->lookahead = 0;
+
+	/* Emit a stored block if pending_buf will be full: */
+ 	max_start = s->block_start + max_block_size;
+        if (s->strstart == 0 || (ulg)s->strstart >= max_start) {
+	    /* strstart == 0 is possible when wraparound on 16-bit machine */
+	    s->lookahead = (uInt)(s->strstart - max_start);
+	    s->strstart = (uInt)max_start;
+            FLUSH_BLOCK(s, 0);
+	}
+	/* Flush if we may have to slide, otherwise block_start may become
+         * negative and the data will be gone:
+         */
+        if (s->strstart - (uInt)s->block_start >= MAX_DIST(s)) {
+            FLUSH_BLOCK(s, 0);
+	}
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
+
+/* ===========================================================================
+ * Compress as much as possible from the input stream, return the current
+ * block state.
+ * This function does not perform lazy evaluation of matches and inserts
+ * new strings in the dictionary only for unmatched strings or for short
+ * matches. It is used only for the fast compression options.
+ */
+local block_state deflate_fast(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    IPos hash_head = NIL; /* head of the hash chain */
+    int bflush;           /* set if current block must be flushed */
+
+    for (;;) {
+        /* Make sure that we always have enough lookahead, except
+         * at the end of the input file. We need MAX_MATCH bytes
+         * for the next match, plus MIN_MATCH bytes to insert the
+         * string following the next match.
+         */
+        if (s->lookahead < MIN_LOOKAHEAD) {
+            fill_window(s);
+            if (s->lookahead < MIN_LOOKAHEAD && flush == Z_NO_FLUSH) {
+	        return need_more;
+	    }
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+
+        /* Insert the string window[strstart .. strstart+2] in the
+         * dictionary, and set hash_head to the head of the hash chain:
+         */
+        if (s->lookahead >= MIN_MATCH) {
+            INSERT_STRING(s, s->strstart, hash_head);
+        }
+
+        /* Find the longest match, discarding those <= prev_length.
+         * At this point we have always match_length < MIN_MATCH
+         */
+        if (hash_head != NIL && s->strstart - hash_head <= MAX_DIST(s)) {
+            /* To simplify the code, we prevent matches with the string
+             * of window index 0 (in particular we have to avoid a match
+             * of the string with itself at the start of the input file).
+             */
+            if (s->strategy != Z_HUFFMAN_ONLY) {
+                s->match_length = longest_match (s, hash_head);
+            }
+            /* longest_match() sets match_start */
+        }
+        if (s->match_length >= MIN_MATCH) {
+            check_match(s, s->strstart, s->match_start, s->match_length);
+
+            bflush = _tr_tally(s, s->strstart - s->match_start,
+                               s->match_length - MIN_MATCH);
+
+            s->lookahead -= s->match_length;
+
+            /* Insert new strings in the hash table only if the match length
+             * is not too large. This saves time but degrades compression.
+             */
+            if (s->match_length <= s->max_insert_length &&
+                s->lookahead >= MIN_MATCH) {
+                s->match_length--; /* string at strstart already in hash table */
+                do {
+                    s->strstart++;
+                    INSERT_STRING(s, s->strstart, hash_head);
+                    /* strstart never exceeds WSIZE-MAX_MATCH, so there are
+                     * always MIN_MATCH bytes ahead.
+                     */
+                } while (--s->match_length != 0);
+                s->strstart++; 
+            } else {
+                s->strstart += s->match_length;
+                s->match_length = 0;
+                s->ins_h = s->window[s->strstart];
+                UPDATE_HASH(s, s->ins_h, s->window[s->strstart+1]);
+#if MIN_MATCH != 3
+                Call UPDATE_HASH() MIN_MATCH-3 more times
+#endif
+                /* If lookahead < MIN_MATCH, ins_h is garbage, but it does not
+                 * matter since it will be recomputed at next deflate call.
+                 */
+            }
+        } else {
+            /* No match, output a literal byte */
+            Tracevv((stderr,"%c", s->window[s->strstart]));
+            bflush = _tr_tally (s, 0, s->window[s->strstart]);
+            s->lookahead--;
+            s->strstart++; 
+        }
+        if (bflush) FLUSH_BLOCK(s, 0);
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
+
+/* ===========================================================================
+ * Same as above, but achieves better compression. We use a lazy
+ * evaluation for matches: a match is finally adopted only if there is
+ * no better match at the next window position.
+ */
+local block_state deflate_slow(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    IPos hash_head = NIL;    /* head of hash chain */
+    int bflush;              /* set if current block must be flushed */
+
+    /* Process the input block. */
+    for (;;) {
+        /* Make sure that we always have enough lookahead, except
+         * at the end of the input file. We need MAX_MATCH bytes
+         * for the next match, plus MIN_MATCH bytes to insert the
+         * string following the next match.
+         */
+        if (s->lookahead < MIN_LOOKAHEAD) {
+            fill_window(s);
+            if (s->lookahead < MIN_LOOKAHEAD && flush == Z_NO_FLUSH) {
+	        return need_more;
+	    }
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+
+        /* Insert the string window[strstart .. strstart+2] in the
+         * dictionary, and set hash_head to the head of the hash chain:
+         */
+        if (s->lookahead >= MIN_MATCH) {
+            INSERT_STRING(s, s->strstart, hash_head);
+        }
+
+        /* Find the longest match, discarding those <= prev_length.
+         */
+        s->prev_length = s->match_length, s->prev_match = s->match_start;
+        s->match_length = MIN_MATCH-1;
+
+        if (hash_head != NIL && s->prev_length < s->max_lazy_match &&
+            s->strstart - hash_head <= MAX_DIST(s)) {
+            /* To simplify the code, we prevent matches with the string
+             * of window index 0 (in particular we have to avoid a match
+             * of the string with itself at the start of the input file).
+             */
+            if (s->strategy != Z_HUFFMAN_ONLY) {
+                s->match_length = longest_match (s, hash_head);
+            }
+            /* longest_match() sets match_start */
+
+            if (s->match_length <= 5 && (s->strategy == Z_FILTERED ||
+                 (s->match_length == MIN_MATCH &&
+                  s->strstart - s->match_start > TOO_FAR))) {
+
+                /* If prev_match is also MIN_MATCH, match_start is garbage
+                 * but we will ignore the current match anyway.
+                 */
+                s->match_length = MIN_MATCH-1;
+            }
+        }
+        /* If there was a match at the previous step and the current
+         * match is not better, output the previous match:
+         */
+        if (s->prev_length >= MIN_MATCH && s->match_length <= s->prev_length) {
+            uInt max_insert = s->strstart + s->lookahead - MIN_MATCH;
+            /* Do not insert strings in hash table beyond this. */
+
+            check_match(s, s->strstart-1, s->prev_match, s->prev_length);
+
+            bflush = _tr_tally(s, s->strstart -1 - s->prev_match,
+                               s->prev_length - MIN_MATCH);
+
+            /* Insert in hash table all strings up to the end of the match.
+             * strstart-1 and strstart are already inserted. If there is not
+             * enough lookahead, the last two strings are not inserted in
+             * the hash table.
+             */
+            s->lookahead -= s->prev_length-1;
+            s->prev_length -= 2;
+            do {
+                if (++s->strstart <= max_insert) {
+                    INSERT_STRING(s, s->strstart, hash_head);
+                }
+            } while (--s->prev_length != 0);
+            s->match_available = 0;
+            s->match_length = MIN_MATCH-1;
+            s->strstart++;
+
+            if (bflush) FLUSH_BLOCK(s, 0);
+
+        } else if (s->match_available) {
+            /* If there was no match at the previous position, output a
+             * single literal. If there was a match but the current match
+             * is longer, truncate the previous match to a single literal.
+             */
+            Tracevv((stderr,"%c", s->window[s->strstart-1]));
+            if (_tr_tally (s, 0, s->window[s->strstart-1])) {
+                FLUSH_BLOCK_ONLY(s, 0);
+            }
+            s->strstart++;
+            s->lookahead--;
+            if (s->strm->avail_out == 0) return need_more;
+        } else {
+            /* There is no previous match to compare with, wait for
+             * the next step to decide.
+             */
+            s->match_available = 1;
+            s->strstart++;
+            s->lookahead--;
+        }
+    }
+    Assert (flush != Z_NO_FLUSH, "no flush?");
+    if (s->match_available) {
+        Tracevv((stderr,"%c", s->window[s->strstart-1]));
+        _tr_tally (s, 0, s->window[s->strstart-1]);
+        s->match_available = 0;
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
+/* --- deflate.c */
+
+/* +++ trees.c */
+/* trees.c -- output deflated data using Huffman coding
+ * Copyright (C) 1995-1996 Jean-loup Gailly
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/*
+ *  ALGORITHM
+ *
+ *      The "deflation" process uses several Huffman trees. The more
+ *      common source values are represented by shorter bit sequences.
+ *
+ *      Each code tree is stored in a compressed form which is itself
+ * a Huffman encoding of the lengths of all the code strings (in
+ * ascending order by source values).  The actual code strings are
+ * reconstructed from the lengths in the inflate process, as described
+ * in the deflate specification.
+ *
+ *  REFERENCES
+ *
+ *      Deutsch, L.P.,"'Deflate' Compressed Data Format Specification".
+ *      Available in ftp.uu.net:/pub/archiving/zip/doc/deflate-1.1.doc
+ *
+ *      Storer, James A.
+ *          Data Compression:  Methods and Theory, pp. 49-50.
+ *          Computer Science Press, 1988.  ISBN 0-7167-8156-5.
+ *
+ *      Sedgewick, R.
+ *          Algorithms, p290.
+ *          Addison-Wesley, 1983. ISBN 0-201-06672-6.
+ */
+
+/* From: trees.c,v 1.11 1996/07/24 13:41:06 me Exp $ */
+
+/* #include "deflate.h" */
+
+#ifdef DEBUG_ZLIB
+#  include <ctype.h>
+#endif
+
+/* ===========================================================================
+ * Constants
+ */
+
+#define MAX_BL_BITS 7
+/* Bit length codes must not exceed MAX_BL_BITS bits */
+
+#define END_BLOCK 256
+/* end of block literal code */
+
+#define REP_3_6      16
+/* repeat previous bit length 3-6 times (2 bits of repeat count) */
+
+#define REPZ_3_10    17
+/* repeat a zero length 3-10 times  (3 bits of repeat count) */
+
+#define REPZ_11_138  18
+/* repeat a zero length 11-138 times  (7 bits of repeat count) */
+
+local int extra_lbits[LENGTH_CODES] /* extra bits for each length code */
+   = {0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,0};
+
+local int extra_dbits[D_CODES] /* extra bits for each distance code */
+   = {0,0,0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13};
+
+local int extra_blbits[BL_CODES]/* extra bits for each bit length code */
+   = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,3,7};
+
+local uch bl_order[BL_CODES]
+   = {16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15};
+/* The lengths of the bit length codes are sent in order of decreasing
+ * probability, to avoid transmitting the lengths for unused bit length codes.
+ */
+
+#define Buf_size (8 * 2*sizeof(char))
+/* Number of bits used within bi_buf. (bi_buf might be implemented on
+ * more than 16 bits on some systems.)
+ */
+
+/* ===========================================================================
+ * Local data. These are initialized only once.
+ */
+
+local ct_data static_ltree[L_CODES+2];
+/* The static literal tree. Since the bit lengths are imposed, there is no
+ * need for the L_CODES extra codes used during heap construction. However
+ * The codes 286 and 287 are needed to build a canonical tree (see _tr_init
+ * below).
+ */
+
+local ct_data static_dtree[D_CODES];
+/* The static distance tree. (Actually a trivial tree since all codes use
+ * 5 bits.)
+ */
+
+local uch dist_code[512];
+/* distance codes. The first 256 values correspond to the distances
+ * 3 .. 258, the last 256 values correspond to the top 8 bits of
+ * the 15 bit distances.
+ */
+
+local uch length_code[MAX_MATCH-MIN_MATCH+1];
+/* length code for each normalized match length (0 == MIN_MATCH) */
+
+local int base_length[LENGTH_CODES];
+/* First normalized length for each code (0 = MIN_MATCH) */
+
+local int base_dist[D_CODES];
+/* First normalized distance for each code (0 = distance of 1) */
+
+struct static_tree_desc_s {
+    ct_data *static_tree;        /* static tree or NULL */
+    intf    *extra_bits;         /* extra bits for each code or NULL */
+    int     extra_base;          /* base index for extra_bits */
+    int     elems;               /* max number of elements in the tree */
+    int     max_length;          /* max bit length for the codes */
+};
+
+local static_tree_desc  static_l_desc =
+{static_ltree, extra_lbits, LITERALS+1, L_CODES, MAX_BITS};
+
+local static_tree_desc  static_d_desc =
+{static_dtree, extra_dbits, 0,          D_CODES, MAX_BITS};
+
+local static_tree_desc  static_bl_desc =
+{(ct_data *)0, extra_blbits, 0,      BL_CODES, MAX_BL_BITS};
+
+/* ===========================================================================
+ * Local (static) routines in this file.
+ */
+
+local void tr_static_init OF((void));
+local void init_block     OF((deflate_state *s));
+local void pqdownheap     OF((deflate_state *s, ct_data *tree, int k));
+local void gen_bitlen     OF((deflate_state *s, tree_desc *desc));
+local void gen_codes      OF((ct_data *tree, int max_code, ushf *bl_count));
+local void build_tree     OF((deflate_state *s, tree_desc *desc));
+local void scan_tree      OF((deflate_state *s, ct_data *tree, int max_code));
+local void send_tree      OF((deflate_state *s, ct_data *tree, int max_code));
+local int  build_bl_tree  OF((deflate_state *s));
+local void send_all_trees OF((deflate_state *s, int lcodes, int dcodes,
+                              int blcodes));
+local void compress_block OF((deflate_state *s, ct_data *ltree,
+                              ct_data *dtree));
+local void set_data_type  OF((deflate_state *s));
+local unsigned bi_reverse OF((unsigned value, int length));
+local void bi_windup      OF((deflate_state *s));
+local void bi_flush       OF((deflate_state *s));
+local void copy_block     OF((deflate_state *s, charf *buf, unsigned len,
+                              int header));
+
+#ifndef DEBUG_ZLIB
+#  define send_code(s, c, tree) send_bits(s, tree[c].Code, tree[c].Len)
+   /* Send a code of the given tree. c and tree must not have side effects */
+
+#else /* DEBUG_ZLIB */
+#  define send_code(s, c, tree) \
+     { if (verbose>2) fprintf(stderr,"\ncd %3d ",(c)); \
+       send_bits(s, tree[c].Code, tree[c].Len); }
+#endif
+
+#define d_code(dist) \
+   ((dist) < 256 ? dist_code[dist] : dist_code[256+((dist)>>7)])
+/* Mapping from a distance to a distance code. dist is the distance - 1 and
+ * must not have side effects. dist_code[256] and dist_code[257] are never
+ * used.
+ */
+
+/* ===========================================================================
+ * Output a short LSB first on the stream.
+ * IN assertion: there is enough room in pendingBuf.
+ */
+#define put_short(s, w) { \
+    put_byte(s, (uch)((w) & 0xff)); \
+    put_byte(s, (uch)((ush)(w) >> 8)); \
+}
+
+/* ===========================================================================
+ * Send a value on a given number of bits.
+ * IN assertion: length <= 16 and value fits in length bits.
+ */
+#ifdef DEBUG_ZLIB
+local void send_bits      OF((deflate_state *s, int value, int length));
+
+local void send_bits(s, value, length)
+    deflate_state *s;
+    int value;  /* value to send */
+    int length; /* number of bits */
+{
+    Tracevv((stderr," l %2d v %4x ", length, value));
+    Assert(length > 0 && length <= 15, "invalid length");
+    s->bits_sent += (ulg)length;
+
+    /* If not enough room in bi_buf, use (valid) bits from bi_buf and
+     * (16 - bi_valid) bits from value, leaving (width - (16-bi_valid))
+     * unused bits in value.
+     */
+    if (s->bi_valid > (int)Buf_size - length) {
+        s->bi_buf |= (value << s->bi_valid);
+        put_short(s, s->bi_buf);
+        s->bi_buf = (ush)value >> (Buf_size - s->bi_valid);
+        s->bi_valid += length - Buf_size;
+    } else {
+        s->bi_buf |= value << s->bi_valid;
+        s->bi_valid += length;
+    }
+}
+#else /* !DEBUG_ZLIB */
+
+#define send_bits(s, value, length) \
+{ int len = length;\
+  if (s->bi_valid > (int)Buf_size - len) {\
+    int val = value;\
+    s->bi_buf |= (val << s->bi_valid);\
+    put_short(s, s->bi_buf);\
+    s->bi_buf = (ush)val >> (Buf_size - s->bi_valid);\
+    s->bi_valid += len - Buf_size;\
+  } else {\
+    s->bi_buf |= (value) << s->bi_valid;\
+    s->bi_valid += len;\
+  }\
+}
+#endif /* DEBUG_ZLIB */
+
+
+#define MAX(a,b) (a >= b ? a : b)
+/* the arguments must not have side effects */
+
+/* ===========================================================================
+ * Initialize the various 'constant' tables. In a multi-threaded environment,
+ * this function may be called by two threads concurrently, but this is
+ * harmless since both invocations do exactly the same thing.
+ */
+local void tr_static_init()
+{
+    static int static_init_done;
+    int n;        /* iterates over tree elements */
+    int bits;     /* bit counter */
+    int length;   /* length value */
+    int code;     /* code value */
+    int dist;     /* distance index */
+    ush bl_count[MAX_BITS+1];
+    /* number of codes at each bit length for an optimal tree */
+
+    if (static_init_done) return;
+
+    /* Initialize the mapping length (0..255) -> length code (0..28) */
+    length = 0;
+    for (code = 0; code < LENGTH_CODES-1; code++) {
+        base_length[code] = length;
+        for (n = 0; n < (1<<extra_lbits[code]); n++) {
+            length_code[length++] = (uch)code;
+        }
+    }
+    Assert (length == 256, "tr_static_init: length != 256");
+    /* Note that the length 255 (match length 258) can be represented
+     * in two different ways: code 284 + 5 bits or code 285, so we
+     * overwrite length_code[255] to use the best encoding:
+     */
+    length_code[length-1] = (uch)code;
+
+    /* Initialize the mapping dist (0..32K) -> dist code (0..29) */
+    dist = 0;
+    for (code = 0 ; code < 16; code++) {
+        base_dist[code] = dist;
+        for (n = 0; n < (1<<extra_dbits[code]); n++) {
+            dist_code[dist++] = (uch)code;
+        }
+    }
+    Assert (dist == 256, "tr_static_init: dist != 256");
+    dist >>= 7; /* from now on, all distances are divided by 128 */
+    for ( ; code < D_CODES; code++) {
+        base_dist[code] = dist << 7;
+        for (n = 0; n < (1<<(extra_dbits[code]-7)); n++) {
+            dist_code[256 + dist++] = (uch)code;
+        }
+    }
+    Assert (dist == 256, "tr_static_init: 256+dist != 512");
+
+    /* Construct the codes of the static literal tree */
+    for (bits = 0; bits <= MAX_BITS; bits++) bl_count[bits] = 0;
+    n = 0;
+    while (n <= 143) static_ltree[n++].Len = 8, bl_count[8]++;
+    while (n <= 255) static_ltree[n++].Len = 9, bl_count[9]++;
+    while (n <= 279) static_ltree[n++].Len = 7, bl_count[7]++;
+    while (n <= 287) static_ltree[n++].Len = 8, bl_count[8]++;
+    /* Codes 286 and 287 do not exist, but we must include them in the
+     * tree construction to get a canonical Huffman tree (longest code
+     * all ones)
+     */
+    gen_codes((ct_data *)static_ltree, L_CODES+1, bl_count);
+
+    /* The static distance tree is trivial: */
+    for (n = 0; n < D_CODES; n++) {
+        static_dtree[n].Len = 5;
+        static_dtree[n].Code = bi_reverse((unsigned)n, 5);
+    }
+    static_init_done = 1;
+}
+
+/* ===========================================================================
+ * Initialize the tree data structures for a new zlib stream.
+ */
+void _tr_init(s)
+    deflate_state *s;
+{
+    tr_static_init();
+
+    s->compressed_len = 0L;
+
+    s->l_desc.dyn_tree = s->dyn_ltree;
+    s->l_desc.stat_desc = &static_l_desc;
+
+    s->d_desc.dyn_tree = s->dyn_dtree;
+    s->d_desc.stat_desc = &static_d_desc;
+
+    s->bl_desc.dyn_tree = s->bl_tree;
+    s->bl_desc.stat_desc = &static_bl_desc;
+
+    s->bi_buf = 0;
+    s->bi_valid = 0;
+    s->last_eob_len = 8; /* enough lookahead for inflate */
+#ifdef DEBUG_ZLIB
+    s->bits_sent = 0L;
+#endif
+
+    /* Initialize the first block of the first file: */
+    init_block(s);
+}
+
+/* ===========================================================================
+ * Initialize a new block.
+ */
+local void init_block(s)
+    deflate_state *s;
+{
+    int n; /* iterates over tree elements */
+
+    /* Initialize the trees. */
+    for (n = 0; n < L_CODES;  n++) s->dyn_ltree[n].Freq = 0;
+    for (n = 0; n < D_CODES;  n++) s->dyn_dtree[n].Freq = 0;
+    for (n = 0; n < BL_CODES; n++) s->bl_tree[n].Freq = 0;
+
+    s->dyn_ltree[END_BLOCK].Freq = 1;
+    s->opt_len = s->static_len = 0L;
+    s->last_lit = s->matches = 0;
+}
+
+#define SMALLEST 1
+/* Index within the heap array of least frequent node in the Huffman tree */
+
+
+/* ===========================================================================
+ * Remove the smallest element from the heap and recreate the heap with
+ * one less element. Updates heap and heap_len.
+ */
+#define pqremove(s, tree, top) \
+{\
+    top = s->heap[SMALLEST]; \
+    s->heap[SMALLEST] = s->heap[s->heap_len--]; \
+    pqdownheap(s, tree, SMALLEST); \
+}
+
+/* ===========================================================================
+ * Compares to subtrees, using the tree depth as tie breaker when
+ * the subtrees have equal frequency. This minimizes the worst case length.
+ */
+#define smaller(tree, n, m, depth) \
+   (tree[n].Freq < tree[m].Freq || \
+   (tree[n].Freq == tree[m].Freq && depth[n] <= depth[m]))
+
+/* ===========================================================================
+ * Restore the heap property by moving down the tree starting at node k,
+ * exchanging a node with the smallest of its two sons if necessary, stopping
+ * when the heap property is re-established (each father smaller than its
+ * two sons).
+ */
+local void pqdownheap(s, tree, k)
+    deflate_state *s;
+    ct_data *tree;  /* the tree to restore */
+    int k;               /* node to move down */
+{
+    int v = s->heap[k];
+    int j = k << 1;  /* left son of k */
+    while (j <= s->heap_len) {
+        /* Set j to the smallest of the two sons: */
+        if (j < s->heap_len &&
+            smaller(tree, s->heap[j+1], s->heap[j], s->depth)) {
+            j++;
+        }
+        /* Exit if v is smaller than both sons */
+        if (smaller(tree, v, s->heap[j], s->depth)) break;
+
+        /* Exchange v with the smallest son */
+        s->heap[k] = s->heap[j];  k = j;
+
+        /* And continue down the tree, setting j to the left son of k */
+        j <<= 1;
+    }
+    s->heap[k] = v;
+}
+
+/* ===========================================================================
+ * Compute the optimal bit lengths for a tree and update the total bit length
+ * for the current block.
+ * IN assertion: the fields freq and dad are set, heap[heap_max] and
+ *    above are the tree nodes sorted by increasing frequency.
+ * OUT assertions: the field len is set to the optimal bit length, the
+ *     array bl_count contains the frequencies for each bit length.
+ *     The length opt_len is updated; static_len is also updated if stree is
+ *     not null.
+ */
+local void gen_bitlen(s, desc)
+    deflate_state *s;
+    tree_desc *desc;    /* the tree descriptor */
+{
+    ct_data *tree  = desc->dyn_tree;
+    int max_code   = desc->max_code;
+    ct_data *stree = desc->stat_desc->static_tree;
+    intf *extra    = desc->stat_desc->extra_bits;
+    int base       = desc->stat_desc->extra_base;
+    int max_length = desc->stat_desc->max_length;
+    int h;              /* heap index */
+    int n, m;           /* iterate over the tree elements */
+    int bits;           /* bit length */
+    int xbits;          /* extra bits */
+    ush f;              /* frequency */
+    int overflow = 0;   /* number of elements with bit length too large */
+
+    for (bits = 0; bits <= MAX_BITS; bits++) s->bl_count[bits] = 0;
+
+    /* In a first pass, compute the optimal bit lengths (which may
+     * overflow in the case of the bit length tree).
+     */
+    tree[s->heap[s->heap_max]].Len = 0; /* root of the heap */
+
+    for (h = s->heap_max+1; h < HEAP_SIZE; h++) {
+        n = s->heap[h];
+        bits = tree[tree[n].Dad].Len + 1;
+        if (bits > max_length) bits = max_length, overflow++;
+        tree[n].Len = (ush)bits;
+        /* We overwrite tree[n].Dad which is no longer needed */
+
+        if (n > max_code) continue; /* not a leaf node */
+
+        s->bl_count[bits]++;
+        xbits = 0;
+        if (n >= base) xbits = extra[n-base];
+        f = tree[n].Freq;
+        s->opt_len += (ulg)f * (bits + xbits);
+        if (stree) s->static_len += (ulg)f * (stree[n].Len + xbits);
+    }
+    if (overflow == 0) return;
+
+    Trace((stderr,"\nbit length overflow\n"));
+    /* This happens for example on obj2 and pic of the Calgary corpus */
+
+    /* Find the first bit length which could increase: */
+    do {
+        bits = max_length-1;
+        while (s->bl_count[bits] == 0) bits--;
+        s->bl_count[bits]--;      /* move one leaf down the tree */
+        s->bl_count[bits+1] += 2; /* move one overflow item as its brother */
+        s->bl_count[max_length]--;
+        /* The brother of the overflow item also moves one step up,
+         * but this does not affect bl_count[max_length]
+         */
+        overflow -= 2;
+    } while (overflow > 0);
+
+    /* Now recompute all bit lengths, scanning in increasing frequency.
+     * h is still equal to HEAP_SIZE. (It is simpler to reconstruct all
+     * lengths instead of fixing only the wrong ones. This idea is taken
+     * from 'ar' written by Haruhiko Okumura.)
+     */
+    for (bits = max_length; bits != 0; bits--) {
+        n = s->bl_count[bits];
+        while (n != 0) {
+            m = s->heap[--h];
+            if (m > max_code) continue;
+            if (tree[m].Len != (unsigned) bits) {
+                Trace((stderr,"code %d bits %d->%d\n", m, tree[m].Len, bits));
+                s->opt_len += ((long)bits - (long)tree[m].Len)
+                              *(long)tree[m].Freq;
+                tree[m].Len = (ush)bits;
+            }
+            n--;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Generate the codes for a given tree and bit counts (which need not be
+ * optimal).
+ * IN assertion: the array bl_count contains the bit length statistics for
+ * the given tree and the field len is set for all tree elements.
+ * OUT assertion: the field code is set for all tree elements of non
+ *     zero code length.
+ */
+local void gen_codes (tree, max_code, bl_count)
+    ct_data *tree;             /* the tree to decorate */
+    int max_code;              /* largest code with non zero frequency */
+    ushf *bl_count;            /* number of codes at each bit length */
+{
+    ush next_code[MAX_BITS+1]; /* next code value for each bit length */
+    ush code = 0;              /* running code value */
+    int bits;                  /* bit index */
+    int n;                     /* code index */
+
+    /* The distribution counts are first used to generate the code values
+     * without bit reversal.
+     */
+    for (bits = 1; bits <= MAX_BITS; bits++) {
+        next_code[bits] = code = (code + bl_count[bits-1]) << 1;
+    }
+    /* Check that the bit counts in bl_count are consistent. The last code
+     * must be all ones.
+     */
+    Assert (code + bl_count[MAX_BITS]-1 == (1<<MAX_BITS)-1,
+            "inconsistent bit counts");
+    Tracev((stderr,"\ngen_codes: max_code %d ", max_code));
+
+    for (n = 0;  n <= max_code; n++) {
+        int len = tree[n].Len;
+        if (len == 0) continue;
+        /* Now reverse the bits */
+        tree[n].Code = bi_reverse(next_code[len]++, len);
+
+        Tracecv(tree != static_ltree, (stderr,"\nn %3d %c l %2d c %4x (%x) ",
+             n, (isgraph(n) ? n : ' '), len, tree[n].Code, next_code[len]-1));
+    }
+}
+
+/* ===========================================================================
+ * Construct one Huffman tree and assigns the code bit strings and lengths.
+ * Update the total bit length for the current block.
+ * IN assertion: the field freq is set for all tree elements.
+ * OUT assertions: the fields len and code are set to the optimal bit length
+ *     and corresponding code. The length opt_len is updated; static_len is
+ *     also updated if stree is not null. The field max_code is set.
+ */
+local void build_tree(s, desc)
+    deflate_state *s;
+    tree_desc *desc; /* the tree descriptor */
+{
+    ct_data *tree   = desc->dyn_tree;
+    ct_data *stree  = desc->stat_desc->static_tree;
+    int elems       = desc->stat_desc->elems;
+    int n, m;          /* iterate over heap elements */
+    int max_code = -1; /* largest code with non zero frequency */
+    int node;          /* new node being created */
+
+    /* Construct the initial heap, with least frequent element in
+     * heap[SMALLEST]. The sons of heap[n] are heap[2*n] and heap[2*n+1].
+     * heap[0] is not used.
+     */
+    s->heap_len = 0, s->heap_max = HEAP_SIZE;
+
+    for (n = 0; n < elems; n++) {
+        if (tree[n].Freq != 0) {
+            s->heap[++(s->heap_len)] = max_code = n;
+            s->depth[n] = 0;
+        } else {
+            tree[n].Len = 0;
+        }
+    }
+
+    /* The pkzip format requires that at least one distance code exists,
+     * and that at least one bit should be sent even if there is only one
+     * possible code. So to avoid special checks later on we force at least
+     * two codes of non zero frequency.
+     */
+    while (s->heap_len < 2) {
+        node = s->heap[++(s->heap_len)] = (max_code < 2 ? ++max_code : 0);
+        tree[node].Freq = 1;
+        s->depth[node] = 0;
+        s->opt_len--; if (stree) s->static_len -= stree[node].Len;
+        /* node is 0 or 1 so it does not have extra bits */
+    }
+    desc->max_code = max_code;
+
+    /* The elements heap[heap_len/2+1 .. heap_len] are leaves of the tree,
+     * establish sub-heaps of increasing lengths:
+     */
+    for (n = s->heap_len/2; n >= 1; n--) pqdownheap(s, tree, n);
+
+    /* Construct the Huffman tree by repeatedly combining the least two
+     * frequent nodes.
+     */
+    node = elems;              /* next internal node of the tree */
+    do {
+        pqremove(s, tree, n);  /* n = node of least frequency */
+        m = s->heap[SMALLEST]; /* m = node of next least frequency */
+
+        s->heap[--(s->heap_max)] = n; /* keep the nodes sorted by frequency */
+        s->heap[--(s->heap_max)] = m;
+
+        /* Create a new node father of n and m */
+        tree[node].Freq = tree[n].Freq + tree[m].Freq;
+        s->depth[node] = (uch) (MAX(s->depth[n], s->depth[m]) + 1);
+        tree[n].Dad = tree[m].Dad = (ush)node;
+#ifdef DUMP_BL_TREE
+        if (tree == s->bl_tree) {
+            fprintf(stderr,"\nnode %d(%d), sons %d(%d) %d(%d)",
+                    node, tree[node].Freq, n, tree[n].Freq, m, tree[m].Freq);
+        }
+#endif
+        /* and insert the new node in the heap */
+        s->heap[SMALLEST] = node++;
+        pqdownheap(s, tree, SMALLEST);
+
+    } while (s->heap_len >= 2);
+
+    s->heap[--(s->heap_max)] = s->heap[SMALLEST];
+
+    /* At this point, the fields freq and dad are set. We can now
+     * generate the bit lengths.
+     */
+    gen_bitlen(s, (tree_desc *)desc);
+
+    /* The field len is now set, we can generate the bit codes */
+    gen_codes ((ct_data *)tree, max_code, s->bl_count);
+}
+
+/* ===========================================================================
+ * Scan a literal or distance tree to determine the frequencies of the codes
+ * in the bit length tree.
+ */
+local void scan_tree (s, tree, max_code)
+    deflate_state *s;
+    ct_data *tree;   /* the tree to be scanned */
+    int max_code;    /* and its largest code of non zero frequency */
+{
+    int n;                     /* iterates over all tree elements */
+    int prevlen = -1;          /* last emitted length */
+    int curlen;                /* length of current code */
+    int nextlen = tree[0].Len; /* length of next code */
+    int count = 0;             /* repeat count of the current code */
+    int max_count = 7;         /* max repeat count */
+    int min_count = 4;         /* min repeat count */
+
+    if (nextlen == 0) max_count = 138, min_count = 3;
+    tree[max_code+1].Len = (ush)0xffff; /* guard */
+
+    for (n = 0; n <= max_code; n++) {
+        curlen = nextlen; nextlen = tree[n+1].Len;
+        if (++count < max_count && curlen == nextlen) {
+            continue;
+        } else if (count < min_count) {
+            s->bl_tree[curlen].Freq += count;
+        } else if (curlen != 0) {
+            if (curlen != prevlen) s->bl_tree[curlen].Freq++;
+            s->bl_tree[REP_3_6].Freq++;
+        } else if (count <= 10) {
+            s->bl_tree[REPZ_3_10].Freq++;
+        } else {
+            s->bl_tree[REPZ_11_138].Freq++;
+        }
+        count = 0; prevlen = curlen;
+        if (nextlen == 0) {
+            max_count = 138, min_count = 3;
+        } else if (curlen == nextlen) {
+            max_count = 6, min_count = 3;
+        } else {
+            max_count = 7, min_count = 4;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Send a literal or distance tree in compressed form, using the codes in
+ * bl_tree.
+ */
+local void send_tree (s, tree, max_code)
+    deflate_state *s;
+    ct_data *tree; /* the tree to be scanned */
+    int max_code;       /* and its largest code of non zero frequency */
+{
+    int n;                     /* iterates over all tree elements */
+    int prevlen = -1;          /* last emitted length */
+    int curlen;                /* length of current code */
+    int nextlen = tree[0].Len; /* length of next code */
+    int count = 0;             /* repeat count of the current code */
+    int max_count = 7;         /* max repeat count */
+    int min_count = 4;         /* min repeat count */
+
+    /* tree[max_code+1].Len = -1; */  /* guard already set */
+    if (nextlen == 0) max_count = 138, min_count = 3;
+
+    for (n = 0; n <= max_code; n++) {
+        curlen = nextlen; nextlen = tree[n+1].Len;
+        if (++count < max_count && curlen == nextlen) {
+            continue;
+        } else if (count < min_count) {
+            do { send_code(s, curlen, s->bl_tree); } while (--count != 0);
+
+        } else if (curlen != 0) {
+            if (curlen != prevlen) {
+                send_code(s, curlen, s->bl_tree); count--;
+            }
+            Assert(count >= 3 && count <= 6, " 3_6?");
+            send_code(s, REP_3_6, s->bl_tree); send_bits(s, count-3, 2);
+
+        } else if (count <= 10) {
+            send_code(s, REPZ_3_10, s->bl_tree); send_bits(s, count-3, 3);
+
+        } else {
+            send_code(s, REPZ_11_138, s->bl_tree); send_bits(s, count-11, 7);
+        }
+        count = 0; prevlen = curlen;
+        if (nextlen == 0) {
+            max_count = 138, min_count = 3;
+        } else if (curlen == nextlen) {
+            max_count = 6, min_count = 3;
+        } else {
+            max_count = 7, min_count = 4;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Construct the Huffman tree for the bit lengths and return the index in
+ * bl_order of the last bit length code to send.
+ */
+local int build_bl_tree(s)
+    deflate_state *s;
+{
+    int max_blindex;  /* index of last bit length code of non zero freq */
+
+    /* Determine the bit length frequencies for literal and distance trees */
+    scan_tree(s, (ct_data *)s->dyn_ltree, s->l_desc.max_code);
+    scan_tree(s, (ct_data *)s->dyn_dtree, s->d_desc.max_code);
+
+    /* Build the bit length tree: */
+    build_tree(s, (tree_desc *)(&(s->bl_desc)));
+    /* opt_len now includes the length of the tree representations, except
+     * the lengths of the bit lengths codes and the 5+5+4 bits for the counts.
+     */
+
+    /* Determine the number of bit length codes to send. The pkzip format
+     * requires that at least 4 bit length codes be sent. (appnote.txt says
+     * 3 but the actual value used is 4.)
+     */
+    for (max_blindex = BL_CODES-1; max_blindex >= 3; max_blindex--) {
+        if (s->bl_tree[bl_order[max_blindex]].Len != 0) break;
+    }
+    /* Update opt_len to include the bit length tree and counts */
+    s->opt_len += 3*(max_blindex+1) + 5+5+4;
+    Tracev((stderr, "\ndyn trees: dyn %ld, stat %ld",
+            s->opt_len, s->static_len));
+
+    return max_blindex;
+}
+
+/* ===========================================================================
+ * Send the header for a block using dynamic Huffman trees: the counts, the
+ * lengths of the bit length codes, the literal tree and the distance tree.
+ * IN assertion: lcodes >= 257, dcodes >= 1, blcodes >= 4.
+ */
+local void send_all_trees(s, lcodes, dcodes, blcodes)
+    deflate_state *s;
+    int lcodes, dcodes, blcodes; /* number of codes for each tree */
+{
+    int rank;                    /* index in bl_order */
+
+    Assert (lcodes >= 257 && dcodes >= 1 && blcodes >= 4, "not enough codes");
+    Assert (lcodes <= L_CODES && dcodes <= D_CODES && blcodes <= BL_CODES,
+            "too many codes");
+    Tracev((stderr, "\nbl counts: "));
+    send_bits(s, lcodes-257, 5); /* not +255 as stated in appnote.txt */
+    send_bits(s, dcodes-1,   5);
+    send_bits(s, blcodes-4,  4); /* not -3 as stated in appnote.txt */
+    for (rank = 0; rank < blcodes; rank++) {
+        Tracev((stderr, "\nbl code %2d ", bl_order[rank]));
+        send_bits(s, s->bl_tree[bl_order[rank]].Len, 3);
+    }
+    Tracev((stderr, "\nbl tree: sent %ld", s->bits_sent));
+
+    send_tree(s, (ct_data *)s->dyn_ltree, lcodes-1); /* literal tree */
+    Tracev((stderr, "\nlit tree: sent %ld", s->bits_sent));
+
+    send_tree(s, (ct_data *)s->dyn_dtree, dcodes-1); /* distance tree */
+    Tracev((stderr, "\ndist tree: sent %ld", s->bits_sent));
+}
+
+/* ===========================================================================
+ * Send a stored block
+ */
+void _tr_stored_block(s, buf, stored_len, eof)
+    deflate_state *s;
+    charf *buf;       /* input block */
+    ulg stored_len;   /* length of input block */
+    int eof;          /* true if this is the last block for a file */
+{
+    send_bits(s, (STORED_BLOCK<<1)+eof, 3);  /* send block type */
+    s->compressed_len = (s->compressed_len + 3 + 7) & (ulg)~7L;
+    s->compressed_len += (stored_len + 4) << 3;
+
+    copy_block(s, buf, (unsigned)stored_len, 1); /* with header */
+}
+
+/* Send just the `stored block' type code without any length bytes or data.
+ */
+void _tr_stored_type_only(s)
+    deflate_state *s;
+{
+    send_bits(s, (STORED_BLOCK << 1), 3);
+    bi_windup(s);
+    s->compressed_len = (s->compressed_len + 3) & ~7L;
+}
+
+
+/* ===========================================================================
+ * Send one empty static block to give enough lookahead for inflate.
+ * This takes 10 bits, of which 7 may remain in the bit buffer.
+ * The current inflate code requires 9 bits of lookahead. If the
+ * last two codes for the previous block (real code plus EOB) were coded
+ * on 5 bits or less, inflate may have only 5+3 bits of lookahead to decode
+ * the last real code. In this case we send two empty static blocks instead
+ * of one. (There are no problems if the previous block is stored or fixed.)
+ * To simplify the code, we assume the worst case of last real code encoded
+ * on one bit only.
+ */
+void _tr_align(s)
+    deflate_state *s;
+{
+    send_bits(s, STATIC_TREES<<1, 3);
+    send_code(s, END_BLOCK, static_ltree);
+    s->compressed_len += 10L; /* 3 for block type, 7 for EOB */
+    bi_flush(s);
+    /* Of the 10 bits for the empty block, we have already sent
+     * (10 - bi_valid) bits. The lookahead for the last real code (before
+     * the EOB of the previous block) was thus at least one plus the length
+     * of the EOB plus what we have just sent of the empty static block.
+     */
+    if (1 + s->last_eob_len + 10 - s->bi_valid < 9) {
+        send_bits(s, STATIC_TREES<<1, 3);
+        send_code(s, END_BLOCK, static_ltree);
+        s->compressed_len += 10L;
+        bi_flush(s);
+    }
+    s->last_eob_len = 7;
+}
+
+/* ===========================================================================
+ * Determine the best encoding for the current block: dynamic trees, static
+ * trees or store, and output the encoded block to the zip file. This function
+ * returns the total compressed length for the file so far.
+ */
+ulg _tr_flush_block(s, buf, stored_len, eof)
+    deflate_state *s;
+    charf *buf;       /* input block, or NULL if too old */
+    ulg stored_len;   /* length of input block */
+    int eof;          /* true if this is the last block for a file */
+{
+    ulg opt_lenb, static_lenb; /* opt_len and static_len in bytes */
+    int max_blindex = 0;  /* index of last bit length code of non zero freq */
+
+    /* Build the Huffman trees unless a stored block is forced */
+    if (s->level > 0) {
+
+	 /* Check if the file is ascii or binary */
+	if (s->data_type == Z_UNKNOWN) set_data_type(s);
+
+	/* Construct the literal and distance trees */
+	build_tree(s, (tree_desc *)(&(s->l_desc)));
+	Tracev((stderr, "\nlit data: dyn %ld, stat %ld", s->opt_len,
+		s->static_len));
+
+	build_tree(s, (tree_desc *)(&(s->d_desc)));
+	Tracev((stderr, "\ndist data: dyn %ld, stat %ld", s->opt_len,
+		s->static_len));
+	/* At this point, opt_len and static_len are the total bit lengths of
+	 * the compressed block data, excluding the tree representations.
+	 */
+
+	/* Build the bit length tree for the above two trees, and get the index
+	 * in bl_order of the last bit length code to send.
+	 */
+	max_blindex = build_bl_tree(s);
+
+	/* Determine the best encoding. Compute first the block length in bytes*/
+	opt_lenb = (s->opt_len+3+7)>>3;
+	static_lenb = (s->static_len+3+7)>>3;
+
+	Tracev((stderr, "\nopt %lu(%lu) stat %lu(%lu) stored %lu lit %u ",
+		opt_lenb, s->opt_len, static_lenb, s->static_len, stored_len,
+		s->last_lit));
+
+	if (static_lenb <= opt_lenb) opt_lenb = static_lenb;
+
+    } else {
+        Assert(buf != (char*)0, "lost buf");
+	opt_lenb = static_lenb = stored_len + 5; /* force a stored block */
+    }
+
+    /* If compression failed and this is the first and last block,
+     * and if the .zip file can be seeked (to rewrite the local header),
+     * the whole file is transformed into a stored file:
+     */
+#ifdef STORED_FILE_OK
+#  ifdef FORCE_STORED_FILE
+    if (eof && s->compressed_len == 0L) { /* force stored file */
+#  else
+    if (stored_len <= opt_lenb && eof && s->compressed_len==0L && seekable()) {
+#  endif
+        /* Since LIT_BUFSIZE <= 2*WSIZE, the input data must be there: */
+        if (buf == (charf*)0) error ("block vanished");
+
+        copy_block(s, buf, (unsigned)stored_len, 0); /* without header */
+        s->compressed_len = stored_len << 3;
+        s->method = STORED;
+    } else
+#endif /* STORED_FILE_OK */
+
+#ifdef FORCE_STORED
+    if (buf != (char*)0) { /* force stored block */
+#else
+    if (stored_len+4 <= opt_lenb && buf != (char*)0) {
+                       /* 4: two words for the lengths */
+#endif
+        /* The test buf != NULL is only necessary if LIT_BUFSIZE > WSIZE.
+         * Otherwise we can't have processed more than WSIZE input bytes since
+         * the last block flush, because compression would have been
+         * successful. If LIT_BUFSIZE <= WSIZE, it is never too late to
+         * transform a block into a stored block.
+         */
+        _tr_stored_block(s, buf, stored_len, eof);
+
+#ifdef FORCE_STATIC
+    } else if (static_lenb >= 0) { /* force static trees */
+#else
+    } else if (static_lenb == opt_lenb) {
+#endif
+        send_bits(s, (STATIC_TREES<<1)+eof, 3);
+        compress_block(s, (ct_data *)static_ltree, (ct_data *)static_dtree);
+        s->compressed_len += 3 + s->static_len;
+    } else {
+        send_bits(s, (DYN_TREES<<1)+eof, 3);
+        send_all_trees(s, s->l_desc.max_code+1, s->d_desc.max_code+1,
+                       max_blindex+1);
+        compress_block(s, (ct_data *)s->dyn_ltree, (ct_data *)s->dyn_dtree);
+        s->compressed_len += 3 + s->opt_len;
+    }
+    Assert (s->compressed_len == s->bits_sent, "bad compressed size");
+    init_block(s);
+
+    if (eof) {
+        bi_windup(s);
+        s->compressed_len += 7;  /* align on byte boundary */
+    }
+    Tracev((stderr,"\ncomprlen %lu(%lu) ", s->compressed_len>>3,
+           s->compressed_len-7*eof));
+
+    return s->compressed_len >> 3;
+}
+
+/* ===========================================================================
+ * Save the match info and tally the frequency counts. Return true if
+ * the current block must be flushed.
+ */
+int _tr_tally (s, dist, lc)
+    deflate_state *s;
+    unsigned dist;  /* distance of matched string */
+    unsigned lc;    /* match length-MIN_MATCH or unmatched char (if dist==0) */
+{
+    s->d_buf[s->last_lit] = (ush)dist;
+    s->l_buf[s->last_lit++] = (uch)lc;
+    if (dist == 0) {
+        /* lc is the unmatched char */
+        s->dyn_ltree[lc].Freq++;
+    } else {
+        s->matches++;
+        /* Here, lc is the match length - MIN_MATCH */
+        dist--;             /* dist = match distance - 1 */
+        Assert((ush)dist < (ush)MAX_DIST(s) &&
+               (ush)lc <= (ush)(MAX_MATCH-MIN_MATCH) &&
+               (ush)d_code(dist) < (ush)D_CODES,  "_tr_tally: bad match");
+
+        s->dyn_ltree[length_code[lc]+LITERALS+1].Freq++;
+        s->dyn_dtree[d_code(dist)].Freq++;
+    }
+
+    /* Try to guess if it is profitable to stop the current block here */
+    if (s->level > 2 && (s->last_lit & 0xfff) == 0) {
+        /* Compute an upper bound for the compressed length */
+        ulg out_length = (ulg)s->last_lit*8L;
+        ulg in_length = (ulg)((long)s->strstart - s->block_start);
+        int dcode;
+        for (dcode = 0; dcode < D_CODES; dcode++) {
+            out_length += (ulg)s->dyn_dtree[dcode].Freq *
+                (5L+extra_dbits[dcode]);
+        }
+        out_length >>= 3;
+        Tracev((stderr,"\nlast_lit %u, in %ld, out ~%ld(%ld%%) ",
+               s->last_lit, in_length, out_length,
+               100L - out_length*100L/in_length));
+        if (s->matches < s->last_lit/2 && out_length < in_length/2) return 1;
+    }
+    return (s->last_lit == s->lit_bufsize-1);
+    /* We avoid equality with lit_bufsize because of wraparound at 64K
+     * on 16 bit machines and because stored blocks are restricted to
+     * 64K-1 bytes.
+     */
+}
+
+/* ===========================================================================
+ * Send the block data compressed using the given Huffman trees
+ */
+local void compress_block(s, ltree, dtree)
+    deflate_state *s;
+    ct_data *ltree; /* literal tree */
+    ct_data *dtree; /* distance tree */
+{
+    unsigned dist;      /* distance of matched string */
+    int lc;             /* match length or unmatched char (if dist == 0) */
+    unsigned lx = 0;    /* running index in l_buf */
+    unsigned code;      /* the code to send */
+    int extra;          /* number of extra bits to send */
+
+    if (s->last_lit != 0) do {
+        dist = s->d_buf[lx];
+        lc = s->l_buf[lx++];
+        if (dist == 0) {
+            send_code(s, lc, ltree); /* send a literal byte */
+            Tracecv(isgraph(lc), (stderr," '%c' ", lc));
+        } else {
+            /* Here, lc is the match length - MIN_MATCH */
+            code = length_code[lc];
+            send_code(s, code+LITERALS+1, ltree); /* send the length code */
+            extra = extra_lbits[code];
+            if (extra != 0) {
+                lc -= base_length[code];
+                send_bits(s, lc, extra);       /* send the extra length bits */
+            }
+            dist--; /* dist is now the match distance - 1 */
+            code = d_code(dist);
+            Assert (code < D_CODES, "bad d_code");
+
+            send_code(s, code, dtree);       /* send the distance code */
+            extra = extra_dbits[code];
+            if (extra != 0) {
+                dist -= base_dist[code];
+                send_bits(s, dist, extra);   /* send the extra distance bits */
+            }
+        } /* literal or match pair ? */
+
+        /* Check that the overlay between pending_buf and d_buf+l_buf is ok: */
+        Assert(s->pending < s->lit_bufsize + 2*lx, "pendingBuf overflow");
+
+    } while (lx < s->last_lit);
+
+    send_code(s, END_BLOCK, ltree);
+    s->last_eob_len = ltree[END_BLOCK].Len;
+}
+
+/* ===========================================================================
+ * Set the data type to ASCII or BINARY, using a crude approximation:
+ * binary if more than 20% of the bytes are <= 6 or >= 128, ascii otherwise.
+ * IN assertion: the fields freq of dyn_ltree are set and the total of all
+ * frequencies does not exceed 64K (to fit in an int on 16 bit machines).
+ */
+local void set_data_type(s)
+    deflate_state *s;
+{
+    int n = 0;
+    unsigned ascii_freq = 0;
+    unsigned bin_freq = 0;
+    while (n < 7)        bin_freq += s->dyn_ltree[n++].Freq;
+    while (n < 128)    ascii_freq += s->dyn_ltree[n++].Freq;
+    while (n < LITERALS) bin_freq += s->dyn_ltree[n++].Freq;
+    s->data_type = (Byte)(bin_freq > (ascii_freq >> 2) ? Z_BINARY : Z_ASCII);
+}
+
+/* ===========================================================================
+ * Reverse the first len bits of a code, using straightforward code (a faster
+ * method would use a table)
+ * IN assertion: 1 <= len <= 15
+ */
+local unsigned bi_reverse(code, len)
+    unsigned code; /* the value to invert */
+    int len;       /* its bit length */
+{
+    register unsigned res = 0;
+    do {
+        res |= code & 1;
+        code >>= 1, res <<= 1;
+    } while (--len > 0);
+    return res >> 1;
+}
+
+/* ===========================================================================
+ * Flush the bit buffer, keeping at most 7 bits in it.
+ */
+local void bi_flush(s)
+    deflate_state *s;
+{
+    if (s->bi_valid == 16) {
+        put_short(s, s->bi_buf);
+        s->bi_buf = 0;
+        s->bi_valid = 0;
+    } else if (s->bi_valid >= 8) {
+        put_byte(s, (Byte)s->bi_buf);
+        s->bi_buf >>= 8;
+        s->bi_valid -= 8;
+    }
+}
+
+/* ===========================================================================
+ * Flush the bit buffer and align the output on a byte boundary
+ */
+local void bi_windup(s)
+    deflate_state *s;
+{
+    if (s->bi_valid > 8) {
+        put_short(s, s->bi_buf);
+    } else if (s->bi_valid > 0) {
+        put_byte(s, (Byte)s->bi_buf);
+    }
+    s->bi_buf = 0;
+    s->bi_valid = 0;
+#ifdef DEBUG_ZLIB
+    s->bits_sent = (s->bits_sent+7) & ~7;
+#endif
+}
+
+/* ===========================================================================
+ * Copy a stored block, storing first the length and its
+ * one's complement if requested.
+ */
+local void copy_block(s, buf, len, header)
+    deflate_state *s;
+    charf    *buf;    /* the input data */
+    unsigned len;     /* its length */
+    int      header;  /* true if block header must be written */
+{
+    bi_windup(s);        /* align on byte boundary */
+    s->last_eob_len = 8; /* enough lookahead for inflate */
+
+    if (header) {
+        put_short(s, (ush)len);   
+        put_short(s, (ush)~len);
+#ifdef DEBUG_ZLIB
+        s->bits_sent += 2*16;
+#endif
+    }
+#ifdef DEBUG_ZLIB
+    s->bits_sent += (ulg)len<<3;
+#endif
+    /* bundle up the put_byte(s, *buf++) calls */
+    zmemcpy(&s->pending_buf[s->pending], buf, len);
+    s->pending += len;
+}
+/* --- trees.c */
+
+/* +++ inflate.c */
+/* inflate.c -- zlib interface to inflate modules
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+
+/* +++ infblock.h */
+/* infblock.h -- header to use infblock.c
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+struct inflate_blocks_state;
+typedef struct inflate_blocks_state FAR inflate_blocks_statef;
+
+extern inflate_blocks_statef * inflate_blocks_new OF((
+    z_streamp z,
+    check_func c,               /* check function */
+    uInt w));                   /* window size */
+
+extern int inflate_blocks OF((
+    inflate_blocks_statef *,
+    z_streamp ,
+    int));                      /* initial return code */
+
+extern void inflate_blocks_reset OF((
+    inflate_blocks_statef *,
+    z_streamp ,
+    uLongf *));                  /* check value on output */
+
+extern int inflate_blocks_free OF((
+    inflate_blocks_statef *,
+    z_streamp ,
+    uLongf *));                  /* check value on output */
+
+extern void inflate_set_dictionary OF((
+    inflate_blocks_statef *s,
+    const Bytef *d,  /* dictionary */
+    uInt  n));       /* dictionary length */
+
+extern int inflate_addhistory OF((
+    inflate_blocks_statef *,
+    z_streamp));
+
+extern int inflate_packet_flush OF((
+    inflate_blocks_statef *));
+/* --- infblock.h */
+
+#ifndef NO_DUMMY_DECL
+struct inflate_blocks_state {int dummy;}; /* for buggy compilers */
+#endif
+
+/* inflate private state */
+struct internal_state {
+
+  /* mode */
+  enum {
+      METHOD,   /* waiting for method byte */
+      FLAG,     /* waiting for flag byte */
+      DICT4,    /* four dictionary check bytes to go */
+      DICT3,    /* three dictionary check bytes to go */
+      DICT2,    /* two dictionary check bytes to go */
+      DICT1,    /* one dictionary check byte to go */
+      DICT0,    /* waiting for inflateSetDictionary */
+      BLOCKS,   /* decompressing blocks */
+      CHECK4,   /* four check bytes to go */
+      CHECK3,   /* three check bytes to go */
+      CHECK2,   /* two check bytes to go */
+      CHECK1,   /* one check byte to go */
+      DONE,     /* finished check, done */
+      BAD}      /* got an error--stay here */
+    mode;               /* current inflate mode */
+
+  /* mode dependent information */
+  union {
+    uInt method;        /* if FLAGS, method byte */
+    struct {
+      uLong was;                /* computed check value */
+      uLong need;               /* stream check value */
+    } check;            /* if CHECK, check values to compare */
+    uInt marker;        /* if BAD, inflateSync's marker bytes count */
+  } sub;        /* submode */
+
+  /* mode independent information */
+  int  nowrap;          /* flag for no wrapper */
+  uInt wbits;           /* log2(window size)  (8..15, defaults to 15) */
+  inflate_blocks_statef 
+    *blocks;            /* current inflate_blocks state */
+
+};
+
+
+int inflateReset(z)
+z_streamp z;
+{
+  uLong c;
+
+  if (z == Z_NULL || z->state == Z_NULL)
+    return Z_STREAM_ERROR;
+  z->total_in = z->total_out = 0;
+  z->msg = Z_NULL;
+  z->state->mode = z->state->nowrap ? BLOCKS : METHOD;
+  inflate_blocks_reset(z->state->blocks, z, &c);
+  Trace((stderr, "inflate: reset\n"));
+  return Z_OK;
+}
+
+
+int inflateEnd(z)
+z_streamp z;
+{
+  uLong c;
+
+  if (z == Z_NULL || z->state == Z_NULL || z->zfree == Z_NULL)
+    return Z_STREAM_ERROR;
+  if (z->state->blocks != Z_NULL)
+    inflate_blocks_free(z->state->blocks, z, &c);
+  ZFREE(z, z->state);
+  z->state = Z_NULL;
+  Trace((stderr, "inflate: end\n"));
+  return Z_OK;
+}
+
+
+int inflateInit2_(z, w, version, stream_size)
+z_streamp z;
+int w;
+const char *version;
+int stream_size;
+{
+  if (version == Z_NULL || version[0] != ZLIB_VERSION[0] ||
+      stream_size != sizeof(z_stream))
+      return Z_VERSION_ERROR;
+
+  /* initialize state */
+  if (z == Z_NULL)
+    return Z_STREAM_ERROR;
+  z->msg = Z_NULL;
+#ifndef NO_ZCFUNCS
+  if (z->zalloc == Z_NULL)
+  {
+    z->zalloc = zcalloc;
+    z->opaque = (voidpf)0;
+  }
+  if (z->zfree == Z_NULL) z->zfree = zcfree;
+#endif
+  if ((z->state = (struct internal_state FAR *)
+       ZALLOC(z,1,sizeof(struct internal_state))) == Z_NULL)
+    return Z_MEM_ERROR;
+  z->state->blocks = Z_NULL;
+
+  /* handle undocumented nowrap option (no zlib header or check) */
+  z->state->nowrap = 0;
+  if (w < 0)
+  {
+    w = - w;
+    z->state->nowrap = 1;
+  }
+
+  /* set window size */
+  if (w < 8 || w > 15)
+  {
+    inflateEnd(z);
+    return Z_STREAM_ERROR;
+  }
+  z->state->wbits = (uInt)w;
+
+  /* create inflate_blocks state */
+  if ((z->state->blocks =
+      inflate_blocks_new(z, z->state->nowrap ? Z_NULL : adler32, (uInt)1 << w))
+      == Z_NULL)
+  {
+    inflateEnd(z);
+    return Z_MEM_ERROR;
+  }
+  Trace((stderr, "inflate: allocated\n"));
+
+  /* reset state */
+  inflateReset(z);
+  return Z_OK;
+}
+
+
+int inflateInit_(z, version, stream_size)
+z_streamp z;
+const char *version;
+int stream_size;
+{
+  return inflateInit2_(z, DEF_WBITS, version, stream_size);
+}
+
+
+#define NEEDBYTE {if(z->avail_in==0)goto empty;r=Z_OK;}
+#define NEXTBYTE (z->avail_in--,z->total_in++,*z->next_in++)
+
+int inflate(z, f)
+z_streamp z;
+int f;
+{
+  int r;
+  uInt b;
+
+  if (z == Z_NULL || z->state == Z_NULL || z->next_in == Z_NULL || f < 0)
+    return Z_STREAM_ERROR;
+  r = Z_BUF_ERROR;
+  while (1) switch (z->state->mode)
+  {
+    case METHOD:
+      NEEDBYTE
+      if (((z->state->sub.method = NEXTBYTE) & 0xf) != Z_DEFLATED)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"unknown compression method";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      if ((z->state->sub.method >> 4) + 8 > z->state->wbits)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"invalid window size";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      z->state->mode = FLAG;
+    case FLAG:
+      NEEDBYTE
+      b = NEXTBYTE;
+      if (((z->state->sub.method << 8) + b) % 31)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"incorrect header check";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      Trace((stderr, "inflate: zlib header ok\n"));
+      if (!(b & PRESET_DICT))
+      {
+        z->state->mode = BLOCKS;
+	break;
+      }
+      z->state->mode = DICT4;
+    case DICT4:
+      NEEDBYTE
+      z->state->sub.check.need = (uLong)NEXTBYTE << 24;
+      z->state->mode = DICT3;
+    case DICT3:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 16;
+      z->state->mode = DICT2;
+    case DICT2:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 8;
+      z->state->mode = DICT1;
+    case DICT1:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE;
+      z->adler = z->state->sub.check.need;
+      z->state->mode = DICT0;
+      return Z_NEED_DICT;
+    case DICT0:
+      z->state->mode = BAD;
+      z->msg = (char*)"need dictionary";
+      z->state->sub.marker = 0;       /* can try inflateSync */
+      return Z_STREAM_ERROR;
+    case BLOCKS:
+      r = inflate_blocks(z->state->blocks, z, r);
+      if (f == Z_PACKET_FLUSH && z->avail_in == 0 && z->avail_out != 0)
+	  r = inflate_packet_flush(z->state->blocks);
+      if (r == Z_DATA_ERROR)
+      {
+        z->state->mode = BAD;
+        z->state->sub.marker = 0;       /* can try inflateSync */
+        break;
+      }
+      if (r != Z_STREAM_END)
+        return r;
+      r = Z_OK;
+      inflate_blocks_reset(z->state->blocks, z, &z->state->sub.check.was);
+      if (z->state->nowrap)
+      {
+        z->state->mode = DONE;
+        break;
+      }
+      z->state->mode = CHECK4;
+    case CHECK4:
+      NEEDBYTE
+      z->state->sub.check.need = (uLong)NEXTBYTE << 24;
+      z->state->mode = CHECK3;
+    case CHECK3:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 16;
+      z->state->mode = CHECK2;
+    case CHECK2:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 8;
+      z->state->mode = CHECK1;
+    case CHECK1:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE;
+
+      if (z->state->sub.check.was != z->state->sub.check.need)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"incorrect data check";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      Trace((stderr, "inflate: zlib check ok\n"));
+      z->state->mode = DONE;
+    case DONE:
+      return Z_STREAM_END;
+    case BAD:
+      return Z_DATA_ERROR;
+    default:
+      return Z_STREAM_ERROR;
+  }
+
+ empty:
+  if (f != Z_PACKET_FLUSH)
+    return r;
+  z->state->mode = BAD;
+  z->msg = (char *)"need more for packet flush";
+  z->state->sub.marker = 0;       /* can try inflateSync */
+  return Z_DATA_ERROR;
+}
+
+
+int inflateSetDictionary(z, dictionary, dictLength)
+z_streamp z;
+const Bytef *dictionary;
+uInt  dictLength;
+{
+  uInt length = dictLength;
+
+  if (z == Z_NULL || z->state == Z_NULL || z->state->mode != DICT0)
+    return Z_STREAM_ERROR;
+
+  if (adler32(1L, dictionary, dictLength) != z->adler) return Z_DATA_ERROR;
+  z->adler = 1L;
+
+  if (length >= ((uInt)1<<z->state->wbits))
+  {
+    length = (1<<z->state->wbits)-1;
+    dictionary += dictLength - length;
+  }
+  inflate_set_dictionary(z->state->blocks, dictionary, length);
+  z->state->mode = BLOCKS;
+  return Z_OK;
+}
+
+/*
+ * This subroutine adds the data at next_in/avail_in to the output history
+ * without performing any output.  The output buffer must be "caught up";
+ * i.e. no pending output (hence s->read equals s->write), and the state must
+ * be BLOCKS (i.e. we should be willing to see the start of a series of
+ * BLOCKS).  On exit, the output will also be caught up, and the checksum
+ * will have been updated if need be.
+ */
+
+int inflateIncomp(z)
+z_stream *z;
+{
+    if (z->state->mode != BLOCKS)
+	return Z_DATA_ERROR;
+    return inflate_addhistory(z->state->blocks, z);
+}
+
+
+int inflateSync(z)
+z_streamp z;
+{
+  uInt n;       /* number of bytes to look at */
+  Bytef *p;     /* pointer to bytes */
+  uInt m;       /* number of marker bytes found in a row */
+  uLong r, w;   /* temporaries to save total_in and total_out */
+
+  /* set up */
+  if (z == Z_NULL || z->state == Z_NULL)
+    return Z_STREAM_ERROR;
+  if (z->state->mode != BAD)
+  {
+    z->state->mode = BAD;
+    z->state->sub.marker = 0;
+  }
+  if ((n = z->avail_in) == 0)
+    return Z_BUF_ERROR;
+  p = z->next_in;
+  m = z->state->sub.marker;
+
+  /* search */
+  while (n && m < 4)
+  {
+    if (*p == (Byte)(m < 2 ? 0 : 0xff))
+      m++;
+    else if (*p)
+      m = 0;
+    else
+      m = 4 - m;
+    p++, n--;
+  }
+
+  /* restore */
+  z->total_in += p - z->next_in;
+  z->next_in = p;
+  z->avail_in = n;
+  z->state->sub.marker = m;
+
+  /* return no joy or set up to restart on a new block */
+  if (m != 4)
+    return Z_DATA_ERROR;
+  r = z->total_in;  w = z->total_out;
+  inflateReset(z);
+  z->total_in = r;  z->total_out = w;
+  z->state->mode = BLOCKS;
+  return Z_OK;
+}
+
+#undef NEEDBYTE
+#undef NEXTBYTE
+/* --- inflate.c */
+
+/* +++ infblock.c */
+/* infblock.c -- interpret and process block types to last block
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+/* #include "infblock.h" */
+
+/* +++ inftrees.h */
+/* inftrees.h -- header to use inftrees.c
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* Huffman code lookup table entry--this entry is four bytes for machines
+   that have 16-bit pointers (e.g. PC's in the small or medium model). */
+
+typedef struct inflate_huft_s FAR inflate_huft;
+
+struct inflate_huft_s {
+  union {
+    struct {
+      Byte Exop;        /* number of extra bits or operation */
+      Byte Bits;        /* number of bits in this code or subcode */
+    } what;
+    Bytef *pad;         /* pad structure to a power of 2 (4 bytes for */
+  } word;               /*  16-bit, 8 bytes for 32-bit machines) */
+  union {
+    uInt Base;          /* literal, length base, or distance base */
+    inflate_huft *Next; /* pointer to next level of table */
+  } more;
+};
+
+#ifdef DEBUG_ZLIB
+  extern uInt inflate_hufts;
+#endif
+
+extern int inflate_trees_bits OF((
+    uIntf *,                    /* 19 code lengths */
+    uIntf *,                    /* bits tree desired/actual depth */
+    inflate_huft * FAR *,       /* bits tree result */
+    z_streamp ));               /* for zalloc, zfree functions */
+
+extern int inflate_trees_dynamic OF((
+    uInt,                       /* number of literal/length codes */
+    uInt,                       /* number of distance codes */
+    uIntf *,                    /* that many (total) code lengths */
+    uIntf *,                    /* literal desired/actual bit depth */
+    uIntf *,                    /* distance desired/actual bit depth */
+    inflate_huft * FAR *,       /* literal/length tree result */
+    inflate_huft * FAR *,       /* distance tree result */
+    z_streamp ));               /* for zalloc, zfree functions */
+
+extern int inflate_trees_fixed OF((
+    uIntf *,                    /* literal desired/actual bit depth */
+    uIntf *,                    /* distance desired/actual bit depth */
+    inflate_huft * FAR *,       /* literal/length tree result */
+    inflate_huft * FAR *));     /* distance tree result */
+
+extern int inflate_trees_free OF((
+    inflate_huft *,             /* tables to free */
+    z_streamp ));               /* for zfree function */
+
+/* --- inftrees.h */
+
+/* +++ infcodes.h */
+/* infcodes.h -- header to use infcodes.c
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+struct inflate_codes_state;
+typedef struct inflate_codes_state FAR inflate_codes_statef;
+
+extern inflate_codes_statef *inflate_codes_new OF((
+    uInt, uInt,
+    inflate_huft *, inflate_huft *,
+    z_streamp ));
+
+extern int inflate_codes OF((
+    inflate_blocks_statef *,
+    z_streamp ,
+    int));
+
+extern void inflate_codes_free OF((
+    inflate_codes_statef *,
+    z_streamp ));
+
+/* --- infcodes.h */
+
+/* +++ infutil.h */
+/* infutil.h -- types and macros common to blocks and codes
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+#ifndef _INFUTIL_H
+#define _INFUTIL_H
+
+typedef enum {
+      TYPE,     /* get type bits (3, including end bit) */
+      LENS,     /* get lengths for stored */
+      STORED,   /* processing stored block */
+      TABLE,    /* get table lengths */
+      BTREE,    /* get bit lengths tree for a dynamic block */
+      DTREE,    /* get length, distance trees for a dynamic block */
+      CODES,    /* processing fixed or dynamic block */
+      DRY,      /* output remaining window bytes */
+      DONEB,    /* finished last block, done */
+      BADB}     /* got a data error--stuck here */
+inflate_block_mode;
+
+/* inflate blocks semi-private state */
+struct inflate_blocks_state {
+
+  /* mode */
+  inflate_block_mode  mode;     /* current inflate_block mode */
+
+  /* mode dependent information */
+  union {
+    uInt left;          /* if STORED, bytes left to copy */
+    struct {
+      uInt table;               /* table lengths (14 bits) */
+      uInt index;               /* index into blens (or border) */
+      uIntf *blens;             /* bit lengths of codes */
+      uInt bb;                  /* bit length tree depth */
+      inflate_huft *tb;         /* bit length decoding tree */
+    } trees;            /* if DTREE, decoding info for trees */
+    struct {
+      inflate_huft *tl;
+      inflate_huft *td;         /* trees to free */
+      inflate_codes_statef 
+         *codes;
+    } decode;           /* if CODES, current state */
+  } sub;                /* submode */
+  uInt last;            /* true if this block is the last block */
+
+  /* mode independent information */
+  uInt bitk;            /* bits in bit buffer */
+  uLong bitb;           /* bit buffer */
+  Bytef *window;        /* sliding window */
+  Bytef *end;           /* one byte after sliding window */
+  Bytef *read;          /* window read pointer */
+  Bytef *write;         /* window write pointer */
+  check_func checkfn;   /* check function */
+  uLong check;          /* check on output */
+
+};
+
+
+/* defines for inflate input/output */
+/*   update pointers and return */
+#define UPDBITS {s->bitb=b;s->bitk=k;}
+#define UPDIN {z->avail_in=n;z->total_in+=p-z->next_in;z->next_in=p;}
+#define UPDOUT {s->write=q;}
+#define UPDATE {UPDBITS UPDIN UPDOUT}
+#define LEAVE {UPDATE return inflate_flush(s,z,r);}
+/*   get bytes and bits */
+#define LOADIN {p=z->next_in;n=z->avail_in;b=s->bitb;k=s->bitk;}
+#define NEEDBYTE {if(n)r=Z_OK;else LEAVE}
+#define NEXTBYTE (n--,*p++)
+#define NEEDBITS(j) {while(k<(j)){NEEDBYTE;b|=((uLong)NEXTBYTE)<<k;k+=8;}}
+#define DUMPBITS(j) {b>>=(j);k-=(j);}
+/*   output bytes */
+#define WAVAIL (uInt)(q<s->read?s->read-q-1:s->end-q)
+#define LOADOUT {q=s->write;m=(uInt)WAVAIL;}
+#define WWRAP {if(q==s->end&&s->read!=s->window){q=s->window;m=(uInt)WAVAIL;}}
+#define FLUSH {UPDOUT r=inflate_flush(s,z,r); LOADOUT}
+#define NEEDOUT {if(m==0){WWRAP if(m==0){FLUSH WWRAP if(m==0) LEAVE}}r=Z_OK;}
+#define OUTBYTE(a) {*q++=(Byte)(a);m--;}
+/*   load local pointers */
+#define LOAD {LOADIN LOADOUT}
+
+/* masks for lower bits (size given to avoid silly warnings with Visual C++) */
+extern uInt inflate_mask[17];
+
+/* copy as much as possible from the sliding window to the output area */
+extern int inflate_flush OF((
+    inflate_blocks_statef *,
+    z_streamp ,
+    int));
+
+#ifndef NO_DUMMY_DECL
+struct internal_state      {int dummy;}; /* for buggy compilers */
+#endif
+
+#endif
+/* --- infutil.h */
+
+#ifndef NO_DUMMY_DECL
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+#endif
+
+/* Table for deflate from PKZIP's appnote.txt. */
+local const uInt border[] = { /* Order of the bit length code lengths */
+        16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};
+
+/*
+   Notes beyond the 1.93a appnote.txt:
+
+   1. Distance pointers never point before the beginning of the output
+      stream.
+   2. Distance pointers can point back across blocks, up to 32k away.
+   3. There is an implied maximum of 7 bits for the bit length table and
+      15 bits for the actual data.
+   4. If only one code exists, then it is encoded using one bit.  (Zero
+      would be more efficient, but perhaps a little confusing.)  If two
+      codes exist, they are coded using one bit each (0 and 1).
+   5. There is no way of sending zero distance codes--a dummy must be
+      sent if there are none.  (History: a pre 2.0 version of PKZIP would
+      store blocks with no distance codes, but this was discovered to be
+      too harsh a criterion.)  Valid only for 1.93a.  2.04c does allow
+      zero distance codes, which is sent as one code of zero bits in
+      length.
+   6. There are up to 286 literal/length codes.  Code 256 represents the
+      end-of-block.  Note however that the static length tree defines
+      288 codes just to fill out the Huffman codes.  Codes 286 and 287
+      cannot be used though, since there is no length base or extra bits
+      defined for them.  Similarily, there are up to 30 distance codes.
+      However, static trees define 32 codes (all 5 bits) to fill out the
+      Huffman codes, but the last two had better not show up in the data.
+   7. Unzip can check dynamic Huffman blocks for complete code sets.
+      The exception is that a single code would not be complete (see #4).
+   8. The five bits following the block type is really the number of
+      literal codes sent minus 257.
+   9. Length codes 8,16,16 are interpreted as 13 length codes of 8 bits
+      (1+6+6).  Therefore, to output three times the length, you output
+      three codes (1+1+1), whereas to output four times the same length,
+      you only need two codes (1+3).  Hmm.
+  10. In the tree reconstruction algorithm, Code = Code + Increment
+      only if BitLength(i) is not zero.  (Pretty obvious.)
+  11. Correction: 4 Bits: # of Bit Length codes - 4     (4 - 19)
+  12. Note: length code 284 can represent 227-258, but length code 285
+      really is 258.  The last length deserves its own, short code
+      since it gets used a lot in very redundant files.  The length
+      258 is special since 258 - 3 (the min match length) is 255.
+  13. The literal/length and distance code bit lengths are read as a
+      single stream of lengths.  It is possible (and advantageous) for
+      a repeat code (16, 17, or 18) to go across the boundary between
+      the two sets of lengths.
+ */
+
+
+void inflate_blocks_reset(s, z, c)
+inflate_blocks_statef *s;
+z_streamp z;
+uLongf *c;
+{
+  if (s->checkfn != Z_NULL)
+    *c = s->check;
+  if (s->mode == BTREE || s->mode == DTREE)
+    ZFREE(z, s->sub.trees.blens);
+  if (s->mode == CODES)
+  {
+    inflate_codes_free(s->sub.decode.codes, z);
+    inflate_trees_free(s->sub.decode.td, z);
+    inflate_trees_free(s->sub.decode.tl, z);
+  }
+  s->mode = TYPE;
+  s->bitk = 0;
+  s->bitb = 0;
+  s->read = s->write = s->window;
+  if (s->checkfn != Z_NULL)
+    z->adler = s->check = (*s->checkfn)(0L, Z_NULL, 0);
+  Trace((stderr, "inflate:   blocks reset\n"));
+}
+
+
+inflate_blocks_statef *inflate_blocks_new(z, c, w)
+z_streamp z;
+check_func c;
+uInt w;
+{
+  inflate_blocks_statef *s;
+
+  if ((s = (inflate_blocks_statef *)ZALLOC
+       (z,1,sizeof(struct inflate_blocks_state))) == Z_NULL)
+    return s;
+  if ((s->window = (Bytef *)ZALLOC(z, 1, w)) == Z_NULL)
+  {
+    ZFREE(z, s);
+    return Z_NULL;
+  }
+  s->end = s->window + w;
+  s->checkfn = c;
+  s->mode = TYPE;
+  Trace((stderr, "inflate:   blocks allocated\n"));
+  inflate_blocks_reset(s, z, &s->check);
+  return s;
+}
+
+
+#ifdef DEBUG_ZLIB
+  extern uInt inflate_hufts;
+#endif
+int inflate_blocks(s, z, r)
+inflate_blocks_statef *s;
+z_streamp z;
+int r;
+{
+  uInt t;               /* temporary storage */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Bytef *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Bytef *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+
+  /* copy input/output information to locals (UPDATE macro restores) */
+  LOAD
+
+  /* process input based on current state */
+  while (1) switch (s->mode)
+  {
+    case TYPE:
+      NEEDBITS(3)
+      t = (uInt)b & 7;
+      s->last = t & 1;
+      switch (t >> 1)
+      {
+        case 0:                         /* stored */
+          Trace((stderr, "inflate:     stored block%s\n",
+                 s->last ? " (last)" : ""));
+          DUMPBITS(3)
+          t = k & 7;                    /* go to byte boundary */
+          DUMPBITS(t)
+          s->mode = LENS;               /* get length of stored block */
+          break;
+        case 1:                         /* fixed */
+          Trace((stderr, "inflate:     fixed codes block%s\n",
+                 s->last ? " (last)" : ""));
+          {
+            uInt bl, bd;
+            inflate_huft *tl, *td;
+
+            inflate_trees_fixed(&bl, &bd, &tl, &td);
+            s->sub.decode.codes = inflate_codes_new(bl, bd, tl, td, z);
+            if (s->sub.decode.codes == Z_NULL)
+            {
+              r = Z_MEM_ERROR;
+              LEAVE
+            }
+            s->sub.decode.tl = Z_NULL;  /* don't try to free these */
+            s->sub.decode.td = Z_NULL;
+          }
+          DUMPBITS(3)
+          s->mode = CODES;
+          break;
+        case 2:                         /* dynamic */
+          Trace((stderr, "inflate:     dynamic codes block%s\n",
+                 s->last ? " (last)" : ""));
+          DUMPBITS(3)
+          s->mode = TABLE;
+          break;
+        case 3:                         /* illegal */
+          DUMPBITS(3)
+          s->mode = BADB;
+          z->msg = (char*)"invalid block type";
+          r = Z_DATA_ERROR;
+          LEAVE
+      }
+      break;
+    case LENS:
+      NEEDBITS(32)
+      if ((((~b) >> 16) & 0xffff) != (b & 0xffff))
+      {
+        s->mode = BADB;
+        z->msg = (char*)"invalid stored block lengths";
+        r = Z_DATA_ERROR;
+        LEAVE
+      }
+      s->sub.left = (uInt)b & 0xffff;
+      b = k = 0;                      /* dump bits */
+      Tracev((stderr, "inflate:       stored length %u\n", s->sub.left));
+      s->mode = s->sub.left ? STORED : (s->last ? DRY : TYPE);
+      break;
+    case STORED:
+      if (n == 0)
+        LEAVE
+      NEEDOUT
+      t = s->sub.left;
+      if (t > n) t = n;
+      if (t > m) t = m;
+      zmemcpy(q, p, t);
+      p += t;  n -= t;
+      q += t;  m -= t;
+      if ((s->sub.left -= t) != 0)
+        break;
+      Tracev((stderr, "inflate:       stored end, %lu total out\n",
+              z->total_out + (q >= s->read ? q - s->read :
+              (s->end - s->read) + (q - s->window))));
+      s->mode = s->last ? DRY : TYPE;
+      break;
+    case TABLE:
+      NEEDBITS(14)
+      s->sub.trees.table = t = (uInt)b & 0x3fff;
+#ifndef PKZIP_BUG_WORKAROUND
+      if ((t & 0x1f) > 29 || ((t >> 5) & 0x1f) > 29)
+      {
+        s->mode = BADB;
+        z->msg = (char*)"too many length or distance symbols";
+        r = Z_DATA_ERROR;
+        LEAVE
+      }
+#endif
+      t = 258 + (t & 0x1f) + ((t >> 5) & 0x1f);
+      if (t < 19)
+        t = 19;
+      if ((s->sub.trees.blens = (uIntf*)ZALLOC(z, t, sizeof(uInt))) == Z_NULL)
+      {
+        r = Z_MEM_ERROR;
+        LEAVE
+      }
+      DUMPBITS(14)
+      s->sub.trees.index = 0;
+      Tracev((stderr, "inflate:       table sizes ok\n"));
+      s->mode = BTREE;
+    case BTREE:
+      while (s->sub.trees.index < 4 + (s->sub.trees.table >> 10))
+      {
+        NEEDBITS(3)
+        s->sub.trees.blens[border[s->sub.trees.index++]] = (uInt)b & 7;
+        DUMPBITS(3)
+      }
+      while (s->sub.trees.index < 19)
+        s->sub.trees.blens[border[s->sub.trees.index++]] = 0;
+      s->sub.trees.bb = 7;
+      t = inflate_trees_bits(s->sub.trees.blens, &s->sub.trees.bb,
+                             &s->sub.trees.tb, z);
+      if (t != Z_OK)
+      {
+        ZFREE(z, s->sub.trees.blens);
+        r = t;
+        if (r == Z_DATA_ERROR)
+          s->mode = BADB;
+        LEAVE
+      }
+      s->sub.trees.index = 0;
+      Tracev((stderr, "inflate:       bits tree ok\n"));
+      s->mode = DTREE;
+    case DTREE:
+      while (t = s->sub.trees.table,
+             s->sub.trees.index < 258 + (t & 0x1f) + ((t >> 5) & 0x1f))
+      {
+        inflate_huft *h;
+        uInt i, j, c;
+
+        t = s->sub.trees.bb;
+        NEEDBITS(t)
+        h = s->sub.trees.tb + ((uInt)b & inflate_mask[t]);
+        t = h->word.what.Bits;
+        c = h->more.Base;
+        if (c < 16)
+        {
+          DUMPBITS(t)
+          s->sub.trees.blens[s->sub.trees.index++] = c;
+        }
+        else /* c == 16..18 */
+        {
+          i = c == 18 ? 7 : c - 14;
+          j = c == 18 ? 11 : 3;
+          NEEDBITS(t + i)
+          DUMPBITS(t)
+          j += (uInt)b & inflate_mask[i];
+          DUMPBITS(i)
+          i = s->sub.trees.index;
+          t = s->sub.trees.table;
+          if (i + j > 258 + (t & 0x1f) + ((t >> 5) & 0x1f) ||
+              (c == 16 && i < 1))
+          {
+            inflate_trees_free(s->sub.trees.tb, z);
+            ZFREE(z, s->sub.trees.blens);
+            s->mode = BADB;
+            z->msg = (char*)"invalid bit length repeat";
+            r = Z_DATA_ERROR;
+            LEAVE
+          }
+          c = c == 16 ? s->sub.trees.blens[i - 1] : 0;
+          do {
+            s->sub.trees.blens[i++] = c;
+          } while (--j);
+          s->sub.trees.index = i;
+        }
+      }
+      inflate_trees_free(s->sub.trees.tb, z);
+      s->sub.trees.tb = Z_NULL;
+      {
+        uInt bl, bd;
+        inflate_huft *tl, *td;
+        inflate_codes_statef *c;
+
+        bl = 9;         /* must be <= 9 for lookahead assumptions */
+        bd = 6;         /* must be <= 9 for lookahead assumptions */
+        t = s->sub.trees.table;
+#ifdef DEBUG_ZLIB
+      inflate_hufts = 0;
+#endif
+        t = inflate_trees_dynamic(257 + (t & 0x1f), 1 + ((t >> 5) & 0x1f),
+                                  s->sub.trees.blens, &bl, &bd, &tl, &td, z);
+        ZFREE(z, s->sub.trees.blens);
+        if (t != Z_OK)
+        {
+          if (t == (uInt)Z_DATA_ERROR)
+            s->mode = BADB;
+          r = t;
+          LEAVE
+        }
+        Tracev((stderr, "inflate:       trees ok, %d * %d bytes used\n",
+              inflate_hufts, sizeof(inflate_huft)));
+        if ((c = inflate_codes_new(bl, bd, tl, td, z)) == Z_NULL)
+        {
+          inflate_trees_free(td, z);
+          inflate_trees_free(tl, z);
+          r = Z_MEM_ERROR;
+          LEAVE
+        }
+        s->sub.decode.codes = c;
+        s->sub.decode.tl = tl;
+        s->sub.decode.td = td;
+      }
+      s->mode = CODES;
+    case CODES:
+      UPDATE
+      if ((r = inflate_codes(s, z, r)) != Z_STREAM_END)
+        return inflate_flush(s, z, r);
+      r = Z_OK;
+      inflate_codes_free(s->sub.decode.codes, z);
+      inflate_trees_free(s->sub.decode.td, z);
+      inflate_trees_free(s->sub.decode.tl, z);
+      LOAD
+      Tracev((stderr, "inflate:       codes end, %lu total out\n",
+              z->total_out + (q >= s->read ? q - s->read :
+              (s->end - s->read) + (q - s->window))));
+      if (!s->last)
+      {
+        s->mode = TYPE;
+        break;
+      }
+      if (k > 7)              /* return unused byte, if any */
+      {
+        Assert(k < 16, "inflate_codes grabbed too many bytes")
+        k -= 8;
+        n++;
+        p--;                    /* can always return one */
+      }
+      s->mode = DRY;
+    case DRY:
+      FLUSH
+      if (s->read != s->write)
+        LEAVE
+      s->mode = DONEB;
+    case DONEB:
+      r = Z_STREAM_END;
+      LEAVE
+    case BADB:
+      r = Z_DATA_ERROR;
+      LEAVE
+    default:
+      r = Z_STREAM_ERROR;
+      LEAVE
+  }
+}
+
+
+int inflate_blocks_free(s, z, c)
+inflate_blocks_statef *s;
+z_streamp z;
+uLongf *c;
+{
+  inflate_blocks_reset(s, z, c);
+  ZFREE(z, s->window);
+  ZFREE(z, s);
+  Trace((stderr, "inflate:   blocks freed\n"));
+  return Z_OK;
+}
+
+
+void inflate_set_dictionary(s, d, n)
+inflate_blocks_statef *s;
+const Bytef *d;
+uInt  n;
+{
+  zmemcpy((charf *)s->window, d, n);
+  s->read = s->write = s->window + n;
+}
+
+/*
+ * This subroutine adds the data at next_in/avail_in to the output history
+ * without performing any output.  The output buffer must be "caught up";
+ * i.e. no pending output (hence s->read equals s->write), and the state must
+ * be BLOCKS (i.e. we should be willing to see the start of a series of
+ * BLOCKS).  On exit, the output will also be caught up, and the checksum
+ * will have been updated if need be.
+ */
+int inflate_addhistory(s, z)
+inflate_blocks_statef *s;
+z_stream *z;
+{
+    uLong b;              /* bit buffer */  /* NOT USED HERE */
+    uInt k;               /* bits in bit buffer */ /* NOT USED HERE */
+    uInt t;               /* temporary storage */
+    Bytef *p;             /* input data pointer */
+    uInt n;               /* bytes available there */
+    Bytef *q;             /* output window write pointer */
+    uInt m;               /* bytes to end of window or read pointer */
+
+    if (s->read != s->write)
+	return Z_STREAM_ERROR;
+    if (s->mode != TYPE)
+	return Z_DATA_ERROR;
+
+    /* we're ready to rock */
+    LOAD
+    /* while there is input ready, copy to output buffer, moving
+     * pointers as needed.
+     */
+    while (n) {
+	t = n;  /* how many to do */
+	/* is there room until end of buffer? */
+	if (t > m) t = m;
+	/* update check information */
+	if (s->checkfn != Z_NULL)
+	    s->check = (*s->checkfn)(s->check, q, t);
+	zmemcpy(q, p, t);
+	q += t;
+	p += t;
+	n -= t;
+	z->total_out += t;
+	s->read = q;    /* drag read pointer forward */
+/*      WWRAP  */ 	/* expand WWRAP macro by hand to handle s->read */
+	if (q == s->end) {
+	    s->read = q = s->window;
+	    m = WAVAIL;
+	}
+    }
+    UPDATE
+    return Z_OK;
+}
+
+
+/*
+ * At the end of a Deflate-compressed PPP packet, we expect to have seen
+ * a `stored' block type value but not the (zero) length bytes.
+ */
+int inflate_packet_flush(s)
+    inflate_blocks_statef *s;
+{
+    if (s->mode != LENS)
+	return Z_DATA_ERROR;
+    s->mode = TYPE;
+    return Z_OK;
+}
+/* --- infblock.c */
+
+/* +++ inftrees.c */
+/* inftrees.c -- generate Huffman trees for efficient decoding
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+/* #include "inftrees.h" */
+
+char inflate_copyright[] = " inflate 1.0.4 Copyright 1995-1996 Mark Adler ";
+/*
+  If you use the zlib library in a product, an acknowledgment is welcome
+  in the documentation of your product. If for some reason you cannot
+  include such an acknowledgment, I would appreciate that you keep this
+  copyright string in the executable of your product.
+ */
+
+#ifndef NO_DUMMY_DECL
+struct internal_state  {int dummy;}; /* for buggy compilers */
+#endif
+
+/* simplify the use of the inflate_huft type with some defines */
+#define base more.Base
+#define next more.Next
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+
+local int huft_build OF((
+    uIntf *,            /* code lengths in bits */
+    uInt,               /* number of codes */
+    uInt,               /* number of "simple" codes */
+    const uIntf *,      /* list of base values for non-simple codes */
+    const uIntf *,      /* list of extra bits for non-simple codes */
+    inflate_huft * FAR*,/* result: starting table */
+    uIntf *,            /* maximum lookup bits (returns actual) */
+    z_streamp ));       /* for zalloc function */
+
+local voidpf falloc OF((
+    voidpf,             /* opaque pointer (not used) */
+    uInt,               /* number of items */
+    uInt));             /* size of item */
+
+/* Tables for deflate from PKZIP's appnote.txt. */
+local const uInt cplens[31] = { /* Copy lengths for literal codes 257..285 */
+        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
+        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
+        /* see note #13 above about 258 */
+local const uInt cplext[31] = { /* Extra bits for literal codes 257..285 */
+        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2,
+        3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0, 112, 112}; /* 112==invalid */
+local const uInt cpdist[30] = { /* Copy offsets for distance codes 0..29 */
+        1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
+        257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
+        8193, 12289, 16385, 24577};
+local const uInt cpdext[30] = { /* Extra bits for distance codes */
+        0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,
+        7, 7, 8, 8, 9, 9, 10, 10, 11, 11,
+        12, 12, 13, 13};
+
+/*
+   Huffman code decoding is performed using a multi-level table lookup.
+   The fastest way to decode is to simply build a lookup table whose
+   size is determined by the longest code.  However, the time it takes
+   to build this table can also be a factor if the data being decoded
+   is not very long.  The most common codes are necessarily the
+   shortest codes, so those codes dominate the decoding time, and hence
+   the speed.  The idea is you can have a shorter table that decodes the
+   shorter, more probable codes, and then point to subsidiary tables for
+   the longer codes.  The time it costs to decode the longer codes is
+   then traded against the time it takes to make longer tables.
+
+   This results of this trade are in the variables lbits and dbits
+   below.  lbits is the number of bits the first level table for literal/
+   length codes can decode in one step, and dbits is the same thing for
+   the distance codes.  Subsequent tables are also less than or equal to
+   those sizes.  These values may be adjusted either when all of the
+   codes are shorter than that, in which case the longest code length in
+   bits is used, or when the shortest code is *longer* than the requested
+   table size, in which case the length of the shortest code in bits is
+   used.
+
+   There are two different values for the two tables, since they code a
+   different number of possibilities each.  The literal/length table
+   codes 286 possible values, or in a flat code, a little over eight
+   bits.  The distance table codes 30 possible values, or a little less
+   than five bits, flat.  The optimum values for speed end up being
+   about one bit more than those, so lbits is 8+1 and dbits is 5+1.
+   The optimum values may differ though from machine to machine, and
+   possibly even between compilers.  Your mileage may vary.
+ */
+
+
+/* If BMAX needs to be larger than 16, then h and x[] should be uLong. */
+#define BMAX 15         /* maximum bit length of any code */
+#define N_MAX 288       /* maximum number of codes in any set */
+
+#ifdef DEBUG_ZLIB
+  uInt inflate_hufts;
+#endif
+
+local int huft_build(b, n, s, d, e, t, m, zs)
+uIntf *b;               /* code lengths in bits (all assumed <= BMAX) */
+uInt n;                 /* number of codes (assumed <= N_MAX) */
+uInt s;                 /* number of simple-valued codes (0..s-1) */
+const uIntf *d;         /* list of base values for non-simple codes */
+const uIntf *e;         /* list of extra bits for non-simple codes */
+inflate_huft * FAR *t;  /* result: starting table */
+uIntf *m;               /* maximum lookup bits, returns actual */
+z_streamp zs;           /* for zalloc function */
+/* Given a list of code lengths and a maximum table size, make a set of
+   tables to decode that set of codes.  Return Z_OK on success, Z_BUF_ERROR
+   if the given code set is incomplete (the tables are still built in this
+   case), Z_DATA_ERROR if the input is invalid (an over-subscribed set of
+   lengths), or Z_MEM_ERROR if not enough memory. */
+{
+
+  uInt a;                       /* counter for codes of length k */
+  uInt c[BMAX+1];               /* bit length count table */
+  uInt f;                       /* i repeats in table every f entries */
+  int g;                        /* maximum code length */
+  int h;                        /* table level */
+  register uInt i;              /* counter, current code */
+  register uInt j;              /* counter */
+  register int k;               /* number of bits in current code */
+  int l;                        /* bits per table (returned in m) */
+  register uIntf *p;            /* pointer into c[], b[], or v[] */
+  inflate_huft *q;              /* points to current table */
+  struct inflate_huft_s r;      /* table entry for structure assignment */
+  inflate_huft *u[BMAX];        /* table stack */
+  uInt v[N_MAX];                /* values in order of bit length */
+  register int w;               /* bits before this table == (l * h) */
+  uInt x[BMAX+1];               /* bit offsets, then code stack */
+  uIntf *xp;                    /* pointer into x */
+  int y;                        /* number of dummy codes added */
+  uInt z;                       /* number of entries in current table */
+
+
+  /* Generate counts for each bit length */
+  p = c;
+#define C0 *p++ = 0;
+#define C2 C0 C0 C0 C0
+#define C4 C2 C2 C2 C2
+  C4                            /* clear c[]--assume BMAX+1 is 16 */
+  p = b;  i = n;
+  do {
+    c[*p++]++;                  /* assume all entries <= BMAX */
+  } while (--i);
+  if (c[0] == n)                /* null input--all zero length codes */
+  {
+    *t = (inflate_huft *)Z_NULL;
+    *m = 0;
+    return Z_OK;
+  }
+
+
+  /* Find minimum and maximum length, bound *m by those */
+  l = *m;
+  for (j = 1; j <= BMAX; j++)
+    if (c[j])
+      break;
+  k = j;                        /* minimum code length */
+  if ((uInt)l < j)
+    l = j;
+  for (i = BMAX; i; i--)
+    if (c[i])
+      break;
+  g = i;                        /* maximum code length */
+  if ((uInt)l > i)
+    l = i;
+  *m = l;
+
+
+  /* Adjust last length count to fill out codes, if needed */
+  for (y = 1 << j; j < i; j++, y <<= 1)
+    if ((y -= c[j]) < 0)
+      return Z_DATA_ERROR;
+  if ((y -= c[i]) < 0)
+    return Z_DATA_ERROR;
+  c[i] += y;
+
+
+  /* Generate starting offsets into the value table for each length */
+  x[1] = j = 0;
+  p = c + 1;  xp = x + 2;
+  while (--i) {                 /* note that i == g from above */
+    *xp++ = (j += *p++);
+  }
+
+
+  /* Make a table of values in order of bit lengths */
+  p = b;  i = 0;
+  do {
+    if ((j = *p++) != 0)
+      v[x[j]++] = i;
+  } while (++i < n);
+  n = x[g];                   /* set n to length of v */
+
+
+  /* Generate the Huffman codes and for each, make the table entries */
+  x[0] = i = 0;                 /* first Huffman code is zero */
+  p = v;                        /* grab values in bit order */
+  h = -1;                       /* no tables yet--level -1 */
+  w = -l;                       /* bits decoded == (l * h) */
+  u[0] = (inflate_huft *)Z_NULL;        /* just to keep compilers happy */
+  q = (inflate_huft *)Z_NULL;   /* ditto */
+  z = 0;                        /* ditto */
+
+  /* go through the bit lengths (k already is bits in shortest code) */
+  for (; k <= g; k++)
+  {
+    a = c[k];
+    while (a--)
+    {
+      /* here i is the Huffman code of length k bits for value *p */
+      /* make tables up to required level */
+      while (k > w + l)
+      {
+        h++;
+        w += l;                 /* previous table always l bits */
+
+        /* compute minimum size table less than or equal to l bits */
+        z = g - w;
+        z = z > (uInt)l ? l : z;        /* table size upper limit */
+        if ((f = 1 << (j = k - w)) > a + 1)     /* try a k-w bit table */
+        {                       /* too few codes for k-w bit table */
+          f -= a + 1;           /* deduct codes from patterns left */
+          xp = c + k;
+          if (j < z)
+            while (++j < z)     /* try smaller tables up to z bits */
+            {
+              if ((f <<= 1) <= *++xp)
+                break;          /* enough codes to use up j bits */
+              f -= *xp;         /* else deduct codes from patterns */
+            }
+        }
+        z = 1 << j;             /* table entries for j-bit table */
+
+        /* allocate and link in new table */
+        if ((q = (inflate_huft *)ZALLOC
+             (zs,z + 1,sizeof(inflate_huft))) == Z_NULL)
+        {
+          if (h)
+            inflate_trees_free(u[0], zs);
+          return Z_MEM_ERROR;   /* not enough memory */
+        }
+#ifdef DEBUG_ZLIB
+        inflate_hufts += z + 1;
+#endif
+        *t = q + 1;             /* link to list for huft_free() */
+        *(t = &(q->next)) = Z_NULL;
+        u[h] = ++q;             /* table starts after link */
+
+        /* connect to last table, if there is one */
+        if (h)
+        {
+          x[h] = i;             /* save pattern for backing up */
+          r.bits = (Byte)l;     /* bits to dump before this table */
+          r.exop = (Byte)j;     /* bits in this table */
+          r.next = q;           /* pointer to this table */
+          j = i >> (w - l);     /* (get around Turbo C bug) */
+          u[h-1][j] = r;        /* connect to last table */
+        }
+      }
+
+      /* set up table entry in r */
+      r.bits = (Byte)(k - w);
+      if (p >= v + n)
+        r.exop = 128 + 64;      /* out of values--invalid code */
+      else if (*p < s)
+      {
+        r.exop = (Byte)(*p < 256 ? 0 : 32 + 64);     /* 256 is end-of-block */
+        r.base = *p++;          /* simple code is just the value */
+      }
+      else
+      {
+        r.exop = (Byte)(e[*p - s] + 16 + 64);/* non-simple--look up in lists */
+        r.base = d[*p++ - s];
+      }
+
+      /* fill code-like entries with r */
+      f = 1 << (k - w);
+      for (j = i >> w; j < z; j += f)
+        q[j] = r;
+
+      /* backwards increment the k-bit code i */
+      for (j = 1 << (k - 1); i & j; j >>= 1)
+        i ^= j;
+      i ^= j;
+
+      /* backup over finished tables */
+      while ((i & ((1 << w) - 1)) != x[h])
+      {
+        h--;                    /* don't need to update q */
+        w -= l;
+      }
+    }
+  }
+
+
+  /* Return Z_BUF_ERROR if we were given an incomplete table */
+  return y != 0 && g != 1 ? Z_BUF_ERROR : Z_OK;
+}
+
+
+int inflate_trees_bits(c, bb, tb, z)
+uIntf *c;               /* 19 code lengths */
+uIntf *bb;              /* bits tree desired/actual depth */
+inflate_huft * FAR *tb; /* bits tree result */
+z_streamp z;            /* for zfree function */
+{
+  int r;
+
+  r = huft_build(c, 19, 19, (uIntf*)Z_NULL, (uIntf*)Z_NULL, tb, bb, z);
+  if (r == Z_DATA_ERROR)
+    z->msg = (char*)"oversubscribed dynamic bit lengths tree";
+  else if (r == Z_BUF_ERROR || *bb == 0)
+  {
+    inflate_trees_free(*tb, z);
+    z->msg = (char*)"incomplete dynamic bit lengths tree";
+    r = Z_DATA_ERROR;
+  }
+  return r;
+}
+
+
+int inflate_trees_dynamic(nl, nd, c, bl, bd, tl, td, z)
+uInt nl;                /* number of literal/length codes */
+uInt nd;                /* number of distance codes */
+uIntf *c;               /* that many (total) code lengths */
+uIntf *bl;              /* literal desired/actual bit depth */
+uIntf *bd;              /* distance desired/actual bit depth */
+inflate_huft * FAR *tl; /* literal/length tree result */
+inflate_huft * FAR *td; /* distance tree result */
+z_streamp z;            /* for zfree function */
+{
+  int r;
+
+  /* build literal/length tree */
+  r = huft_build(c, nl, 257, cplens, cplext, tl, bl, z);
+  if (r != Z_OK || *bl == 0)
+  {
+    if (r == Z_DATA_ERROR)
+      z->msg = (char*)"oversubscribed literal/length tree";
+    else if (r != Z_MEM_ERROR)
+    {
+      inflate_trees_free(*tl, z);
+      z->msg = (char*)"incomplete literal/length tree";
+      r = Z_DATA_ERROR;
+    }
+    return r;
+  }
+
+  /* build distance tree */
+  r = huft_build(c + nl, nd, 0, cpdist, cpdext, td, bd, z);
+  if (r != Z_OK || (*bd == 0 && nl > 257))
+  {
+    if (r == Z_DATA_ERROR)
+      z->msg = (char*)"oversubscribed distance tree";
+    else if (r == Z_BUF_ERROR) {
+#ifdef PKZIP_BUG_WORKAROUND
+      r = Z_OK;
+    }
+#else
+      inflate_trees_free(*td, z);
+      z->msg = (char*)"incomplete distance tree";
+      r = Z_DATA_ERROR;
+    }
+    else if (r != Z_MEM_ERROR)
+    {
+      z->msg = (char*)"empty distance tree with lengths";
+      r = Z_DATA_ERROR;
+    }
+    inflate_trees_free(*tl, z);
+    return r;
+#endif
+  }
+
+  /* done */
+  return Z_OK;
+}
+
+
+/* build fixed tables only once--keep them here */
+local int fixed_built = 0;
+#define FIXEDH 530      /* number of hufts used by fixed tables */
+local inflate_huft fixed_mem[FIXEDH];
+local uInt fixed_bl;
+local uInt fixed_bd;
+local inflate_huft *fixed_tl;
+local inflate_huft *fixed_td;
+
+
+local voidpf falloc(q, n, s)
+voidpf q;       /* opaque pointer */
+uInt n;         /* number of items */
+uInt s;         /* size of item */
+{
+  Assert(s == sizeof(inflate_huft) && n <= *(intf *)q,
+         "inflate_trees falloc overflow");
+  *(intf *)q -= n+s-s; /* s-s to avoid warning */
+  return (voidpf)(fixed_mem + *(intf *)q);
+}
+
+
+int inflate_trees_fixed(bl, bd, tl, td)
+uIntf *bl;               /* literal desired/actual bit depth */
+uIntf *bd;               /* distance desired/actual bit depth */
+inflate_huft * FAR *tl;  /* literal/length tree result */
+inflate_huft * FAR *td;  /* distance tree result */
+{
+  /* build fixed tables if not already (multiple overlapped executions ok) */
+  if (!fixed_built)
+  {
+    int k;              /* temporary variable */
+    unsigned c[288];    /* length list for huft_build */
+    z_stream z;         /* for falloc function */
+    int f = FIXEDH;     /* number of hufts left in fixed_mem */
+
+    /* set up fake z_stream for memory routines */
+    z.zalloc = falloc;
+    z.zfree = Z_NULL;
+    z.opaque = (voidpf)&f;
+
+    /* literal table */
+    for (k = 0; k < 144; k++)
+      c[k] = 8;
+    for (; k < 256; k++)
+      c[k] = 9;
+    for (; k < 280; k++)
+      c[k] = 7;
+    for (; k < 288; k++)
+      c[k] = 8;
+    fixed_bl = 7;
+    huft_build(c, 288, 257, cplens, cplext, &fixed_tl, &fixed_bl, &z);
+
+    /* distance table */
+    for (k = 0; k < 30; k++)
+      c[k] = 5;
+    fixed_bd = 5;
+    huft_build(c, 30, 0, cpdist, cpdext, &fixed_td, &fixed_bd, &z);
+
+    /* done */
+    Assert(f == 0, "invalid build of fixed tables");
+    fixed_built = 1;
+  }
+  *bl = fixed_bl;
+  *bd = fixed_bd;
+  *tl = fixed_tl;
+  *td = fixed_td;
+  return Z_OK;
+}
+
+
+int inflate_trees_free(t, z)
+inflate_huft *t;        /* table to free */
+z_streamp z;            /* for zfree function */
+/* Free the malloc'ed tables built by huft_build(), which makes a linked
+   list of the tables it made, with the links in a dummy first entry of
+   each table. */
+{
+  register inflate_huft *p, *q, *r;
+
+  /* Reverse linked list */
+  p = Z_NULL;
+  q = t;
+  while (q != Z_NULL)
+  {
+    r = (q - 1)->next;
+    (q - 1)->next = p;
+    p = q;
+    q = r;
+  }
+  /* Go through linked list, freeing from the malloced (t[-1]) address. */
+  while (p != Z_NULL)
+  {
+    q = (--p)->next;
+    ZFREE(z,p);
+    p = q;
+  } 
+  return Z_OK;
+}
+/* --- inftrees.c */
+
+/* +++ infcodes.c */
+/* infcodes.c -- process literals and length/distance pairs
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+/* #include "inftrees.h" */
+/* #include "infblock.h" */
+/* #include "infcodes.h" */
+/* #include "infutil.h" */
+
+/* +++ inffast.h */
+/* inffast.h -- header to use inffast.c
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+extern int inflate_fast OF((
+    uInt,
+    uInt,
+    inflate_huft *,
+    inflate_huft *,
+    inflate_blocks_statef *,
+    z_streamp ));
+/* --- inffast.h */
+
+/* simplify the use of the inflate_huft type with some defines */
+#define base more.Base
+#define next more.Next
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+/* inflate codes private state */
+struct inflate_codes_state {
+
+  /* mode */
+  enum {        /* waiting for "i:"=input, "o:"=output, "x:"=nothing */
+      START,    /* x: set up for LEN */
+      LEN,      /* i: get length/literal/eob next */
+      LENEXT,   /* i: getting length extra (have base) */
+      DIST,     /* i: get distance next */
+      DISTEXT,  /* i: getting distance extra */
+      COPY,     /* o: copying bytes in window, waiting for space */
+      LIT,      /* o: got literal, waiting for output space */
+      WASH,     /* o: got eob, possibly still output waiting */
+      END,      /* x: got eob and all data flushed */
+      BADCODE}  /* x: got error */
+    mode;               /* current inflate_codes mode */
+
+  /* mode dependent information */
+  uInt len;
+  union {
+    struct {
+      inflate_huft *tree;       /* pointer into tree */
+      uInt need;                /* bits needed */
+    } code;             /* if LEN or DIST, where in tree */
+    uInt lit;           /* if LIT, literal */
+    struct {
+      uInt get;                 /* bits to get for extra */
+      uInt dist;                /* distance back to copy from */
+    } copy;             /* if EXT or COPY, where and how much */
+  } sub;                /* submode */
+
+  /* mode independent information */
+  Byte lbits;           /* ltree bits decoded per branch */
+  Byte dbits;           /* dtree bits decoder per branch */
+  inflate_huft *ltree;          /* literal/length/eob tree */
+  inflate_huft *dtree;          /* distance tree */
+
+};
+
+
+inflate_codes_statef *inflate_codes_new(bl, bd, tl, td, z)
+uInt bl, bd;
+inflate_huft *tl;
+inflate_huft *td; /* need separate declaration for Borland C++ */
+z_streamp z;
+{
+  inflate_codes_statef *c;
+
+  if ((c = (inflate_codes_statef *)
+       ZALLOC(z,1,sizeof(struct inflate_codes_state))) != Z_NULL)
+  {
+    c->mode = START;
+    c->lbits = (Byte)bl;
+    c->dbits = (Byte)bd;
+    c->ltree = tl;
+    c->dtree = td;
+    Tracev((stderr, "inflate:       codes new\n"));
+  }
+  return c;
+}
+
+
+int inflate_codes(s, z, r)
+inflate_blocks_statef *s;
+z_streamp z;
+int r;
+{
+  uInt j;               /* temporary storage */
+  inflate_huft *t;      /* temporary pointer */
+  uInt e;               /* extra bits or operation */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Bytef *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Bytef *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+  Bytef *f;             /* pointer to copy strings from */
+  inflate_codes_statef *c = s->sub.decode.codes;  /* codes state */
+
+  /* copy input/output information to locals (UPDATE macro restores) */
+  LOAD
+
+  /* process input and output based on current state */
+  while (1) switch (c->mode)
+  {             /* waiting for "i:"=input, "o:"=output, "x:"=nothing */
+    case START:         /* x: set up for LEN */
+#ifndef SLOW
+      if (m >= 258 && n >= 10)
+      {
+        UPDATE
+        r = inflate_fast(c->lbits, c->dbits, c->ltree, c->dtree, s, z);
+        LOAD
+        if (r != Z_OK)
+        {
+          c->mode = r == Z_STREAM_END ? WASH : BADCODE;
+          break;
+        }
+      }
+#endif /* !SLOW */
+      c->sub.code.need = c->lbits;
+      c->sub.code.tree = c->ltree;
+      c->mode = LEN;
+    case LEN:           /* i: get length/literal/eob next */
+      j = c->sub.code.need;
+      NEEDBITS(j)
+      t = c->sub.code.tree + ((uInt)b & inflate_mask[j]);
+      DUMPBITS(t->bits)
+      e = (uInt)(t->exop);
+      if (e == 0)               /* literal */
+      {
+        c->sub.lit = t->base;
+        Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                 "inflate:         literal '%c'\n" :
+                 "inflate:         literal 0x%02x\n", t->base));
+        c->mode = LIT;
+        break;
+      }
+      if (e & 16)               /* length */
+      {
+        c->sub.copy.get = e & 15;
+        c->len = t->base;
+        c->mode = LENEXT;
+        break;
+      }
+      if ((e & 64) == 0)        /* next table */
+      {
+        c->sub.code.need = e;
+        c->sub.code.tree = t->next;
+        break;
+      }
+      if (e & 32)               /* end of block */
+      {
+        Tracevv((stderr, "inflate:         end of block\n"));
+        c->mode = WASH;
+        break;
+      }
+      c->mode = BADCODE;        /* invalid code */
+      z->msg = (char*)"invalid literal/length code";
+      r = Z_DATA_ERROR;
+      LEAVE
+    case LENEXT:        /* i: getting length extra (have base) */
+      j = c->sub.copy.get;
+      NEEDBITS(j)
+      c->len += (uInt)b & inflate_mask[j];
+      DUMPBITS(j)
+      c->sub.code.need = c->dbits;
+      c->sub.code.tree = c->dtree;
+      Tracevv((stderr, "inflate:         length %u\n", c->len));
+      c->mode = DIST;
+    case DIST:          /* i: get distance next */
+      j = c->sub.code.need;
+      NEEDBITS(j)
+      t = c->sub.code.tree + ((uInt)b & inflate_mask[j]);
+      DUMPBITS(t->bits)
+      e = (uInt)(t->exop);
+      if (e & 16)               /* distance */
+      {
+        c->sub.copy.get = e & 15;
+        c->sub.copy.dist = t->base;
+        c->mode = DISTEXT;
+        break;
+      }
+      if ((e & 64) == 0)        /* next table */
+      {
+        c->sub.code.need = e;
+        c->sub.code.tree = t->next;
+        break;
+      }
+      c->mode = BADCODE;        /* invalid code */
+      z->msg = (char*)"invalid distance code";
+      r = Z_DATA_ERROR;
+      LEAVE
+    case DISTEXT:       /* i: getting distance extra */
+      j = c->sub.copy.get;
+      NEEDBITS(j)
+      c->sub.copy.dist += (uInt)b & inflate_mask[j];
+      DUMPBITS(j)
+      Tracevv((stderr, "inflate:         distance %u\n", c->sub.copy.dist));
+      c->mode = COPY;
+    case COPY:          /* o: copying bytes in window, waiting for space */
+#ifndef __TURBOC__ /* Turbo C bug for following expression */
+      f = (uInt)(q - s->window) < c->sub.copy.dist ?
+          s->end - (c->sub.copy.dist - (q - s->window)) :
+          q - c->sub.copy.dist;
+#else
+      f = q - c->sub.copy.dist;
+      if ((uInt)(q - s->window) < c->sub.copy.dist)
+        f = s->end - (c->sub.copy.dist - (uInt)(q - s->window));
+#endif
+      while (c->len)
+      {
+        NEEDOUT
+        OUTBYTE(*f++)
+        if (f == s->end)
+          f = s->window;
+        c->len--;
+      }
+      c->mode = START;
+      break;
+    case LIT:           /* o: got literal, waiting for output space */
+      NEEDOUT
+      OUTBYTE(c->sub.lit)
+      c->mode = START;
+      break;
+    case WASH:          /* o: got eob, possibly more output */
+      FLUSH
+      if (s->read != s->write)
+        LEAVE
+      c->mode = END;
+    case END:
+      r = Z_STREAM_END;
+      LEAVE
+    case BADCODE:       /* x: got error */
+      r = Z_DATA_ERROR;
+      LEAVE
+    default:
+      r = Z_STREAM_ERROR;
+      LEAVE
+  }
+}
+
+
+void inflate_codes_free(c, z)
+inflate_codes_statef *c;
+z_streamp z;
+{
+  ZFREE(z, c);
+  Tracev((stderr, "inflate:       codes free\n"));
+}
+/* --- infcodes.c */
+
+/* +++ infutil.c */
+/* inflate_util.c -- data and routines common to blocks and codes
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+/* #include "infblock.h" */
+/* #include "inftrees.h" */
+/* #include "infcodes.h" */
+/* #include "infutil.h" */
+
+#ifndef NO_DUMMY_DECL
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+#endif
+
+/* And'ing with mask[n] masks the lower n bits */
+uInt inflate_mask[17] = {
+    0x0000,
+    0x0001, 0x0003, 0x0007, 0x000f, 0x001f, 0x003f, 0x007f, 0x00ff,
+    0x01ff, 0x03ff, 0x07ff, 0x0fff, 0x1fff, 0x3fff, 0x7fff, 0xffff
+};
+
+
+/* copy as much as possible from the sliding window to the output area */
+int inflate_flush(s, z, r)
+inflate_blocks_statef *s;
+z_streamp z;
+int r;
+{
+  uInt n;
+  Bytef *p;
+  Bytef *q;
+
+  /* local copies of source and destination pointers */
+  p = z->next_out;
+  q = s->read;
+
+  /* compute number of bytes to copy as far as end of window */
+  n = (uInt)((q <= s->write ? s->write : s->end) - q);
+  if (n > z->avail_out) n = z->avail_out;
+  if (n && r == Z_BUF_ERROR) r = Z_OK;
+
+  /* update counters */
+  z->avail_out -= n;
+  z->total_out += n;
+
+  /* update check information */
+  if (s->checkfn != Z_NULL)
+    z->adler = s->check = (*s->checkfn)(s->check, q, n);
+
+  /* copy as far as end of window */
+  if (p != Z_NULL) {
+    zmemcpy(p, q, n);
+    p += n;
+  }
+  q += n;
+
+  /* see if more to copy at beginning of window */
+  if (q == s->end)
+  {
+    /* wrap pointers */
+    q = s->window;
+    if (s->write == s->end)
+      s->write = s->window;
+
+    /* compute bytes to copy */
+    n = (uInt)(s->write - q);
+    if (n > z->avail_out) n = z->avail_out;
+    if (n && r == Z_BUF_ERROR) r = Z_OK;
+
+    /* update counters */
+    z->avail_out -= n;
+    z->total_out += n;
+
+    /* update check information */
+    if (s->checkfn != Z_NULL)
+      z->adler = s->check = (*s->checkfn)(s->check, q, n);
+
+    /* copy */
+    if (p != Z_NULL) {
+      zmemcpy(p, q, n);
+      p += n;
+    }
+    q += n;
+  }
+
+  /* update pointers */
+  z->next_out = p;
+  s->read = q;
+
+  /* done */
+  return r;
+}
+/* --- infutil.c */
+
+/* +++ inffast.c */
+/* inffast.c -- process literals and length/distance pairs fast
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* #include "zutil.h" */
+/* #include "inftrees.h" */
+/* #include "infblock.h" */
+/* #include "infcodes.h" */
+/* #include "infutil.h" */
+/* #include "inffast.h" */
+
+#ifndef NO_DUMMY_DECL
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+#endif
+
+/* simplify the use of the inflate_huft type with some defines */
+#define base more.Base
+#define next more.Next
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+/* macros for bit input with no checking and for returning unused bytes */
+#define GRABBITS(j) {while(k<(j)){b|=((uLong)NEXTBYTE)<<k;k+=8;}}
+#define UNGRAB {n+=(c=k>>3);p-=c;k&=7;}
+
+/* Called with number of bytes left to write in window at least 258
+   (the maximum string length) and number of input bytes available
+   at least ten.  The ten bytes are six bytes for the longest length/
+   distance pair plus four bytes for overloading the bit buffer. */
+
+int inflate_fast(bl, bd, tl, td, s, z)
+uInt bl, bd;
+inflate_huft *tl;
+inflate_huft *td; /* need separate declaration for Borland C++ */
+inflate_blocks_statef *s;
+z_streamp z;
+{
+  inflate_huft *t;      /* temporary pointer */
+  uInt e;               /* extra bits or operation */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Bytef *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Bytef *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+  uInt ml;              /* mask for literal/length tree */
+  uInt md;              /* mask for distance tree */
+  uInt c;               /* bytes to copy */
+  uInt d;               /* distance back to copy from */
+  Bytef *r;             /* copy source pointer */
+
+  /* load input, output, bit values */
+  LOAD
+
+  /* initialize masks */
+  ml = inflate_mask[bl];
+  md = inflate_mask[bd];
+
+  /* do until not enough input or output space for fast loop */
+  do {                          /* assume called with m >= 258 && n >= 10 */
+    /* get literal/length code */
+    GRABBITS(20)                /* max bits for literal/length code */
+    if ((e = (t = tl + ((uInt)b & ml))->exop) == 0)
+    {
+      DUMPBITS(t->bits)
+      Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                "inflate:         * literal '%c'\n" :
+                "inflate:         * literal 0x%02x\n", t->base));
+      *q++ = (Byte)t->base;
+      m--;
+      continue;
+    }
+    do {
+      DUMPBITS(t->bits)
+      if (e & 16)
+      {
+        /* get extra bits for length */
+        e &= 15;
+        c = t->base + ((uInt)b & inflate_mask[e]);
+        DUMPBITS(e)
+        Tracevv((stderr, "inflate:         * length %u\n", c));
+
+        /* decode distance base of block to copy */
+        GRABBITS(15);           /* max bits for distance code */
+        e = (t = td + ((uInt)b & md))->exop;
+        do {
+          DUMPBITS(t->bits)
+          if (e & 16)
+          {
+            /* get extra bits to add to distance base */
+            e &= 15;
+            GRABBITS(e)         /* get extra bits (up to 13) */
+            d = t->base + ((uInt)b & inflate_mask[e]);
+            DUMPBITS(e)
+            Tracevv((stderr, "inflate:         * distance %u\n", d));
+
+            /* do the copy */
+            m -= c;
+            if ((uInt)(q - s->window) >= d)     /* offset before dest */
+            {                                   /*  just copy */
+              r = q - d;
+              *q++ = *r++;  c--;        /* minimum count is three, */
+              *q++ = *r++;  c--;        /*  so unroll loop a little */
+            }
+            else                        /* else offset after destination */
+            {
+              e = d - (uInt)(q - s->window); /* bytes from offset to end */
+              r = s->end - e;           /* pointer to offset */
+              if (c > e)                /* if source crosses, */
+              {
+                c -= e;                 /* copy to end of window */
+                do {
+                  *q++ = *r++;
+                } while (--e);
+                r = s->window;          /* copy rest from start of window */
+              }
+            }
+            do {                        /* copy all or what's left */
+              *q++ = *r++;
+            } while (--c);
+            break;
+          }
+          else if ((e & 64) == 0)
+            e = (t = t->next + ((uInt)b & inflate_mask[e]))->exop;
+          else
+          {
+            z->msg = (char*)"invalid distance code";
+            UNGRAB
+            UPDATE
+            return Z_DATA_ERROR;
+          }
+        } while (1);
+        break;
+      }
+      if ((e & 64) == 0)
+      {
+        if ((e = (t = t->next + ((uInt)b & inflate_mask[e]))->exop) == 0)
+        {
+          DUMPBITS(t->bits)
+          Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                    "inflate:         * literal '%c'\n" :
+                    "inflate:         * literal 0x%02x\n", t->base));
+          *q++ = (Byte)t->base;
+          m--;
+          break;
+        }
+      }
+      else if (e & 32)
+      {
+        Tracevv((stderr, "inflate:         * end of block\n"));
+        UNGRAB
+        UPDATE
+        return Z_STREAM_END;
+      }
+      else
+      {
+        z->msg = (char*)"invalid literal/length code";
+        UNGRAB
+        UPDATE
+        return Z_DATA_ERROR;
+      }
+    } while (1);
+  } while (m >= 258 && n >= 10);
+
+  /* not enough input or output--restore pointers and return */
+  UNGRAB
+  UPDATE
+  return Z_OK;
+}
+/* --- inffast.c */
+
+/* +++ zutil.c */
+/* zutil.c -- target dependent utility functions for the compression library
+ * Copyright (C) 1995-1996 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* From: zutil.c,v 1.17 1996/07/24 13:41:12 me Exp $ */
+
+/* #include "zutil.h" */
+
+#ifndef NO_DUMMY_DECL
+struct internal_state      {int dummy;}; /* for buggy compilers */
+#endif
+
+#ifndef STDC
+extern void exit OF((int));
+#endif
+
+const char *z_errmsg[10] = {
+"need dictionary",     /* Z_NEED_DICT       2  */
+"stream end",          /* Z_STREAM_END      1  */
+"",                    /* Z_OK              0  */
+"file error",          /* Z_ERRNO         (-1) */
+"stream error",        /* Z_STREAM_ERROR  (-2) */
+"data error",          /* Z_DATA_ERROR    (-3) */
+"insufficient memory", /* Z_MEM_ERROR     (-4) */
+"buffer error",        /* Z_BUF_ERROR     (-5) */
+"incompatible version",/* Z_VERSION_ERROR (-6) */
+""};
+
+
+const char *zlibVersion()
+{
+    return ZLIB_VERSION;
+}
+
+#ifdef DEBUG_ZLIB
+void z_error (m)
+    char *m;
+{
+    fprintf(stderr, "%s\n", m);
+    exit(1);
+}
+#endif
+
+#ifndef HAVE_MEMCPY
+
+void zmemcpy(dest, source, len)
+    Bytef* dest;
+    Bytef* source;
+    uInt  len;
+{
+    if (len == 0) return;
+    do {
+        *dest++ = *source++; /* ??? to be unrolled */
+    } while (--len != 0);
+}
+
+int zmemcmp(s1, s2, len)
+    Bytef* s1;
+    Bytef* s2;
+    uInt  len;
+{
+    uInt j;
+
+    for (j = 0; j < len; j++) {
+        if (s1[j] != s2[j]) return 2*(s1[j] > s2[j])-1;
+    }
+    return 0;
+}
+
+void zmemzero(dest, len)
+    Bytef* dest;
+    uInt  len;
+{
+    if (len == 0) return;
+    do {
+        *dest++ = 0;  /* ??? to be unrolled */
+    } while (--len != 0);
+}
+#endif
+
+#ifdef __TURBOC__
+#if (defined( __BORLANDC__) || !defined(SMALL_MEDIUM)) && !defined(__32BIT__)
+/* Small and medium model in Turbo C are for now limited to near allocation
+ * with reduced MAX_WBITS and MAX_MEM_LEVEL
+ */
+#  define MY_ZCALLOC
+
+/* Turbo C malloc() does not allow dynamic allocation of 64K bytes
+ * and farmalloc(64K) returns a pointer with an offset of 8, so we
+ * must fix the pointer. Warning: the pointer must be put back to its
+ * original form in order to free it, use zcfree().
+ */
+
+#define MAX_PTR 10
+/* 10*64K = 640K */
+
+local int next_ptr = 0;
+
+typedef struct ptr_table_s {
+    voidpf org_ptr;
+    voidpf new_ptr;
+} ptr_table;
+
+local ptr_table table[MAX_PTR];
+/* This table is used to remember the original form of pointers
+ * to large buffers (64K). Such pointers are normalized with a zero offset.
+ * Since MSDOS is not a preemptive multitasking OS, this table is not
+ * protected from concurrent access. This hack doesn't work anyway on
+ * a protected system like OS/2. Use Microsoft C instead.
+ */
+
+voidpf zcalloc (voidpf opaque, unsigned items, unsigned size)
+{
+    voidpf buf = opaque; /* just to make some compilers happy */
+    ulg bsize = (ulg)items*size;
+
+    /* If we allocate less than 65520 bytes, we assume that farmalloc
+     * will return a usable pointer which doesn't have to be normalized.
+     */
+    if (bsize < 65520L) {
+        buf = farmalloc(bsize);
+        if (*(ush*)&buf != 0) return buf;
+    } else {
+        buf = farmalloc(bsize + 16L);
+    }
+    if (buf == NULL || next_ptr >= MAX_PTR) return NULL;
+    table[next_ptr].org_ptr = buf;
+
+    /* Normalize the pointer to seg:0 */
+    *((ush*)&buf+1) += ((ush)((uch*)buf-0) + 15) >> 4;
+    *(ush*)&buf = 0;
+    table[next_ptr++].new_ptr = buf;
+    return buf;
+}
+
+void  zcfree (voidpf opaque, voidpf ptr)
+{
+    int n;
+    if (*(ush*)&ptr != 0) { /* object < 64K */
+        farfree(ptr);
+        return;
+    }
+    /* Find the original pointer */
+    for (n = 0; n < next_ptr; n++) {
+        if (ptr != table[n].new_ptr) continue;
+
+        farfree(table[n].org_ptr);
+        while (++n < next_ptr) {
+            table[n-1] = table[n];
+        }
+        next_ptr--;
+        return;
+    }
+    ptr = opaque; /* just to make some compilers happy */
+    Assert(0, "zcfree: ptr not found");
+}
+#endif
+#endif /* __TURBOC__ */
+
+
+#if defined(M_I86) && !defined(__32BIT__)
+/* Microsoft C in 16-bit mode */
+
+#  define MY_ZCALLOC
+
+#if (!defined(_MSC_VER) || (_MSC_VER < 600))
+#  define _halloc  halloc
+#  define _hfree   hfree
+#endif
+
+voidpf zcalloc (voidpf opaque, unsigned items, unsigned size)
+{
+    if (opaque) opaque = 0; /* to make compiler happy */
+    return _halloc((long)items, size);
+}
+
+void  zcfree (voidpf opaque, voidpf ptr)
+{
+    if (opaque) opaque = 0; /* to make compiler happy */
+    _hfree(ptr);
+}
+
+#endif /* MSC */
+
+
+#ifndef MY_ZCALLOC /* Any system without a special alloc function */
+
+#ifndef STDC
+extern voidp  calloc OF((uInt items, uInt size));
+extern void   free   OF((voidpf ptr));
+#endif
+
+voidpf zcalloc (opaque, items, size)
+    voidpf opaque;
+    unsigned items;
+    unsigned size;
+{
+    if (opaque) items += size - size; /* make compiler happy */
+    return (voidpf)calloc(items, size);
+}
+
+void  zcfree (opaque, ptr)
+    voidpf opaque;
+    voidpf ptr;
+{
+    free(ptr);
+    if (opaque) return; /* make compiler happy */
+}
+
+#endif /* MY_ZCALLOC */
+/* --- zutil.c */
+
+/* +++ adler32.c */
+/* adler32.c -- compute the Adler-32 checksum of a data stream
+ * Copyright (C) 1995-1996 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* From: adler32.c,v 1.10 1996/05/22 11:52:18 me Exp $ */
+
+/* #include "zlib.h" */
+
+#define BASE 65521L /* largest prime smaller than 65536 */
+#define NMAX 5552
+/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
+
+#define DO1(buf,i)  {s1 += buf[i]; s2 += s1;}
+#define DO2(buf,i)  DO1(buf,i); DO1(buf,i+1);
+#define DO4(buf,i)  DO2(buf,i); DO2(buf,i+2);
+#define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
+#define DO16(buf)   DO8(buf,0); DO8(buf,8);
+
+/* ========================================================================= */
+uLong adler32(adler, buf, len)
+    uLong adler;
+    const Bytef *buf;
+    uInt len;
+{
+    unsigned long s1 = adler & 0xffff;
+    unsigned long s2 = (adler >> 16) & 0xffff;
+    int k;
+
+    if (buf == Z_NULL) return 1L;
+
+    while (len > 0) {
+        k = len < NMAX ? len : NMAX;
+        len -= k;
+        while (k >= 16) {
+            DO16(buf);
+	    buf += 16;
+            k -= 16;
+        }
+        if (k != 0) do {
+            s1 += *buf++;
+	    s2 += s1;
+        } while (--k);
+        s1 %= BASE;
+        s2 %= BASE;
+    }
+    return (s2 << 16) | s1;
+}
+/* --- adler32.c */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/dump/dump_zlib.h linuxppc64_2_4/drivers/dump/dump_zlib.h
--- linux-2.4.19/drivers/dump/dump_zlib.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/dump/dump_zlib.h	Mon Jul  8 14:13:07 2002
@@ -0,0 +1,1010 @@
+/*	$Id: dump_zlib.h,v 1.1 2002/07/08 19:13:07 tinglett Exp $	*/
+
+/*
+ * This file is derived from zlib.h and zconf.h from the zlib-1.0.4
+ * distribution by Jean-loup Gailly and Mark Adler, with some additions
+ * by Paul Mackerras to aid in implementing Deflate compression and
+ * decompression for PPP packets.
+ */
+
+/*
+ *  ==FILEVERSION 971127==
+ *
+ * This marker is used by the Linux installation script to determine
+ * whether an up-to-date version of this file is already installed.
+ */
+
+
+/* +++ zlib.h */
+/* zlib.h -- interface of the 'zlib' general purpose compression library
+  version 1.0.4, Jul 24th, 1996.
+
+  Copyright (C) 1995-1996 Jean-loup Gailly and Mark Adler
+
+  This software is provided 'as-is', without any express or implied
+  warranty.  In no event will the authors be held liable for any damages
+  arising from the use of this software.
+
+  Permission is granted to anyone to use this software for any purpose,
+  including commercial applications, and to alter it and redistribute it
+  freely, subject to the following restrictions:
+
+  1. The origin of this software must not be misrepresented; you must not
+     claim that you wrote the original software. If you use this software
+     in a product, an acknowledgment in the product documentation would be
+     appreciated but is not required.
+  2. Altered source versions must be plainly marked as such, and must not be
+     misrepresented as being the original software.
+  3. This notice may not be removed or altered from any source distribution.
+
+  Jean-loup Gailly        Mark Adler
+  gzip@prep.ai.mit.edu    madler@alumni.caltech.edu
+
+
+  The data format used by the zlib library is described by RFCs (Request for
+  Comments) 1950 to 1952 in the files ftp://ds.internic.net/rfc/rfc1950.txt
+  (zlib format), rfc1951.txt (deflate format) and rfc1952.txt (gzip format).
+*/
+
+#ifndef _ZLIB_H
+#define _ZLIB_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/* +++ zconf.h */
+/* zconf.h -- configuration of the zlib compression library
+ * Copyright (C) 1995-1996 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* From: zconf.h,v 1.20 1996/07/02 15:09:28 me Exp $ */
+
+#ifndef _ZCONF_H
+#define _ZCONF_H
+
+/*
+ * If you *really* need a unique prefix for all types and library functions,
+ * compile with -DZ_PREFIX. The "standard" zlib should be compiled without it.
+ */
+#ifdef Z_PREFIX
+#  define deflateInit_	z_deflateInit_
+#  define deflate	z_deflate
+#  define deflateEnd	z_deflateEnd
+#  define inflateInit_ 	z_inflateInit_
+#  define inflate	z_inflate
+#  define inflateEnd	z_inflateEnd
+#  define deflateInit2_	z_deflateInit2_
+#  define deflateSetDictionary z_deflateSetDictionary
+#  define deflateCopy	z_deflateCopy
+#  define deflateReset	z_deflateReset
+#  define deflateParams	z_deflateParams
+#  define inflateInit2_	z_inflateInit2_
+#  define inflateSetDictionary z_inflateSetDictionary
+#  define inflateSync	z_inflateSync
+#  define inflateReset	z_inflateReset
+#  define compress	z_compress
+#  define uncompress	z_uncompress
+#  define adler32	z_adler32
+#  define crc32		z_crc32
+#  define get_crc_table z_get_crc_table
+
+#  define Byte		z_Byte
+#  define uInt		z_uInt
+#  define uLong		z_uLong
+#  define Bytef	        z_Bytef
+#  define charf		z_charf
+#  define intf		z_intf
+#  define uIntf		z_uIntf
+#  define uLongf	z_uLongf
+#  define voidpf	z_voidpf
+#  define voidp		z_voidp
+#endif
+
+#if (defined(_WIN32) || defined(__WIN32__)) && !defined(WIN32)
+#  define WIN32
+#endif
+#if defined(__GNUC__) || defined(WIN32) || defined(__386__) || defined(i386)
+#  ifndef __32BIT__
+#    define __32BIT__
+#  endif
+#endif
+#if defined(__MSDOS__) && !defined(MSDOS)
+#  define MSDOS
+#endif
+
+/*
+ * Compile with -DMAXSEG_64K if the alloc function cannot allocate more
+ * than 64k bytes at a time (needed on systems with 16-bit int).
+ */
+#if defined(MSDOS) && !defined(__32BIT__)
+#  define MAXSEG_64K
+#endif
+#ifdef MSDOS
+#  define UNALIGNED_OK
+#endif
+
+#if (defined(MSDOS) || defined(_WINDOWS) || defined(WIN32))  && !defined(STDC)
+#  define STDC
+#endif
+#if (defined(__STDC__) || defined(__cplusplus)) && !defined(STDC)
+#  define STDC
+#endif
+
+#ifndef STDC
+#  ifndef const /* cannot use !defined(STDC) && !defined(const) on Mac */
+#    define const
+#  endif
+#endif
+
+/* Some Mac compilers merge all .h files incorrectly: */
+#if defined(__MWERKS__) || defined(applec) ||defined(THINK_C) ||defined(__SC__)
+#  define NO_DUMMY_DECL
+#endif
+
+/* Maximum value for memLevel in deflateInit2 */
+#ifndef MAX_MEM_LEVEL
+#  ifdef MAXSEG_64K
+#    define MAX_MEM_LEVEL 8
+#  else
+#    define MAX_MEM_LEVEL 9
+#  endif
+#endif
+
+/* Maximum value for windowBits in deflateInit2 and inflateInit2 */
+#ifndef MAX_WBITS
+#  define MAX_WBITS   15 /* 32K LZ77 window */
+#endif
+
+/* The memory requirements for deflate are (in bytes):
+            1 << (windowBits+2)   +  1 << (memLevel+9)
+ that is: 128K for windowBits=15  +  128K for memLevel = 8  (default values)
+ plus a few kilobytes for small objects. For example, if you want to reduce
+ the default memory requirements from 256K to 128K, compile with
+     make CFLAGS="-O -DMAX_WBITS=14 -DMAX_MEM_LEVEL=7"
+ Of course this will generally degrade compression (there's no free lunch).
+
+   The memory requirements for inflate are (in bytes) 1 << windowBits
+ that is, 32K for windowBits=15 (default value) plus a few kilobytes
+ for small objects.
+*/
+
+                        /* Type declarations */
+
+#ifndef OF /* function prototypes */
+#  ifdef STDC
+#    define OF(args)  args
+#  else
+#    define OF(args)  ()
+#  endif
+#endif
+
+/* The following definitions for FAR are needed only for MSDOS mixed
+ * model programming (small or medium model with some far allocations).
+ * This was tested only with MSC; for other MSDOS compilers you may have
+ * to define NO_MEMCPY in zutil.h.  If you don't need the mixed model,
+ * just define FAR to be empty.
+ */
+#if (defined(M_I86SM) || defined(M_I86MM)) && !defined(__32BIT__)
+   /* MSC small or medium model */
+#  define SMALL_MEDIUM
+#  ifdef _MSC_VER
+#    define FAR __far
+#  else
+#    define FAR far
+#  endif
+#endif
+#if defined(__BORLANDC__) && (defined(__SMALL__) || defined(__MEDIUM__))
+#  ifndef __32BIT__
+#    define SMALL_MEDIUM
+#    define FAR __far
+#  endif
+#endif
+#ifndef FAR
+#   define FAR
+#endif
+
+typedef unsigned char  Byte;  /* 8 bits */
+typedef unsigned int   uInt;  /* 16 bits or more */
+typedef unsigned long  uLong; /* 32 bits or more */
+
+#if defined(__BORLANDC__) && defined(SMALL_MEDIUM)
+   /* Borland C/C++ ignores FAR inside typedef */
+#  define Bytef Byte FAR
+#else
+   typedef Byte  FAR Bytef;
+#endif
+typedef char  FAR charf;
+typedef int   FAR intf;
+typedef uInt  FAR uIntf;
+typedef uLong FAR uLongf;
+
+#ifdef STDC
+   typedef void FAR *voidpf;
+   typedef void     *voidp;
+#else
+   typedef Byte FAR *voidpf;
+   typedef Byte     *voidp;
+#endif
+
+
+/* Compile with -DZLIB_DLL for Windows DLL support */
+#if (defined(_WINDOWS) || defined(WINDOWS)) && defined(ZLIB_DLL)
+#  include <windows.h>
+#  define EXPORT  WINAPI
+#else
+#  define EXPORT
+#endif
+
+#endif /* _ZCONF_H */
+/* --- zconf.h */
+
+#define ZLIB_VERSION "1.0.4P"
+
+/* 
+     The 'zlib' compression library provides in-memory compression and
+  decompression functions, including integrity checks of the uncompressed
+  data.  This version of the library supports only one compression method
+  (deflation) but other algorithms may be added later and will have the same
+  stream interface.
+
+     For compression the application must provide the output buffer and
+  may optionally provide the input buffer for optimization. For decompression,
+  the application must provide the input buffer and may optionally provide
+  the output buffer for optimization.
+
+     Compression can be done in a single step if the buffers are large
+  enough (for example if an input file is mmap'ed), or can be done by
+  repeated calls of the compression function.  In the latter case, the
+  application must provide more input and/or consume the output
+  (providing more output space) before each call.
+
+     The library does not install any signal handler. It is recommended to
+  add at least a handler for SIGSEGV when decompressing; the library checks
+  the consistency of the input data whenever possible but may go nuts
+  for some forms of corrupted input.
+*/
+
+typedef voidpf (*alloc_func) OF((voidpf opaque, uInt items, uInt size));
+typedef void   (*free_func)  OF((voidpf opaque, voidpf address));
+
+struct internal_state;
+
+typedef struct z_stream_s {
+    Bytef    *next_in;  /* next input byte */
+    uInt     avail_in;  /* number of bytes available at next_in */
+    uLong    total_in;  /* total nb of input bytes read so far */
+
+    Bytef    *next_out; /* next output byte should be put there */
+    uInt     avail_out; /* remaining free space at next_out */
+    uLong    total_out; /* total nb of bytes output so far */
+
+    char     *msg;      /* last error message, NULL if no error */
+    struct internal_state FAR *state; /* not visible by applications */
+
+    alloc_func zalloc;  /* used to allocate the internal state */
+    free_func  zfree;   /* used to free the internal state */
+    voidpf     opaque;  /* private data object passed to zalloc and zfree */
+
+    int     data_type;  /* best guess about the data type: ascii or binary */
+    uLong   adler;      /* adler32 value of the uncompressed data */
+    uLong   reserved;   /* reserved for future use */
+} z_stream;
+
+typedef z_stream FAR *z_streamp;
+
+/*
+   The application must update next_in and avail_in when avail_in has
+   dropped to zero. It must update next_out and avail_out when avail_out
+   has dropped to zero. The application must initialize zalloc, zfree and
+   opaque before calling the init function. All other fields are set by the
+   compression library and must not be updated by the application.
+
+   The opaque value provided by the application will be passed as the first
+   parameter for calls of zalloc and zfree. This can be useful for custom
+   memory management. The compression library attaches no meaning to the
+   opaque value.
+
+   zalloc must return Z_NULL if there is not enough memory for the object.
+   On 16-bit systems, the functions zalloc and zfree must be able to allocate
+   exactly 65536 bytes, but will not be required to allocate more than this
+   if the symbol MAXSEG_64K is defined (see zconf.h). WARNING: On MSDOS,
+   pointers returned by zalloc for objects of exactly 65536 bytes *must*
+   have their offset normalized to zero. The default allocation function
+   provided by this library ensures this (see zutil.c). To reduce memory
+   requirements and avoid any allocation of 64K objects, at the expense of
+   compression ratio, compile the library with -DMAX_WBITS=14 (see zconf.h).
+
+   The fields total_in and total_out can be used for statistics or
+   progress reports. After compression, total_in holds the total size of
+   the uncompressed data and may be saved for use in the decompressor
+   (particularly if the decompressor wants to decompress everything in
+   a single step).
+*/
+
+                        /* constants */
+
+#define Z_NO_FLUSH      0
+#define Z_PARTIAL_FLUSH 1
+#define Z_PACKET_FLUSH	2
+#define Z_SYNC_FLUSH    3
+#define Z_FULL_FLUSH    4
+#define Z_FINISH        5
+/* Allowed flush values; see deflate() below for details */
+
+#define Z_OK            0
+#define Z_STREAM_END    1
+#define Z_NEED_DICT     2
+#define Z_ERRNO        (-1)
+#define Z_STREAM_ERROR (-2)
+#define Z_DATA_ERROR   (-3)
+#define Z_MEM_ERROR    (-4)
+#define Z_BUF_ERROR    (-5)
+#define Z_VERSION_ERROR (-6)
+/* Return codes for the compression/decompression functions. Negative
+ * values are errors, positive values are used for special but normal events.
+ */
+
+#define Z_NO_COMPRESSION         0
+#define Z_BEST_SPEED             1
+#define Z_BEST_COMPRESSION       9
+#define Z_DEFAULT_COMPRESSION  (-1)
+/* compression levels */
+
+#define Z_FILTERED            1
+#define Z_HUFFMAN_ONLY        2
+#define Z_DEFAULT_STRATEGY    0
+/* compression strategy; see deflateInit2() below for details */
+
+#define Z_BINARY   0
+#define Z_ASCII    1
+#define Z_UNKNOWN  2
+/* Possible values of the data_type field */
+
+#define Z_DEFLATED   8
+/* The deflate compression method (the only one supported in this version) */
+
+#define Z_NULL  0  /* for initializing zalloc, zfree, opaque */
+
+#define zlib_version zlibVersion()
+/* for compatibility with versions < 1.0.2 */
+
+                        /* basic functions */
+
+extern const char * EXPORT zlibVersion OF((void));
+/* The application can compare zlibVersion and ZLIB_VERSION for consistency.
+   If the first character differs, the library code actually used is
+   not compatible with the zlib.h header file used by the application.
+   This check is automatically made by deflateInit and inflateInit.
+ */
+
+/* 
+extern int EXPORT deflateInit OF((z_streamp strm, int level));
+
+     Initializes the internal stream state for compression. The fields
+   zalloc, zfree and opaque must be initialized before by the caller.
+   If zalloc and zfree are set to Z_NULL, deflateInit updates them to
+   use default allocation functions.
+
+     The compression level must be Z_DEFAULT_COMPRESSION, or between 0 and 9:
+   1 gives best speed, 9 gives best compression, 0 gives no compression at
+   all (the input data is simply copied a block at a time).
+   Z_DEFAULT_COMPRESSION requests a default compromise between speed and
+   compression (currently equivalent to level 6).
+
+     deflateInit returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_STREAM_ERROR if level is not a valid compression level,
+   Z_VERSION_ERROR if the zlib library version (zlib_version) is incompatible
+   with the version assumed by the caller (ZLIB_VERSION).
+   msg is set to null if there is no error message.  deflateInit does not
+   perform any compression: this will be done by deflate().
+*/
+
+
+extern int EXPORT deflate OF((z_streamp strm, int flush));
+/*
+  Performs one or both of the following actions:
+
+  - Compress more input starting at next_in and update next_in and avail_in
+    accordingly. If not all input can be processed (because there is not
+    enough room in the output buffer), next_in and avail_in are updated and
+    processing will resume at this point for the next call of deflate().
+
+  - Provide more output starting at next_out and update next_out and avail_out
+    accordingly. This action is forced if the parameter flush is non zero.
+    Forcing flush frequently degrades the compression ratio, so this parameter
+    should be set only when necessary (in interactive applications).
+    Some output may be provided even if flush is not set.
+
+  Before the call of deflate(), the application should ensure that at least
+  one of the actions is possible, by providing more input and/or consuming
+  more output, and updating avail_in or avail_out accordingly; avail_out
+  should never be zero before the call. The application can consume the
+  compressed output when it wants, for example when the output buffer is full
+  (avail_out == 0), or after each call of deflate(). If deflate returns Z_OK
+  and with zero avail_out, it must be called again after making room in the
+  output buffer because there might be more output pending.
+
+    If the parameter flush is set to Z_PARTIAL_FLUSH, the current compression
+  block is terminated and flushed to the output buffer so that the
+  decompressor can get all input data available so far. For method 9, a future
+  variant on method 8, the current block will be flushed but not terminated.
+  Z_SYNC_FLUSH has the same effect as partial flush except that the compressed
+  output is byte aligned (the compressor can clear its internal bit buffer)
+  and the current block is always terminated; this can be useful if the
+  compressor has to be restarted from scratch after an interruption (in which
+  case the internal state of the compressor may be lost).
+    If flush is set to Z_FULL_FLUSH, the compression block is terminated, a
+  special marker is output and the compression dictionary is discarded; this
+  is useful to allow the decompressor to synchronize if one compressed block
+  has been damaged (see inflateSync below).  Flushing degrades compression and
+  so should be used only when necessary.  Using Z_FULL_FLUSH too often can
+  seriously degrade the compression. If deflate returns with avail_out == 0,
+  this function must be called again with the same value of the flush
+  parameter and more output space (updated avail_out), until the flush is
+  complete (deflate returns with non-zero avail_out).
+
+    If the parameter flush is set to Z_PACKET_FLUSH, the compression
+  block is terminated, and a zero-length stored block is output,
+  omitting the length bytes (the effect of this is that the 3-bit type
+  code 000 for a stored block is output, and the output is then
+  byte-aligned).  This is designed for use at the end of a PPP packet.
+
+    If the parameter flush is set to Z_FINISH, pending input is processed,
+  pending output is flushed and deflate returns with Z_STREAM_END if there
+  was enough output space; if deflate returns with Z_OK, this function must be
+  called again with Z_FINISH and more output space (updated avail_out) but no
+  more input data, until it returns with Z_STREAM_END or an error. After
+  deflate has returned Z_STREAM_END, the only possible operations on the
+  stream are deflateReset or deflateEnd.
+  
+    Z_FINISH can be used immediately after deflateInit if all the compression
+  is to be done in a single step. In this case, avail_out must be at least
+  0.1% larger than avail_in plus 12 bytes.  If deflate does not return
+  Z_STREAM_END, then it must be called again as described above.
+
+    deflate() may update data_type if it can make a good guess about
+  the input data type (Z_ASCII or Z_BINARY). In doubt, the data is considered
+  binary. This field is only for information purposes and does not affect
+  the compression algorithm in any manner.
+
+    deflate() returns Z_OK if some progress has been made (more input
+  processed or more output produced), Z_STREAM_END if all input has been
+  consumed and all output has been produced (only when flush is set to
+  Z_FINISH), Z_STREAM_ERROR if the stream state was inconsistent (for example
+  if next_in or next_out was NULL), Z_BUF_ERROR if no progress is possible.
+*/
+
+
+extern int EXPORT deflateEnd OF((z_streamp strm));
+/*
+     All dynamically allocated data structures for this stream are freed.
+   This function discards any unprocessed input and does not flush any
+   pending output.
+
+     deflateEnd returns Z_OK if success, Z_STREAM_ERROR if the
+   stream state was inconsistent, Z_DATA_ERROR if the stream was freed
+   prematurely (some input or output was discarded). In the error case,
+   msg may be set but then points to a static string (which must not be
+   deallocated).
+*/
+
+
+/* 
+extern int EXPORT inflateInit OF((z_streamp strm));
+
+     Initializes the internal stream state for decompression. The fields
+   zalloc, zfree and opaque must be initialized before by the caller.  If
+   zalloc and zfree are set to Z_NULL, inflateInit updates them to use default
+   allocation functions.
+
+     inflateInit returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_VERSION_ERROR if the zlib library version is incompatible
+   with the version assumed by the caller.  msg is set to null if there is no
+   error message. inflateInit does not perform any decompression: this will be
+   done by inflate().
+*/
+
+
+extern int EXPORT inflate OF((z_streamp strm, int flush));
+/*
+  Performs one or both of the following actions:
+
+  - Decompress more input starting at next_in and update next_in and avail_in
+    accordingly. If not all input can be processed (because there is not
+    enough room in the output buffer), next_in is updated and processing
+    will resume at this point for the next call of inflate().
+
+  - Provide more output starting at next_out and update next_out and avail_out
+    accordingly.  inflate() provides as much output as possible, until there
+    is no more input data or no more space in the output buffer (see below
+    about the flush parameter).
+
+  Before the call of inflate(), the application should ensure that at least
+  one of the actions is possible, by providing more input and/or consuming
+  more output, and updating the next_* and avail_* values accordingly.
+  The application can consume the uncompressed output when it wants, for
+  example when the output buffer is full (avail_out == 0), or after each
+  call of inflate(). If inflate returns Z_OK and with zero avail_out, it
+  must be called again after making room in the output buffer because there
+  might be more output pending.
+
+    If the parameter flush is set to Z_PARTIAL_FLUSH or Z_PACKET_FLUSH,
+  inflate flushes as much output as possible to the output buffer. The
+  flushing behavior of inflate is not specified for values of the flush
+  parameter other than Z_PARTIAL_FLUSH, Z_PACKET_FLUSH or Z_FINISH, but the
+  current implementation actually flushes as much output as possible
+  anyway.  For Z_PACKET_FLUSH, inflate checks that once all the input data
+  has been consumed, it is expecting to see the length field of a stored
+  block; if not, it returns Z_DATA_ERROR.
+
+    inflate() should normally be called until it returns Z_STREAM_END or an
+  error. However if all decompression is to be performed in a single step
+  (a single call of inflate), the parameter flush should be set to
+  Z_FINISH. In this case all pending input is processed and all pending
+  output is flushed; avail_out must be large enough to hold all the
+  uncompressed data. (The size of the uncompressed data may have been saved
+  by the compressor for this purpose.) The next operation on this stream must
+  be inflateEnd to deallocate the decompression state. The use of Z_FINISH
+  is never required, but can be used to inform inflate that a faster routine
+  may be used for the single inflate() call.
+
+    inflate() returns Z_OK if some progress has been made (more input
+  processed or more output produced), Z_STREAM_END if the end of the
+  compressed data has been reached and all uncompressed output has been
+  produced, Z_NEED_DICT if a preset dictionary is needed at this point (see
+  inflateSetDictionary below), Z_DATA_ERROR if the input data was corrupted,
+  Z_STREAM_ERROR if the stream structure was inconsistent (for example if
+  next_in or next_out was NULL), Z_MEM_ERROR if there was not enough memory,
+  Z_BUF_ERROR if no progress is possible or if there was not enough room in
+  the output buffer when Z_FINISH is used. In the Z_DATA_ERROR case, the
+  application may then call inflateSync to look for a good compression block.
+  In the Z_NEED_DICT case, strm->adler is set to the Adler32 value of the
+  dictionary chosen by the compressor.
+*/
+
+
+extern int EXPORT inflateEnd OF((z_streamp strm));
+/*
+     All dynamically allocated data structures for this stream are freed.
+   This function discards any unprocessed input and does not flush any
+   pending output.
+
+     inflateEnd returns Z_OK if success, Z_STREAM_ERROR if the stream state
+   was inconsistent. In the error case, msg may be set but then points to a
+   static string (which must not be deallocated).
+*/
+
+                        /* Advanced functions */
+
+/*
+    The following functions are needed only in some special applications.
+*/
+
+/*   
+extern int EXPORT deflateInit2 OF((z_streamp strm,
+                                   int  level,
+                                   int  method,
+                                   int  windowBits,
+                                   int  memLevel,
+                                   int  strategy));
+
+     This is another version of deflateInit with more compression options. The
+   fields next_in, zalloc, zfree and opaque must be initialized before by
+   the caller.
+
+     The method parameter is the compression method. It must be Z_DEFLATED in
+   this version of the library. (Method 9 will allow a 64K history buffer and
+   partial block flushes.)
+
+     The windowBits parameter is the base two logarithm of the window size
+   (the size of the history buffer).  It should be in the range 8..15 for this
+   version of the library (the value 16 will be allowed for method 9). Larger
+   values of this parameter result in better compression at the expense of
+   memory usage. The default value is 15 if deflateInit is used instead.
+
+     The memLevel parameter specifies how much memory should be allocated
+   for the internal compression state. memLevel=1 uses minimum memory but
+   is slow and reduces compression ratio; memLevel=9 uses maximum memory
+   for optimal speed. The default value is 8. See zconf.h for total memory
+   usage as a function of windowBits and memLevel.
+
+     The strategy parameter is used to tune the compression algorithm. Use the
+   value Z_DEFAULT_STRATEGY for normal data, Z_FILTERED for data produced by a
+   filter (or predictor), or Z_HUFFMAN_ONLY to force Huffman encoding only (no
+   string match).  Filtered data consists mostly of small values with a
+   somewhat random distribution. In this case, the compression algorithm is
+   tuned to compress them better. The effect of Z_FILTERED is to force more
+   Huffman coding and less string matching; it is somewhat intermediate
+   between Z_DEFAULT and Z_HUFFMAN_ONLY. The strategy parameter only affects
+   the compression ratio but not the correctness of the compressed output even
+   if it is not set appropriately.
+
+     If next_in is not null, the library will use this buffer to hold also
+   some history information; the buffer must either hold the entire input
+   data, or have at least 1<<(windowBits+1) bytes and be writable. If next_in
+   is null, the library will allocate its own history buffer (and leave next_in
+   null). next_out need not be provided here but must be provided by the
+   application for the next call of deflate().
+
+     If the history buffer is provided by the application, next_in must
+   must never be changed by the application since the compressor maintains
+   information inside this buffer from call to call; the application
+   must provide more input only by increasing avail_in. next_in is always
+   reset by the library in this case.
+
+      deflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was
+   not enough memory, Z_STREAM_ERROR if a parameter is invalid (such as
+   an invalid method). msg is set to null if there is no error message.
+   deflateInit2 does not perform any compression: this will be done by
+   deflate(). 
+*/
+                            
+extern int EXPORT deflateSetDictionary OF((z_streamp strm,
+                                           const Bytef *dictionary,
+				           uInt  dictLength));
+/*
+     Initializes the compression dictionary (history buffer) from the given
+   byte sequence without producing any compressed output. This function must
+   be called immediately after deflateInit or deflateInit2, before any call
+   of deflate. The compressor and decompressor must use exactly the same
+   dictionary (see inflateSetDictionary).
+     The dictionary should consist of strings (byte sequences) that are likely
+   to be encountered later in the data to be compressed, with the most commonly
+   used strings preferably put towards the end of the dictionary. Using a
+   dictionary is most useful when the data to be compressed is short and
+   can be predicted with good accuracy; the data can then be compressed better
+   than with the default empty dictionary. In this version of the library,
+   only the last 32K bytes of the dictionary are used.
+     Upon return of this function, strm->adler is set to the Adler32 value
+   of the dictionary; the decompressor may later use this value to determine
+   which dictionary has been used by the compressor. (The Adler32 value
+   applies to the whole dictionary even if only a subset of the dictionary is
+   actually used by the compressor.)
+
+     deflateSetDictionary returns Z_OK if success, or Z_STREAM_ERROR if a
+   parameter is invalid (such as NULL dictionary) or the stream state
+   is inconsistent (for example if deflate has already been called for this
+   stream). deflateSetDictionary does not perform any compression: this will
+   be done by deflate(). 
+*/
+
+extern int EXPORT deflateCopy OF((z_streamp dest,
+                                  z_streamp source));
+/*
+     Sets the destination stream as a complete copy of the source stream.  If
+   the source stream is using an application-supplied history buffer, a new
+   buffer is allocated for the destination stream.  The compressed output
+   buffer is always application-supplied. It's the responsibility of the
+   application to provide the correct values of next_out and avail_out for the
+   next call of deflate.
+
+     This function can be useful when several compression strategies will be
+   tried, for example when there are several ways of pre-processing the input
+   data with a filter. The streams that will be discarded should then be freed
+   by calling deflateEnd.  Note that deflateCopy duplicates the internal
+   compression state which can be quite large, so this strategy is slow and
+   can consume lots of memory.
+
+     deflateCopy returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_STREAM_ERROR if the source stream state was inconsistent
+   (such as zalloc being NULL). msg is left unchanged in both source and
+   destination.
+*/
+
+extern int EXPORT deflateReset OF((z_streamp strm));
+/*
+     This function is equivalent to deflateEnd followed by deflateInit,
+   but does not free and reallocate all the internal compression state.
+   The stream will keep the same compression level and any other attributes
+   that may have been set by deflateInit2.
+
+      deflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
+   stream state was inconsistent (such as zalloc or state being NULL).
+*/
+
+extern int EXPORT deflateParams OF((z_streamp strm, int level, int strategy));
+/*
+     Dynamically update the compression level and compression strategy.
+   This can be used to switch between compression and straight copy of
+   the input data, or to switch to a different kind of input data requiring
+   a different strategy. If the compression level is changed, the input
+   available so far is compressed with the old level (and may be flushed);
+   the new level will take effect only at the next call of deflate().
+
+     Before the call of deflateParams, the stream state must be set as for
+   a call of deflate(), since the currently available input may have to
+   be compressed and flushed. In particular, strm->avail_out must be non-zero.
+
+     deflateParams returns Z_OK if success, Z_STREAM_ERROR if the source
+   stream state was inconsistent or if a parameter was invalid, Z_BUF_ERROR
+   if strm->avail_out was zero.
+*/
+
+extern int EXPORT deflateOutputPending OF((z_streamp strm));
+/*
+     Returns the number of bytes of output which are immediately
+   available from the compressor (i.e. without any further input
+   or flush).
+*/
+
+/*   
+extern int EXPORT inflateInit2 OF((z_streamp strm,
+                                   int  windowBits));
+
+     This is another version of inflateInit with more compression options. The
+   fields next_out, zalloc, zfree and opaque must be initialized before by
+   the caller.
+
+     The windowBits parameter is the base two logarithm of the maximum window
+   size (the size of the history buffer).  It should be in the range 8..15 for
+   this version of the library (the value 16 will be allowed soon). The
+   default value is 15 if inflateInit is used instead. If a compressed stream
+   with a larger window size is given as input, inflate() will return with
+   the error code Z_DATA_ERROR instead of trying to allocate a larger window.
+
+     If next_out is not null, the library will use this buffer for the history
+   buffer; the buffer must either be large enough to hold the entire output
+   data, or have at least 1<<windowBits bytes.  If next_out is null, the
+   library will allocate its own buffer (and leave next_out null). next_in
+   need not be provided here but must be provided by the application for the
+   next call of inflate().
+
+     If the history buffer is provided by the application, next_out must
+   never be changed by the application since the decompressor maintains
+   history information inside this buffer from call to call; the application
+   can only reset next_out to the beginning of the history buffer when
+   avail_out is zero and all output has been consumed.
+
+      inflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was
+   not enough memory, Z_STREAM_ERROR if a parameter is invalid (such as
+   windowBits < 8). msg is set to null if there is no error message.
+   inflateInit2 does not perform any decompression: this will be done by
+   inflate().
+*/
+
+extern int EXPORT inflateSetDictionary OF((z_streamp strm,
+				           const Bytef *dictionary,
+					   uInt  dictLength));
+/*
+     Initializes the decompression dictionary (history buffer) from the given
+   uncompressed byte sequence. This function must be called immediately after
+   a call of inflate if this call returned Z_NEED_DICT. The dictionary chosen
+   by the compressor can be determined from the Adler32 value returned by this
+   call of inflate. The compressor and decompressor must use exactly the same
+   dictionary (see deflateSetDictionary).
+
+     inflateSetDictionary returns Z_OK if success, Z_STREAM_ERROR if a
+   parameter is invalid (such as NULL dictionary) or the stream state is
+   inconsistent, Z_DATA_ERROR if the given dictionary doesn't match the
+   expected one (incorrect Adler32 value). inflateSetDictionary does not
+   perform any decompression: this will be done by subsequent calls of
+   inflate().
+*/
+
+extern int EXPORT inflateSync OF((z_streamp strm));
+/* 
+    Skips invalid compressed data until the special marker (see deflate()
+  above) can be found, or until all available input is skipped. No output
+  is provided.
+
+    inflateSync returns Z_OK if the special marker has been found, Z_BUF_ERROR
+  if no more input was provided, Z_DATA_ERROR if no marker has been found,
+  or Z_STREAM_ERROR if the stream structure was inconsistent. In the success
+  case, the application may save the current current value of total_in which
+  indicates where valid compressed data was found. In the error case, the
+  application may repeatedly call inflateSync, providing more input each time,
+  until success or end of the input data.
+*/
+
+extern int EXPORT inflateReset OF((z_streamp strm));
+/*
+     This function is equivalent to inflateEnd followed by inflateInit,
+   but does not free and reallocate all the internal decompression state.
+   The stream will keep attributes that may have been set by inflateInit2.
+
+      inflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
+   stream state was inconsistent (such as zalloc or state being NULL).
+*/
+
+extern int inflateIncomp OF((z_stream *strm));
+/*
+     This function adds the data at next_in (avail_in bytes) to the output
+   history without performing any output.  There must be no pending output,
+   and the decompressor must be expecting to see the start of a block.
+   Calling this function is equivalent to decompressing a stored block
+   containing the data at next_in (except that the data is not output).
+*/
+
+                        /* utility functions */
+
+/*
+     The following utility functions are implemented on top of the
+   basic stream-oriented functions. To simplify the interface, some
+   default options are assumed (compression level, window size,
+   standard memory allocation functions). The source code of these
+   utility functions can easily be modified if you need special options.
+*/
+
+extern int EXPORT compress OF((Bytef *dest,   uLongf *destLen,
+			       const Bytef *source, uLong sourceLen));
+/*
+     Compresses the source buffer into the destination buffer.  sourceLen is
+   the byte length of the source buffer. Upon entry, destLen is the total
+   size of the destination buffer, which must be at least 0.1% larger than
+   sourceLen plus 12 bytes. Upon exit, destLen is the actual size of the
+   compressed buffer.
+     This function can be used to compress a whole file at once if the
+   input file is mmap'ed.
+     compress returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_BUF_ERROR if there was not enough room in the output
+   buffer.
+*/
+
+extern int EXPORT uncompress OF((Bytef *dest,   uLongf *destLen,
+				 const Bytef *source, uLong sourceLen));
+/*
+     Decompresses the source buffer into the destination buffer.  sourceLen is
+   the byte length of the source buffer. Upon entry, destLen is the total
+   size of the destination buffer, which must be large enough to hold the
+   entire uncompressed data. (The size of the uncompressed data must have
+   been saved previously by the compressor and transmitted to the decompressor
+   by some mechanism outside the scope of this compression library.)
+   Upon exit, destLen is the actual size of the compressed buffer.
+     This function can be used to decompress a whole file at once if the
+   input file is mmap'ed.
+
+     uncompress returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_BUF_ERROR if there was not enough room in the output
+   buffer, or Z_DATA_ERROR if the input data was corrupted.
+*/
+
+
+typedef voidp gzFile;
+
+extern gzFile EXPORT gzopen  OF((const char *path, const char *mode));
+/*
+     Opens a gzip (.gz) file for reading or writing. The mode parameter
+   is as in fopen ("rb" or "wb") but can also include a compression level
+   ("wb9").  gzopen can be used to read a file which is not in gzip format;
+   in this case gzread will directly read from the file without decompression.
+     gzopen returns NULL if the file could not be opened or if there was
+   insufficient memory to allocate the (de)compression state; errno
+   can be checked to distinguish the two cases (if errno is zero, the
+   zlib error is Z_MEM_ERROR).
+*/
+
+extern gzFile EXPORT gzdopen  OF((int fd, const char *mode));
+/*
+     gzdopen() associates a gzFile with the file descriptor fd.  File
+   descriptors are obtained from calls like open, dup, creat, pipe or
+   fileno (in the file has been previously opened with fopen).
+   The mode parameter is as in gzopen.
+     The next call of gzclose on the returned gzFile will also close the
+   file descriptor fd, just like fclose(fdopen(fd), mode) closes the file
+   descriptor fd. If you want to keep fd open, use gzdopen(dup(fd), mode).
+     gzdopen returns NULL if there was insufficient memory to allocate
+   the (de)compression state.
+*/
+
+extern int EXPORT    gzread  OF((gzFile file, voidp buf, unsigned len));
+/*
+     Reads the given number of uncompressed bytes from the compressed file.
+   If the input file was not in gzip format, gzread copies the given number
+   of bytes into the buffer.
+     gzread returns the number of uncompressed bytes actually read (0 for
+   end of file, -1 for error). */
+
+extern int EXPORT    gzwrite OF((gzFile file, const voidp buf, unsigned len));
+/*
+     Writes the given number of uncompressed bytes into the compressed file.
+   gzwrite returns the number of uncompressed bytes actually written
+   (0 in case of error).
+*/
+
+extern int EXPORT    gzflush OF((gzFile file, int flush));
+/*
+     Flushes all pending output into the compressed file. The parameter
+   flush is as in the deflate() function. The return value is the zlib
+   error number (see function gzerror below). gzflush returns Z_OK if
+   the flush parameter is Z_FINISH and all output could be flushed.
+     gzflush should be called only when strictly necessary because it can
+   degrade compression.
+*/
+
+extern int EXPORT    gzclose OF((gzFile file));
+/*
+     Flushes all pending output if necessary, closes the compressed file
+   and deallocates all the (de)compression state. The return value is the zlib
+   error number (see function gzerror below).
+*/
+
+extern const char * EXPORT gzerror OF((gzFile file, int *errnum));
+/*
+     Returns the error message for the last error which occurred on the
+   given compressed file. errnum is set to zlib error number. If an
+   error occurred in the file system and not in the compression library,
+   errnum is set to Z_ERRNO and the application may consult errno
+   to get the exact error code.
+*/
+
+                        /* checksum functions */
+
+/*
+     These functions are not related to compression but are exported
+   anyway because they might be useful in applications using the
+   compression library.
+*/
+
+extern uLong EXPORT adler32 OF((uLong adler, const Bytef *buf, uInt len));
+
+/*
+     Update a running Adler-32 checksum with the bytes buf[0..len-1] and
+   return the updated checksum. If buf is NULL, this function returns
+   the required initial value for the checksum.
+   An Adler-32 checksum is almost as reliable as a CRC32 but can be computed
+   much faster. Usage example:
+
+     uLong adler = adler32(0L, Z_NULL, 0);
+
+     while (read_buffer(buffer, length) != EOF) {
+       adler = adler32(adler, buffer, length);
+     }
+     if (adler != original_adler) error();
+*/
+
+extern uLong EXPORT crc32   OF((uLong crc, const Bytef *buf, uInt len));
+/*
+     Update a running crc with the bytes buf[0..len-1] and return the updated
+   crc. If buf is NULL, this function returns the required initial value
+   for the crc. Pre- and post-conditioning (one's complement) is performed
+   within this function so it shouldn't be done by the application.
+   Usage example:
+
+     uLong crc = crc32(0L, Z_NULL, 0);
+
+     while (read_buffer(buffer, length) != EOF) {
+       crc = crc32(crc, buffer, length);
+     }
+     if (crc != original_crc) error();
+*/
+
+
+                        /* various hacks, don't look :) */
+
+/* deflateInit and inflateInit are macros to allow checking the zlib version
+ * and the compiler's view of z_stream:
+ */
+extern int EXPORT deflateInit_ OF((z_streamp strm, int level,
+			           const char *version, int stream_size));
+extern int EXPORT inflateInit_ OF((z_streamp strm,
+				   const char *version, int stream_size));
+extern int EXPORT deflateInit2_ OF((z_streamp strm, int  level, int  method,
+				    int windowBits, int memLevel, int strategy,
+				    const char *version, int stream_size));
+extern int EXPORT inflateInit2_ OF((z_streamp strm, int  windowBits,
+				    const char *version, int stream_size));
+#define deflateInit(strm, level) \
+        deflateInit_((strm), (level),       ZLIB_VERSION, sizeof(z_stream))
+#define inflateInit(strm) \
+        inflateInit_((strm),                ZLIB_VERSION, sizeof(z_stream))
+#define deflateInit2(strm, level, method, windowBits, memLevel, strategy) \
+        deflateInit2_((strm),(level),(method),(windowBits),(memLevel),\
+		      (strategy),           ZLIB_VERSION, sizeof(z_stream))
+#define inflateInit2(strm, windowBits) \
+        inflateInit2_((strm), (windowBits), ZLIB_VERSION, sizeof(z_stream))
+
+#if !defined(_Z_UTIL_H) && !defined(NO_DUMMY_DECL)
+    struct internal_state {int dummy;}; /* hack for buggy compilers */
+#endif
+
+uLongf *get_crc_table OF((void)); /* can be used by asm versions of crc32() */
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _ZLIB_H */
+/* --- zlib.h */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/ide/sl82c105.c linuxppc64_2_4/drivers/ide/sl82c105.c
--- linux-2.4.19/drivers/ide/sl82c105.c	Fri Sep  7 12:45:30 2001
+++ linuxppc64_2_4/drivers/ide/sl82c105.c	Mon Dec 17 15:04:57 2001
@@ -156,6 +156,29 @@
 }
 
 /*
+ * Reset the controller.
+ * If we are using INTC under a w83c553 we need to use a magic test
+ * bit to do this.  Return zero if successful (or applicable).
+ * 
+ */
+static int sl82c105_hard_reset(ide_drive_t *drive)
+{
+	ide_hwif_t *hwif = HWIF(drive);
+	struct pci_dev *dev = hwif->pci_dev;
+	unsigned int reg;
+
+	pci_read_config_dword(dev, 0x40, &reg);	/* LEGIRQ register */
+	if (reg & (1<<11)) {	/* Using INTC? */
+		printk("sl82c105: resetting device\n");
+		pci_read_config_dword(dev, 0x7e, &reg);
+		pci_write_config_word(dev, 0x7e, reg | (1<<2));
+		pci_write_config_word(dev, 0x7e, reg & (~(1<<2)));
+		return 0;
+	}
+	return 1;
+}
+
+/*
  * Our own dmaproc, only to intercept ide_dma_check
  */
 static int sl82c105_dmaproc(ide_dma_action_t func, ide_drive_t *drive)
@@ -171,6 +194,11 @@
 	case ide_dma_off:
 		config_for_pio(drive, 4, 0);
 		break;
+	case ide_dma_lostirq:
+	case ide_dma_timeout:
+	        if (sl82c105_hard_reset(drive) == 0)
+			return 0;
+	        break;
 	default:
 		break;
 	}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/Makefile linuxppc64_2_4/drivers/iseries/Makefile
--- linux-2.4.19/drivers/iseries/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/Makefile	Thu Oct 11 11:10:49 2001
@@ -0,0 +1,43 @@
+#
+# Makefile for the iSeries-specific device drivers.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now inherited from the
+# parent makes..
+#
+
+# The target object and module list name.
+
+# O_TARGET	:= macintosh.o
+
+O_TARGET  := iseries.o
+
+# Objects that export symbols.
+
+# export-objs	:= adb.o rtc.o mac_hid.o via-pmu.o
+
+export-objs := veth.o viocons.o viotape.o viodasd.o viocd.o viopath.o
+
+# Object file lists.
+
+obj-y	:=
+obj-m	:=
+obj-n	:=
+obj-	:=
+
+# Each configuration option enables a list of files.
+
+obj-$(CONFIG_VETH) += veth.o
+obj-$(CONFIG_VIOCONS) += viocons.o
+obj-$(CONFIG_VIOPATH) += viopath.o
+obj-$(CONFIG_VIOTAPE) += viotape.o
+obj-$(CONFIG_VIODASD) += viodasd.o
+obj-$(CONFIG_VIOCD)   += viocd.o
+
+# The global Rules.make.
+
+include $(TOPDIR)/Rules.make
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/veth.c linuxppc64_2_4/drivers/iseries/veth.c
--- linux-2.4.19/drivers/iseries/veth.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/veth.c	Thu Aug  8 12:54:35 2002
@@ -0,0 +1,1729 @@
+/* File veth.c created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/**************************************************************************/
+/*                                                                        */
+/* IBM eServer iSeries Virtual Ethernet Device Driver                     */
+/* Copyright (C) 2001 Kyle A. Lucke (klucke@us.ibm.com), IBM Corp.        */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the implementation of a virtual ethernet device   */
+/* for use with iSeries LPAR Linux.  It utilizes low-level message passing*/
+/* provided by the hypervisor to enable an ethernet-like network device   */
+/* that can be used to enable inter-partition communications on the same  */
+/* physical iSeries.                                                      */
+/*                                                                        */
+/* The iSeries LPAR hypervisor has currently defined the ability for a    */
+/* partition to communicate on up to 16 different virtual ethernets, all  */
+/* dynamically configurable, at least for an OS/400 partition.  The       */
+/* dynamic nature is not supported for Linux yet.                         */
+/*                                                                        */
+/* Each virtual ethernet a given Linux partition participates in will     */
+/* cause a network device with the form ethXX to be created,              */
+/*                                                                        */
+/* The virtual ethernet a given ethXX virtual ethernet device talks on    */
+/* can be determined either by dumping /proc/iSeries/veth/vethX, where    */
+/* X is the virtual ethernet number, and the netdevice name will be       */
+/* printed out.  The virtual ethernet a given ethX device communicates on */
+/* is also printed to the printk() buffer at module load time.            */
+/*                                                                        */
+/* This driver (and others like it on other partitions) is responsible for*/
+/* routing packets to and from other partitions.  The MAC addresses used  */
+/* by the virtual ethernets contain meaning, and should not be modified.  */
+/* Doing so could disable the ability of your Linux partition to          */
+/* communicate with the other OS/400 partitions on your physical iSeries. */
+/* Similarly, setting the MAC address to something other than the         */
+/* "virtual burned-in" address is not allowed, for the same reason.       */
+/*                                                                        */
+/* Notes:                                                                 */
+/*                                                                        */
+/* 1. Although there is the capability to talk on multiple shared         */
+/*    ethernets to communicate to the same partition, each shared         */
+/*    ethernet to a given partition X will use a finite, shared amount    */
+/*    of hypervisor messages to do the communication.  So having 2 shared */
+/*    ethernets to the same remote partition DOES NOT double the          */
+/*    available bandwidth.  Each of the 2 shared ethernets will share the */
+/*    same bandwidth available to another.                                */
+/*                                                                        */
+/* 2. It is allowed to have a virtual ethernet that does not communicate  */
+/*    with any other partition.  It won't do anything, but it's allowed.  */
+/*                                                                        */
+/* 3. There is no "loopback" mode for a virtual ethernet device.  If you  */
+/*    send a packet to your own mac address, it will just be dropped, you */
+/*    won't get it on the receive side.  Such a thing could be done,      */
+/*    but my default driver DOES NOT do so.                               */
+/*                                                                        */
+/* 4. Multicast addressing is implemented via broadcasting the multicast  */
+/*    frames to other partitions.  It is the responsibility of the        */
+/*    receiving partition to filter the addresses desired.                */
+/*                                                                        */
+/* 5. This module utilizes several different bottom half handlers for     */
+/*    non-high-use path function (setup, error handling, etc.).  Multiple */
+/*    bottom halves were used because only one would not keep up to the   */
+/*    much faster iSeries device drivers this Linux driver is talking to. */
+/*    All hi-priority work (receiving frames, handling frame acks) is done*/
+/*    in the interrupt handler for maximum performance.                   */
+/*                                                                        */
+/* Tunable parameters:                                                    */
+/*                                                                        */
+/* VethBuffersToAllocate: This compile time option defaults to 120. It can*/
+/* be safely changed to something greater or less than the default.  It   */
+/* controls how much memory Linux will allocate per remote partition it is*/
+/* communicating with.  The user can play with this to see how it affects */
+/* performance, packets dropped, etc.  Without trying to understand the   */
+/* complete driver, it can be thought of as the maximum number of packets */
+/* outstanding to a remote partition at a time.                           */
+/*                                                                        */
+/**************************************************************************/
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#include <asm/iSeries/mf.h>
+#include <asm/uaccess.h>
+
+#ifndef _VETH_H
+#include "veth.h"
+#endif
+#ifndef _HVLPCONFIG_H
+#include <asm/iSeries/HvLpConfig.h>
+#endif
+#ifndef _VETH_PROC_H
+#include <asm/iSeries/veth-proc.h>
+#endif
+#ifndef _HVTYPES_H
+#include <asm/iSeries/HvTypes.h>
+#endif
+#ifndef _ISERIES_PROC_H
+#include <asm/iSeries/iSeries_proc.h>
+#endif
+#include <asm/semaphore.h>
+#include <linux/proc_fs.h>
+
+
+#define veth_printk(fmt, args...) \
+printk(KERN_INFO "%s: " fmt, __FILE__, ## args)
+
+#define veth_error_printk(fmt, args...) \
+printk(KERN_ERR "(%s:%3.3d) ERROR: " fmt, __FILE__, __LINE__ , ## args)
+
+static const char *version __initdata = "v1.0 03/11/2002  Kyle Lucke, klucke@us.ibm.com\n";
+
+static int probed __initdata = 0;
+#define VethBuffersToAllocate 120
+
+static struct VethFabricMgr *mFabricMgr = NULL;
+static struct proc_dir_entry *veth_proc_root = NULL;
+
+DECLARE_MUTEX_LOCKED(VethProcSemaphore);
+
+static int veth_open(struct net_device *dev);
+static int veth_close(struct net_device *dev);
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static void veth_handleEvent(struct HvLpEvent *, struct pt_regs *);
+static void veth_handleAck(struct HvLpEvent *);
+static void veth_handleInt(struct HvLpEvent *);
+static void veth_openConnections(void);
+static void veth_openConnection(u8, int lockMe);
+static void veth_closeConnection(u8, int lockMe);
+static void veth_intFinishOpeningConnections(void *, int number);
+static void veth_finishOpeningConnections(void *);
+static void veth_finishOpeningConnectionsLocked(struct VethLpConnection *);
+static int veth_multicast_wanted(struct VethPort *port, u64 dest);
+static void veth_set_multicast_list(struct net_device *dev);
+
+static void veth_sendCap(struct VethLpConnection *);
+static void veth_sendMonitor(struct VethLpConnection *);
+static void veth_takeCap(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_takeCapAck(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_takeMonitorAck(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_msgsInit(struct VethLpConnection *connection);
+static void veth_recycleMsgByNum(struct VethLpConnection *, u16);
+static void veth_recycleMsg(struct VethLpConnection *, struct VethMsg *);
+static void veth_capBh(struct VethLpConnection *);
+static void veth_capAckBh(struct VethLpConnection *);
+static void veth_monitorAckBh(struct VethLpConnection *);
+static void veth_takeFrames(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_pTransmit(struct sk_buff *skb, HvLpIndex remoteLp, struct net_device *dev);
+static struct net_device_stats *veth_get_stats(struct net_device *dev);
+static void veth_intFinishMsgsInit(void *, int);
+static void veth_finishMsgsInit(struct VethLpConnection *connection);
+static void veth_intFinishCapBh(void *, int);
+static void veth_finishCapBh(struct VethLpConnection *connection);
+static void veth_finishCapBhLocked(struct VethLpConnection *connection);
+static void veth_finishSendCap(struct VethLpConnection *connection);
+static void veth_timedAck(unsigned long connectionPtr);
+#ifdef MODULE
+static void veth_waitForEnd(void);
+#endif
+static void veth_failMe(struct VethLpConnection *connection);
+
+extern struct pci_dev *iSeries_veth_dev;
+
+int __init veth_probe(void)
+{
+	struct net_device *dev = NULL;
+	struct VethPort *port = NULL;
+	int vlansFound = 0;
+	int displayVersion = 0;
+
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	int vlanIndex = 0;
+
+	if (probed)
+		return -ENODEV;
+	probed = 1;
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			vlansFound++;
+
+			dev = init_etherdev(NULL, sizeof(struct VethPort));
+
+			if (dev == NULL) {
+				veth_error_printk("Unable to allocate net_device structure!\n");
+				break;
+			}
+
+			if (!dev->priv)
+				dev->priv = kmalloc(sizeof(struct VethPort), GFP_KERNEL);
+			if (!dev->priv) {
+				veth_error_printk("Unable to allocate memory\n");
+				return -ENOMEM;
+			}
+
+			veth_printk("Found an ethernet device %s (veth=%d) (addr=%p)\n", dev->name, vlanIndex, dev);
+			port = mFabricMgr->mPorts[vlanIndex] = (struct VethPort *) dev->priv;
+			memset(port, 0, sizeof(struct VethPort));
+			rwlock_init(&(port->mMcastGate));
+			mFabricMgr->mPorts[vlanIndex]->mDev = dev;
+
+			dev->dev_addr[0] = 0x02;
+			dev->dev_addr[1] = 0x01;
+			dev->dev_addr[2] = 0xFF;
+			dev->dev_addr[3] = vlanIndex;
+			dev->dev_addr[4] = 0xFF;
+			dev->dev_addr[5] = HvLpConfig_getLpIndex_outline();
+			dev->mtu = 9000;
+
+			memcpy(&(port->mMyAddress), dev->dev_addr, 6);
+
+			dev->open = &veth_open;
+			dev->hard_start_xmit = &veth_start_xmit;
+			dev->stop = &veth_close;
+			dev->get_stats = veth_get_stats;
+			dev->set_multicast_list = &veth_set_multicast_list;
+			dev->do_ioctl = &veth_ioctl;
+			dev->features |= NETIF_F_SG;
+
+			/* display version info if adapter is found */
+			if (!displayVersion) {
+				/* set display flag to TRUE so that */
+				/* we only display this string ONCE */
+				displayVersion = 1;
+				veth_printk("%s", version);
+			}
+
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	if (vlansFound > 0)
+		return 0;
+	else
+		return -ENODEV;
+}
+
+#ifdef MODULE
+MODULE_AUTHOR("Kyle Lucke <klucke@us.ibm.com>");
+MODULE_DESCRIPTION("iSeries Virtual ethernet driver");
+MODULE_LICENSE("GPL");
+
+DECLARE_MUTEX_LOCKED(VethModuleBhDone);
+int VethModuleReopen = 1;
+
+void veth_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	int i = 0;
+	HvLpIndex thisLp = HvLpConfig_getLpIndex_outline();
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	int vlanIndex = 0;
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		if (i != thisLp) {
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(thisLp, i)) {
+				char name[10] = "";
+				sprintf(name, "lpar%d", i);
+				remove_proc_entry(name, veth_proc_root);
+			}
+		}
+	}
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			char name[10] = "";
+			sprintf(name, "veth%d", vlanIndex);
+			remove_proc_entry(name, veth_proc_root);
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	remove_proc_entry("veth", iSeries_proc);
+
+	up(&VethProcSemaphore);
+}
+
+void veth_waitForEnd(void)
+{
+	up(&VethModuleBhDone);
+}
+
+void __exit veth_module_cleanup(void)
+{
+	int i;
+	struct VethFabricMgr *myFm = mFabricMgr;
+	struct tq_struct myBottomHalf;
+	struct net_device *thisOne = NULL;
+
+	VethModuleReopen = 0;
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		veth_closeConnection(i, 1);
+	}
+
+	myBottomHalf.routine = (void *) (void *) veth_waitForEnd;
+
+	queue_task(&myBottomHalf, &tq_immediate);
+	mark_bh(IMMEDIATE_BH);
+
+	down(&VethModuleBhDone);
+
+	HvLpEvent_unregisterHandler(HvLpEvent_Type_VirtualLan);
+
+	mb();
+	mFabricMgr = NULL;
+	mb();
+
+	down(&VethProcSemaphore);
+
+	iSeries_proc_callback(&veth_proc_delete);
+
+	down(&VethProcSemaphore);
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		if (myFm->mConnection[i].mNumberAllocated + myFm->mConnection[i].mNumberRcvMsgs > 0) {
+			mf_deallocateLpEvents(myFm->mConnection[i].mRemoteLp,
+					      HvLpEvent_Type_VirtualLan,
+					      myFm->mConnection[i].mNumberAllocated + myFm->mConnection[i].mNumberRcvMsgs,
+					      NULL, NULL);
+		}
+
+		if (myFm->mConnection[i].mMsgs != NULL) {
+			kfree(myFm->mConnection[i].mMsgs);
+		}
+	}
+
+	for (i = 0; i < HvMaxArchitectedVirtualLans; ++i) {
+		if (myFm->mPorts[i] != NULL) {
+			thisOne = myFm->mPorts[i]->mDev;
+			myFm->mPorts[i] = NULL;
+
+			mb();
+
+			if (thisOne != NULL) {
+				veth_printk("Unregistering %s (veth=%d)\n", thisOne->name, i);
+				unregister_netdev(thisOne);
+			}
+		}
+	}
+
+	kfree(myFm);
+}
+
+module_exit(veth_module_cleanup);
+#endif
+
+
+void veth_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	long i = 0;
+	HvLpIndex thisLp = HvLpConfig_getLpIndex_outline();
+	u16 vlanMap = HvLpConfig_getVirtualLanIndexMap();
+	long vlanIndex = 0;
+
+
+	veth_proc_root = proc_mkdir("veth", iSeries_proc);
+	if (!veth_proc_root)
+		return;
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		if (i != thisLp) {
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(thisLp, i)) {
+				struct proc_dir_entry *ent;
+				char name[10] = "";
+				sprintf(name, "lpar%d", (int) i);
+				ent = create_proc_entry(name, S_IFREG | S_IRUSR, veth_proc_root);
+				if (!ent)
+					return;
+				ent->nlink = 1;
+				ent->data = (void *) i;
+				ent->read_proc = proc_veth_dump_connection;
+				ent->write_proc = NULL;
+			}
+		}
+	}
+
+	while (vlanMap != 0) {
+		int bitOn = vlanMap & 0x8000;
+
+		if (bitOn) {
+			struct proc_dir_entry *ent;
+			char name[10] = "";
+			sprintf(name, "veth%d", (int) vlanIndex);
+			ent = create_proc_entry(name, S_IFREG | S_IRUSR, veth_proc_root);
+			if (!ent)
+				return;
+			ent->nlink = 1;
+			ent->data = (void *) vlanIndex;
+			ent->read_proc = proc_veth_dump_port;
+			ent->write_proc = NULL;
+		}
+
+		++vlanIndex;
+		vlanMap = vlanMap << 1;
+	}
+
+	up(&VethProcSemaphore);
+}
+
+int __init veth_module_init(void)
+{
+	int status;
+	int i;
+
+	mFabricMgr = kmalloc(sizeof(struct VethFabricMgr), GFP_KERNEL);
+	memset(mFabricMgr, 0, sizeof(struct VethFabricMgr));
+	veth_printk("Initializing veth module, fabric mgr (address=%p)\n", mFabricMgr);
+
+	mFabricMgr->mEyecatcher = 0x56455448464D4752ULL;
+	mFabricMgr->mThisLp = HvLpConfig_getLpIndex_outline();
+
+	for (i = 0; i < HvMaxArchitectedLps; ++i) {
+		mFabricMgr->mConnection[i].mEyecatcher = 0x564554484C50434EULL;
+		veth_failMe(mFabricMgr->mConnection + i);
+		spin_lock_init(&mFabricMgr->mConnection[i].mAckGate);
+		spin_lock_init(&mFabricMgr->mConnection[i].mStatusGate);
+	}
+
+	status = veth_probe();
+
+	if (status == 0) {
+		veth_openConnections();
+		iSeries_proc_callback(&veth_proc_init);
+	}
+
+	return status;
+}
+
+module_init(veth_module_init);
+
+static void veth_failMe(struct VethLpConnection *connection)
+{
+	connection->mConnectionStatus.mSentCap = 0;
+	connection->mConnectionStatus.mCapAcked = 0;
+	connection->mConnectionStatus.mGotCap = 0;
+	connection->mConnectionStatus.mGotCapAcked = 0;
+	connection->mConnectionStatus.mSentMonitor = 0;
+	connection->mConnectionStatus.mFailed = 1;
+}
+
+static int veth_open(struct net_device *dev)
+{
+	struct VethPort *port = (struct VethPort *) dev->priv;
+
+	memset(&port->mStats, 0, sizeof(port->mStats));
+	MOD_INC_USE_COUNT;
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int veth_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+
+	MOD_DEC_USE_COUNT;
+
+	return 0;
+}
+
+static struct net_device_stats *veth_get_stats(struct net_device *dev)
+{
+	struct VethPort *port = (struct VethPort *) dev->priv;
+
+	return (&port->mStats);
+}
+
+
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned char *frame = skb->data;
+	HvLpIndex remoteLp = frame[5];
+	int i = 0;
+	int clone = 0;
+
+	if (mFabricMgr == NULL) {
+		veth_error_printk("NULL fabric manager with active ports!\n");
+		netif_stop_queue(dev);
+		return 1;
+	}
+
+	mb();
+
+	if ((*frame & 0x01) != 0x01) {	/* broadcast or multicast */
+		if ((remoteLp != mFabricMgr->mThisLp) && (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, remoteLp)))
+			veth_pTransmit(skb, remoteLp, dev);
+	} else {
+		for (i = 0; i < HvMaxArchitectedLps; ++i) {
+			if (i != mFabricMgr->mThisLp) {
+				if (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, i)) {
+					if (clone)
+						skb = skb_clone(skb, GFP_ATOMIC);
+					else
+						clone = 1;
+
+					/* the ack handles deleting the skb */
+					veth_pTransmit(skb, i, dev);
+				}
+			}
+		}
+	}
+
+	return 0;
+}
+
+static void veth_pTransmit(struct sk_buff *skb, HvLpIndex remoteLp, struct net_device *dev)
+{
+	struct VethLpConnection *connection = mFabricMgr->mConnection + remoteLp;
+	HvLpEvent_Rc returnCode;
+	struct scatterlist sg[VethMaxFramesPerMsg];
+	int nfrags = 0;
+	int nsg;
+
+	if (connection->mConnectionStatus.mFailed != 1) {
+		int rc = 0;
+		struct VethMsg *msg = NULL;
+		VETHSTACKPOP(&(connection->mMsgStack), msg);
+
+		/* We can't handle a fragmented frame if it has 
+		   more than VethMaxFramesPerMsg fragments. 
+		   Attempt to coalesce the fragments if possible,
+		   otherwise drop the frame */
+
+		if ((skb_shinfo(skb)->nr_frags + 1) > VethMaxFramesPerMsg) {
+			veth_printk("Linearizing frame to handle > 6 frags\n");
+			rc = skb_linearize(skb, GFP_ATOMIC);
+		}
+
+		if (msg != NULL && rc == 0 && ((skb->len - 14) <= 9000)) {
+			/* Use a scatterlist for both the fraged un-fraged
+			   case. pci_map_sg has a fastpast for the single
+			   case so we can simplify this code and not take
+			   too big of a perf hit.
+			 */
+
+			if (skb_shinfo(skb)->nr_frags) {	/* fragmented frame */
+				int i = 0;
+
+				sg[nfrags].address = skb->data;
+				sg[nfrags].length = skb->len - skb->data_len;
+				++nfrags;
+
+				do {
+					skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+					sg[nfrags].address = page_address(frag->page) + frag->page_offset;
+					sg[nfrags].length = frag->size;
+					++nfrags;
+					++i;
+				} while (i < skb_shinfo(skb)->nr_frags);
+			} else {	/* unfragmented frame */
+				sg[nfrags].address = skb->data;
+				sg[nfrags].length = skb->len;
+				++nfrags;
+			}
+			/*
+			   Note: nsg may be less than nfrags. Each frag entry
+			   in the skb can only be a page in size, so they can
+			   be contigous in memory but be spread over multiple
+			   frag entries in the skb. pci_map_sg will coalesce 
+			   contiguous fragments and into a single dma address.
+			 */
+
+			nsg = pci_map_sg(iSeries_veth_dev, sg, nfrags, PCI_DMA_TODEVICE);
+
+			/* Is it really necessary to check the length and address fields of the 
+			   first entry here? */
+			if (nsg) {
+				int i = 0;
+				msg->mSkb = skb;
+				do {
+					msg->mEvent.mSendData.mAddress[i] = sg[i].dma_address;
+					msg->mEvent.mSendData.mLength[i] = sg[i].dma_length;
+					++i;
+				} while (i < nsg);
+
+				msg->mEvent.mSendData.mEofMask = (1 << (nsg - 1));
+				test_and_set_bit(0, &(msg->mInUse));
+
+				returnCode = HvCallEvent_signalLpEventFast(remoteLp,
+									   HvLpEvent_Type_VirtualLan,
+									   VethEventTypeFrames,
+									   HvLpEvent_AckInd_NoAck,
+									   HvLpEvent_AckType_ImmediateAck,
+									   connection->mSourceInst,
+									   connection->mTargetInst,
+									   msg->mIndex,
+									   msg->mEvent.mFpData.mData1,
+									   msg->mEvent.mFpData.mData2,
+									   msg->mEvent.mFpData.mData3,
+									   msg->mEvent.mFpData.mData4,
+									   msg->mEvent.mFpData.mData5);
+			} else {
+				returnCode = -1;	/* Bad return code */
+			}
+
+			if (returnCode != HvLpEvent_Rc_Good) {
+				struct VethPort *port = (struct VethPort *) dev->priv;
+				veth_recycleMsg(connection, msg);
+				port->mStats.tx_dropped++;
+			} else {
+				struct VethPort *port = (struct VethPort *) dev->priv;
+				port->mStats.tx_packets++;
+				port->mStats.tx_bytes += skb->len;
+			}
+		} else {
+			struct VethPort *port = (struct VethPort *) dev->priv;
+			port->mStats.tx_dropped++;
+			if (rc)
+				port->mLinearized++;
+			dev_kfree_skb_any(skb);
+		}
+	} else {
+		struct VethPort *port = (struct VethPort *) dev->priv;
+		port->mStats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+	}
+}
+
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+    struct ethtool_cmd ecmd;
+
+    if (cmd != SIOCETHTOOL)
+	return -EOPNOTSUPP;
+    if (copy_from_user(&ecmd, ifr->ifr_data, sizeof(ecmd)))
+	return -EFAULT;
+    switch (ecmd.cmd) {
+	case ETHTOOL_GSET:
+	    ecmd.supported =
+	      (SUPPORTED_1000baseT_Full |
+	       SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+	    ecmd.advertising =
+	      (SUPPORTED_1000baseT_Full |
+	       SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+
+	    ecmd.port = PORT_FIBRE;
+	    ecmd.transceiver = XCVR_INTERNAL;
+	    ecmd.phy_address = 0;
+	    ecmd.speed = SPEED_1000;
+	    ecmd.duplex = DUPLEX_FULL;
+	    ecmd.autoneg = AUTONEG_ENABLE;
+	    ecmd.maxtxpkt = 120;
+	    ecmd.maxrxpkt = 120;
+	    if(copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+		return -EFAULT;
+	    return 0;
+
+	case ETHTOOL_GDRVINFO: {
+		struct ethtool_drvinfo info = {ETHTOOL_GDRVINFO};
+		strncpy(info.driver, "veth", sizeof(info.driver) - 1);
+		info.driver[sizeof(info.driver) - 1] = '\0';
+		strncpy(info.version, "1.0", sizeof(info.version) - 1);
+		if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+		    return -EFAULT;
+		return 0;
+	    }
+	    /* get link status */
+	case ETHTOOL_GLINK: {
+		struct ethtool_value edata = {ETHTOOL_GLINK};
+		edata.data = 1;
+		if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+		    return -EFAULT;
+		return 0;
+	    }
+
+	default:
+	    break;
+    }
+
+#endif
+	return -EOPNOTSUPP;
+}
+
+static void veth_set_multicast_list(struct net_device *dev)
+{
+	char *addrs;
+	struct VethPort *port = (struct VethPort *) dev->priv;
+	u64 newAddress = 0;
+	unsigned long flags;
+
+	write_lock_irqsave(&port->mMcastGate, flags);
+
+	if (dev->flags & IFF_PROMISC) {	/* set promiscuous mode */
+		port->mPromiscuous = 1;
+	} else {
+		struct dev_mc_list *dmi = dev->mc_list;
+
+		if (dev->flags & IFF_ALLMULTI) {
+			port->mAllMcast = 1;
+		} else {
+			int i;
+			/* Update table */
+			port->mNumAddrs = 0;
+
+			for (i = 0; ((i < dev->mc_count) && (i < 12)); i++) {	/* for each address in the list */
+				addrs = dmi->dmi_addr;
+				dmi = dmi->next;
+				if ((*addrs & 0x01) == 1) {	/* multicast address? */
+					memcpy(&newAddress, addrs, 6);
+					newAddress &= 0xFFFFFFFFFFFF0000;
+
+					port->mMcasts[port->mNumAddrs] = newAddress;
+					mb();
+					port->mNumAddrs = port->mNumAddrs + 1;
+				}
+			}
+		}
+	}
+
+	write_unlock_irqrestore(&port->mMcastGate, flags);
+}
+
+
+static void veth_handleEvent(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack) {
+		veth_handleAck(event);
+	} else if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		veth_handleInt(event);
+	}
+}
+
+static void veth_handleAck(struct HvLpEvent *event)
+{
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[event->xTargetLp]);
+	struct VethLpEvent *vethEvent = (struct VethLpEvent *) event;
+
+	switch (event->xSubtype) {
+	case VethEventTypeCap:
+		{
+			veth_takeCapAck(connection, vethEvent);
+			break;
+		}
+	case VethEventTypeMonitor:
+		{
+			veth_takeMonitorAck(connection, vethEvent);
+			break;
+		}
+	default:
+		{
+			veth_error_printk("Unknown ack type %d from lpar %d\n", event->xSubtype, connection->mRemoteLp);
+		}
+	};
+}
+
+static void veth_handleInt(struct HvLpEvent *event)
+{
+	int i = 0;
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[event->xSourceLp]);
+	struct VethLpEvent *vethEvent = (struct VethLpEvent *) event;
+
+	switch (event->xSubtype) {
+	case VethEventTypeCap:
+		{
+			veth_takeCap(connection, vethEvent);
+			break;
+		}
+	case VethEventTypeMonitor:
+		{
+			/* do nothing... this'll hang out here til we're dead, and the hypervisor will return it for us. */
+			break;
+		}
+	case VethEventTypeFramesAck:
+		{
+			for (i = 0; i < VethMaxFramesMsgsAcked; ++i) {
+				u16 msg = vethEvent->mDerivedData.mFramesAckData.mToken[i];
+				veth_recycleMsgByNum(connection, msg);
+			}
+			break;
+		}
+	case VethEventTypeFrames:
+		{
+			veth_takeFrames(connection, vethEvent);
+			break;
+		}
+	default:
+		{
+			veth_error_printk("Unknown interrupt type %d from lpar %d\n", event->xSubtype, connection->mRemoteLp);
+		}
+	};
+}
+
+static void veth_openConnections()
+{
+	int i = 0;
+
+	HvLpEvent_registerHandler(HvLpEvent_Type_VirtualLan, &veth_handleEvent);
+
+	/* Now I need to run through the active lps and open connections to the ones I'm supposed to
+	   open to. */
+
+	for (i = HvMaxArchitectedLps - 1; i >= 0; --i) {
+		if (i != mFabricMgr->mThisLp) {
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan(mFabricMgr->mThisLp, i)) {
+				veth_openConnection(i, 1);
+			} else {
+				veth_closeConnection(i, 1);
+			}
+		}
+	}
+}
+
+static void veth_intFinishOpeningConnections(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocBhTq.data = parm;
+	connection->mNumberAllocated = number;
+	queue_task(&connection->mAllocBhTq, &tq_immediate);
+	mark_bh(IMMEDIATE_BH);
+}
+
+static void veth_finishOpeningConnections(void *parm)
+{
+	unsigned long flags;
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	veth_finishOpeningConnectionsLocked(connection);
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_finishOpeningConnectionsLocked(struct VethLpConnection *connection)
+{
+	if (connection->mNumberAllocated >= 2) {
+		connection->mConnectionStatus.mCapMonAlloced = 1;
+		veth_sendCap(connection);
+	} else {
+		veth_error_printk("Couldn't allocate base msgs for lpar %d, only got %d\n", connection->mRemoteLp,
+				  connection->mNumberAllocated);
+		veth_failMe(connection);
+	}
+}
+
+static void veth_openConnection(u8 remoteLp, int lockMe)
+{
+	unsigned long flags;
+	unsigned long flags2;
+	HvLpInstanceId source;
+	HvLpInstanceId target;
+	u64 i = 0;
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[remoteLp]);
+
+	memset(&connection->mCapBhTq, 0, sizeof(connection->mCapBhTq));
+	connection->mCapBhTq.routine = (void *) (void *) veth_capBh;
+
+	memset(&connection->mCapAckBhTq, 0, sizeof(connection->mCapAckBhTq));
+	connection->mCapAckBhTq.routine = (void *) (void *) veth_capAckBh;
+
+	memset(&connection->mMonitorAckBhTq, 0, sizeof(connection->mMonitorAckBhTq));
+	connection->mMonitorAckBhTq.routine = (void *) (void *) veth_monitorAckBh;
+
+	memset(&connection->mAllocBhTq, 0, sizeof(connection->mAllocBhTq));
+	connection->mAllocBhTq.routine = (void *) (void *) veth_finishOpeningConnections;
+
+	if (lockMe)
+		spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	connection->mRemoteLp = remoteLp;
+
+	spin_lock_irqsave(&connection->mAckGate, flags2);
+
+	memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+	connection->mNumAcks = 0;
+
+	HvCallEvent_openLpEventPath(remoteLp, HvLpEvent_Type_VirtualLan);
+
+	/* clean up non-acked msgs */
+	for (i = 0; i < connection->mNumMsgs; ++i) {
+		veth_recycleMsgByNum(connection, i);
+	}
+
+	connection->mConnectionStatus.mOpen = 1;
+
+	source = connection->mSourceInst = HvCallEvent_getSourceLpInstanceId(remoteLp, HvLpEvent_Type_VirtualLan);
+	target = connection->mTargetInst = HvCallEvent_getTargetLpInstanceId(remoteLp, HvLpEvent_Type_VirtualLan);
+
+	if (connection->mConnectionStatus.mCapMonAlloced != 1) {
+		connection->mAllocBhTq.routine = (void *) (void *) veth_finishOpeningConnections;
+		mf_allocateLpEvents(remoteLp,
+				    HvLpEvent_Type_VirtualLan,
+				    sizeof(struct VethLpEvent), 2, &veth_intFinishOpeningConnections, connection);
+	} else {
+		veth_finishOpeningConnectionsLocked(connection);
+	}
+
+	spin_unlock_irqrestore(&connection->mAckGate, flags2);
+
+	if (lockMe)
+		spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_closeConnection(u8 remoteLp, int lockMe)
+{
+	struct VethLpConnection *connection = &(mFabricMgr->mConnection[remoteLp]);
+	unsigned long flags;
+	unsigned long flags2;
+	if (lockMe)
+		spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	del_timer(&connection->mAckTimer);
+
+	if (connection->mConnectionStatus.mOpen == 1) {
+		HvCallEvent_closeLpEventPath(remoteLp, HvLpEvent_Type_VirtualLan);
+		connection->mConnectionStatus.mOpen = 0;
+		veth_failMe(connection);
+
+		/* reset ack data */
+		spin_lock_irqsave(&connection->mAckGate, flags2);
+
+		memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+		connection->mNumAcks = 0;
+
+		spin_unlock_irqrestore(&connection->mAckGate, flags2);
+	}
+
+	if (lockMe)
+		spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_msgsInit(struct VethLpConnection *connection)
+{
+	connection->mAllocBhTq.routine = (void *) (void *) veth_finishMsgsInit;
+	mf_allocateLpEvents(connection->mRemoteLp,
+			    HvLpEvent_Type_VirtualLan,
+			    sizeof(struct VethLpEvent),
+			    connection->mMyCap.mUnionData.mFields.mNumberBuffers, &veth_intFinishMsgsInit, connection);
+}
+
+static void veth_intFinishMsgsInit(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocBhTq.data = parm;
+	connection->mNumberRcvMsgs = number;
+	queue_task(&connection->mAllocBhTq, &tq_immediate);
+	mark_bh(IMMEDIATE_BH);
+}
+
+static void veth_intFinishCapBh(void *parm, int number)
+{
+	struct VethLpConnection *connection = (struct VethLpConnection *) parm;
+	connection->mAllocBhTq.data = parm;
+	if (number > 0)
+		connection->mNumberLpAcksAlloced += number;
+
+	queue_task(&connection->mAllocBhTq, &tq_immediate);
+	mark_bh(IMMEDIATE_BH);
+}
+
+static void veth_finishMsgsInit(struct VethLpConnection *connection)
+{
+	int i = 0;
+	unsigned int numberGotten = 0;
+	u64 amountOfHeapToGet = connection->mMyCap.mUnionData.mFields.mNumberBuffers * sizeof(struct VethMsg);
+	char *msgs = NULL;
+	unsigned long flags;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	if (connection->mNumberRcvMsgs >= connection->mMyCap.mUnionData.mFields.mNumberBuffers) {
+		msgs = kmalloc(amountOfHeapToGet, GFP_ATOMIC);
+
+		connection->mMsgs = (struct VethMsg *) msgs;
+
+		if (msgs != NULL) {
+			memset(msgs, 0, amountOfHeapToGet);
+
+			for (i = 0; i < connection->mMyCap.mUnionData.mFields.mNumberBuffers; ++i) {
+				connection->mMsgs[i].mIndex = i;
+				++numberGotten;
+				VETHSTACKPUSH(&(connection->mMsgStack), (connection->mMsgs + i));
+			}
+			if (numberGotten > 0) {
+				connection->mNumMsgs = numberGotten;
+			}
+		} else {
+			kfree(msgs);
+			connection->mMsgs = NULL;
+		}
+	}
+
+	connection->mMyCap.mUnionData.mFields.mNumberBuffers = connection->mNumMsgs;
+
+	if (connection->mNumMsgs < 10)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 1;
+	else if (connection->mNumMsgs < 20)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 4;
+	else if (connection->mNumMsgs < 40)
+		connection->mMyCap.mUnionData.mFields.mThreshold = 10;
+	else
+		connection->mMyCap.mUnionData.mFields.mThreshold = 20;
+
+	connection->mMyCap.mUnionData.mFields.mTimer = VethAckTimeoutUsec;
+
+	veth_finishSendCap(connection);
+
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_sendCap(struct VethLpConnection *connection)
+{
+	if (connection->mMsgs == NULL) {
+		connection->mMyCap.mUnionData.mFields.mNumberBuffers = VethBuffersToAllocate;
+		veth_msgsInit(connection);
+	} else {
+		veth_finishSendCap(connection);
+	}
+}
+
+static void veth_finishSendCap(struct VethLpConnection *connection)
+{
+	HvLpEvent_Rc returnCode = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+								HvLpEvent_Type_VirtualLan,
+								VethEventTypeCap,
+								HvLpEvent_AckInd_DoAck,
+								HvLpEvent_AckType_ImmediateAck,
+								connection->mSourceInst,
+								connection->mTargetInst,
+								0,
+								connection->mMyCap.mUnionData.mNoFields.mReserved1,
+								connection->mMyCap.mUnionData.mNoFields.mReserved2,
+								connection->mMyCap.mUnionData.mNoFields.mReserved3,
+								connection->mMyCap.mUnionData.mNoFields.mReserved4,
+								connection->mMyCap.mUnionData.mNoFields.mReserved5);
+
+	if ((returnCode == HvLpEvent_Rc_PartitionDead) || (returnCode == HvLpEvent_Rc_PathClosed)) {
+		connection->mConnectionStatus.mSentCap = 0;
+	} else if (returnCode != HvLpEvent_Rc_Good) {
+		veth_error_printk("Couldn't send cap to lpar %d, rc %x\n", connection->mRemoteLp, (int) returnCode);
+		veth_failMe(connection);
+	} else {
+		connection->mConnectionStatus.mSentCap = 1;
+	}
+}
+
+static void veth_takeCap(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mCapBhPending))) {
+		connection->mCapBhTq.data = connection;
+		memcpy(&connection->mCapEvent, event, sizeof(connection->mCapEvent));
+		queue_task(&connection->mCapBhTq, &tq_immediate);
+		mark_bh(IMMEDIATE_BH);
+	} else {
+		veth_error_printk("Received a capabilities from lpar %d while already processing one\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+}
+
+static void veth_takeCapAck(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mCapAckBhPending))) {
+		connection->mCapAckBhTq.data = connection;
+		memcpy(&connection->mCapAckEvent, event, sizeof(connection->mCapAckEvent));
+		queue_task(&connection->mCapAckBhTq, &tq_immediate);
+		mark_bh(IMMEDIATE_BH);
+	} else {
+		veth_error_printk("Received a capabilities ack from lpar %d while already processing one\n",
+				  connection->mRemoteLp);
+	}
+}
+
+static void veth_takeMonitorAck(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	if (!test_and_set_bit(0, &(connection->mMonitorAckBhPending))) {
+		connection->mMonitorAckBhTq.data = connection;
+		memcpy(&connection->mMonitorAckEvent, event, sizeof(connection->mMonitorAckEvent));
+		queue_task(&connection->mMonitorAckBhTq, &tq_immediate);
+		mark_bh(IMMEDIATE_BH);
+	} else {
+		veth_error_printk("Received a monitor ack from lpar %d while already processing one\n", connection->mRemoteLp);
+	}
+}
+
+static void veth_recycleMsgByNum(struct VethLpConnection *connection, u16 msg)
+{
+	if (msg < connection->mNumMsgs) {
+		struct VethMsg *myMsg = connection->mMsgs + msg;
+		veth_recycleMsg(connection, myMsg);
+	}
+}
+
+static void veth_recycleMsg(struct VethLpConnection *connection, struct VethMsg *myMsg)
+{
+	struct scatterlist sg[VethMaxFramesPerMsg];
+	if (test_and_clear_bit(0, &(myMsg->mInUse))) {
+	        int i;
+	        int nsg = 0;
+		for (i = 0; i < VethMaxFramesPerMsg; i++) {
+		        if (myMsg->mEvent.mSendData.mAddress[i] != 0) {
+			        sg[nsg].dma_address = myMsg->mEvent.mSendData.mAddress[i];
+			        sg[nsg].dma_length = myMsg->mEvent.mSendData.mLength[i];
+			        nsg++;
+		        }
+		}
+	        pci_unmap_sg(iSeries_veth_dev, sg, nsg, PCI_DMA_TODEVICE);
+
+	        dev_kfree_skb_any(myMsg->mSkb);
+
+	        myMsg->mSkb = NULL;
+	        memset(&(myMsg->mEvent.mSendData), 0, sizeof(struct VethFramesData));
+	        VETHSTACKPUSH(&connection->mMsgStack, myMsg);
+	} else {
+	        if (connection->mConnectionStatus.mOpen) {
+		        veth_error_printk("Received a frames ack for msg %d from lpar %d while not outstanding\n", myMsg->mIndex,
+				  connection->mRemoteLp);
+		}
+	}
+}
+
+static void veth_capBh(struct VethLpConnection *connection)
+{
+	struct VethLpEvent *event = &connection->mCapEvent;
+	unsigned long flags;
+	struct VethCapData *remoteCap = &(connection->mRemoteCap);
+	u64 numAcks = 0;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	connection->mConnectionStatus.mGotCap = 1;
+
+	memcpy(remoteCap, &(event->mDerivedData.mCapabilitiesData), sizeof(connection->mRemoteCap));
+
+	if ((remoteCap->mUnionData.mFields.mNumberBuffers <= VethMaxFramesMsgs) &&
+	    (remoteCap->mUnionData.mFields.mNumberBuffers != 0) &&
+	    (remoteCap->mUnionData.mFields.mThreshold <= VethMaxFramesMsgsAcked) &&
+	    (remoteCap->mUnionData.mFields.mThreshold != 0)) {
+		numAcks = (remoteCap->mUnionData.mFields.mNumberBuffers / remoteCap->mUnionData.mFields.mThreshold) + 1;
+
+		if (connection->mNumberLpAcksAlloced < numAcks) {
+			numAcks = numAcks - connection->mNumberLpAcksAlloced;
+			connection->mAllocBhTq.routine = (void *) (void *) veth_finishCapBh;
+			mf_allocateLpEvents(connection->mRemoteLp,
+					    HvLpEvent_Type_VirtualLan,
+					    sizeof(struct VethLpEvent), numAcks, &veth_intFinishCapBh, connection);
+		} else
+			veth_finishCapBhLocked(connection);
+	} else {
+		veth_error_printk("Received incompatible capabilities from lpar %d\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_InvalidSubtypeData;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+
+	clear_bit(0, &(connection->mCapBhPending));
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_capAckBh(struct VethLpConnection *connection)
+{
+	struct VethLpEvent *event = &connection->mCapAckEvent;
+	unsigned long flags;
+
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	if (event->mBaseEvent.xRc == HvLpEvent_Rc_Good) {
+		connection->mConnectionStatus.mCapAcked = 1;
+
+		if ((connection->mConnectionStatus.mGotCap == 1) && (connection->mConnectionStatus.mGotCapAcked == 1)) {
+			if (connection->mConnectionStatus.mSentMonitor != 1)
+				veth_sendMonitor(connection);
+		}
+	} else {
+		veth_printk("Bad rc(%d) from lpar %d on capabilities\n", event->mBaseEvent.xRc, connection->mRemoteLp);
+		veth_failMe(connection);
+	}
+
+	clear_bit(0, &(connection->mCapAckBhPending));
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_monitorAckBh(struct VethLpConnection *connection)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+
+	veth_failMe(connection);
+
+	veth_printk("Monitor ack returned for lpar %d\n", connection->mRemoteLp);
+
+	if (connection->mConnectionStatus.mOpen) {
+		veth_closeConnection(connection->mRemoteLp, 0);
+
+		udelay(100);
+
+		queue_task(&connection->mMonitorAckBhTq, &tq_immediate);
+		mark_bh(IMMEDIATE_BH);
+	} else {
+#ifdef MODULE
+		if (VethModuleReopen)
+#endif
+			veth_openConnection(connection->mRemoteLp, 0);
+#ifdef MODULE
+		else {
+			int i = 0;
+
+			for (i = 0; i < connection->mNumMsgs; ++i) {
+				veth_recycleMsgByNum(connection, i);
+			}
+		}
+#endif
+		clear_bit(0, &(connection->mMonitorAckBhPending));
+	}
+
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+#define number_of_pages(v, l) ((((unsigned long)(v) & ((1 << 12) - 1)) + (l) + 4096 - 1) / 4096)
+#define page_offset(v) ((unsigned long)(v) & ((1 << 12) - 1))
+
+static void veth_takeFrames(struct VethLpConnection *connection, struct VethLpEvent *event)
+{
+	int i = 0;
+	struct VethPort *port = NULL;
+	struct BufList {
+		union {
+			struct {
+				u32 token2;
+				u32 garbage;
+			} token1;
+			u64 address;
+		} addr;
+		u64 size;
+	};
+
+	struct BufList myBufList[4];	/* max pages per frame */
+	struct BufList remoteList[VethMaxFramesPerMsg];	/* max frags per frame */
+
+	do {
+		int nfrags = 0;
+		u16 length = 0;
+
+		/* a 0 address marks the end of the valid entries */
+		if (event->mDerivedData.mSendData.mAddress[i] == 0)
+			break;
+
+		/* make sure that we have at least 1 EOF entry in the remaining entries */
+		if (!(event->mDerivedData.mSendData.mEofMask >> i)) {
+			veth_printk("bad lp event: missing EOF frag in event mEofMask 0x%x i %d\n",
+				    event->mDerivedData.mSendData.mEofMask, i);
+			break;
+		}
+
+		/* add up length of non-EOF frags */
+		do {
+			remoteList[nfrags].addr.token1.token2 = event->mDerivedData.mSendData.mAddress[i + nfrags];
+			remoteList[nfrags].addr.token1.garbage = 0;
+			length += remoteList[nfrags].size = event->mDerivedData.mSendData.mLength[i + nfrags];
+		}
+		while (!(event->mDerivedData.mSendData.mEofMask & (1 << (i + nfrags++))));
+
+
+		/* length == total length of all framgents */
+		/* nfrags == # of fragments in this frame */
+
+		if ((length - 14) <= 9000) {	/* save as 13 < length <= 9014 */
+			struct sk_buff *skb = alloc_skb(length, GFP_ATOMIC);
+			if (skb != NULL) {
+				HvLpDma_Rc returnCode = HvLpDma_Rc_Good;
+
+				/* build the buffer list for the dma operation */
+				int numPages = number_of_pages((skb->data), length);	/* number of pages in this fragment of the complete buffer */
+				myBufList[0].addr.address =
+				    (0x8000000000000000LL | (virt_to_absolute((unsigned long) skb->data)));
+				myBufList[0].size = (numPages > 1) ? (4096 - page_offset(skb->data)) : length;
+				if (numPages > 1) {
+					myBufList[1].addr.address =
+					    (0x8000000000000000LL |
+					     (virt_to_absolute((unsigned long) skb->data + myBufList[0].size)));
+					myBufList[1].size = (numPages > 2) ? 4096 : length - myBufList[0].size;
+					if (numPages > 2) {
+						myBufList[2].addr.address =
+						    (0x8000000000000000LL |
+						     (virt_to_absolute
+						      ((unsigned long) skb->data + myBufList[0].size + myBufList[1].size)));
+						myBufList[2].size =
+						    (numPages > 3) ? 4096 : length - myBufList[0].size - myBufList[1].size;
+						if (numPages > 3) {
+							myBufList[3].addr.address =
+							    0x8000000000000000LL |
+							    (virt_to_absolute
+							     ((unsigned long) skb->data + myBufList[0].size + myBufList[1].size +
+							      myBufList[2].size));
+							myBufList[3].size =
+							    length - myBufList[0].size - myBufList[1].size - myBufList[2].size;
+						}
+					}
+				}
+				returnCode = HvCallEvent_dmaBufList(HvLpEvent_Type_VirtualLan,
+								    event->mBaseEvent.xSourceLp,
+								    HvLpDma_Direction_RemoteToLocal,
+								    connection->mSourceInst,
+								    connection->mTargetInst,
+								    HvLpDma_AddressType_RealAddress,
+								    HvLpDma_AddressType_TceIndex,
+								    0x8000000000000000LL |
+								    (virt_to_absolute((unsigned long) &myBufList)),
+								    0x8000000000000000LL |
+								    (virt_to_absolute((unsigned long) &remoteList)), length);
+
+				if (returnCode == HvLpDma_Rc_Good) {
+					HvLpVirtualLanIndex vlan = skb->data[9];
+					u64 dest = *((u64 *) skb->data) & 0xFFFFFFFFFFFF0000;
+
+					if (((vlan < HvMaxArchitectedVirtualLans) && ((port = mFabricMgr->mPorts[vlan]) != NULL)) && ((dest == port->mMyAddress) ||	/* it's for me */
+																      (dest == 0xFFFFFFFFFFFF0000) ||	/* it's a broadcast */
+																      (veth_multicast_wanted(port, dest)) ||	/* it's one of my multicasts */
+																      (port->mPromiscuous == 1))) {	/* I'm promiscuous */
+						skb_put(skb, length);
+						skb->dev = port->mDev;
+						skb->protocol = eth_type_trans(skb, port->mDev);
+						skb->ip_summed = CHECKSUM_NONE;
+						netif_rx(skb);	/* send it up */
+						port->mStats.rx_packets++;
+						port->mStats.rx_bytes += length;
+
+					} else {
+						dev_kfree_skb_irq(skb);
+					}
+				} else {
+					dev_kfree_skb_irq(skb);
+				}
+			}
+		} else {
+			break;
+		}
+		i += nfrags;
+	} while (i < VethMaxFramesPerMsg);
+
+	/* Ack it */
+
+	{
+		unsigned long flags;
+		spin_lock_irqsave(&connection->mAckGate, flags);
+
+		if (connection->mNumAcks < VethMaxFramesMsgsAcked) {
+			connection->mEventData.mAckData.mToken[connection->mNumAcks] = event->mBaseEvent.xCorrelationToken;
+			++connection->mNumAcks;
+
+			if (connection->mNumAcks == connection->mRemoteCap.mUnionData.mFields.mThreshold) {
+				HvLpEvent_Rc rc = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+										HvLpEvent_Type_VirtualLan,
+										VethEventTypeFramesAck,
+										HvLpEvent_AckInd_NoAck,
+										HvLpEvent_AckType_ImmediateAck,
+										connection->mSourceInst,
+										connection->mTargetInst,
+										0,
+										connection->mEventData.mFpData.mData1,
+										connection->mEventData.mFpData.mData2,
+										connection->mEventData.mFpData.mData3,
+										connection->mEventData.mFpData.mData4,
+										connection->mEventData.mFpData.mData5);
+
+				if (rc != HvLpEvent_Rc_Good) {
+					veth_error_printk("Bad lp event return code(%x) acking frames from lpar %d\n", (int) rc,
+							  connection->mRemoteLp);
+				}
+
+				connection->mNumAcks = 0;
+
+				memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+			}
+
+		}
+
+		spin_unlock_irqrestore(&connection->mAckGate, flags);
+	}
+}
+
+#undef number_of_pages
+#undef page_offset
+
+static void veth_timedAck(unsigned long connectionPtr)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+	struct VethLpConnection *connection = (struct VethLpConnection *) connectionPtr;
+	/* Ack all the events */
+	spin_lock_irqsave(&connection->mAckGate, flags);
+
+	if (connection->mNumAcks > 0) {
+		rc = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+						   HvLpEvent_Type_VirtualLan,
+						   VethEventTypeFramesAck,
+						   HvLpEvent_AckInd_NoAck,
+						   HvLpEvent_AckType_ImmediateAck,
+						   connection->mSourceInst,
+						   connection->mTargetInst,
+						   0,
+						   connection->mEventData.mFpData.mData1,
+						   connection->mEventData.mFpData.mData2,
+						   connection->mEventData.mFpData.mData3,
+						   connection->mEventData.mFpData.mData4, connection->mEventData.mFpData.mData5);
+
+		if (rc != HvLpEvent_Rc_Good) {
+			veth_error_printk("Bad lp event return code(%x) acking frames from lpar %d!\n", (int) rc,
+					  connection->mRemoteLp);
+		}
+
+		connection->mNumAcks = 0;
+
+		memset(&connection->mEventData, 0xFF, sizeof(connection->mEventData));
+	}
+
+	spin_unlock_irqrestore(&connection->mAckGate, flags);
+
+	/* Reschedule the timer */
+	connection->mAckTimer.expires = jiffies + connection->mTimeout;
+	add_timer(&connection->mAckTimer);
+}
+
+static int veth_multicast_wanted(struct VethPort *port, u64 thatAddr)
+{
+	int returnParm = 0;
+	int i;
+	unsigned long flags;
+
+	if ((*((char *) &thatAddr) & 0x01) != 1)
+		return 0;
+
+	read_lock_irqsave(&port->mMcastGate, flags);
+	if (port->mAllMcast) {
+		read_unlock_irqrestore(&port->mMcastGate, flags);
+		return 1;
+	}
+
+	for (i = 0; i < port->mNumAddrs; ++i) {
+		u64 thisAddr = port->mMcasts[i];
+
+		if (thisAddr == thatAddr) {
+			returnParm = 1;
+			break;
+		}
+	}
+	read_unlock_irqrestore(&port->mMcastGate, flags);
+
+	return returnParm;
+}
+
+static void veth_sendMonitor(struct VethLpConnection *connection)
+{
+	HvLpEvent_Rc returnCode = HvCallEvent_signalLpEventFast(connection->mRemoteLp,
+								HvLpEvent_Type_VirtualLan,
+								VethEventTypeMonitor,
+								HvLpEvent_AckInd_DoAck,
+								HvLpEvent_AckType_DeferredAck,
+								connection->mSourceInst,
+								connection->mTargetInst,
+								0, 0, 0, 0, 0, 0);
+
+	if (returnCode == HvLpEvent_Rc_Good) {
+		connection->mConnectionStatus.mSentMonitor = 1;
+		connection->mConnectionStatus.mFailed = 0;
+
+		/* Start the ACK timer */
+		init_timer(&connection->mAckTimer);
+		connection->mAckTimer.function = veth_timedAck;
+		connection->mAckTimer.data = (unsigned long) connection;
+		connection->mAckTimer.expires = jiffies + connection->mTimeout;
+		add_timer(&connection->mAckTimer);
+
+	} else {
+		veth_error_printk("Monitor send to lpar %d failed with rc %x\n", connection->mRemoteLp, (int) returnCode);
+		veth_failMe(connection);
+	}
+}
+
+static void veth_finishCapBh(struct VethLpConnection *connection)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&connection->mStatusGate, flags);
+	veth_finishCapBhLocked(connection);
+	spin_unlock_irqrestore(&connection->mStatusGate, flags);
+}
+
+static void veth_finishCapBhLocked(struct VethLpConnection *connection)
+{
+	struct VethLpEvent *event = &connection->mCapEvent;
+	struct VethCapData *remoteCap = &(connection->mRemoteCap);
+	int numAcks = (remoteCap->mUnionData.mFields.mNumberBuffers / remoteCap->mUnionData.mFields.mThreshold) + 1;
+
+	/* Convert timer to jiffies */
+	if (connection->mMyCap.mUnionData.mFields.mTimer)
+		connection->mTimeout = remoteCap->mUnionData.mFields.mTimer * HZ / 1000000;
+	else
+		connection->mTimeout = VethAckTimeoutUsec * HZ / 1000000;
+
+	if (connection->mNumberLpAcksAlloced >= numAcks) {
+		HvLpEvent_Rc returnCode = HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+
+		if (returnCode == HvLpEvent_Rc_Good) {
+			connection->mConnectionStatus.mGotCapAcked = 1;
+
+			if (connection->mConnectionStatus.mSentCap != 1) {
+				connection->mTargetInst =
+				    HvCallEvent_getTargetLpInstanceId(connection->mRemoteLp, HvLpEvent_Type_VirtualLan);
+
+				veth_sendCap(connection);
+			} else if (connection->mConnectionStatus.mCapAcked == 1) {
+				if (connection->mConnectionStatus.mSentMonitor != 1)
+					veth_sendMonitor(connection);
+			}
+		} else {
+			veth_error_printk("Failed to ack remote cap for lpar %d with rc %x\n", connection->mRemoteLp,
+					  (int) returnCode);
+			veth_failMe(connection);
+		}
+	} else {
+		veth_error_printk("Couldn't allocate all the frames ack events for lpar %d\n", connection->mRemoteLp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+	}
+}
+
+int proc_veth_dump_connection(char *page, char **start, off_t off, int count, int *eof, void *data) {
+	char *out = page;
+	long whichConnection = (long) data;
+	int len = 0;
+	struct VethLpConnection *connection = NULL;
+
+	if ((whichConnection < 0) || (whichConnection > HvMaxArchitectedLps) || (mFabricMgr == NULL)) {
+		veth_error_printk("Got bad data from /proc file system\n");
+		len = sprintf(page, "ERROR\n");
+	} else {
+		int thereWasStuffBefore = 0;
+		connection = &(mFabricMgr->mConnection[whichConnection]);
+
+		out += sprintf(out, "Remote Lp:\t%d\n", connection->mRemoteLp);
+		out += sprintf(out, "Source Inst:\t%04X\n", connection->mSourceInst);
+		out += sprintf(out, "Target Inst:\t%04X\n", connection->mTargetInst);
+		out += sprintf(out, "Num Msgs:\t%d\n", connection->mNumMsgs);
+		out += sprintf(out, "Num Lp Acks:\t%d\n", connection->mNumberLpAcksAlloced);
+		out += sprintf(out, "Num Acks:\t%d\n", connection->mNumAcks);
+
+		if (connection->mConnectionStatus.mOpen) {
+			out += sprintf(out, "<Open");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mCapMonAlloced) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "CapMonAlloced");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mBaseMsgsAlloced) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "BaseMsgsAlloced");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mSentCap) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "SentCap");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mCapAcked) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "CapAcked");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mGotCap) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "GotCap");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mGotCapAcked) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "GotCapAcked");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mSentMonitor) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "SentMonitor");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mPopulatedRings) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "PopulatedRings");
+			thereWasStuffBefore = 1;
+		}
+
+		if (connection->mConnectionStatus.mFailed) {
+			if (thereWasStuffBefore)
+				out += sprintf(out, "/");
+			else
+				out += sprintf(out, "<");
+			out += sprintf(out, "Failed");
+			thereWasStuffBefore = 1;
+		}
+
+		if (thereWasStuffBefore)
+			out += sprintf(out, ">");
+
+		out += sprintf(out, "\n");
+
+		out += sprintf(out, "Capabilities (System:<Version/Buffers/Threshold/Timeout>):\n");
+		out += sprintf(out, "\tLocal:<");
+		out += sprintf(out, "%d/%d/%d/%d>\n",
+			       connection->mMyCap.mUnionData.mFields.mVersion,
+			       connection->mMyCap.mUnionData.mFields.mNumberBuffers,
+			       connection->mMyCap.mUnionData.mFields.mThreshold, connection->mMyCap.mUnionData.mFields.mTimer);
+		out += sprintf(out, "\tRemote:<");
+		out += sprintf(out, "%d/%d/%d/%d>\n",
+			       connection->mRemoteCap.mUnionData.mFields.mVersion,
+			       connection->mRemoteCap.mUnionData.mFields.mNumberBuffers,
+			       connection->mRemoteCap.mUnionData.mFields.mThreshold,
+			       connection->mRemoteCap.mUnionData.mFields.mTimer);
+		len = out - page;
+	}
+	len -= off;
+	if (len < count) {
+		*eof = 1;
+		if (len <= 0)
+			return 0;
+	} else
+		len = count;
+	*start = page + off;
+	return len;
+}
+
+int proc_veth_dump_port(char *page, char **start, off_t off, int count, int *eof, void *data) {
+	char *out = page;
+	long whichPort = (long) data;
+	int len = 0;
+	struct VethPort *port = NULL;
+
+	if ((whichPort < 0) || (whichPort > HvMaxArchitectedVirtualLans) || (mFabricMgr == NULL))
+		len = sprintf(page, "Virtual ethernet is not configured.\n");
+	else {
+		int i = 0;
+		u32 *myAddr;
+		u16 *myEndAddr;
+		port = mFabricMgr->mPorts[whichPort];
+
+		if (port != NULL) {
+			myAddr = (u32 *) & (port->mMyAddress);
+			myEndAddr = (u16 *) (myAddr + 1);
+			out += sprintf(out, "Net device:\t%p\n", port->mDev);
+			out += sprintf(out, "Net device name:\t%s\n", port->mDev->name);
+			out += sprintf(out, "Address:\t%08X%04X\n", myAddr[0], myEndAddr[0]);
+			out += sprintf(out, "Promiscuous:\t%d\n", port->mPromiscuous);
+			out += sprintf(out, "All multicast:\t%d\n", port->mAllMcast);
+			out += sprintf(out, "Number sk_buffs linearized:\t%u\n", port->mLinearized);
+			out += sprintf(out, "Number multicast:\t%d\n", port->mNumAddrs);
+
+			for (i = 0; i < port->mNumAddrs; ++i) {
+				u32 *multi = (u32 *) & (port->mMcasts[i]);
+				u16 *multiEnd = (u16 *) (multi + 1);
+				out += sprintf(out, "   %08X%04X\n", multi[0], multiEnd[0]);
+			}
+		} else {
+			out += sprintf(page, "veth%d is not configured.\n", (int) whichPort);
+		}
+
+		len = out - page;
+	}
+	len -= off;
+	if (len < count) {
+		*eof = 1;
+		if (len <= 0)
+			return 0;
+	} else
+		len = count;
+	*start = page + off;
+	return len;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/veth.h linuxppc64_2_4/drivers/iseries/veth.h
--- linux-2.4.19/drivers/iseries/veth.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/veth.h	Tue Mar 12 08:51:03 2002
@@ -0,0 +1,242 @@
+/* File veth.h created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/* Change Activity: */
+/* End Change Activity */
+
+#ifndef _VETH_H
+#define _VETH_H
+
+#ifndef _HVTYPES_H
+#include <asm/iSeries/HvTypes.h>
+#endif
+#ifndef _HVLPEVENT_H
+#include <asm/iSeries/HvLpEvent.h>
+#endif
+#include <linux/netdevice.h>
+
+#define VethEventNumTypes (4)
+#define VethEventTypeCap (0)
+#define VethEventTypeFrames (1)
+#define VethEventTypeMonitor (2)
+#define VethEventTypeFramesAck (3)
+
+#define VethMaxFramesMsgsAcked (20)
+#define VethMaxFramesMsgs (0xFFFF)
+#define VethMaxFramesPerMsg (6)
+#define VethAckTimeoutUsec (1000000)
+
+#define VETHSTACKTYPE(T) struct VethStack##T
+#define VETHSTACK(T) \
+VETHSTACKTYPE(T) \
+{ \
+struct T *head; \
+spinlock_t lock; \
+}
+#define VETHSTACKCTOR(s) do { (s)->head = NULL; spin_lock_init(&(s)->lock); } while(0)
+#define VETHSTACKPUSH(s, p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(s)->lock,flags); \
+(p)->next = (s)->head; \
+(s)->head = (p); \
+spin_unlock_irqrestore(&(s)->lock, flags); \
+} while(0)
+
+#define VETHSTACKPOP(s,p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(s)->lock,flags); \
+(p) = (s)->head; \
+if ((s)->head != NULL) \
+{ \
+(s)->head = (s)->head->next; \
+} \
+spin_unlock_irqrestore(&(s)->lock, flags); \
+} while(0)
+
+#define VETHQUEUE(T) \
+struct VethQueue##T \
+{ \
+T *head; \
+T *tail; \
+spinlock_t lock; \
+}
+#define VETHQUEUECTOR(q) do { (q)->head = NULL; (q)->tail = NULL; spin_lock_init(&(q)->lock); } while(0)
+#define VETHQUEUEENQ(q, p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(q)->lock,flags); \
+(p)->next = NULL; \
+if ((q)->head != NULL) \
+{ \
+(q)->head->next = (p); \
+(q)->head = (p); \
+} \
+else \
+{ \
+(q)->tail = (q)->head = (p); \
+} \
+spin_unlock_irqrestore(&(q)->lock, flags); \
+} while(0)
+
+#define VETHQUEUEDEQ(q,p) \
+do { \
+unsigned long flags; \
+spin_lock_irqsave(&(q)->lock,flags); \
+(p) = (q)->tail; \
+if ((p) != NULL) \
+{ \
+(q)->tail = (p)->next; \
+(p)->next = NULL; \
+} \
+if ((q)->tail == NULL) \
+(q)->head = NULL; \
+spin_unlock_irqrestore(&(q)->lock, flags); \
+} while(0)
+
+struct VethFramesData {
+	u32 mAddress[6];
+	u16 mLength[6];
+	u32 mEofMask:6;
+	u32 mReserved:26;
+};
+
+struct VethFramesAckData {
+	u16 mToken[VethMaxFramesMsgsAcked];
+};
+
+struct VethCapData {
+	union {
+		struct Fields {
+			u8 mVersion;
+			u8 mReserved1;
+			u16 mNumberBuffers;
+			u16 mThreshold;
+			u16 mReserved2;
+			u32 mTimer;
+			u32 mReserved3;
+			u64 mReserved4;
+			u64 mReserved5;
+			u64 mReserved6;
+		} mFields;
+		struct NoFields {
+			u64 mReserved1;
+			u64 mReserved2;
+			u64 mReserved3;
+			u64 mReserved4;
+			u64 mReserved5;
+		} mNoFields;
+	} mUnionData;
+};
+
+struct VethFastPathData {
+	u64 mData1;
+	u64 mData2;
+	u64 mData3;
+	u64 mData4;
+	u64 mData5;
+};
+
+struct VethLpEvent {
+	struct HvLpEvent mBaseEvent;
+	union {
+		struct VethFramesData mSendData;
+		struct VethCapData mCapabilitiesData;
+		struct VethFramesAckData mFramesAckData;
+		struct VethFastPathData mFastPathData;
+	} mDerivedData;
+
+};
+
+struct VethMsg {
+	struct VethMsg *next;
+	union {
+		struct VethFramesData mSendData;
+		struct VethFastPathData mFpData;
+	} mEvent;
+	int mIndex;
+	unsigned long mInUse;
+	struct sk_buff *mSkb;
+};
+
+
+struct VethControlBlock {
+	struct net_device *mDev;
+	struct VethControlBlock *mNext;
+	HvLpVirtualLanIndex mVlanId;
+};
+
+struct VethLpConnection {
+	u64 mEyecatcher;
+	HvLpIndex mRemoteLp;
+	HvLpInstanceId mSourceInst;
+	HvLpInstanceId mTargetInst;
+	u32 mNumMsgs;
+	struct VethMsg *mMsgs;
+	int mNumberRcvMsgs;
+	int mNumberLpAcksAlloced;
+	union {
+		struct VethFramesAckData mAckData;
+		struct VethFastPathData mFpData;
+	} mEventData;
+	spinlock_t mAckGate;
+	u32 mNumAcks;
+	spinlock_t mStatusGate;
+	struct {
+		u64 mOpen:1;
+		u64 mCapMonAlloced:1;
+		u64 mBaseMsgsAlloced:1;
+		u64 mSentCap:1;
+		u64 mCapAcked:1;
+		u64 mGotCap:1;
+		u64 mGotCapAcked:1;
+		u64 mSentMonitor:1;
+		u64 mPopulatedRings:1;
+		u64 mReserved:54;
+		u64 mFailed:1;
+	} mConnectionStatus;
+	struct VethCapData mMyCap;
+	struct VethCapData mRemoteCap;
+	unsigned long mCapAckBhPending;
+	struct tq_struct mCapAckBhTq;
+	struct VethLpEvent mCapAckEvent;
+	unsigned long mCapBhPending;
+	struct tq_struct mCapBhTq;
+	struct VethLpEvent mCapEvent;
+	unsigned long mMonitorAckBhPending;
+	struct tq_struct mMonitorAckBhTq;
+	struct VethLpEvent mMonitorAckEvent;
+	unsigned long mAllocBhPending;
+	struct tq_struct mAllocBhTq;
+	int mNumberAllocated;
+	struct timer_list mAckTimer;
+	u32 mTimeout;
+	 VETHSTACK(VethMsg) mMsgStack;
+};
+#define HVMAXARCHITECTEDVIRTUALLANS 16
+struct VethPort {
+	struct net_device *mDev;
+	struct net_device_stats mStats;
+	int mLock;
+	u64 mMyAddress;
+	int mPromiscuous;
+	int mAllMcast;
+	rwlock_t mMcastGate;
+	int mNumAddrs;
+	u64 mMcasts[12];
+	u32 mLinearized;
+};
+
+struct VethFabricMgr {
+	u64 mEyecatcher;
+	HvLpIndex mThisLp;
+	struct VethLpConnection mConnection[HVMAXARCHITECTEDLPS];
+	spinlock_t mPortListGate;
+	u64 mNumPorts;
+	struct VethPort *mPorts[HVMAXARCHITECTEDVIRTUALLANS];
+};
+
+int proc_veth_dump_connection(char *page, char **start, off_t off, int count, int *eof, void *data);
+int proc_veth_dump_port(char *page, char **start, off_t off, int count, int *eof, void *data);
+
+#endif				/* _VETH_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/vio.h linuxppc64_2_4/drivers/iseries/vio.h
--- linux-2.4.19/drivers/iseries/vio.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/vio.h	Wed Feb 27 11:09:05 2002
@@ -0,0 +1,130 @@
+/* -*- linux-c -*-
+ *  drivers/char/vio.h
+ *
+ *  iSeries Virtual I/O Message Path header
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This header file is used by the iSeries virtual I/O device
+ * drivers.  It defines the interfaces to the common functions
+ * (implemented in drivers/char/viopath.h) as well as defining
+ * common functions and structures.  Currently (at the time I 
+ * wrote this comment) the iSeries virtual I/O device drivers
+ * that use this are 
+ *   drivers/block/viodasd.c 
+ *   drivers/char/viocons.c
+ *   drivers/char/viotape.c
+ *   drivers/cdrom/viocd.c
+ *
+ * The iSeries virtual ethernet support (veth.c) uses a whole
+ * different set of functions.
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef _VIO_H
+#define _VIO_H
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+
+/* iSeries virtual I/O events use the subtype field in
+ * HvLpEvent to figure out what kind of vio event is coming
+ * in.  We use a table to route these, and this defines
+ * the maximum number of distinct subtypes
+ */
+#define VIO_MAX_SUBTYPES 7
+
+/* Each subtype can register a handler to process their events.
+ * The handler must have this interface.
+ */
+typedef void (vio_event_handler_t) (struct HvLpEvent * event);
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq);
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq);
+int vio_setHandler(int subtype, vio_event_handler_t * beh);
+int vio_clearHandler(int subtype);
+int viopath_isactive(HvLpIndex lp);
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp);
+HvLpInstanceId viopath_targetinst(HvLpIndex lp);
+void vio_set_hostlp(void);
+void *vio_get_event_buffer(int subtype);
+void vio_free_event_buffer(int subtype, void *buffer);
+
+extern HvLpIndex viopath_hostLp;
+extern HvLpIndex viopath_ourLp;
+
+#define VIO_MESSAGE "iSeries virtual I/O: "
+#define KERN_DEBUG_VIO KERN_DEBUG VIO_MESSAGE
+#define KERN_INFO_VIO KERN_INFO VIO_MESSAGE
+#define KERN_WARNING_VIO KERN_WARNING VIO_MESSAGE
+
+#define VIOCHAR_MAX_DATA 200
+
+#define VIOMAJOR_SUBTYPE_MASK 0xff00
+#define VIOMINOR_SUBTYPE_MASK 0x00ff
+#define VIOMAJOR_SUBTYPE_SHIFT 8
+
+#define VIOVERSION            0x0101
+
+/*
+This is the general structure for VIO errors; each module should have a table
+of them, and each table should be terminated by an entry of { 0, 0, NULL }.
+Then, to find a specific error message, a module should pass its local table
+and the return code.
+*/
+struct vio_error_entry {
+	u16 rc;
+	int errno;
+	const char *msg;
+};
+const struct vio_error_entry *vio_lookup_rc(const struct vio_error_entry
+					    *local_table, u16 rc);
+
+enum viosubtypes {
+	viomajorsubtype_monitor = 0x0100,
+	viomajorsubtype_blockio = 0x0200,
+	viomajorsubtype_chario = 0x0300,
+	viomajorsubtype_config = 0x0400,
+	viomajorsubtype_cdio = 0x0500,
+	viomajorsubtype_tape = 0x0600
+};
+
+
+enum vioconfigsubtype {
+	vioconfigget = 0x0001,
+};
+
+enum viorc {
+	viorc_good = 0x0000,
+	viorc_noConnection = 0x0001,
+	viorc_noReceiver = 0x0002,
+	viorc_noBufferAvailable = 0x0003,
+	viorc_invalidMessageType = 0x0004,
+	viorc_invalidRange = 0x0201,
+	viorc_invalidToken = 0x0202,
+	viorc_DMAError = 0x0203,
+	viorc_useError = 0x0204,
+	viorc_releaseError = 0x0205,
+	viorc_invalidDisk = 0x0206,
+	viorc_openRejected = 0x0301
+};
+
+
+#endif				/* _VIO_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/viocd.c linuxppc64_2_4/drivers/iseries/viocd.c
--- linux-2.4.19/drivers/iseries/viocd.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/viocd.c	Fri Mar 29 11:51:14 2002
@@ -0,0 +1,818 @@
+/* -*- linux-c -*-
+ *  drivers/cdrom/viocd.c
+ *
+ ***************************************************************************
+ *  iSeries Virtual CD Rom
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *************************************************************************** 
+ * This routine provides access to CD ROM drives owned and managed by an 
+ * OS/400 partition running on the same box as this Linux partition.
+ *
+ * All operations are performed by sending messages back and forth to 
+ * the OS/400 partition.  
+ *
+ * 
+ * This device driver can either use it's own major number, or it can
+ * pretend to be an AZTECH drive. This is controlled with a 
+ * CONFIG option.  You can either call this an elegant solution to the 
+ * fact that a lot of software doesn't recognize a new CD major number...
+ * or you can call this a really ugly hack.  Your choice.
+ *
+ */
+
+#include <linux/major.h>
+#include <linux/config.h>
+
+/* Decide on the proper naming convention to use for our device */
+#ifdef CONFIG_DEVFS_FS
+#define VIOCD_DEVICE "cdroms/cdrom%d"
+#define VIOCD_DEVICE_OFFSET 0
+#else
+#ifdef CONFIG_VIOCD_AZTECH
+#define VIOCD_DEVICE "aztcd"
+#define VIOCD_DEVICE_OFFSET 0
+#else
+#define VIOCD_DEVICE "iseries/vcd%c"
+#define VIOCD_DEVICE_OFFSET 'a'
+#endif
+#endif
+
+/***************************************************************************
+ * Decide if we are using our own major or pretending to be an AZTECH drive
+ ***************************************************************************/
+#ifdef CONFIG_VIOCD_AZTECH
+#define MAJOR_NR AZTECH_CDROM_MAJOR
+#define do_viocd_request do_aztcd_request
+#else
+#define MAJOR_NR VIOCD_MAJOR
+#endif
+
+#define VIOCD_VERS "1.04"
+
+#include <linux/blk.h>
+#include <linux/cdrom.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include "vio.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+extern struct pci_dev * iSeries_vio_dev;
+
+#define signalLpEventFast HvCallEvent_signalLpEventFast
+
+struct viocdlpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	u32 mToken;
+	u64 mOffset;		// On open, the max number of disks
+	u64 mLen;		// On open, the size of the disk
+	u32 mBlockSize;		// Only set on open
+	u32 mMediaSize;		// Only set on open
+};
+
+enum viocdsubtype {
+	viocdopen = 0x0001,
+	viocdclose = 0x0002,
+	viocdread = 0x0003,
+	viocdwrite = 0x0004,
+	viocdlockdoor = 0x0005,
+	viocdgetinfo = 0x0006,
+	viocdcheck = 0x0007
+};
+
+/* Should probably make this a module parameter....sigh
+ */
+#define VIOCD_MAX_CD 8
+int viocd_blocksizes[VIOCD_MAX_CD];
+static u64 viocd_size_in_bytes[VIOCD_MAX_CD];
+
+static const struct vio_error_entry viocd_err_table[] = {
+	{0x0201, EINVAL, "Invalid Range"},
+	{0x0202, EINVAL, "Invalid Token"},
+	{0x0203, EIO, "DMA Error"},
+	{0x0204, EIO, "Use Error"},
+	{0x0205, EIO, "Release Error"},
+	{0x0206, EINVAL, "Invalid CD"},
+	{0x020C, EROFS, "Read Only Device"},
+	{0x020D, EIO, "Changed or Missing Volume (or Varied Off?)"},
+	{0x020E, EIO, "Optical System Error (Varied Off?)"},
+	{0x02FF, EIO, "Internal Error"},
+	{0x3010, EIO, "Changed Volume"},
+	{0xC100, EIO, "Optical System Error"},
+	{0x0000, 0, NULL},
+};
+
+/* This is the structure we use to exchange info between driver and interrupt
+ * handler
+ */
+struct viocd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	u16 subtypeRc;
+	int changed;
+};
+
+/* this is a lookup table for the true capabilities of a device */
+struct capability_entry {
+	char *type;
+	int capability;
+};
+
+static struct capability_entry capability_table[] = {
+	{ "6330", CDC_LOCK | CDC_DVD_RAM },
+	{ "6321", CDC_LOCK },
+	{ "632B", 0 },
+	{ NULL  , CDC_LOCK },
+};
+
+struct block_device_operations viocd_fops =
+{
+	owner:			THIS_MODULE,
+	open:			cdrom_open,
+	release:		cdrom_release,
+	ioctl:			cdrom_ioctl,
+	check_media_change:	cdrom_media_changed,
+};
+
+/* These are our internal structures for keeping track of devices
+ */
+static int viocd_numdev;
+
+struct cdrom_info {
+	char rsrcname[10];
+	char type[4];
+	char model[3];
+};
+static struct cdrom_info *viocd_unitinfo = NULL;
+
+struct disk_info{
+	u32 useCount;
+	u32 blocksize;
+	u32 mediasize;
+};
+static struct disk_info viocd_diskinfo[VIOCD_MAX_CD];
+
+static struct cdrom_device_info viocd_info[VIOCD_MAX_CD];
+
+static spinlock_t viocd_lock = SPIN_LOCK_UNLOCKED;
+
+#define MAX_CD_REQ 1
+static LIST_HEAD(reqlist);
+
+/* End a request
+ */
+static int viocd_end_request(struct request *req, int uptodate)
+{
+	if (end_that_request_first(req, uptodate, DEVICE_NAME))
+		return 0;
+	end_that_request_last(req);
+	return 1;
+}
+
+
+/* Get info on CD devices from OS/400
+ */
+static void get_viocd_info(void)
+{
+	dma_addr_t dmaaddr;
+	HvLpEvent_Rc hvrc;
+	int i;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we;
+
+	// If we don't have a host, bail out
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return;
+
+	if (viocd_unitinfo == NULL)
+		viocd_unitinfo =
+		    kmalloc(sizeof(struct cdrom_info) * VIOCD_MAX_CD,
+			    GFP_KERNEL);
+
+	memset(viocd_unitinfo, 0x00,
+	       sizeof(struct cdrom_info) * VIOCD_MAX_CD);
+
+	dmaaddr = pci_map_single(iSeries_vio_dev, viocd_unitinfo,
+				 sizeof(struct cdrom_info) * VIOCD_MAX_CD,
+				 PCI_DMA_FROMDEVICE);
+	if (dmaaddr == 0xFFFFFFFF) {
+		printk(KERN_WARNING_VIO "error allocating tce\n");
+		return;
+	}
+
+	we.sem = &Semaphore;
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdgetinfo,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     dmaaddr,
+			     0,
+			     sizeof(struct cdrom_info) * VIOCD_MAX_CD,
+			     0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "cdrom error sending event. rc %d\n", (int) hvrc);
+		return;
+	}
+
+	down(&Semaphore);
+
+	if (we.rc) {
+		const struct vio_error_entry *err = vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on getinfo: %s\n", we.rc, we.subtypeRc, err->msg);
+		return;
+	}
+
+
+	for (i = 0; (i < VIOCD_MAX_CD) && (viocd_unitinfo[i].rsrcname[0]); i++) {
+		viocd_numdev++;
+	}
+}
+
+/* Open a device
+ */
+static int viocd_open(struct cdrom_device_info *cdi, int purpose)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int device_no = MINOR(cdi->dev);
+	HvLpEvent_Rc hvrc;
+	struct viocd_waitevent we;
+	struct disk_info *diskinfo = &viocd_diskinfo[device_no];
+
+	// If we don't have a host, bail out
+	if (viopath_hostLp == HvLpIndexInvalid || device_no >= viocd_numdev)
+		return -ENODEV;
+
+	we.sem = &Semaphore;
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdopen,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (we.rc) {
+		const struct vio_error_entry *err = vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on open: %s\n", we.rc, we.subtypeRc, err->msg);
+		return -err->errno;
+	}
+
+	if (diskinfo->useCount == 0) {
+		if(diskinfo->blocksize > 0) {
+			viocd_blocksizes[device_no] = diskinfo->blocksize;
+			viocd_size_in_bytes[device_no] = diskinfo->blocksize * diskinfo->mediasize;
+		} else {
+			viocd_size_in_bytes[device_no] = 0xFFFFFFFFFFFFFFFF;
+		}
+	}
+	MOD_INC_USE_COUNT;
+	return 0;
+}
+
+/* Release a device
+ */
+static void viocd_release(struct cdrom_device_info *cdi)
+{
+	int device_no = MINOR(cdi->dev);
+	HvLpEvent_Rc hvrc;
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid
+	    || device_no >= viocd_numdev)
+		return;
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdclose,
+			     HvLpEvent_AckInd_NoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     0,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return;
+	}
+
+	MOD_DEC_USE_COUNT;
+}
+
+/* Send a read or write request to OS/400
+ */
+static int send_request(struct request *req)
+{
+	HvLpEvent_Rc hvrc;
+	dma_addr_t dmaaddr;
+	int device_no = DEVICE_NR(req->rq_dev);
+	u64 start = req->sector * 512,
+	    len = req->current_nr_sectors * 512;
+	char reading = req->cmd == READ;
+	u16 command = reading ? viocdread : viocdwrite;
+
+
+	if(start + len > viocd_size_in_bytes[device_no]) {
+		printk(KERN_WARNING_VIO "viocd%d; access position %lx, past size %lx\n",
+		       device_no, start + len, viocd_size_in_bytes[device_no]);
+		return -1;
+	}
+	
+	dmaaddr = pci_map_single(iSeries_vio_dev, req->buffer, len,
+				 reading ? PCI_DMA_FROMDEVICE : PCI_DMA_TODEVICE);
+	if (dmaaddr == 0xFFFFFFFF) {
+		printk(KERN_WARNING_VIO "error allocating tce for address %p len %ld\n",
+			   req->buffer, len);
+		return -1;
+	}
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | command,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) req->buffer,
+	                     VIOVERSION << 16,
+			     ((u64) device_no << 48) | dmaaddr,
+			     start, len, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "hv error on op %d\n", (int) hvrc);
+		return -1;
+	}
+
+	return 0;
+}
+
+
+/* Do a request
+ */
+static int rwreq;
+static void do_viocd_request(request_queue_t * q)
+{
+	for (;;) {
+		struct request *req;
+		char err_str[80] = "";
+		int device_no;
+
+		INIT_REQUEST;
+		if (rwreq >= MAX_CD_REQ) {
+			return;
+		}
+
+		device_no = CURRENT_DEV;
+
+		/* remove the current request from the queue */
+		req = CURRENT;
+		blkdev_dequeue_request(req);
+
+		/* check for any kind of error */
+		if (device_no > viocd_numdev)
+			sprintf(err_str, "Invalid device number %d", device_no);
+		else if (send_request(req) < 0)
+			strcpy(err_str, "unable to send message to OS/400!");
+
+		/* if we had any sort of error, log it and cancel the request */
+		if (*err_str) {
+			printk(KERN_WARNING_VIO "%s\n", err_str);
+			viocd_end_request(req, 0);
+		} else {
+			spin_lock(&viocd_lock);
+			list_add_tail(&req->queue, &reqlist);
+			++rwreq;
+			spin_unlock(&viocd_lock);
+		}
+	}
+}
+
+/* Check if the CD changed
+ */
+static int viocd_media_changed(struct cdrom_device_info *cdi, int disc_nr)
+{
+	struct viocd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = MINOR(cdi->dev);
+
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400                                         */
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdcheck,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     ((u64) device_no << 48),
+			     0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc) {
+		const struct vio_error_entry *err = vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on check_change: %s; Assuming no change\n", we.rc, we.subtypeRc, err->msg);
+		return 0;
+	}
+
+	return we.changed;
+}
+
+static int viocd_lock_door(struct cdrom_device_info *cdi, int locking)
+{
+	HvLpEvent_Rc hvrc;
+	u64 device_no = MINOR(cdi->dev);
+	/* NOTE: flags is 1 or 0 so it won't overwrite the device_no             */
+	u64 flags = !!locking;
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we = { sem:&Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the lockdoor event to OS/400                                     */
+	hvrc = signalLpEventFast(viopath_hostLp,
+			     HvLpEvent_Type_VirtualIo,
+			     viomajorsubtype_cdio | viocdlockdoor,
+			     HvLpEvent_AckInd_DoAck,
+			     HvLpEvent_AckType_ImmediateAck,
+			     viopath_sourceinst(viopath_hostLp),
+			     viopath_targetinst(viopath_hostLp),
+			     (u64) (unsigned long) &we,
+			     VIOVERSION << 16,
+			     (device_no << 48) | (flags << 32),
+			     0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n", (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc != 0) {
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/* This routine handles incoming CD LP events
+ */
+static void vioHandleCDEvent(struct HvLpEvent *event)
+{
+	struct viocdlpevent *bevent = (struct viocdlpevent *) event;
+	struct viocd_waitevent *pwe;
+
+	if (event == NULL) {
+		/* Notification that a partition went away! */
+		return;
+	}
+	/* First, we should NEVER get an int here...only acks */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO "Yikes! got an int in viocd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+	case viocdopen:
+		viocd_diskinfo[bevent->mDisk].blocksize = bevent->mBlockSize;
+		viocd_diskinfo[bevent->mDisk].mediasize = bevent->mMediaSize;
+		/* FALLTHROUGH !! */
+	case viocdgetinfo:
+	case viocdlockdoor:
+		pwe = (struct viocd_waitevent *) (unsigned long) event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		up(pwe->sem);
+		break;
+
+	case viocdclose:
+		break;
+
+	case viocdwrite:
+	case viocdread:{
+		unsigned long flags;
+		int reading = ((event->xSubtype & VIOMINOR_SUBTYPE_MASK) == viocdread);
+		struct request *req = blkdev_entry_to_request(reqlist.next);
+		/* Since this is running in interrupt mode, we need to make sure we're not
+		 * stepping on any global I/O operations
+		 */
+		spin_lock_irqsave(&io_request_lock, flags);
+
+		pci_unmap_single(iSeries_vio_dev,
+				 bevent->mToken,
+				 bevent->mLen,
+				 reading ? PCI_DMA_FROMDEVICE : PCI_DMA_TODEVICE);
+
+		/* find the event to which this is a response */
+		while ((&req->queue != &reqlist) &&
+		       ((u64) (unsigned long) req->buffer != bevent->event.xCorrelationToken))
+			req = blkdev_entry_to_request(req->queue.next);
+
+		/* if the event was not there, then what are we responding to?? */
+		if (&req->queue == &reqlist) {
+			printk(KERN_WARNING_VIO "Yikes! we never enqueued this guy!\n");
+			spin_unlock_irqrestore(&io_request_lock,
+					       flags);
+			break;
+		}
+
+		/* we don't need to keep it around anymore... */
+		spin_lock(&viocd_lock);
+		list_del(&req->queue);
+		--rwreq;
+		spin_unlock(&viocd_lock);
+		{
+			char stat = event->xRc == HvLpEvent_Rc_Good;
+			int nsect = bevent->mLen >> 9;
+
+			if (!stat) {
+				const struct vio_error_entry *err =
+				    vio_lookup_rc(viocd_err_table, bevent->mSubTypeRc);
+				printk(KERN_WARNING_VIO "request %p failed with rc %d:0x%04X: %s\n",
+				       req->buffer, event->xRc, bevent->mSubTypeRc, err->msg);
+			}
+			while ((nsect > 0) && (req->bh)) {
+				nsect -= req->current_nr_sectors;
+				viocd_end_request(req, stat);
+			}
+			/* we weren't done yet */
+			if (req->bh) {
+				if (send_request(req) < 0) {
+					printk(KERN_WARNING_VIO
+					    "couldn't re-submit req %p\n", req->buffer);
+					viocd_end_request(req, 0);
+				} else {
+					spin_lock(&viocd_lock);
+					list_add_tail(&req->queue, &reqlist);
+					++rwreq;
+					spin_unlock(&viocd_lock);
+				}
+			}
+		}
+
+		/* restart handling of incoming requests */
+		do_viocd_request(NULL);
+		spin_unlock_irqrestore(&io_request_lock, flags);
+		break;
+	}
+	case viocdcheck:
+		pwe = (struct viocd_waitevent *) (unsigned long) event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		pwe->changed = bevent->mFlags;
+		up(pwe->sem);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO "message with invalid subtype %0x04X!\n", event->xSubtype & VIOMINOR_SUBTYPE_MASK);
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+/* Our file operations table
+ */
+static struct cdrom_device_ops viocd_dops = {
+	open:viocd_open,
+	release:viocd_release,
+	media_changed:viocd_media_changed,
+	lock_door:viocd_lock_door,
+	capability:CDC_CLOSE_TRAY | CDC_OPEN_TRAY | CDC_LOCK | CDC_SELECT_SPEED | CDC_SELECT_DISC | CDC_MULTI_SESSION | CDC_MCN | CDC_MEDIA_CHANGED | CDC_PLAY_AUDIO | CDC_RESET | CDC_IOCTLS | CDC_DRIVE_STATUS | CDC_GENERIC_PACKET | CDC_CD_R | CDC_CD_RW | CDC_DVD | CDC_DVD_R | CDC_DVD_RAM
+};
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+
+	for (i = 0; i < viocd_numdev; i++) {
+		len +=
+		    sprintf(buf + len,
+			    "viocd device %d is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			    i, viocd_unitinfo[i].rsrcname,
+			    viocd_unitinfo[i].type,
+			    viocd_unitinfo[i].model);
+	}
+	*eof = 1;
+	return len;
+}
+
+
+/* setup our proc file system entries
+ */
+void viocd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("viocd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+}
+
+/* clean up our proc file system entries
+ */
+void viocd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viocd", iSeries_proc);
+}
+
+static int find_capability(const char *type)
+{
+	struct capability_entry *entry;
+	for(entry = capability_table; entry->type; ++entry)
+		if(!strncmp(entry->type, type, 4))
+			break;
+	return entry->capability;
+}
+
+/* Initialize the whole device driver.  Handle module and non-module
+ * versions
+ */
+__init int viocd_init(void)
+{
+	int i, rc;
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return -ENODEV;
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+	if (rc) {
+		printk(KERN_WARNING_VIO "error opening path to host partition %d\n",
+			   viopath_hostLp);
+		return rc;
+	}
+
+	/* Initialize our request handler
+	 */
+	rwreq = 0;
+	vio_setHandler(viomajorsubtype_cdio, vioHandleCDEvent);
+
+	memset(&viocd_diskinfo, 0x00, sizeof(viocd_diskinfo));
+
+	get_viocd_info();
+
+	if (viocd_numdev == 0) {
+		vio_clearHandler(viomajorsubtype_cdio);
+		viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+		return 0;
+	}
+
+	printk(KERN_INFO_VIO
+	       "%s: iSeries Virtual CD vers %s, major %d, max disks %d, hosting partition %d\n",
+	       DEVICE_NAME, VIOCD_VERS, MAJOR_NR, VIOCD_MAX_CD, viopath_hostLp);
+
+	if (devfs_register_blkdev(MAJOR_NR, "viocd", &viocd_fops) != 0) {
+		printk(KERN_WARNING_VIO "Unable to get major %d for viocd CD-ROM\n", MAJOR_NR);
+		return -EIO;
+	}
+
+	blksize_size[MAJOR_NR] = viocd_blocksizes;
+	blk_init_queue(BLK_DEFAULT_QUEUE(MAJOR_NR), DEVICE_REQUEST);
+	read_ahead[MAJOR_NR] = 4;
+
+	memset(&viocd_info, 0x00, sizeof(viocd_info));
+	for (i = 0; i < viocd_numdev; i++) {
+		viocd_info[i].dev = MKDEV(MAJOR_NR, i);
+		viocd_info[i].ops = &viocd_dops;
+		viocd_info[i].speed = 4;
+		viocd_info[i].capacity = 1;
+		viocd_info[i].mask = ~find_capability(viocd_unitinfo[i].type);
+		sprintf(viocd_info[i].name, VIOCD_DEVICE, VIOCD_DEVICE_OFFSET + i);
+		if (register_cdrom(&viocd_info[i]) != 0) {
+			printk(KERN_WARNING_VIO "Cannot register viocd CD-ROM %s!\n", viocd_info[i].name);
+		} else {
+			printk(KERN_INFO_VIO 
+			       "cd %s is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			       viocd_info[i].name,
+			       viocd_unitinfo[i].rsrcname,
+			       viocd_unitinfo[i].type,
+			       viocd_unitinfo[i].model);
+		}
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viocd_proc_init);
+
+	return 0;
+}
+
+#ifdef MODULE
+void viocd_exit(void)
+{
+	int i;
+	for (i = 0; i < viocd_numdev; i++) {
+		if (unregister_cdrom(&viocd_info[i]) != 0) {
+			printk(KERN_WARNING_VIO "Cannot unregister viocd CD-ROM %s!\n", viocd_info[i].name);
+		}
+	}
+	if ((devfs_unregister_blkdev(MAJOR_NR, "viocd") == -EINVAL)) {
+		printk(KERN_WARNING_VIO "can't unregister viocd\n");
+		return;
+	}
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(MAJOR_NR));
+	if (viocd_unitinfo)
+		kfree(viocd_unitinfo);
+
+	iSeries_proc_callback(&viocd_proc_delete);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+	vio_clearHandler(viomajorsubtype_cdio);
+}
+#endif
+
+#ifdef MODULE
+module_init(viocd_init);
+module_exit(viocd_exit);
+MODULE_LICENSE("GPL");
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/viocons.c linuxppc64_2_4/drivers/iseries/viocons.c
--- linux-2.4.19/drivers/iseries/viocons.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/viocons.c	Thu Apr 25 19:59:10 2002
@@ -0,0 +1,1390 @@
+/* -*- linux-c -*-
+ *  drivers/char/viocons.c
+ *
+ *  iSeries Virtual Terminal
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/console.h>
+#include <linux/module.h>
+#include <asm/uaccess.h>
+#include <linux/init.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <asm/ioctls.h>
+#include <linux/kd.h>
+
+#include "vio.h"
+
+#include <asm/iSeries/HvLpEvent.h>
+#include "asm/iSeries/HvCallEvent.h"
+#include "asm/iSeries/HvLpConfig.h"
+#include "asm/iSeries/HvCall.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+/* Check that the tty_driver_data actually points to our stuff
+ */
+#define VIOTTY_PARANOIA_CHECK 1
+#define VIOTTY_MAGIC (0x0DCB)
+
+static int debug;
+
+static DECLARE_WAIT_QUEUE_HEAD(viocons_wait_queue);
+
+#define VTTY_PORTS 10
+#define VIOTTY_SERIAL_START 65
+
+static u64 sndMsgSeq[VTTY_PORTS];
+static u64 sndMsgAck[VTTY_PORTS];
+
+static spinlock_t consolelock = SPIN_LOCK_UNLOCKED;
+
+/* THe structure of the events that flow between us and OS/400.  You can't
+ * mess with this unless the OS/400 side changes too
+ */
+struct viocharlpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u8 virtualDevice;
+	u8 immediateDataLen;
+	u8 immediateData[VIOCHAR_MAX_DATA];
+};
+
+#define viochar_window (10)
+#define viochar_highwatermark (3)
+
+enum viocharsubtype {
+	viocharopen = 0x0001,
+	viocharclose = 0x0002,
+	viochardata = 0x0003,
+	viocharack = 0x0004,
+	viocharconfig = 0x0005
+};
+
+enum viochar_rc {
+	viochar_rc_ebusy = 1
+};
+
+/* When we get writes faster than we can send it to the partition,
+ * buffer the data here.  There is one set of buffers for each virtual
+ * port.
+ * Note that bufferUsed is a bit map of used buffers.
+ * It had better have enough bits to hold NUM_BUF
+ * the bitops assume it is a multiple of unsigned long
+ */
+#define NUM_BUF (8)
+#define OVERFLOW_SIZE VIOCHAR_MAX_DATA
+
+static struct overflowBuffers {
+	unsigned long bufferUsed;
+	u8 *buffer[NUM_BUF];
+	int bufferBytes[NUM_BUF];
+	int curbuf;
+	int bufferOverflow;
+	int overflowMessage;
+} overflow[VTTY_PORTS];
+
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp);
+
+static struct tty_driver viotty_driver;
+static struct tty_driver viottyS_driver;
+static int viotty_refcount;
+
+static struct tty_struct *viotty_table[VTTY_PORTS];
+static struct tty_struct *viottyS_table[VTTY_PORTS];
+static struct termios *viotty_termios[VTTY_PORTS];
+static struct termios *viottyS_termios[VTTY_PORTS];
+static struct termios *viotty_termios_locked[VTTY_PORTS];
+static struct termios *viottyS_termios_locked[VTTY_PORTS];
+
+void hvlog(char *fmt, ...)
+{
+	int i;
+	static char buf[256];
+	va_list args;
+	va_start(args, fmt);
+	i = vsprintf(buf, fmt, args);
+	va_end(args);
+	HvCall_writeLogBuffer(buf, i);
+	HvCall_writeLogBuffer("\r", 1);
+
+}
+
+/* Our port information.  We store a pointer to one entry in the
+ * tty_driver_data
+ */
+static struct port_info_tag {
+	int magic;
+	struct tty_struct *tty;
+	HvLpIndex lp;
+	u8 vcons;
+	u8 port;
+} port_info[VTTY_PORTS];
+
+/* Make sure we're pointing to a valid port_info structure.  Shamelessly
+ * plagerized from serial.c
+ */
+static inline int viotty_paranoia_check(struct port_info_tag *pi,
+					kdev_t device, const char *routine)
+{
+#ifdef VIOTTY_PARANOIA_CHECK
+	static const char *badmagic =
+	    "%s Warning: bad magic number for port_info struct (%s) in %s\n";
+	static const char *badinfo =
+	    "%s Warning: null port_info for (%s) in %s\n";
+
+	if (!pi) {
+		printk(badinfo, KERN_WARNING_VIO, kdevname(device),
+		       routine);
+		return 1;
+	}
+	if (pi->magic != VIOTTY_MAGIC) {
+		printk(badmagic, KERN_WARNING_VIO, kdevname(device),
+		       routine);
+		return 1;
+	}
+#endif
+	return 0;
+}
+
+/*
+ * Handle reads from the proc file system.  Right now we just dump the
+ * state of the first TTY
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	struct tty_struct *tty = viotty_table[0];
+	struct termios *termios;
+	if (tty == NULL) {
+		len += sprintf(buf + len, "no tty\n");
+		*eof = 1;
+		return len;
+	}
+
+	len +=
+	    sprintf(buf + len,
+		    "tty info: COOK_OUT %ld COOK_IN %ld, NO_WRITE_SPLIT %ld\n",
+		    tty->flags & TTY_HW_COOK_OUT,
+		    tty->flags & TTY_HW_COOK_IN,
+		    tty->flags & TTY_NO_WRITE_SPLIT);
+
+	termios = tty->termios;
+	if (termios == NULL) {
+		len += sprintf(buf + len, "no termios\n");
+		*eof = 1;
+		return len;
+	}
+	len += sprintf(buf + len, "INTR_CHAR     %2.2x\n", INTR_CHAR(tty));
+	len += sprintf(buf + len, "QUIT_CHAR     %2.2x\n", QUIT_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "ERASE_CHAR    %2.2x\n", ERASE_CHAR(tty));
+	len += sprintf(buf + len, "KILL_CHAR     %2.2x\n", KILL_CHAR(tty));
+	len += sprintf(buf + len, "EOF_CHAR      %2.2x\n", EOF_CHAR(tty));
+	len += sprintf(buf + len, "TIME_CHAR     %2.2x\n", TIME_CHAR(tty));
+	len += sprintf(buf + len, "MIN_CHAR      %2.2x\n", MIN_CHAR(tty));
+	len += sprintf(buf + len, "SWTC_CHAR     %2.2x\n", SWTC_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "START_CHAR    %2.2x\n", START_CHAR(tty));
+	len += sprintf(buf + len, "STOP_CHAR     %2.2x\n", STOP_CHAR(tty));
+	len += sprintf(buf + len, "SUSP_CHAR     %2.2x\n", SUSP_CHAR(tty));
+	len += sprintf(buf + len, "EOL_CHAR      %2.2x\n", EOL_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "REPRINT_CHAR  %2.2x\n", REPRINT_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "DISCARD_CHAR  %2.2x\n", DISCARD_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "WERASE_CHAR   %2.2x\n", WERASE_CHAR(tty));
+	len +=
+	    sprintf(buf + len, "LNEXT_CHAR    %2.2x\n", LNEXT_CHAR(tty));
+	len += sprintf(buf + len, "EOL2_CHAR     %2.2x\n", EOL2_CHAR(tty));
+
+	len += sprintf(buf + len, "I_IGNBRK      %4.4x\n", I_IGNBRK(tty));
+	len += sprintf(buf + len, "I_BRKINT      %4.4x\n", I_BRKINT(tty));
+	len += sprintf(buf + len, "I_IGNPAR      %4.4x\n", I_IGNPAR(tty));
+	len += sprintf(buf + len, "I_PARMRK      %4.4x\n", I_PARMRK(tty));
+	len += sprintf(buf + len, "I_INPCK       %4.4x\n", I_INPCK(tty));
+	len += sprintf(buf + len, "I_ISTRIP      %4.4x\n", I_ISTRIP(tty));
+	len += sprintf(buf + len, "I_INLCR       %4.4x\n", I_INLCR(tty));
+	len += sprintf(buf + len, "I_IGNCR       %4.4x\n", I_IGNCR(tty));
+	len += sprintf(buf + len, "I_ICRNL       %4.4x\n", I_ICRNL(tty));
+	len += sprintf(buf + len, "I_IUCLC       %4.4x\n", I_IUCLC(tty));
+	len += sprintf(buf + len, "I_IXON        %4.4x\n", I_IXON(tty));
+	len += sprintf(buf + len, "I_IXANY       %4.4x\n", I_IXANY(tty));
+	len += sprintf(buf + len, "I_IXOFF       %4.4x\n", I_IXOFF(tty));
+	len += sprintf(buf + len, "I_IMAXBEL     %4.4x\n", I_IMAXBEL(tty));
+
+	len += sprintf(buf + len, "O_OPOST       %4.4x\n", O_OPOST(tty));
+	len += sprintf(buf + len, "O_OLCUC       %4.4x\n", O_OLCUC(tty));
+	len += sprintf(buf + len, "O_ONLCR       %4.4x\n", O_ONLCR(tty));
+	len += sprintf(buf + len, "O_OCRNL       %4.4x\n", O_OCRNL(tty));
+	len += sprintf(buf + len, "O_ONOCR       %4.4x\n", O_ONOCR(tty));
+	len += sprintf(buf + len, "O_ONLRET      %4.4x\n", O_ONLRET(tty));
+	len += sprintf(buf + len, "O_OFILL       %4.4x\n", O_OFILL(tty));
+	len += sprintf(buf + len, "O_OFDEL       %4.4x\n", O_OFDEL(tty));
+	len += sprintf(buf + len, "O_NLDLY       %4.4x\n", O_NLDLY(tty));
+	len += sprintf(buf + len, "O_CRDLY       %4.4x\n", O_CRDLY(tty));
+	len += sprintf(buf + len, "O_TABDLY      %4.4x\n", O_TABDLY(tty));
+	len += sprintf(buf + len, "O_BSDLY       %4.4x\n", O_BSDLY(tty));
+	len += sprintf(buf + len, "O_VTDLY       %4.4x\n", O_VTDLY(tty));
+	len += sprintf(buf + len, "O_FFDLY       %4.4x\n", O_FFDLY(tty));
+
+	len += sprintf(buf + len, "C_BAUD        %4.4x\n", C_BAUD(tty));
+	len += sprintf(buf + len, "C_CSIZE       %4.4x\n", C_CSIZE(tty));
+	len += sprintf(buf + len, "C_CSTOPB      %4.4x\n", C_CSTOPB(tty));
+	len += sprintf(buf + len, "C_CREAD       %4.4x\n", C_CREAD(tty));
+	len += sprintf(buf + len, "C_PARENB      %4.4x\n", C_PARENB(tty));
+	len += sprintf(buf + len, "C_PARODD      %4.4x\n", C_PARODD(tty));
+	len += sprintf(buf + len, "C_HUPCL       %4.4x\n", C_HUPCL(tty));
+	len += sprintf(buf + len, "C_CLOCAL      %4.4x\n", C_CLOCAL(tty));
+	len += sprintf(buf + len, "C_CRTSCTS     %4.4x\n", C_CRTSCTS(tty));
+
+	len += sprintf(buf + len, "L_ISIG        %4.4x\n", L_ISIG(tty));
+	len += sprintf(buf + len, "L_ICANON      %4.4x\n", L_ICANON(tty));
+	len += sprintf(buf + len, "L_XCASE       %4.4x\n", L_XCASE(tty));
+	len += sprintf(buf + len, "L_ECHO        %4.4x\n", L_ECHO(tty));
+	len += sprintf(buf + len, "L_ECHOE       %4.4x\n", L_ECHOE(tty));
+	len += sprintf(buf + len, "L_ECHOK       %4.4x\n", L_ECHOK(tty));
+	len += sprintf(buf + len, "L_ECHONL      %4.4x\n", L_ECHONL(tty));
+	len += sprintf(buf + len, "L_NOFLSH      %4.4x\n", L_NOFLSH(tty));
+	len += sprintf(buf + len, "L_TOSTOP      %4.4x\n", L_TOSTOP(tty));
+	len += sprintf(buf + len, "L_ECHOCTL     %4.4x\n", L_ECHOCTL(tty));
+	len += sprintf(buf + len, "L_ECHOPRT     %4.4x\n", L_ECHOPRT(tty));
+	len += sprintf(buf + len, "L_ECHOKE      %4.4x\n", L_ECHOKE(tty));
+	len += sprintf(buf + len, "L_FLUSHO      %4.4x\n", L_FLUSHO(tty));
+	len += sprintf(buf + len, "L_PENDIN      %4.4x\n", L_PENDIN(tty));
+	len += sprintf(buf + len, "L_IEXTEN      %4.4x\n", L_IEXTEN(tty));
+
+	*eof = 1;
+	return len;
+}
+
+/*
+ * Handle writes to our proc file system.  Right now just turns on and off
+ * our debug flag
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	if (count) {
+		if (buffer[0] == '1') {
+			printk("viocons: debugging on\n");
+			debug = 1;
+		} else {
+			printk("viocons: debugging off\n");
+			debug = 0;
+		}
+	}
+	return count;
+}
+
+/*
+ * setup our proc file system entries
+ */
+void viocons_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viocons", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/*
+ * clean up our proc file system entries
+ */
+void viocons_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viocons", iSeries_proc);
+}
+
+/*
+ * Add data to our pending-send buffers.  
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+static int bufferAdd(u8 port, const char *buf, size_t len, int userFlag)
+{
+	size_t bleft = len;
+	size_t curlen;
+	char *cbuf = (char *) buf;
+	int nextbuf;
+	struct overflowBuffers *pov = &overflow[port];
+	while (bleft > 0) {
+		/* If there is no space left in the current buffer, we have
+		 * filled everything up, so return.  If we filled the previous
+		 * buffer we would already have moved to the next one.
+		 */
+		if (pov->bufferBytes[pov->curbuf] == OVERFLOW_SIZE) {
+			hvlog("buffer %d full.  no more space\n",
+			      pov->curbuf);
+			pov->bufferOverflow++;
+			pov->overflowMessage = 1;
+			return len - bleft;
+		}
+
+		/* Turn on the "used" bit for this buffer.  If it's already on, that's
+		 * fine.
+		 */
+		set_bit(pov->curbuf, &pov->bufferUsed);
+
+		/* 
+		 * See if this buffer has been allocated.  If not, allocate it
+		 */
+		if (pov->buffer[pov->curbuf] == NULL)
+			pov->buffer[pov->curbuf] =
+			    kmalloc(OVERFLOW_SIZE, GFP_ATOMIC);
+
+		/*
+		 * Figure out how much we can copy into this buffer
+		 */
+		if (bleft <
+		    (OVERFLOW_SIZE - pov->bufferBytes[pov->curbuf]))
+			curlen = bleft;
+		else
+			curlen =
+			    OVERFLOW_SIZE - pov->bufferBytes[pov->curbuf];
+
+		/*
+		 * Copy the data into the buffer                      
+		 */
+		if (userFlag)
+			copy_from_user(pov->buffer[pov->curbuf] +
+				       pov->bufferBytes[pov->curbuf], cbuf,
+				       curlen);
+		else
+			memcpy(pov->buffer[pov->curbuf] +
+			       pov->bufferBytes[pov->curbuf], cbuf,
+			       curlen);
+
+		pov->bufferBytes[pov->curbuf] += curlen;
+		cbuf += curlen;
+		bleft -= curlen;
+
+		/*
+		 * Now see if we've filled this buffer
+		 */
+		if (pov->bufferBytes[pov->curbuf] == OVERFLOW_SIZE) {
+			nextbuf = (pov->curbuf + 1) % NUM_BUF;
+
+			/*
+			 * Move to the next buffer if it hasn't been used yet
+			 */
+			if (test_bit(nextbuf, &pov->bufferUsed) == 0) {
+				pov->curbuf = nextbuf;
+			}
+		}
+	}
+	return len;
+}
+
+/* Send pending data
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+void sendBuffers(u8 port, HvLpIndex lp)
+{
+	HvLpEvent_Rc hvrc;
+	int nextbuf;
+	struct viocharlpevent *viochar;
+	unsigned long flags;
+	struct overflowBuffers *pov = &overflow[port];
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	viochar = (struct viocharlpevent *)
+	    vio_get_event_buffer(viomajorsubtype_chario);
+
+	/* Make sure we got a buffer
+	 */
+	if (viochar == NULL) {
+		hvlog("Yikes...can't get viochar buffer");
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	if (pov->bufferUsed == 0) {
+		hvlog("in sendbuffers, but no buffers used\n");
+		vio_free_event_buffer(viomajorsubtype_chario, viochar);
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	/*
+	 * curbuf points to the buffer we're filling.  We want to start sending AFTER
+	 * this one.  
+	 */
+	nextbuf = (pov->curbuf + 1) % NUM_BUF;
+
+	/*
+	 * Loop until we find a buffer with the bufferUsed bit on
+	 */
+	while (test_bit(nextbuf, &pov->bufferUsed) == 0)
+		nextbuf = (nextbuf + 1) % NUM_BUF;
+
+	initDataEvent(viochar, lp);
+
+	/*
+	 * While we have buffers with data, and our send window is open, send them
+	 */
+	while ((test_bit(nextbuf, &pov->bufferUsed)) &&
+	       ((sndMsgSeq[port] - sndMsgAck[port]) < viochar_window)) {
+		viochar->immediateDataLen = pov->bufferBytes[nextbuf];
+		viochar->event.xCorrelationToken = sndMsgSeq[port]++;
+		viochar->event.xSizeMinus1 =
+		    offsetof(struct viocharlpevent,
+			     immediateData) + viochar->immediateDataLen;
+
+		memcpy(viochar->immediateData, pov->buffer[nextbuf],
+		       viochar->immediateDataLen);
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			/*
+			 * MUST unlock the spinlock before doing a printk
+			 */
+			vio_free_event_buffer(viomajorsubtype_chario,
+					      viochar);
+			spin_unlock_irqrestore(&consolelock, flags);
+
+			printk(KERN_WARNING_VIO
+			       "console error sending event! return code %d\n",
+			       (int) hvrc);
+			return;
+		}
+
+		/*
+		 * clear the bufferUsed bit, zero the number of bytes in this buffer,
+		 * and move to the next buffer
+		 */
+		clear_bit(nextbuf, &pov->bufferUsed);
+		pov->bufferBytes[nextbuf] = 0;
+		nextbuf = (nextbuf + 1) % NUM_BUF;
+	}
+
+
+	/*
+	 * If we have emptied all the buffers, start at 0 again.
+	 * this will re-use any allocated buffers
+	 */
+	if (pov->bufferUsed == 0) {
+		pov->curbuf = 0;
+
+		if (pov->overflowMessage)
+			pov->overflowMessage = 0;
+
+		if (port_info[port].tty) {
+			if ((port_info[port].tty->
+			     flags & (1 << TTY_DO_WRITE_WAKEUP))
+			    && (port_info[port].tty->ldisc.write_wakeup))
+				(port_info[port].tty->ldisc.
+				 write_wakeup) (port_info[port].tty);
+			wake_up_interruptible(&port_info[port].tty->
+					      write_wait);
+		}
+	}
+
+	vio_free_event_buffer(viomajorsubtype_chario, viochar);
+	spin_unlock_irqrestore(&consolelock, flags);
+
+}
+
+/* Our internal writer.  Gets called both from the console device and
+ * the tty device.  the tty pointer will be NULL if called from the console.
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog can be
+ * used to log to the hypervisor buffer
+ */
+static int internal_write(struct tty_struct *tty, const char *buf,
+			  size_t len, int userFlag)
+{
+	HvLpEvent_Rc hvrc;
+	size_t bleft = len;
+	size_t curlen;
+	const char *curbuf = buf;
+	struct viocharlpevent *viochar;
+	unsigned long flags;
+	struct port_info_tag *pi = NULL;
+	HvLpIndex lp;
+	u8 port;
+
+	if (tty) {
+		pi = (struct port_info_tag *) tty->driver_data;
+
+		if (!pi
+		    || viotty_paranoia_check(pi, tty->device,
+					     "viotty_internal_write"))
+			return -ENODEV;
+
+		lp = pi->lp;
+		port = pi->port;
+	} else {
+		/* If this is the console device, use the lp from the first port entry
+		 */
+		port = 0;
+		lp = port_info[0].lp;
+	}
+
+	/* Always put console output in the hypervisor console log
+	 */
+	if (port == 0)
+		HvCall_writeLogBuffer(buf, len);
+
+	/* If the path to this LP is closed, don't bother doing anything more.
+	 * just dump the data on the floor
+	 */
+	if (!viopath_isactive(lp))
+		return len;
+
+	/*
+	 * If there is already data queued for this port, send it
+	 */
+	if (overflow[port].bufferUsed)
+		sendBuffers(port, lp);
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	viochar = (struct viocharlpevent *)
+	    vio_get_event_buffer(viomajorsubtype_chario);
+	/* Make sure we got a buffer
+	 */
+	if (viochar == NULL) {
+		hvlog("Yikes...can't get viochar buffer");
+		spin_unlock_irqrestore(&consolelock, flags);
+		return -1;
+	}
+
+	initDataEvent(viochar, lp);
+
+	/* Got the lock, don't cause console output */
+	while ((bleft > 0) &&
+	       (overflow[port].bufferUsed == 0) &&
+	       ((sndMsgSeq[port] - sndMsgAck[port]) < viochar_window)) {
+		if (bleft > VIOCHAR_MAX_DATA)
+			curlen = VIOCHAR_MAX_DATA;
+		else
+			curlen = bleft;
+
+		viochar->immediateDataLen = curlen;
+		viochar->event.xCorrelationToken = sndMsgSeq[port]++;
+
+		if (userFlag)
+			copy_from_user(viochar->immediateData, curbuf,
+				       curlen);
+		else
+			memcpy(viochar->immediateData, curbuf, curlen);
+
+		viochar->event.xSizeMinus1 =
+		    offsetof(struct viocharlpevent,
+			     immediateData) + curlen;
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			/*
+			 * MUST unlock the spinlock before doing a printk
+			 */
+			vio_free_event_buffer(viomajorsubtype_chario,
+					      viochar);
+			spin_unlock_irqrestore(&consolelock, flags);
+
+			hvlog("viocons: error sending event! %d\n",
+			      (int) hvrc);
+			return len - bleft;
+		}
+
+		curbuf += curlen;
+		bleft -= curlen;
+	}
+
+	/*
+	 * If we didn't send it all, buffer it
+	 */
+	if (bleft > 0) {
+		bleft -= bufferAdd(port, curbuf, bleft, userFlag);
+	}
+	vio_free_event_buffer(viomajorsubtype_chario, viochar);
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return len - bleft;
+}
+
+/* Initialize the common fields in a charLpEvent
+ */
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp)
+{
+	memset(viochar, 0x00, sizeof(struct viocharlpevent));
+
+	viochar->event.xFlags.xValid = 1;
+	viochar->event.xFlags.xFunction = HvLpEvent_Function_Int;
+	viochar->event.xFlags.xAckInd = HvLpEvent_AckInd_NoAck;
+	viochar->event.xFlags.xAckType = HvLpEvent_AckType_DeferredAck;
+	viochar->event.xType = HvLpEvent_Type_VirtualIo;
+	viochar->event.xSubtype = viomajorsubtype_chario | viochardata;
+	viochar->event.xSourceLp = HvLpConfig_getLpIndex();
+	viochar->event.xTargetLp = lp;
+	viochar->event.xSizeMinus1 = sizeof(struct viocharlpevent);
+	viochar->event.xSourceInstanceId = viopath_sourceinst(lp);
+	viochar->event.xTargetInstanceId = viopath_targetinst(lp);
+}
+
+
+/* console device write
+ */
+static void viocons_write(struct console *co, const char *s,
+			  unsigned count)
+{
+	/* This parser will ensure that all single instances of either \n or \r are
+	 * matched into carriage return/line feed combinations.  It also allows for
+	 * instances where there already exist \n\r combinations as well as the
+	 * reverse, \r\n combinations.
+	 */
+
+	int index;
+	char charptr[1];
+	int foundcr;
+	int slicebegin;
+	int sliceend;
+
+	foundcr = 0;
+	slicebegin = 0;
+	sliceend = 0;
+
+	for (index = 0; index < count; index++) {
+		if (!foundcr && s[index] == 0x0a) {
+			if ((slicebegin - sliceend > 0)
+			    && sliceend < count) {
+				internal_write(NULL, &s[slicebegin],
+					       sliceend - slicebegin, 0);
+				slicebegin = sliceend;
+			}
+			charptr[0] = '\r';
+			internal_write(NULL, charptr, 1, 0);
+		}
+		if (foundcr && s[index] != 0x0a) {
+			if ((index - 2) >= 0) {
+				if (s[index - 2] != 0x0a) {
+					internal_write(NULL,
+						       &s[slicebegin],
+						       sliceend -
+						       slicebegin, 0);
+					slicebegin = sliceend;
+					charptr[0] = '\n';
+					internal_write(NULL, charptr, 1,
+						       0);
+				}
+			}
+		}
+		sliceend++;
+
+		if (s[index] == 0x0d)
+			foundcr = 1;
+		else
+			foundcr = 0;
+	}
+
+	internal_write(NULL, &s[slicebegin], sliceend - slicebegin, 0);
+
+	if (count > 1) {
+		if (foundcr == 1 && s[count - 1] != 0x0a) {
+			charptr[0] = '\n';
+			internal_write(NULL, charptr, 1, 0);
+		} else if (s[count - 1] == 0x0a && s[count - 2] != 0x0d) {
+
+			charptr[0] = '\r';
+			internal_write(NULL, charptr, 1, 0);
+		}
+	}
+}
+
+/* Work out a the device associate with this console
+ */
+static kdev_t viocons_device(struct console *c)
+{
+	return MKDEV(TTY_MAJOR, c->index + viotty_driver.minor_start);
+}
+
+/* console device read method
+ */
+static int viocons_read(struct console *co, const char *s, unsigned count)
+{
+	printk(KERN_DEBUG_VIO "viocons_read\n");
+	// Implement me
+	interruptible_sleep_on(&viocons_wait_queue);
+	return 0;
+}
+
+/* Do console device setup
+ */
+static int __init viocons_setup(struct console *co, char *options)
+{
+	return 0;
+}
+
+/* console device I/O methods
+ */
+static struct console viocons = {
+	name:"ttyS",
+	write:viocons_write,
+	read:viocons_read,
+	device:viocons_device,
+	setup:viocons_setup,
+	flags:CON_PRINTBUFFER,
+};
+
+
+/* TTY Open method
+ */
+static int viotty_open(struct tty_struct *tty, struct file *filp)
+{
+	int port;
+	unsigned long flags;
+	MOD_INC_USE_COUNT;
+	port = MINOR(tty->device) - tty->driver.minor_start;
+
+	if (port >= VIOTTY_SERIAL_START)
+		port -= VIOTTY_SERIAL_START;
+
+	if ((port < 0) || (port >= VTTY_PORTS)) {
+		MOD_DEC_USE_COUNT;
+		return -ENODEV;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	/*
+	 * If some other TTY is already connected here, reject the open
+	 */
+	if ((port_info[port].tty) && (port_info[port].tty != tty)) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		MOD_DEC_USE_COUNT;
+		printk(KERN_WARNING_VIO
+		       "console attempt to open device twice from different ttys\n");
+		return -EBUSY;
+	}
+	tty->driver_data = &port_info[port];
+	port_info[port].tty = tty;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return 0;
+}
+
+/* TTY Close method
+ */
+static void viotty_close(struct tty_struct *tty, struct file *filp)
+{
+	unsigned long flags;
+	struct port_info_tag *pi =
+	    (struct port_info_tag *) tty->driver_data;
+
+	if (!pi || viotty_paranoia_check(pi, tty->device, "viotty_close"))
+		return;
+
+	spin_lock_irqsave(&consolelock, flags);
+	if (tty->count == 1) {
+		pi->tty = NULL;
+	}
+
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	MOD_DEC_USE_COUNT;
+}
+
+/* TTY Write method
+ */
+static int viotty_write(struct tty_struct *tty, int from_user,
+			const unsigned char *buf, int count)
+{
+	return internal_write(tty, buf, count, from_user);
+}
+
+/* TTY put_char method
+ */
+static void viotty_put_char(struct tty_struct *tty, unsigned char ch)
+{
+	internal_write(tty, &ch, 1, 0);
+}
+
+/* TTY flush_chars method
+ */
+static void viotty_flush_chars(struct tty_struct *tty)
+{
+}
+
+/* TTY write_room method
+ */
+static int viotty_write_room(struct tty_struct *tty)
+{
+	int i;
+	int room = 0;
+	struct port_info_tag *pi =
+	    (struct port_info_tag *) tty->driver_data;
+
+	if (!pi
+	    || viotty_paranoia_check(pi, tty->device,
+				     "viotty_sendbuffers"))
+		return 0;
+
+	// If no buffers are used, return the max size
+	if (overflow[pi->port].bufferUsed == 0)
+		return VIOCHAR_MAX_DATA * NUM_BUF;
+
+	for (i = 0; ((i < NUM_BUF) && (room < VIOCHAR_MAX_DATA)); i++) {
+		room +=
+		    (OVERFLOW_SIZE - overflow[pi->port].bufferBytes[i]);
+	}
+
+	if (room > VIOCHAR_MAX_DATA)
+		return VIOCHAR_MAX_DATA;
+	else
+		return room;
+}
+
+/* TTY chars_in_buffer_room method
+ */
+static int viotty_chars_in_buffer(struct tty_struct *tty)
+{
+	return 0;
+}
+
+static void viotty_flush_buffer(struct tty_struct *tty)
+{
+}
+
+static int viotty_ioctl(struct tty_struct *tty, struct file *file,
+			unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+		/* the ioctls below read/set the flags usually shown in the leds */
+		/* don't use them - they will go away without warning */
+	case KDGETLED:
+	case KDGKBLED:
+		return put_user(0, (char *) arg);
+
+	case KDSKBLED:
+		return 0;
+	}
+
+	return n_tty_ioctl(tty, file, cmd, arg);
+}
+
+static void viotty_throttle(struct tty_struct *tty)
+{
+}
+
+static void viotty_unthrottle(struct tty_struct *tty)
+{
+}
+
+static void viotty_set_termios(struct tty_struct *tty,
+			       struct termios *old_termios)
+{
+}
+
+static void viotty_stop(struct tty_struct *tty)
+{
+}
+
+static void viotty_start(struct tty_struct *tty)
+{
+}
+
+static void viotty_hangup(struct tty_struct *tty)
+{
+}
+
+static void viotty_break(struct tty_struct *tty, int break_state)
+{
+}
+
+static void viotty_send_xchar(struct tty_struct *tty, char ch)
+{
+}
+
+static void viotty_wait_until_sent(struct tty_struct *tty, int timeout)
+{
+}
+
+/* Handle an open charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleOpenEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	u8 eventRc;
+	u16 eventSubtypeRc;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	u8 port = cevent->virtualDevice;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack) {
+		if (port >= VTTY_PORTS)
+			return;
+
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			sndMsgSeq[port] = sndMsgAck[port] = 0;
+		}
+
+		port_info[port].lp = event->xTargetLp;
+
+		spin_unlock_irqrestore(&consolelock, flags);
+
+		if (event->xCorrelationToken != 0) {
+			unsigned long semptr = event->xCorrelationToken;
+			up((struct semaphore *) semptr);
+		} else
+			printk(KERN_WARNING_VIO
+			       "console: wierd...got open ack without semaphore\n");
+	} else {
+		/* This had better require an ack, otherwise complain
+		 */
+		if (event->xFlags.xAckInd != HvLpEvent_AckInd_DoAck) {
+			printk(KERN_WARNING_VIO
+			       "console: viocharopen without ack bit!\n");
+			return;
+		}
+
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		/* Make sure this is a good virtual tty */
+		if (port >= VTTY_PORTS) {
+			eventRc = HvLpEvent_Rc_SubtypeError;
+			eventSubtypeRc = viorc_openRejected;
+		}
+
+		/* If this is tty is already connected to a different
+		   partition, fail */
+		else if ((port_info[port].lp != HvLpIndexInvalid) &&
+			 (port_info[port].lp != event->xSourceLp)) {
+			eventRc = HvLpEvent_Rc_SubtypeError;
+			eventSubtypeRc = viorc_openRejected;
+		} else {
+			port_info[port].lp = event->xSourceLp;
+			eventRc = HvLpEvent_Rc_Good;
+			eventSubtypeRc = viorc_good;
+			sndMsgSeq[port] = sndMsgAck[port] = 0;
+		}
+
+		spin_unlock_irqrestore(&consolelock, flags);
+
+		/* Return the acknowledgement */
+		HvCallEvent_ackLpEvent(event);
+	}
+}
+
+/* Handle a close open charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleCloseEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	u8 port = cevent->virtualDevice;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		if (port >= VTTY_PORTS)
+			return;
+
+		/* For closes, just mark the console partition invalid */
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		if (port_info[port].lp == event->xSourceLp)
+			port_info[port].lp = HvLpIndexInvalid;
+
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_INFO_VIO
+		       "console close from %d\n", event->xSourceLp);
+	} else {
+		printk(KERN_WARNING_VIO
+		       "console got unexpected close acknowlegement\n");
+	}
+}
+
+/* Handle a config charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleConfig(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	int len;
+
+	len = cevent->immediateDataLen;
+	HvCall_writeLogBuffer(cevent->immediateData,
+			      cevent->immediateDataLen);
+
+	if (cevent->immediateData[0] == 0x01) {
+		printk(KERN_INFO_VIO
+		       "console window resized to %d: %d: %d: %d\n",
+		       cevent->immediateData[1],
+		       cevent->immediateData[2],
+		       cevent->immediateData[3], cevent->immediateData[4]);
+	} else {
+		printk(KERN_WARNING_VIO "console unknown config event\n");
+	}
+	return;
+}
+
+/* Handle a data charLpEvent. 
+ */
+static void vioHandleData(struct HvLpEvent *event)
+{
+	struct tty_struct *tty;
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	struct port_info_tag *pi;
+	int len;
+	u8 port = cevent->virtualDevice;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "console data on invalid virtual device %d\n",
+		       port);
+		return;
+	}
+
+	tty = port_info[port].tty;
+
+	if (tty == NULL) {
+		printk(KERN_WARNING_VIO
+		       "no tty for virtual device %d\n", port);
+		return;
+	}
+
+	if (tty->magic != TTY_MAGIC) {
+		printk(KERN_WARNING_VIO "tty bad magic\n");
+		return;
+	}
+
+	/*
+	 * Just to be paranoid, make sure the tty points back to this port
+	 */
+	pi = (struct port_info_tag *) tty->driver_data;
+
+	if (!pi || viotty_paranoia_check(pi, tty->device, "vioHandleData"))
+		return;
+
+	len = cevent->immediateDataLen;
+
+	if (len == 0)
+		return;
+
+	/*
+	 * Log port 0 data to the hypervisor log
+	 */
+	if (port == 0)
+		HvCall_writeLogBuffer(cevent->immediateData,
+				      cevent->immediateDataLen);
+
+	/* Don't copy more bytes than there is room for in the buffer */
+	if (tty->flip.count + len > TTY_FLIPBUF_SIZE) {
+		len = TTY_FLIPBUF_SIZE - tty->flip.count;
+		printk(KERN_WARNING_VIO
+		       "console input buffer overflow!\n");
+	}
+
+	memcpy(tty->flip.char_buf_ptr, cevent->immediateData, len);
+	memset(tty->flip.flag_buf_ptr, TTY_NORMAL, len);
+
+	/* Update the kernel buffer end */
+	tty->flip.count += len;
+	tty->flip.char_buf_ptr += len;
+
+	tty->flip.flag_buf_ptr += len;
+
+	tty_flip_buffer_push(tty);
+}
+
+/* Handle an ack charLpEvent. 
+ */
+static void vioHandleAck(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *) event;
+	unsigned long flags;
+	u8 port = cevent->virtualDevice;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "viocons: data on invalid virtual device\n");
+		return;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+	sndMsgAck[port] = event->xCorrelationToken;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (overflow[port].bufferUsed)
+		sendBuffers(port, port_info[port].lp);
+}
+
+/* Handle charLpEvents and route to the appropriate routine
+ */
+static void vioHandleCharEvent(struct HvLpEvent *event)
+{
+	int charminor;
+
+	if (event == NULL) {
+		return;
+	}
+	charminor = event->xSubtype & VIOMINOR_SUBTYPE_MASK;
+	switch (charminor) {
+	case viocharopen:
+		vioHandleOpenEvent(event);
+		break;
+	case viocharclose:
+		vioHandleCloseEvent(event);
+		break;
+	case viochardata:
+		vioHandleData(event);
+		break;
+	case viocharack:
+		vioHandleAck(event);
+		break;
+	case viocharconfig:
+		vioHandleConfig(event);
+		break;
+	default:
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+/* Send an open event
+ */
+static int viocons_sendOpen(HvLpIndex remoteLp, u8 port, void *sem)
+{
+	return HvCallEvent_signalLpEventFast(remoteLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_chario
+					     | viocharopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (remoteLp),
+					     viopath_targetinst
+					     (remoteLp),
+					     (u64) (unsigned long)
+					     sem, VIOVERSION << 16,
+					     ((u64) port << 48), 0, 0, 0);
+
+}
+
+int __init viocons_init2(void)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int rc;
+
+	/*
+	 * Now open to the primary LP
+	 */
+	printk(KERN_INFO_VIO "console open path to primary\n");
+	rc = viopath_open(HvLpConfig_getPrimaryLpIndex(), viomajorsubtype_chario, viochar_window + 2);	/* +2 for fudge */
+	if (rc) {
+		printk(KERN_WARNING_VIO
+		       "console error opening to primary %d\n", rc);
+	}
+
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		vio_set_hostlp();
+	}
+
+	/*
+	 * And if the primary is not the same as the hosting LP, open to the 
+	 * hosting lp
+	 */
+	if ((viopath_hostLp != HvLpIndexInvalid) &&
+	    (viopath_hostLp != HvLpConfig_getPrimaryLpIndex())) {
+		printk(KERN_INFO_VIO
+		       "console open path to hosting (%d)\n",
+		       viopath_hostLp);
+		rc = viopath_open(viopath_hostLp, viomajorsubtype_chario, viochar_window + 2);	/* +2 for fudge */
+		if (rc) {
+			printk(KERN_WARNING_VIO
+			       "console error opening to partition %d: %d\n",
+			       viopath_hostLp, rc);
+		}
+	}
+
+	if (vio_setHandler(viomajorsubtype_chario, vioHandleCharEvent) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Error seting handler for console events!\n");
+	}
+
+	printk(KERN_INFO_VIO "console major number is %d\n", TTY_MAJOR);
+
+	/* First, try to open the console to the hosting lp.
+	 * Wait on a semaphore for the response.
+	 */
+	if ((viopath_isactive(viopath_hostLp)) &&
+	    (viocons_sendOpen(viopath_hostLp, 0, &Semaphore) == 0)) {
+		printk(KERN_INFO_VIO
+		       "opening console to hosting partition %d\n",
+		       viopath_hostLp);
+		down(&Semaphore);
+	}
+
+	/*
+	 * If we don't have an active console, try the primary
+	 */
+	if ((!viopath_isactive(port_info[0].lp)) &&
+	    (viopath_isactive(HvLpConfig_getPrimaryLpIndex())) &&
+	    (viocons_sendOpen
+	     (HvLpConfig_getPrimaryLpIndex(), 0, &Semaphore) == 0)) {
+		printk(KERN_INFO_VIO
+		       "opening console to primary partition\n");
+		down(&Semaphore);
+	}
+
+	/* Initialize the tty_driver structure */
+	memset(&viotty_driver, 0, sizeof(struct tty_driver));
+	viotty_driver.magic = TTY_DRIVER_MAGIC;
+	viotty_driver.driver_name = "vioconsole";
+#if defined(CONFIG_DEVFS_FS)
+	viotty_driver.name = "tty%d";
+#else
+	viotty_driver.name = "tty";
+#endif
+	viotty_driver.major = TTY_MAJOR;
+	viotty_driver.minor_start = 1;
+	viotty_driver.name_base = 1;
+	viotty_driver.num = VTTY_PORTS;
+	viotty_driver.type = TTY_DRIVER_TYPE_CONSOLE;
+	viotty_driver.subtype = 1;
+	viotty_driver.init_termios = tty_std_termios;
+	viotty_driver.flags =
+	    TTY_DRIVER_REAL_RAW | TTY_DRIVER_RESET_TERMIOS;
+	viotty_driver.refcount = &viotty_refcount;
+	viotty_driver.table = viotty_table;
+	viotty_driver.termios = viotty_termios;
+	viotty_driver.termios_locked = viotty_termios_locked;
+
+	viotty_driver.open = viotty_open;
+	viotty_driver.close = viotty_close;
+	viotty_driver.write = viotty_write;
+	viotty_driver.put_char = viotty_put_char;
+	viotty_driver.flush_chars = viotty_flush_chars;
+	viotty_driver.write_room = viotty_write_room;
+	viotty_driver.chars_in_buffer = viotty_chars_in_buffer;
+	viotty_driver.flush_buffer = viotty_flush_buffer;
+	viotty_driver.ioctl = viotty_ioctl;
+	viotty_driver.throttle = viotty_throttle;
+	viotty_driver.unthrottle = viotty_unthrottle;
+	viotty_driver.set_termios = viotty_set_termios;
+	viotty_driver.stop = viotty_stop;
+	viotty_driver.start = viotty_start;
+	viotty_driver.hangup = viotty_hangup;
+	viotty_driver.break_ctl = viotty_break;
+	viotty_driver.send_xchar = viotty_send_xchar;
+	viotty_driver.wait_until_sent = viotty_wait_until_sent;
+
+	viottyS_driver = viotty_driver;
+#if defined(CONFIG_DEVFS_FS)
+	viottyS_driver.name = "ttyS%d";
+#else
+	viottyS_driver.name = "ttyS";
+#endif
+	viottyS_driver.major = TTY_MAJOR;
+	viottyS_driver.minor_start = VIOTTY_SERIAL_START;
+	viottyS_driver.type = TTY_DRIVER_TYPE_SERIAL;
+	viottyS_driver.table = viottyS_table;
+	viottyS_driver.termios = viottyS_termios;
+	viottyS_driver.termios_locked = viottyS_termios_locked;
+
+	if (tty_register_driver(&viotty_driver)) {
+		printk(KERN_WARNING_VIO
+		       "Couldn't register console driver\n");
+	}
+
+	if (tty_register_driver(&viottyS_driver)) {
+		printk(KERN_WARNING_VIO
+		       "Couldn't register console S driver\n");
+	}
+	/* Now create the vcs and vcsa devfs entries so mingetty works */
+#if defined(CONFIG_DEVFS_FS)
+	{
+		struct tty_driver temp_driver = viotty_driver;
+		int i;
+
+		temp_driver.name = "vcs%d";
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+
+		temp_driver.name = "vcsa%d";
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+
+		// For compatibility with some earlier code only!
+		// This will go away!!!
+		temp_driver.name = "viocons/%d";
+		temp_driver.name_base = 0;
+		for (i = 0; i < VTTY_PORTS; i++)
+			tty_register_devfs(&temp_driver,
+					   0, i + temp_driver.minor_start);
+	}
+#endif
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viocons_proc_init);
+
+	return 0;
+}
+
+void __init viocons_init(void)
+{
+	int i;
+	printk(KERN_INFO_VIO "registering console\n");
+
+	memset(&port_info, 0x00, sizeof(port_info));
+	for (i = 0; i < VTTY_PORTS; i++) {
+		sndMsgSeq[i] = sndMsgAck[i] = 0;
+		port_info[i].port = i;
+		port_info[i].lp = HvLpIndexInvalid;
+		port_info[i].magic = VIOTTY_MAGIC;
+	}
+
+	register_console(&viocons);
+	memset(overflow, 0x00, sizeof(overflow));
+	debug = 0;
+
+	HvCall_setLogBufferFormatAndCodepage(HvCall_LogBuffer_ASCII, 437);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/viodasd.c linuxppc64_2_4/drivers/iseries/viodasd.c
--- linux-2.4.19/drivers/iseries/viodasd.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/viodasd.c	Mon Aug 19 08:42:36 2002
@@ -0,0 +1,1654 @@
+/* -*- linux-c -*-
+ * viodasd.c
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ * 
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA 
+ ***************************************************************************
+ * This routine provides access to disk space (termed "DASD" in historical
+ * IBM terms) owned and managed by an OS/400 partition running on the
+ * same box as this Linux partition.
+ *
+ * All disk operations are performed by sending messages back and forth to 
+ * the OS/400 partition. 
+ * 
+ * This device driver can either use its own major number, or it can
+ * pretend to be an IDE drive (grep 'IDE[0-9]_MAJOR' ../../include/linux/major.h).
+ * This is controlled with a CONFIG option.  You can either call this an
+ * elegant solution to the fact that a lot of software doesn't recognize
+ * a new disk major number...or you can call this a really ugly hack.
+ * Your choice.
+ */
+
+#include <linux/major.h>
+#include <linux/config.h>
+
+#include <linux/fs.h>
+#include <linux/blkpg.h>
+
+/* Changelog:
+	2001-11-27	devilbis	Added first pass at complete IDE emulation
+	2002-07-07      boutcher        Added randomness
+ */
+
+/* Decide if we are using our own major or pretending to be an IDE drive
+ *
+ * If we are using our own major, we only support 7 partitions per physical
+ * disk....so with minor numbers 0-255 we get a maximum of 32 disks.  If we
+ * are emulating IDE, we get 63 partitions per disk, with a maximum of 4
+ * disks per major, but common practice is to place only 2 devices in /dev
+ * for each IDE major, for a total of 20 (since there are 10 IDE majors).
+ */
+
+#ifdef CONFIG_VIODASD_IDE
+static const int major_table[] = {
+	IDE0_MAJOR,
+	IDE1_MAJOR,
+	IDE2_MAJOR,
+	IDE3_MAJOR,
+	IDE4_MAJOR,
+	IDE5_MAJOR,
+	IDE6_MAJOR,
+	IDE7_MAJOR,
+	IDE8_MAJOR,
+	IDE9_MAJOR,
+};
+enum {
+	DEV_PER_MAJOR = 2,
+	PARTITION_SHIFT = 6,
+};
+static int major_to_index(int major)
+{
+	switch(major) {
+	case IDE0_MAJOR: return 0;
+	case IDE1_MAJOR: return 1;
+	case IDE2_MAJOR: return 2;
+	case IDE3_MAJOR: return 3;
+	case IDE4_MAJOR: return 4;
+	case IDE5_MAJOR: return 5;
+	case IDE6_MAJOR: return 6;
+	case IDE7_MAJOR: return 7;
+	case IDE8_MAJOR: return 8;
+	case IDE9_MAJOR: return 9;
+	default:
+		return -1;
+	}
+}
+#define do_viodasd_request do_hd_request
+#define VIOD_DEVICE_NAME "ide"
+#define VIOD_GENHD_NAME "hd"
+#else				/* !CONFIG_VIODASD_IDE */
+static const int major_table[] = {
+	VIODASD_MAJOR,
+};
+enum {
+	DEV_PER_MAJOR = 32,
+	PARTITION_SHIFT = 3,
+};
+static int major_to_index(int major)
+{
+	if(major != VIODASD_MAJOR)
+		return -1;
+	return 0;
+}
+#define VIOD_DEVICE_NAME "viod"
+#ifdef CONFIG_DEVFS_FS
+#define VIOD_GENHD_NAME "viod"
+#else
+#define VIOD_GENHD_NAME "iseries/vd"
+#endif
+#endif				/* CONFIG_VIODASD_IDE */
+
+#define DEVICE_NR(dev) (devt_to_diskno(dev))
+#define LOCAL_END_REQUEST
+
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/blk.h>
+#include <linux/genhd.h>
+#include <linux/hdreg.h>
+#include <linux/fd.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/pci.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include "vio.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+MODULE_DESCRIPTION("iSeries Virtual DASD");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+#define VIODASD_VERS "1.50"
+
+enum {
+	NUM_MAJORS = sizeof(major_table) / sizeof(major_table[0]),
+	MAX_DISKNO = DEV_PER_MAJOR * NUM_MAJORS,
+	MAX_MAJOR_NAME = 16 + 1, /* maximum length of a gendisk->name */
+};
+
+static volatile int viodasd_max_disk = MAX_DISKNO - 1;
+
+static int diskno_to_major(int diskno)
+{
+	if (diskno >= MAX_DISKNO)
+		return -1;
+	return major_table[diskno / DEV_PER_MAJOR];
+}
+static int devt_to_diskno(kdev_t dev)
+{
+	return major_to_index(MAJOR(dev)) * DEV_PER_MAJOR +
+	    (MINOR(dev) >> PARTITION_SHIFT);
+}
+static int diskno_to_devt(int diskno, int partition)
+{
+	return MKDEV(diskno_to_major(diskno),
+		     ((diskno % DEV_PER_MAJOR) << PARTITION_SHIFT) +
+		     partition);
+}
+
+#define VIOMAXREQ 16
+#define VIOMAXBLOCKDMA        12
+
+extern struct pci_dev *iSeries_vio_dev;
+
+struct openData {
+	u64 mDiskLen;
+	u16 mMaxDisks;
+	u16 mCylinders;
+	u16 mTracks;
+	u16 mSectors;
+	u16 mBytesPerSector;
+};
+
+struct rwData {			// Used during rw
+	u64 mOffset;
+	struct {
+		u32 mToken;
+		u32 reserved;
+		u64 mLen;
+	} dmaInfo[VIOMAXBLOCKDMA];
+};
+
+struct vioblocklpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	union {
+		struct openData openData;
+		struct rwData rwData;
+		struct {
+			u64 changed;
+		} check;
+	} u;
+};
+
+#define vioblockflags_ro   0x0001
+
+enum vioblocksubtype {
+	vioblockopen = 0x0001,
+	vioblockclose = 0x0002,
+	vioblockread = 0x0003,
+	vioblockwrite = 0x0004,
+	vioblockflush = 0x0005,
+	vioblockcheck = 0x0007
+};
+
+/* In a perfect world we will perform better if we get page-aligned I/O
+ * requests, in multiples of pages.  At least peg our block size to the
+ * actual page size.
+ */
+static int blksize = HVPAGESIZE;	/* in bytes */
+
+static DECLARE_WAIT_QUEUE_HEAD(viodasd_wait);
+struct viodasd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	union {
+		int changed;	/* Used only for check_change */
+		u16 subRC;
+	} data;
+};
+
+static const struct vio_error_entry viodasd_err_table[] = {
+	{0x0201, EINVAL, "Invalid Range"},
+	{0x0202, EINVAL, "Invalid Token"},
+	{0x0203, EIO, "DMA Error"},
+	{0x0204, EIO, "Use Error"},
+	{0x0205, EIO, "Release Error"},
+	{0x0206, EINVAL, "Invalid Disk"},
+	{0x0207, EBUSY, "Cant Lock"},
+	{0x0208, EIO, "Already Locked"},
+	{0x0209, EIO, "Already Unlocked"},
+	{0x020A, EIO, "Invalid Arg"},
+	{0x020B, EIO, "Bad IFS File"},
+	{0x020C, EROFS, "Read Only Device"},
+	{0x02FF, EIO, "Internal Error"},
+	{0x0000, 0, NULL},
+};
+
+/* Our gendisk table
+ */
+static struct gendisk viodasd_gendisk[NUM_MAJORS];
+
+static struct gendisk *major_to_gendisk(int major)
+{
+	int index = major_to_index(major);
+	return index < 0 ? NULL : &viodasd_gendisk[index];
+}
+static struct hd_struct *devt_to_partition(kdev_t dev)
+{
+	return &major_to_gendisk(MAJOR(dev))->part[MINOR(dev)];
+}
+
+/* Figure out the biggest I/O request (in sectors) we can accept
+ */
+#define VIODASD_MAXSECTORS (4096 / 512 * VIOMAXBLOCKDMA)
+
+/* Keep some statistics on what's happening for the PROC file system
+ */
+static struct {
+	long tot;
+	long nobh;
+	long ntce[VIOMAXBLOCKDMA];
+} viod_stats[MAX_DISKNO][2];
+
+/* Number of disk I/O requests we've sent to OS/400
+ */
+static int num_req_outstanding;
+
+/* This is our internal structure for keeping track of disk devices
+ */
+struct viodasd_device {
+	int useCount;
+	u16 cylinders;
+	u16 tracks;
+	u16 sectors;
+	u16 bytesPerSector;
+	u64 size;
+	int readOnly;
+} *viodasd_devices;
+
+/* When we get a disk I/O request we take it off the general request queue
+ * and put it here.
+ */
+static LIST_HEAD(reqlist);
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+	int j;
+
+#if defined(MODULE)
+	len +=
+	    sprintf(buf + len,
+		    "viod Module opened %d times.  Major number %d\n",
+		    MOD_IN_USE, major_table[0]);
+#endif
+	len +=
+	    sprintf(buf + len, "viod %d possible devices\n", MAX_DISKNO);
+
+	for (i = 0; i < 16; i++) {
+		if (viod_stats[i][0].tot || viod_stats[i][1].tot) {
+			len +=
+			    sprintf(buf + len,
+				    "DISK %2.2d: rd %-10.10ld wr %-10.10ld (no buffer list rd %-10.10ld wr %-10.10ld\n",
+				    i, viod_stats[i][0].tot,
+				    viod_stats[i][1].tot,
+				    viod_stats[i][0].nobh,
+				    viod_stats[i][1].nobh);
+
+			len += sprintf(buf + len, "rd DMA: ");
+
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				len += sprintf(buf + len, " [%2.2d] %ld",
+					       j,
+					       viod_stats[i][0].ntce[j]);
+
+			len += sprintf(buf + len, "\nwr DMA: ");
+
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				len += sprintf(buf + len, " [%2.2d] %ld",
+					       j,
+					       viod_stats[i][1].ntce[j]);
+			len += sprintf(buf + len, "\n");
+		}
+	}
+
+	*eof = 1;
+	return len;
+}
+
+/* Handle writes to our proc file system
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	return count;
+}
+
+/* setup our proc file system entries
+ */
+void viodasd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viodasd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/* clean up our proc file system entries
+ */
+void viodasd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viodasd", iSeries_proc);
+}
+
+/* End a request
+ */
+static void viodasd_end_request(struct request *req, int uptodate)
+{
+	if (end_that_request_first(req, uptodate, VIOD_DEVICE_NAME))
+		return;
+
+	add_blkdev_randomness(MAJOR(req->rq_dev));
+
+	end_that_request_last(req);
+}
+
+/* This rebuilds the partition information for a single disk device
+ */
+static int viodasd_revalidate(kdev_t dev)
+{
+	int i;
+	int device_no = DEVICE_NR(dev);
+	int dev_within_major = device_no % DEV_PER_MAJOR;
+	int part0 = (dev_within_major << PARTITION_SHIFT);
+	int npart = (1 << PARTITION_SHIFT);
+	int major = MAJOR(dev);
+	struct gendisk *gendisk = major_to_gendisk(major);
+
+	if (viodasd_devices[device_no].size == 0)
+		return 0;
+
+	for (i = npart - 1; i >= 0; i--) {
+		int minor = part0 + i;
+		struct hd_struct *partition = &gendisk->part[minor];
+
+		if (partition->nr_sects != 0) {
+			kdev_t devp = MKDEV(major, minor);
+			struct super_block *sb;
+			fsync_dev(devp);
+
+			sb = get_super(devp);
+			if (sb)
+				invalidate_inodes(sb);
+
+			invalidate_buffers(devp);
+		}
+
+		partition->start_sect = 0;
+		partition->nr_sects = 0;
+	}
+
+	grok_partitions(gendisk, dev_within_major, npart,
+			viodasd_devices[device_no].size >> 9);
+
+	return 0;
+}
+
+
+static u16 access_flags(mode_t mode)
+{
+	u16 flags = 0;
+	if (!(mode & FMODE_WRITE))
+		flags |= vioblockflags_ro;
+	return flags;
+}
+
+static void internal_register_disk(int diskno);
+
+/* This is the actual open code.  It gets called from the external
+ * open entry point, as well as from the init code when we're figuring
+ * out what disks we have
+ */
+static int internal_open(int device_no, u16 flags)
+{
+	int i;
+	const int dev_within_major = device_no % DEV_PER_MAJOR;
+	struct gendisk *gendisk =
+	    major_to_gendisk(diskno_to_major(device_no));
+	HvLpEvent_Rc hvrc;
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viodasd_waitevent we = { sem:&Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	/* Send the open event to OS/400                                         */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_blockio |
+					     vioblockopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) &we,
+					     VIOVERSION << 16,
+					     ((u64) device_no << 48) |
+					     ((u64) flags << 32), 0, 0, 0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code                                                 */
+	if (we.rc != 0) {
+		const struct vio_error_entry *err =
+		    vio_lookup_rc(viodasd_err_table, we.data.subRC);
+		printk(KERN_WARNING_VIO
+		       "bad rc opening disk: %d:0x%04x (%s)\n",
+		       (int) we.rc, we.data.subRC, err->msg);
+		return -err->errno;
+	}
+	
+	/* If this is the first open of this device, update the device information */
+	/* If this is NOT the first open, assume that it isn't changing            */
+	if (viodasd_devices[device_no].useCount == 0) {
+		if (viodasd_devices[device_no].size > 0) {
+			/* divide by 512 */
+			u64 tmpint = viodasd_devices[device_no].size >> 9;
+			gendisk->part[dev_within_major << PARTITION_SHIFT].nr_sects = tmpint;
+			/* Now the value divided by 1024 */
+			tmpint = tmpint >> 1;
+			gendisk->sizes[dev_within_major << PARTITION_SHIFT] = tmpint;
+
+			for (i = dev_within_major << PARTITION_SHIFT;
+			     i < ((dev_within_major + 1) << PARTITION_SHIFT);
+			     i++)
+			{
+				hardsect_size[diskno_to_major(device_no)][i] =
+				    viodasd_devices[device_no].bytesPerSector;
+			}
+		}
+	} else {
+		/* If the size of the device changed, weird things are happening!     */
+		if (gendisk->sizes[dev_within_major << PARTITION_SHIFT] !=
+		    viodasd_devices[device_no].size >> 10) {
+			printk(KERN_WARNING_VIO
+			       "disk size change (%dK to %dK) for device %d\n",
+			       gendisk->sizes[dev_within_major << PARTITION_SHIFT],
+			       (int) viodasd_devices[device_no].size >> 10, device_no);
+		}
+	}
+
+	internal_register_disk(device_no);
+
+	/* Bump the use count                                                      */
+	viodasd_devices[device_no].useCount++;
+	return 0;
+}
+
+/* This is the actual release code.  It gets called from the external
+ * release entry point, as well as from the init code when we're figuring
+ * out what disks we have
+ */
+static int internal_release(int device_no, u16 flags)
+{
+	/* Send the event to OS/400.  We DON'T expect a response                 */
+	HvLpEvent_Rc hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+							  HvLpEvent_Type_VirtualIo,
+							  viomajorsubtype_blockio
+							  | vioblockclose,
+							  HvLpEvent_AckInd_NoAck,
+							  HvLpEvent_AckType_ImmediateAck,
+							  viopath_sourceinst
+							  (viopath_hostLp),
+							  viopath_targetinst
+							  (viopath_hostLp),
+							  0,
+							  VIOVERSION << 16,
+							  ((u64) device_no
+							   << 48) | ((u64)
+								     flags
+								     <<
+								     32),
+							  0, 0, 0);
+
+	viodasd_devices[device_no].useCount--;
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO
+		       "bad rc sending event to OS/400 %d\n", (int) hvrc);
+		return -EIO;
+	}
+	return 0;
+}
+
+
+/* External open entry point.
+ */
+static int viodasd_open(struct inode *ino, struct file *fil)
+{
+	int device_no;
+	int old_max_disk = viodasd_max_disk;
+
+	/* Do a bunch of sanity checks                                           */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in open\n");
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on open\n");
+		return -ENODEV;
+	}
+
+	device_no = DEVICE_NR(ino->i_rdev);
+	if (device_no > MAX_DISKNO || device_no < 0) {
+		printk(KERN_WARNING_VIO
+		       "Invalid device number %d in open\n", device_no);
+		return -ENODEV;
+	}
+
+	/* Call the actual open code                                             */
+	if (internal_open(device_no, access_flags(fil ? fil->f_mode : 0)) == 0) {
+		int i;
+		MOD_INC_USE_COUNT;
+		/* For each new disk: */
+		/* update the disk's geometry via internal_open and register it */
+		for (i = old_max_disk + 1; i <= viodasd_max_disk; ++i) {
+			internal_open(i, vioblockflags_ro);
+			internal_release(i, vioblockflags_ro);
+		}
+		return 0;
+	} else {
+		return -EIO;
+	}
+}
+
+/* External release entry point.
+ */
+static int viodasd_release(struct inode *ino, struct file *fil)
+{
+	int device_no;
+
+	/* Do a bunch of sanity checks                                           */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in release\n");
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on release\n");
+		return -ENODEV;
+	}
+
+	device_no = DEVICE_NR(ino->i_rdev);
+
+	if (device_no > MAX_DISKNO || device_no < 0) {
+		printk("Tried to release invalid disk number %d\n",
+		       device_no);
+		return -ENODEV;
+	}
+
+	/* Call the actual release code                                          */
+	internal_release(device_no, access_flags(fil ? fil->f_mode : 0));
+
+	MOD_DEC_USE_COUNT;
+	return 0;
+}
+
+/* External ioctl entry point.
+ */
+static int viodasd_ioctl(struct inode *ino, struct file *fil,
+			 unsigned int cmd, unsigned long arg)
+{
+	int device_no;
+	int err;
+	HvLpEvent_Rc hvrc;
+	struct hd_struct *partition;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Sanity checks                                                        */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in ioctl\n");
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on ioctl\n");
+		return -ENODEV;
+	}
+
+	partition = devt_to_partition(ino->i_rdev);
+
+	device_no = DEVICE_NR(ino->i_rdev);
+	if (device_no > viodasd_max_disk) {
+		printk(KERN_WARNING_VIO
+		       "Invalid device number %d in ioctl\n", device_no);
+		return -ENODEV;
+	}
+
+	switch (cmd) {
+	case BLKPG:
+		return blk_ioctl(ino->i_rdev, cmd, arg);
+	case BLKGETSIZE:
+		/* return the device size in sectors */
+		if (!arg)
+			return -EINVAL;
+		err =
+		    verify_area(VERIFY_WRITE, (long *) arg, sizeof(long));
+		if (err)
+			return err;
+
+		put_user(partition->nr_sects, (long *) arg);
+		return 0;
+
+	case FDFLUSH:
+	case BLKFLSBUF:
+		if (!suser())
+			return -EACCES;
+		fsync_dev(ino->i_rdev);
+		invalidate_buffers(ino->i_rdev);
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_blockio
+						     | vioblockflush,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     &Semaphore,
+						     VIOVERSION << 16,
+						     ((u64) device_no <<
+						      48), 0, 0, 0);
+
+
+		if (hvrc != 0) {
+			printk(KERN_WARNING_VIO
+			       "bad rc on sync signalLpEvent %d\n",
+			       (int) hvrc);
+			return -EIO;
+		}
+
+		down(&Semaphore);
+
+		return 0;
+
+	case BLKRAGET:
+		if (!arg)
+			return -EINVAL;
+		err =
+		    verify_area(VERIFY_WRITE, (long *) arg, sizeof(long));
+		if (err)
+			return err;
+		put_user(read_ahead[MAJOR(ino->i_rdev)], (long *) arg);
+		return 0;
+
+	case BLKRASET:
+		if (!suser())
+			return -EACCES;
+		if (arg > 0x00ff)
+			return -EINVAL;
+		read_ahead[MAJOR(ino->i_rdev)] = arg;
+		return 0;
+
+	case BLKRRPART:
+		viodasd_revalidate(ino->i_rdev);
+		return 0;
+
+	case HDIO_GETGEO:
+		{
+			unsigned char sectors;
+			unsigned char heads;
+			unsigned short cylinders;
+
+			struct hd_geometry *geo =
+			    (struct hd_geometry *) arg;
+			if (geo == NULL)
+				return -EINVAL;
+
+			err = verify_area(VERIFY_WRITE, geo, sizeof(*geo));
+			if (err)
+				return err;
+
+			sectors = viodasd_devices[device_no].sectors;
+			if (sectors == 0)
+				sectors = 32;
+
+			heads = viodasd_devices[device_no].tracks;
+			if (heads == 0)
+				heads = 64;
+
+			cylinders = viodasd_devices[device_no].cylinders;
+			if (cylinders == 0)
+				cylinders =
+				    partition->nr_sects / (sectors *
+							   heads);
+
+			put_user(sectors, &geo->sectors);
+			put_user(heads, &geo->heads);
+			put_user(cylinders, &geo->cylinders);
+
+			put_user(partition->start_sect,
+				 (long *) &geo->start);
+
+			return 0;
+		}
+
+#define PRTIOC(x) case x: printk(KERN_WARNING_VIO "got unsupported FD ioctl " #x "\n"); \
+                          return -EINVAL;
+
+		PRTIOC(FDCLRPRM);
+		PRTIOC(FDSETPRM);
+		PRTIOC(FDDEFPRM);
+		PRTIOC(FDGETPRM);
+		PRTIOC(FDMSGON);
+		PRTIOC(FDMSGOFF);
+		PRTIOC(FDFMTBEG);
+		PRTIOC(FDFMTTRK);
+		PRTIOC(FDFMTEND);
+		PRTIOC(FDSETEMSGTRESH);
+		PRTIOC(FDSETMAXERRS);
+		PRTIOC(FDGETMAXERRS);
+		PRTIOC(FDGETDRVTYP);
+		PRTIOC(FDSETDRVPRM);
+		PRTIOC(FDGETDRVPRM);
+		PRTIOC(FDGETDRVSTAT);
+		PRTIOC(FDPOLLDRVSTAT);
+		PRTIOC(FDRESET);
+		PRTIOC(FDGETFDCSTAT);
+		PRTIOC(FDWERRORCLR);
+		PRTIOC(FDWERRORGET);
+		PRTIOC(FDRAWCMD);
+		PRTIOC(FDEJECT);
+		PRTIOC(FDTWADDLE);
+
+	}
+
+	return -EINVAL;
+}
+
+/* Send an actual I/O request to OS/400
+ */
+static int send_request(struct request *req)
+{
+	u64 sect_size;
+	u64 start;
+	u64 len;
+	int direction;
+	int nsg;
+	u16 viocmd;
+	HvLpEvent_Rc hvrc;
+	struct vioblocklpevent *bevent;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	struct buffer_head *bh;
+	int sgindex;
+	int device_no = DEVICE_NR(req->rq_dev);
+	int dev_within_major = device_no % DEV_PER_MAJOR;
+	int statindex;
+	struct hd_struct *partition = devt_to_partition(req->rq_dev);
+
+	if (device_no > viodasd_max_disk || device_no < 0) {
+		printk
+		    ("yikes! sending a request to device %d of %d possible?\n",
+		     device_no, viodasd_max_disk + 1);
+	}
+	
+	/* Note that this SHOULD always be 512...but lets be architecturally correct */
+	sect_size = hardsect_size[MAJOR(req->rq_dev)][dev_within_major];
+
+	/* Figure out the starting sector and length                                 */
+	start = (req->sector + partition->start_sect) * sect_size;
+	len = req->nr_sectors * sect_size;
+
+	/* More paranoia checks                                                      */
+	if ((req->sector + req->nr_sectors) >
+	    (partition->start_sect + partition->nr_sects)) {
+		printk(KERN_WARNING_VIO
+		       "Invalid request offset & length\n");
+		printk(KERN_WARNING_VIO
+		       "req->sector: %ld, req->nr_sectors: %ld\n",
+		       req->sector, req->nr_sectors);
+		printk(KERN_WARNING_VIO "major: %d, minor: %d\n",
+		       MAJOR(req->rq_dev), MINOR(req->rq_dev));
+		return -1;
+	}
+
+	if (req->cmd == READ) {
+		direction = PCI_DMA_FROMDEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockread;
+		statindex = 0;
+	} else {
+		direction = PCI_DMA_TODEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockwrite;
+		statindex = 1;
+	}
+
+	/* Update totals */
+	viod_stats[device_no][statindex].tot++;
+
+	/* Now build the scatter-gather list                                        */
+	memset(&sg, 0x00, sizeof(sg));
+	sgindex = 0;
+
+	/* See if this is a swap I/O (without a bh pointer) or a regular I/O        */
+	if (req->bh) {
+		/* OK...this loop takes buffers from the request and adds them to the SG
+		   until we're done, or until we hit a maximum.  If we hit a maximum we'll
+		   just finish this request later                                       */
+		bh = req->bh;
+		while ((bh) && (sgindex < VIOMAXBLOCKDMA)) {
+			sg[sgindex].address = bh->b_data;
+			sg[sgindex].length = bh->b_size;
+
+			sgindex++;
+			bh = bh->b_reqnext;
+		}
+		nsg = pci_map_sg(iSeries_vio_dev, sg, sgindex, direction);
+		if ((nsg == 0) || (sg[0].dma_length == 0)
+		    || (sg[0].dma_address == 0xFFFFFFFF)) {
+			printk(KERN_WARNING_VIO "error getting sg tces\n");
+			return -1;
+		}
+
+	} else {
+		/* Update stats */
+		viod_stats[device_no][statindex].nobh++;
+
+		sg[0].dma_address =
+		    pci_map_single(iSeries_vio_dev, req->buffer, len,
+				   direction);
+		if (sg[0].dma_address == 0xFFFFFFFF) {
+			printk(KERN_WARNING_VIO
+			       "error allocating tce for address %p len %ld\n",
+			       req->buffer, (long) len);
+			return -1;
+		}
+		sg[0].dma_length = len;
+		nsg = 1;
+	}
+
+	/* Update stats */
+	viod_stats[device_no][statindex].ntce[sgindex]++;
+
+	/* This optimization handles a single DMA block                          */
+	if (sgindex == 1) {
+		/* Send the open event to OS/400                                         */
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_blockio
+						     | viocmd,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     req->buffer,
+						     VIOVERSION << 16,
+						     ((u64) device_no <<
+						      48), start,
+						     ((u64) sg[0].
+						      dma_address) << 32,
+						     sg[0].dma_length);
+	} else {
+		bevent =
+		    (struct vioblocklpevent *)
+		    vio_get_event_buffer(viomajorsubtype_blockio);
+		if (bevent == NULL) {
+			printk(KERN_WARNING_VIO
+			       "error allocating disk event buffer\n");
+			return -1;
+		}
+
+		/* Now build up the actual request.  Note that we store the pointer      */
+		/* to the request buffer in the correlation token so we can match        */
+		/* this response up later                                                */
+		memset(bevent, 0x00, sizeof(struct vioblocklpevent));
+		bevent->event.xFlags.xValid = 1;
+		bevent->event.xFlags.xFunction = HvLpEvent_Function_Int;
+		bevent->event.xFlags.xAckInd = HvLpEvent_AckInd_DoAck;
+		bevent->event.xFlags.xAckType =
+		    HvLpEvent_AckType_ImmediateAck;
+		bevent->event.xType = HvLpEvent_Type_VirtualIo;
+		bevent->event.xSubtype = viocmd;
+		bevent->event.xSourceLp = HvLpConfig_getLpIndex();
+		bevent->event.xTargetLp = viopath_hostLp;
+		bevent->event.xSizeMinus1 =
+		    offsetof(struct vioblocklpevent,
+			     u.rwData.dmaInfo) +
+		    (sizeof(bevent->u.rwData.dmaInfo[0]) * (sgindex)) - 1;
+		bevent->event.xSizeMinus1 =
+		    sizeof(struct vioblocklpevent) - 1;
+		bevent->event.xSourceInstanceId =
+		    viopath_sourceinst(viopath_hostLp);
+		bevent->event.xTargetInstanceId =
+		    viopath_targetinst(viopath_hostLp);
+		bevent->event.xCorrelationToken =
+		    (u64) (unsigned long) req->buffer;
+		bevent->mVersion = VIOVERSION;
+		bevent->mDisk = device_no;
+		bevent->u.rwData.mOffset = start;
+
+		/* Copy just the dma information from the sg list into the request */
+		for (sgindex = 0; sgindex < nsg; sgindex++) {
+			bevent->u.rwData.dmaInfo[sgindex].mToken =
+			    sg[sgindex].dma_address;
+			bevent->u.rwData.dmaInfo[sgindex].mLen =
+			    sg[sgindex].dma_length;
+		}
+
+		/* Send the request                                               */
+		hvrc = HvCallEvent_signalLpEvent(&bevent->event);
+		vio_free_event_buffer(viomajorsubtype_blockio, bevent);
+	}
+
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO
+		       "error sending disk event to OS/400 (rc %d)\n",
+		       (int) hvrc);
+		return -1;
+	} else {
+		/* If the request was successful, bump the number of outstanding */
+		num_req_outstanding++;
+	}
+	return 0;
+}
+
+/* This is the external request processing routine
+ */
+static void do_viodasd_request(request_queue_t * q)
+{
+	int device_no;
+	for (;;) {
+		struct request *req;
+		struct gendisk *gendisk;
+
+		/* inlined INIT_REQUEST here because we don't define MAJOR_NR before blk.h */
+		if (list_empty(&q->queue_head))
+			return;
+		req = blkdev_entry_next_request(&q->queue_head);
+		if (major_to_index(MAJOR(req->rq_dev)) < 0)
+			panic(VIOD_DEVICE_NAME ": request list destroyed");
+		if (req->bh) {
+			if (!buffer_locked(req->bh))
+				panic(VIOD_DEVICE_NAME
+				      ": block not locked");
+		}
+
+		gendisk = major_to_gendisk(MAJOR(req->rq_dev));
+
+		device_no = DEVICE_NR(req->rq_dev);
+		if (device_no > MAX_DISKNO || device_no < 0) {
+			printk(KERN_WARNING_VIO "Invalid device # %d\n",
+			       device_no);
+			viodasd_end_request(req, 0);
+			continue;
+		}
+		
+		if (gendisk->sizes == NULL) {
+			printk(KERN_WARNING_VIO
+			       "Ouch! gendisk->sizes is NULL\n");
+			viodasd_end_request(req, 0);
+			continue;
+		}
+
+		/* If the queue is plugged, don't dequeue anything right now */
+		if ((q) && (q->plugged)) {
+			return;
+		}
+
+		/* If we already have the maximum number of requests outstanding to OS/400
+		   just bail out. We'll come back later                              */
+		if (num_req_outstanding >= VIOMAXREQ) {
+			return;
+		}
+
+		/* get the current request, then dequeue it from the queue           */
+		blkdev_dequeue_request(req);
+
+		/* Try sending the request                                           */
+		if (send_request(req) == 0) {
+			list_add_tail(&req->queue, &reqlist);
+		} else {
+			viodasd_end_request(req, 0);
+		}
+	}
+}
+
+/* Check for changed disks
+ */
+static int viodasd_check_change(kdev_t dev)
+{
+	struct viodasd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = DEVICE_NR(dev);
+
+	/* This semaphore is raised in the interrupt handler                     */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition              */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400                                         */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_blockio |
+					     vioblockcheck,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) &we,
+					     VIOVERSION << 16,
+					     ((u64) device_no << 48), 0, 0,
+					     0);
+
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response                    */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change                      */
+	if (we.rc != 0) {
+		printk(KERN_WARNING_VIO
+		       "bad rc %d on check_change. Assuming no change\n",
+		       (int) we.rc);
+		return 0;
+	}
+
+	return we.data.changed;
+}
+
+/* Our file operations table
+ */
+static struct block_device_operations viodasd_fops = {
+	open:viodasd_open,
+	release:viodasd_release,
+	ioctl:viodasd_ioctl,
+	check_media_change:viodasd_check_change,
+	revalidate:viodasd_revalidate
+};
+
+/* returns the total number of scatterlist elements converted */
+static int block_event_to_scatterlist(const struct vioblocklpevent *bevent,
+				      struct scatterlist *sg,
+				      int *total_len)
+{
+	int i, numsg;
+	const struct rwData *rwData = &bevent->u.rwData;
+	static const int offset =
+	    offsetof(struct vioblocklpevent, u.rwData.dmaInfo);
+	static const int element_size = sizeof(rwData->dmaInfo[0]);
+
+	numsg = ((bevent->event.xSizeMinus1 + 1) - offset) / element_size;
+	if (numsg > VIOMAXBLOCKDMA)
+		numsg = VIOMAXBLOCKDMA;
+
+	*total_len = 0;
+	memset(sg, 0x00, sizeof(sg[0]) * VIOMAXBLOCKDMA);
+
+	for (i = 0; (i < numsg) && (rwData->dmaInfo[i].mLen > 0); ++i) {
+		sg[i].dma_address = rwData->dmaInfo[i].mToken;
+		sg[i].dma_length = rwData->dmaInfo[i].mLen;
+		*total_len += rwData->dmaInfo[i].mLen;
+	}
+	return i;
+}
+
+static struct request *find_request_with_token(u64 token)
+{
+	struct request *req = blkdev_entry_to_request(reqlist.next);
+	while ((&req->queue != &reqlist) &&
+	       ((u64) (unsigned long) req->buffer != token))
+		req = blkdev_entry_to_request(req->queue.next);
+	if (&req->queue == &reqlist) {
+		return NULL;
+	}
+	return req;
+}
+
+/* Restart all queues, starting with the one _after_ the major given, */
+/* thus reducing the chance of starvation of disks with late majors. */
+static void viodasd_restart_all_queues_starting_from(int first_major)
+{
+	int i, first_index = major_to_index(first_major);
+	for(i = first_index + 1; i < NUM_MAJORS; ++i)
+		do_viodasd_request(BLK_DEFAULT_QUEUE(major_table[i]));
+	for(i = 0; i <= first_index; ++i)
+		do_viodasd_request(BLK_DEFAULT_QUEUE(major_table[i]));
+}
+
+/* For read and write requests, decrement the number of outstanding requests,
+ * Free the DMA buffers we allocated, and find the matching request by
+ * using the buffer pointer we stored in the correlation token.
+ */
+static int viodasd_handleReadWrite(struct vioblocklpevent *bevent)
+{
+	int num_sg, num_sect, pci_direction, total_len, major;
+	struct request *req;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	struct HvLpEvent *event = &bevent->event;
+	unsigned long irq_flags;
+
+	num_sg = block_event_to_scatterlist(bevent, sg, &total_len);
+	num_sect = total_len >> 9;
+	if (event->xSubtype == (viomajorsubtype_blockio | vioblockread))
+		pci_direction = PCI_DMA_FROMDEVICE;
+	else
+		pci_direction = PCI_DMA_TODEVICE;
+	pci_unmap_sg(iSeries_vio_dev, sg, num_sg, pci_direction);
+
+
+	/* Since this is running in interrupt mode, we need to make sure we're not
+	 * stepping on any global I/O operations
+	 */
+	spin_lock_irqsave(&io_request_lock, irq_flags);
+
+	num_req_outstanding--;
+
+	/* Now find the matching request in OUR list (remember we moved the request
+	 * from the global list to our list when we got it)
+	 */
+	req = find_request_with_token(bevent->event.xCorrelationToken);
+	if (req == NULL) {
+		printk(KERN_WARNING_VIO
+		       "Yikes! No request matching 0x%lx found\n",
+		       bevent->event.xCorrelationToken);
+		spin_unlock_irqrestore(&io_request_lock, irq_flags);
+		return -1;
+	}
+
+	/* Remove the request from our list */
+	list_del(&req->queue);
+	/* Record this event's major number so we can check that queue again */
+	major = MAJOR(req->rq_dev);
+
+	if (!req->bh) {
+		if (event->xRc != HvLpEvent_Rc_Good) {
+			const struct vio_error_entry *err =
+			    vio_lookup_rc(viodasd_err_table,
+					  bevent->mSubTypeRc);
+			printk(KERN_WARNING_VIO
+			       "read/write error %d:0x%04x (%s)\n",
+			       event->xRc, bevent->mSubTypeRc, err->msg);
+			viodasd_end_request(req, 0);
+		} else {
+			if (num_sect != req->current_nr_sectors) {
+				printk(KERN_WARNING_VIO
+				       "Yikes...non bh i/o # sect doesn't match!!!\n");
+			}
+			viodasd_end_request(req, 1);
+		}
+	} else {
+		/* record having received the answers we did */
+		while ((num_sect > 0) && (req->bh)) {
+			num_sect -= req->current_nr_sectors;
+			viodasd_end_request(req, 1);
+		}
+		/* if they somehow answered _more_ than we asked for...something weird happened */
+		if (num_sect)
+			printk(KERN_WARNING_VIO
+			       "Yikes...sectors left over on a request!!!\n");
+
+		/* if they didn't answer the whole request this time, re-submit the request */
+		if (req->bh) {
+			if (send_request(req) == 0) {
+				list_add_tail(&req->queue, &reqlist);
+			} else {
+				viodasd_end_request(req, 0);
+			}
+		}
+	}
+
+	/* Finally, try to get more requests off of this device's queue */
+	viodasd_restart_all_queues_starting_from(major);
+
+	spin_unlock_irqrestore(&io_request_lock, irq_flags);
+
+	return 0;
+}
+
+/* This routine handles incoming block LP events */
+static void vioHandleBlockEvent(struct HvLpEvent *event)
+{
+	struct vioblocklpevent *bevent = (struct vioblocklpevent *) event;
+	struct viodasd_waitevent *pwe;
+
+	if (event == NULL) {
+		/* Notification that a partition went away! */
+		return;
+	}
+	// First, we should NEVER get an int here...only acks
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "Yikes! got an int in viodasd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+
+		/* Handle a response to an open request.  We get all the disk information
+		 * in the response, so update it.  The correlation token contains a pointer to
+		 * a waitevent structure that has a semaphore in it.  update the return code
+		 * in the waitevent structure and post the semaphore to wake up the guy who
+		 * sent the request */
+	case vioblockopen:
+		pwe =
+		    (struct viodasd_waitevent *) (unsigned long) event->
+		    xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.subRC = bevent->mSubTypeRc;
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			const struct openData *data = &bevent->u.openData;
+			struct viodasd_device *device =
+			    &viodasd_devices[bevent->mDisk];
+			device->readOnly =
+			    bevent->mFlags & vioblockflags_ro;
+			device->size = data->mDiskLen;
+			device->cylinders = data->mCylinders;
+			device->tracks = data->mTracks;
+			device->sectors = data->mSectors;
+			device->bytesPerSector = data->mBytesPerSector;
+			viodasd_max_disk = data->mMaxDisks;
+		}
+		up(pwe->sem);
+		break;
+	case vioblockclose:
+		break;
+	case vioblockcheck:
+		pwe =
+		    (struct viodasd_waitevent *) (unsigned long) event->
+		    xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.changed = bevent->u.check.changed;
+		up(pwe->sem);
+		break;
+	case vioblockflush:
+		up((void *) (unsigned long) event->xCorrelationToken);
+		break;
+	case vioblockread:
+	case vioblockwrite:
+		viodasd_handleReadWrite(bevent);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO "invalid subtype!");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+static const char *major_name(int major)
+{
+	static char major_names[NUM_MAJORS][MAX_MAJOR_NAME];
+	int index = major_to_index(major);
+
+	if(index < 0)
+		return NULL;
+	if(major_names[index][0] == '\0') {
+		if(index == 0)
+			strcpy(major_names[index], VIOD_GENHD_NAME);
+		else
+			sprintf(major_names[index], VIOD_GENHD_NAME"%d", index);
+	}
+	return major_names[index];
+}
+
+static const char *device_name(int major)
+{
+	static char device_names[NUM_MAJORS][MAX_MAJOR_NAME];
+	int index = major_to_index(major);
+
+	if(index < 0)
+		return NULL;
+	if(device_names[index][0] == '\0') {
+#ifdef CONFIG_VIODASD_IDE
+		sprintf(device_names[index], VIOD_DEVICE_NAME"%d", index);
+#else
+		strcpy(device_names[index], VIOD_DEVICE_NAME);
+#endif
+	}
+	return device_names[index];
+}
+
+/* This routine tries to clean up anything we allocated/registered
+ */
+static void viodasd_cleanup_major(int major)
+{
+	const int num_partitions = DEV_PER_MAJOR << PARTITION_SHIFT;
+	int minor;
+
+#define CLEANIT(x) if (x) {kfree(x); x=NULL;}
+
+	for (minor = 0; minor < num_partitions; minor++)
+		fsync_dev(MKDEV(major, minor));
+
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(major));
+
+	read_ahead[major] = 0;
+
+	CLEANIT(blk_size[major]);
+	CLEANIT(blksize_size[major]);
+	CLEANIT(hardsect_size[major]);
+	CLEANIT(max_sectors[major]);
+	CLEANIT(major_to_gendisk(major)->part);
+
+	blk_cleanup_queue(BLK_DEFAULT_QUEUE(major));
+
+	devfs_unregister_blkdev(major, device_name(major));
+}
+
+/* in case of bad return code, caller must cleanup2() for this major */
+static int viodasd_init_major(int major)
+{
+	int i;
+	const int numpart = DEV_PER_MAJOR << PARTITION_SHIFT;
+	int *sizes, *sectsizes, *blksizes, *maxsectors;
+	struct hd_struct *partitions;
+	struct gendisk *gendisk = major_to_gendisk(major);
+
+	/*
+	 * Do the devfs_register.  This works even if devfs is not
+	 * configured
+	 */
+	if (devfs_register_blkdev(major, device_name(major), &viodasd_fops)) {
+		printk(KERN_WARNING_VIO
+		       "%s: can't register major number %d\n",
+		       device_name(major), major);
+		return -1;
+	}
+
+	blk_init_queue(BLK_DEFAULT_QUEUE(major), do_viodasd_request);
+
+	read_ahead[major] = 8;	/* 8 sector (4kB) read ahead */
+
+	/* initialize the struct */
+	gendisk->major = major;
+	gendisk->major_name = major_name(major);
+	gendisk->minor_shift = PARTITION_SHIFT;
+	gendisk->max_p = 1 << PARTITION_SHIFT;
+	gendisk->nr_real = DEV_PER_MAJOR;
+	gendisk->fops = &viodasd_fops;
+
+	/* to be assigned later */
+	gendisk->next = NULL;
+	gendisk->part = NULL;
+	gendisk->sizes = NULL;
+	gendisk->de_arr = NULL;
+	gendisk->flags = NULL;
+
+	/* register us in the global list */
+	add_gendisk(gendisk);
+
+	/*
+	 * Now fill in all the device driver info     
+	 */
+	sizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!sizes)
+		return -ENOMEM;
+	memset(sizes, 0x00, numpart * sizeof(int));
+	blk_size[major] = gendisk->sizes = sizes;
+
+	partitions =
+	    kmalloc(numpart * sizeof(struct hd_struct), GFP_KERNEL);
+	if (!partitions)
+		return -ENOMEM;
+	memset(partitions, 0x00, numpart * sizeof(struct hd_struct));
+	gendisk->part = partitions;
+
+	blksizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!blksizes)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		blksizes[i] = blksize;
+	blksize_size[major] = blksizes;
+
+	sectsizes = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!sectsizes)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		sectsizes[i] = 0;
+	hardsect_size[major] = sectsizes;
+
+	maxsectors = kmalloc(numpart * sizeof(int), GFP_KERNEL);
+	if (!maxsectors)
+		return -ENOMEM;
+	for (i = 0; i < numpart; i++)
+		maxsectors[i] = VIODASD_MAXSECTORS;
+	max_sectors[major] = maxsectors;
+
+	return 0;
+}
+
+static void internal_register_disk(int diskno)
+{
+	static int registered[MAX_DISKNO] = { 0, };
+	int major = diskno_to_major(diskno);
+	int dev_within_major = diskno % DEV_PER_MAJOR;
+	struct gendisk *gendisk = major_to_gendisk(major);
+	int i;
+
+	if(registered[diskno])
+		return;
+	registered[diskno] = 1;
+
+	if (diskno == 0) {
+		printk(KERN_INFO_VIO
+		       "%s: Currently %d disks connected\n",
+		       VIOD_DEVICE_NAME, (int) viodasd_max_disk + 1);
+		if (viodasd_max_disk > MAX_DISKNO - 1)
+			printk(KERN_INFO_VIO
+			       "Only examining the first %d\n",
+			       MAX_DISKNO);
+	}
+
+	register_disk(gendisk,
+		      MKDEV(major,
+			    dev_within_major <<
+			    PARTITION_SHIFT),
+		      1 << PARTITION_SHIFT, &viodasd_fops,
+		      gendisk->
+		      part[dev_within_major << PARTITION_SHIFT].nr_sects);
+
+	printk(KERN_INFO_VIO
+	       "%s: Disk %2.2d size %dM, sectors %d, heads %d, cylinders %d, sectsize %d\n",
+	       VIOD_DEVICE_NAME,
+	       diskno,
+	       (int) (viodasd_devices[diskno].size /
+		      (1024 * 1024)),
+	       (int) viodasd_devices[diskno].sectors,
+	       (int) viodasd_devices[diskno].tracks,
+	       (int) viodasd_devices[diskno].cylinders,
+	       (int) hardsect_size[major][dev_within_major <<
+					  PARTITION_SHIFT]);
+
+	for (i = 1; i < (1 << PARTITION_SHIFT); ++i) {
+		int minor = (dev_within_major << PARTITION_SHIFT) + i;
+		struct hd_struct *partition = &gendisk->part[minor];
+		if (partition->nr_sects)
+			printk(KERN_INFO_VIO
+			       "%s: Disk %2.2d partition %2.2d start sector %ld, # sector %ld\n",
+			       VIOD_DEVICE_NAME, diskno, i,
+			       partition->start_sect, partition->nr_sects);
+	}
+}
+
+/* Initialize the whole device driver.  Handle module and non-module
+ * versions
+ */
+__init int viodasd_init(void)
+{
+	int i, j;
+	int rc;
+
+	/* Try to open to our host lp
+	 */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		vio_set_hostlp();
+	}
+
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "%s: invalid hosting partition\n",
+		       VIOD_DEVICE_NAME);
+		return -EIO;
+	}
+
+	printk(KERN_INFO_VIO
+	       "%s: Disk vers %s, major %d, max disks %d, hosting partition %d\n",
+	       VIOD_DEVICE_NAME, VIODASD_VERS, major_table[0], MAX_DISKNO,
+	       viopath_hostLp);
+
+	if (ROOT_DEV == NODEV) {
+		/* first disk, first partition */
+		ROOT_DEV = diskno_to_devt(0, 1);
+
+		printk(KERN_INFO_VIO
+		       "Claiming root file system as first partition of first virtual disk");
+	}
+
+	/* Actually open the path to the hosting partition           */
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_blockio,
+			  VIOMAXREQ + 2);
+	if (rc) {
+		printk(KERN_WARNING_VIO
+		       "error opening path to host partition %d\n",
+		       viopath_hostLp);
+		return -EIO;
+	} else {
+		printk("%s: opened path to hosting partition %d\n",
+		       VIOD_DEVICE_NAME, viopath_hostLp);
+	}
+
+	viodasd_devices =
+	    kmalloc(MAX_DISKNO * sizeof(struct viodasd_device),
+		    GFP_KERNEL);
+	if (!viodasd_devices)
+		return -ENOMEM;
+	memset(viodasd_devices, 0x00,
+	       MAX_DISKNO * sizeof(struct viodasd_device));
+
+	/*
+	 * Initialize our request handler
+	 */
+	vio_setHandler(viomajorsubtype_blockio, vioHandleBlockEvent);
+
+	for (i = 0; i < NUM_MAJORS; ++i) {
+		int init_rc = viodasd_init_major(major_table[i]);
+		if (init_rc < 0) {
+			for (j = 0; j <= i; ++j)
+				viodasd_cleanup_major(major_table[j]);
+			return init_rc;
+		}
+	}
+
+	viodasd_max_disk = MAX_DISKNO - 1;
+	for (i = 0; i <= viodasd_max_disk && i < MAX_DISKNO; i++) {
+		// Note that internal_open has side effects:
+		//  a) it updates the size of the disk
+		//  b) it updates viodasd_max_disk
+		//  c) it registers the disk if it has not done so already
+		if (internal_open(i, vioblockflags_ro) == 0)
+			internal_release(i, vioblockflags_ro);
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viodasd_proc_init);
+
+	return 0;
+}
+
+#ifdef MODULE
+void viodasd_exit(void)
+{
+	int i;
+	for(i = 0; i < NUM_MAJORS; ++i)
+		viodasd_cleanup_major(major_table[i]);
+
+	CLEANIT(viodasd_devices);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_blockio, VIOMAXREQ + 2);
+	iSeries_proc_callback(&viodasd_proc_delete);
+
+}
+#endif
+
+#ifdef MODULE
+module_init(viodasd_init);
+module_exit(viodasd_exit);
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/viopath.c linuxppc64_2_4/drivers/iseries/viopath.c
--- linux-2.4.19/drivers/iseries/viopath.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/viopath.c	Sun Jul  7 14:47:22 2002
@@ -0,0 +1,712 @@
+/* -*- linux-c -*-
+ *  arch/ppc64/viopath.c
+ *
+ *  iSeries Virtual I/O Message Path code
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This code is used by the iSeries virtual disk, cd,
+ * tape, and console to communicate with OS/400 in another
+ * partition.
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/config.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/proc_fs.h>
+#include <linux/pci.h>
+#include <linux/wait.h>
+
+#include <asm/iSeries/LparData.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvCallCfg.h>
+#include <asm/iSeries/mf.h>
+#include <asm/iSeries/iSeries_proc.h>
+
+#include "vio.h"
+
+EXPORT_SYMBOL(viopath_hostLp);
+EXPORT_SYMBOL(viopath_ourLp);
+EXPORT_SYMBOL(vio_set_hostlp);
+EXPORT_SYMBOL(vio_lookup_rc);
+EXPORT_SYMBOL(viopath_open);
+EXPORT_SYMBOL(viopath_close);
+EXPORT_SYMBOL(viopath_isactive);
+EXPORT_SYMBOL(viopath_sourceinst);
+EXPORT_SYMBOL(viopath_targetinst);
+EXPORT_SYMBOL(vio_setHandler);
+EXPORT_SYMBOL(vio_clearHandler);
+EXPORT_SYMBOL(vio_get_event_buffer);
+EXPORT_SYMBOL(vio_free_event_buffer);
+
+extern struct pci_dev * iSeries_vio_dev;
+
+/* Status of the path to each other partition in the system.
+ * This is overkill, since we will only ever establish connections
+ * to our hosting partition and the primary partition on the system.
+ * But this allows for other support in the future.
+ */
+static struct viopathStatus {
+	int isOpen:1;		/* Did we open the path?            */
+	int isActive:1;		/* Do we have a mon msg outstanding */
+	int users[VIO_MAX_SUBTYPES];
+	HvLpInstanceId mSourceInst;
+	HvLpInstanceId mTargetInst;
+	int numberAllocated;
+} viopathStatus[HVMAXARCHITECTEDLPS];
+
+static spinlock_t statuslock = SPIN_LOCK_UNLOCKED;
+
+/*
+ * For each kind of event we allocate a buffer that is
+ * guaranteed not to cross a page boundary
+ */
+static void *event_buffer[VIO_MAX_SUBTYPES];
+static atomic_t event_buffer_available[VIO_MAX_SUBTYPES];
+
+static void handleMonitorEvent(struct HvLpEvent *event);
+
+/* We use this structure to handle asynchronous responses.  The caller
+ * blocks on the semaphore and the handler posts the semaphore.
+ */
+struct doneAllocParms_t {
+	struct semaphore *sem;
+	int number;
+};
+
+/* Put a sequence number in each mon msg.  The value is not
+ * important.  Start at something other than 0 just for
+ * readability.  wrapping this is ok.
+ */
+static u8 viomonseq = 22;
+
+/* Our hosting logical partition.  We get this at startup
+ * time, and different modules access this variable directly.
+ */
+HvLpIndex viopath_hostLp = 0xff;	/* HvLpIndexInvalid */
+HvLpIndex viopath_ourLp = 0xff;
+
+/* For each kind of incoming event we set a pointer to a
+ * routine to call.
+ */
+static vio_event_handler_t *vio_handler[VIO_MAX_SUBTYPES];
+
+static char e2a(char x) {
+        switch (x) {
+        case 0xF0: return '0';
+        case 0xF1: return '1';
+        case 0xF2: return '2';
+        case 0xF3: return '3';
+        case 0xF4: return '4';
+        case 0xF5: return '5';
+        case 0xF6: return '6';
+        case 0xF7: return '7';
+        case 0xF8: return '8';
+        case 0xF9: return '9';
+        case 0xC1: return 'A';
+        case 0xC2: return 'B';
+        case 0xC3: return 'C';
+        case 0xC4: return 'D';
+        case 0xC5: return 'E';
+        case 0xC6: return 'F';
+        case 0xC7: return 'G';
+        case 0xC8: return 'H';
+        case 0xC9: return 'I';
+        case 0xD1: return 'J';
+        case 0xD2: return 'K';
+        case 0xD3: return 'L';
+        case 0xD4: return 'M';
+        case 0xD5: return 'N';
+        case 0xD6: return 'O';
+        case 0xD7: return 'P';
+        case 0xD8: return 'Q';
+        case 0xD9: return 'R';
+        case 0xE2: return 'S';
+        case 0xE3: return 'T';
+        case 0xE4: return 'U';
+        case 0xE5: return 'V';
+        case 0xE6: return 'W';
+        case 0xE7: return 'X';
+        case 0xE8: return 'Y';
+        case 0xE9: return 'Z';
+        }
+        return ' ';
+}
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	HvLpEvent_Rc hvrc;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	dma_addr_t dmaa =
+	    pci_map_single(iSeries_vio_dev, buf, PAGE_SIZE, PCI_DMA_FROMDEVICE);
+	int len = PAGE_SIZE;
+
+	if (len > blen)
+		len = blen;
+
+	memset(buf, 0x00, len);
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_config |
+					     vioconfigget,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long)
+					     &Semaphore, VIOVERSION << 16,
+					     ((u64) dmaa) << 32, len, 0,
+					     0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk("viopath hv error on op %d\n", (int) hvrc);
+	}
+
+	down(&Semaphore);
+
+	pci_unmap_single(iSeries_vio_dev, dmaa, PAGE_SIZE, PCI_DMA_FROMDEVICE);
+
+	sprintf(buf+strlen(buf),"SRLNBR=");
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.mfgID[2]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.mfgID[3]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[1]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[2]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[3]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[4]);
+        buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[5]);
+        buf[strlen(buf)] = '\n';	*eof = 1;
+	return strlen(buf);
+}
+
+/* Handle writes to our proc file system
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	/* Doesn't do anything today!!!
+	 */
+	return count;
+}
+
+/* setup our proc file system entries
+ */
+static void vio_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("config", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/* See if a given LP is active.  Allow for invalid lps to be passed in
+ * and just return invalid
+ */
+int viopath_isactive(HvLpIndex lp)
+{
+	if (lp == HvLpIndexInvalid)
+		return 0;
+	if (lp < HVMAXARCHITECTEDLPS)
+		return viopathStatus[lp].isActive;
+	else
+		return 0;
+}
+
+/* We cache the source and target instance ids for each
+ * partition.  
+ */
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mSourceInst;
+}
+
+HvLpInstanceId viopath_targetinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mTargetInst;
+}
+
+/* Send a monitor message.  This is a message with the acknowledge
+ * bit on that the other side will NOT explicitly acknowledge.  When
+ * the other side goes down, the hypervisor will acknowledge any
+ * outstanding messages....so we will know when the other side dies.
+ */
+static void sendMonMsg(HvLpIndex remoteLp)
+{
+	HvLpEvent_Rc hvrc;
+
+	viopathStatus[remoteLp].mSourceInst =
+	    HvCallEvent_getSourceLpInstanceId(remoteLp,
+					      HvLpEvent_Type_VirtualIo);
+	viopathStatus[remoteLp].mTargetInst =
+	    HvCallEvent_getTargetLpInstanceId(remoteLp,
+					      HvLpEvent_Type_VirtualIo);
+
+	/* Deliberately ignore the return code here.  if we call this
+	 * more than once, we don't care.
+	 */
+	vio_setHandler(viomajorsubtype_monitor, handleMonitorEvent);
+
+	hvrc = HvCallEvent_signalLpEventFast(remoteLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_monitor,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_DeferredAck,
+					     viopathStatus[remoteLp].
+					     mSourceInst,
+					     viopathStatus[remoteLp].
+					     mTargetInst, viomonseq++,
+					     0, 0, 0, 0, 0);
+
+	if (hvrc == HvLpEvent_Rc_Good) {
+		viopathStatus[remoteLp].isActive = 1;
+	} else {
+		printk(KERN_WARNING_VIO
+		       "could not connect to partition %d\n", remoteLp);
+		viopathStatus[remoteLp].isActive = 0;
+	}
+}
+
+static void handleMonitorEvent(struct HvLpEvent *event)
+{
+	HvLpIndex remoteLp;
+	int i;
+
+	/* This handler is _also_ called as part of the loop
+	 * at the end of this routine, so it must be able to
+	 * ignore NULL events...
+	 */
+	if(!event)
+		return;
+
+	/* First see if this is just a normal monitor message from the
+	 * other partition
+	 */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		if (!viopathStatus[remoteLp].isActive)
+			sendMonMsg(remoteLp);
+		return;
+	}
+
+	/* This path is for an acknowledgement; the other partition
+	 * died
+	 */
+	remoteLp = event->xTargetLp;
+	if ((event->xSourceInstanceId !=
+	     viopathStatus[remoteLp].mSourceInst)
+	    || (event->xTargetInstanceId !=
+		viopathStatus[remoteLp].mTargetInst)) {
+		printk(KERN_WARNING_VIO
+		       "ignoring ack....mismatched instances\n");
+		return;
+	}
+
+	printk(KERN_WARNING_VIO "partition %d ended\n", remoteLp);
+
+	viopathStatus[remoteLp].isActive = 0;
+
+	/* For each active handler, pass them a NULL
+	 * message to indicate that the other partition
+	 * died
+	 */
+	for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+		if (vio_handler[i] != NULL)
+			(*vio_handler[i]) (NULL);
+	}
+}
+
+int vio_setHandler(int subtype, vio_event_handler_t * beh)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	if (vio_handler[subtype] != NULL)
+		return -EBUSY;
+
+	vio_handler[subtype] = beh;
+	return 0;
+}
+
+int vio_clearHandler(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	if (vio_handler[subtype] == NULL)
+		return -EAGAIN;
+
+	vio_handler[subtype] = NULL;
+	return 0;
+}
+
+static void handleConfig(struct HvLpEvent *event)
+{
+	if(!event)
+		return;
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "unexpected config request from partition %d",
+		       event->xSourceLp);
+
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	up((struct semaphore *) event->xCorrelationToken);
+}
+
+/* Initialization of the hosting partition
+ */
+void vio_set_hostlp(void)
+{
+	/* If this has already been set then we DON'T want to either change
+	 * it or re-register the proc file system
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid)
+		return;
+
+	/* Figure out our hosting partition.  This isn't allowed to change
+	 * while we're active
+	 */
+	viopath_ourLp = HvLpConfig_getLpIndex();
+	viopath_hostLp = HvCallCfg_getHostingLpIndex(viopath_ourLp);
+
+	/* If we have a valid hosting LP, create a proc file system entry
+	 * for config information
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid) {
+		iSeries_proc_callback(&vio_proc_init);
+		vio_setHandler(viomajorsubtype_config, handleConfig);
+	}
+}
+
+static void vio_handleEvent(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	HvLpIndex remoteLp;
+	int subtype =
+	    (event->
+	     xSubtype & VIOMAJOR_SUBTYPE_MASK) >> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		if (event->xSourceInstanceId !=
+		    viopathStatus[remoteLp].mTargetInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (event->xTargetInstanceId !=
+		    viopathStatus[remoteLp].mSourceInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	} else {
+		remoteLp = event->xTargetLp;
+		if (event->xSourceInstanceId !=
+		    viopathStatus[remoteLp].mSourceInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "ack msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (event->xTargetInstanceId !=
+		    viopathStatus[remoteLp].mTargetInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "viopath: ack msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	}
+
+	if (vio_handler[subtype] == NULL) {
+		printk(KERN_WARNING_VIO
+		       "unexpected virtual io event subtype %d from partition %d\n",
+		       event->xSubtype, remoteLp);
+		/* No handler.  Ack if necessary
+		 */
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	/* This innocuous little line is where all the real work happens
+	 */
+	(*vio_handler[subtype]) (event);
+}
+
+static void viopath_donealloc(void *parm, int number)
+{
+	struct doneAllocParms_t *doneAllocParmsp =
+	    (struct doneAllocParms_t *) parm;
+	doneAllocParmsp->number = number;
+	up(doneAllocParmsp->sem);
+}
+
+static int allocateEvents(HvLpIndex remoteLp, int numEvents)
+{
+	struct doneAllocParms_t doneAllocParms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	doneAllocParms.sem = &Semaphore;
+
+	mf_allocateLpEvents(remoteLp, HvLpEvent_Type_VirtualIo, 250,	/* It would be nice to put a real number here! */
+			    numEvents,
+			    &viopath_donealloc, &doneAllocParms);
+
+	down(&Semaphore);
+
+	return doneAllocParms.number;
+}
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	int i;
+	unsigned long flags;
+
+	if ((remoteLp >= HvMaxArchitectedLps)
+	    || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	spin_lock_irqsave(&statuslock, flags);
+
+	/* OK...we can fit 4 maximum-sized events (256 bytes) in
+	 * each page (4096).  Get a new page every 4
+	 */
+	if (event_buffer[0] == NULL) {
+		for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+			if ((i % 4) == 0) {
+				event_buffer[i] =
+				    (void *) get_free_page(GFP_KERNEL);
+				if (event_buffer[i] == NULL) {
+					spin_unlock_irqrestore(&statuslock, flags);
+					return -ENOMEM;
+				}
+			} else {
+				event_buffer[i] =
+				    event_buffer[i - 1] + 256;
+			}
+			atomic_set(&event_buffer_available[i], 1);
+		}
+	}
+
+	viopathStatus[remoteLp].users[subtype]++;
+
+	if (!viopathStatus[remoteLp].isOpen) {
+		HvCallEvent_openLpEventPath(remoteLp,
+					    HvLpEvent_Type_VirtualIo);
+
+		viopathStatus[remoteLp].numberAllocated +=
+		    allocateEvents(remoteLp, 1);
+
+		if (viopathStatus[remoteLp].numberAllocated == 0) {
+			HvCallEvent_closeLpEventPath(remoteLp,
+						     HvLpEvent_Type_VirtualIo);
+			
+			spin_unlock_irqrestore(&statuslock, flags);
+			return -ENOMEM;
+		}
+
+		viopathStatus[remoteLp].mSourceInst =
+		    HvCallEvent_getSourceLpInstanceId(remoteLp,
+						      HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].mTargetInst =
+		    HvCallEvent_getTargetLpInstanceId(remoteLp,
+						      HvLpEvent_Type_VirtualIo);
+
+		HvLpEvent_registerHandler(HvLpEvent_Type_VirtualIo,
+					  &vio_handleEvent);
+
+		viopathStatus[remoteLp].isOpen = 1;
+
+		sendMonMsg(remoteLp);
+
+		printk(KERN_INFO_VIO
+		       "Opening connection to partition %d, setting sinst %d, tinst %d\n",
+		       remoteLp,
+		       viopathStatus[remoteLp].mSourceInst,
+		       viopathStatus[remoteLp].mTargetInst);
+	}
+
+	viopathStatus[remoteLp].numberAllocated +=
+	    allocateEvents(remoteLp, numReq);
+	spin_unlock_irqrestore(&statuslock, flags);
+
+	return 0;
+}
+
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	unsigned long flags;
+	int i;
+	int numOpen;
+	struct doneAllocParms_t doneAllocParms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	doneAllocParms.sem = &Semaphore;
+
+	if ((remoteLp >= HvMaxArchitectedLps)
+	    || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	spin_lock_irqsave(&statuslock, flags);
+
+	viopathStatus[remoteLp].users[subtype]--;
+	
+	mf_deallocateLpEvents( remoteLp,HvLpEvent_Type_VirtualIo,
+			       numReq,
+			       &viopath_donealloc,
+			       &doneAllocParms );
+	down(&Semaphore);
+
+	for (i = 0, numOpen = 0; i < VIO_MAX_SUBTYPES; i++) {
+		numOpen += viopathStatus[remoteLp].users[i];
+	}
+	
+	if ((viopathStatus[remoteLp].isOpen) && (numOpen == 0)) {
+		printk(KERN_INFO_VIO
+		       "Closing connection to partition %d", remoteLp);
+
+		HvCallEvent_closeLpEventPath(remoteLp,
+					     HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].isOpen = 0;
+		viopathStatus[remoteLp].isActive = 0;
+
+		for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+			atomic_set(&event_buffer_available[i], 0);
+			
+			for (i = 0; i < VIO_MAX_SUBTYPES; i += 4) {
+				free_page((unsigned long) event_buffer[i]);
+			}
+		}
+
+	}
+	spin_unlock_irqrestore(&statuslock, flags);
+	return 0;
+}
+
+void *vio_get_event_buffer(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return NULL;
+
+	if (atomic_dec_if_positive(&event_buffer_available[subtype]) == 0)
+		return event_buffer[subtype];
+	else
+		return NULL;
+}
+
+void vio_free_event_buffer(int subtype, void *buffer)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES)) {
+		printk(KERN_WARNING_VIO
+		       "unexpected subtype %d freeing event buffer\n",
+		       subtype);
+		return;
+	}
+
+	if (atomic_read(&event_buffer_available[subtype]) != 0) {
+		printk(KERN_WARNING_VIO
+		       "freeing unallocated event buffer, subtype %d\n",
+		       subtype);
+		return;
+	}
+
+	if (buffer != event_buffer[subtype]) {
+		printk(KERN_WARNING_VIO
+		       "freeing invalid event buffer, subtype %d\n",
+		       subtype);
+	}
+
+	atomic_set(&event_buffer_available[subtype], 1);
+}
+
+static const struct vio_error_entry vio_no_error =
+    { 0, 0, "Non-VIO Error" };
+static const struct vio_error_entry vio_unknown_error =
+    { 0, EIO, "Unknown Error" };
+
+static const struct vio_error_entry vio_default_errors[] = {
+	{0x0001, EIO, "No Connection"},
+	{0x0002, EIO, "No Receiver"},
+	{0x0003, EIO, "No Buffer Available"},
+	{0x0004, EBADRQC, "Invalid Message Type"},
+	{0x0000, 0, NULL},
+};
+
+const struct vio_error_entry *vio_lookup_rc(const struct vio_error_entry
+					    *local_table, u16 rc)
+{
+	const struct vio_error_entry *cur;
+	if (!rc)
+		return &vio_no_error;
+	if (local_table)
+		for (cur = local_table; cur->rc; ++cur)
+			if (cur->rc == rc)
+				return cur;
+	for (cur = vio_default_errors; cur->rc; ++cur)
+		if (cur->rc == rc)
+			return cur;
+	return &vio_unknown_error;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/iseries/viotape.c linuxppc64_2_4/drivers/iseries/viotape.c
--- linux-2.4.19/drivers/iseries/viotape.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/iseries/viotape.c	Wed Dec 12 13:48:34 2001
@@ -0,0 +1,1185 @@
+/* -*- linux-c -*-
+ *  drivers/char/viotape.c
+ *
+ *  iSeries Virtual Tape
+ ***************************************************************************
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ ***************************************************************************
+ * This routine provides access to tape drives owned and managed by an OS/400 
+ * partition running on the same box as this Linux partition.
+ *
+ * All tape operations are performed by sending messages back and forth to 
+ * the OS/400 partition.  The format of the messages is defined in
+ * iSeries/vio.h
+ * 
+ */
+
+
+#undef VIOT_DEBUG
+
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <asm/ioctls.h>
+#include <linux/mtio.h>
+#include <linux/pci.h>
+#include <linux/devfs_fs.h>
+#include <linux/devfs_fs_kernel.h>
+#include <asm/uaccess.h>
+
+#include "vio.h"
+#include <asm/iSeries/HvLpEvent.h>
+#include "asm/iSeries/HvCallEvent.h"
+#include "asm/iSeries/HvLpConfig.h"
+#include <asm/iSeries/iSeries_proc.h>
+
+extern struct pci_dev * iSeries_vio_dev;
+
+static int viotape_major = 230;
+static int viotape_numdev = 0;
+
+#define VIOTAPE_MAXREQ 1
+
+/* version number for viotape driver */
+static unsigned int version_major = 1;
+static unsigned int version_minor = 0;
+
+static u64 sndMsgSeq;
+static u64 sndMsgAck;
+static u64 rcvMsgSeq;
+static u64 rcvMsgAck;
+
+/***************************************************************************
+ * The minor number follows the conventions of the SCSI tape drives.  The
+ * rewind and mode are encoded in the minor #.  We use this struct to break
+ * them out
+ ***************************************************************************/
+struct viot_devinfo_struct {
+	int major;
+	int minor;
+	int devno;
+	int mode;
+	int rewind;
+};
+
+#define VIOTAPOP_RESET          0
+#define VIOTAPOP_FSF	        1
+#define VIOTAPOP_BSF	        2
+#define VIOTAPOP_FSR	        3
+#define VIOTAPOP_BSR	        4
+#define VIOTAPOP_WEOF	        5
+#define VIOTAPOP_REW	        6
+#define VIOTAPOP_NOP	        7
+#define VIOTAPOP_EOM	        8
+#define VIOTAPOP_ERASE          9
+#define VIOTAPOP_SETBLK        10
+#define VIOTAPOP_SETDENSITY    11
+#define VIOTAPOP_SETPOS	       12
+#define VIOTAPOP_GETPOS	       13
+#define VIOTAPOP_SETPART       14
+
+struct viotapelpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mTape;
+	u16 mFlags;
+	u32 mToken;
+	u64 mLen;
+	union {
+		struct {
+			u32 mTapeOp;
+			u32 mCount;
+		} tapeOp;
+		struct {
+			u32 mType;
+			u32 mResid;
+			u32 mDsreg;
+			u32 mGstat;
+			u32 mErreg;
+			u32 mFileNo;
+			u32 mBlkNo;
+		} getStatus;
+		struct {
+			u32 mBlkNo;
+		} getPos;
+	} u;
+};
+enum viotapesubtype {
+	viotapeopen = 0x0001,
+	viotapeclose = 0x0002,
+	viotaperead = 0x0003,
+	viotapewrite = 0x0004,
+	viotapegetinfo = 0x0005,
+	viotapeop = 0x0006,
+	viotapegetpos = 0x0007,
+	viotapesetpos = 0x0008,
+	viotapegetstatus = 0x0009
+};
+
+enum viotapeRc {
+	viotape_InvalidRange = 0x0601,
+	viotape_InvalidToken = 0x0602,
+	viotape_DMAError = 0x0603,
+	viotape_UseError = 0x0604,
+	viotape_ReleaseError = 0x0605,
+	viotape_InvalidTape = 0x0606,
+	viotape_InvalidOp = 0x0607,
+	viotape_TapeErr = 0x0608,
+
+	viotape_AllocTimedOut = 0x0640,
+	viotape_BOTEnc = 0x0641,
+	viotape_BlankTape = 0x0642,
+	viotape_BufferEmpty = 0x0643,
+	viotape_CleanCartFound = 0x0644,
+	viotape_CmdNotAllowed = 0x0645,
+	viotape_CmdNotSupported = 0x0646,
+	viotape_DataCheck = 0x0647,
+	viotape_DecompressErr = 0x0648,
+	viotape_DeviceTimeout = 0x0649,
+	viotape_DeviceUnavail = 0x064a,
+	viotape_DeviceBusy = 0x064b,
+	viotape_EndOfMedia = 0x064c,
+	viotape_EndOfTape = 0x064d,
+	viotape_EquipCheck = 0x064e,
+	viotape_InsufficientRs = 0x064f,
+	viotape_InvalidLogBlk = 0x0650,
+	viotape_LengthError = 0x0651,
+	viotape_LibDoorOpen = 0x0652,
+	viotape_LoadFailure = 0x0653,
+	viotape_NotCapable = 0x0654,
+	viotape_NotOperational = 0x0655,
+	viotape_NotReady = 0x0656,
+	viotape_OpCancelled = 0x0657,
+	viotape_PhyLinkErr = 0x0658,
+	viotape_RdyNotBOT = 0x0659,
+	viotape_TapeMark = 0x065a,
+	viotape_WriteProt = 0x065b
+};
+
+static const struct vio_error_entry viotape_err_table[] = {
+	{viotape_InvalidRange, EIO, "Internal error"},
+	{viotape_InvalidToken, EIO, "Internal error"},
+	{viotape_DMAError, EIO, "DMA error"},
+	{viotape_UseError, EIO, "Internal error"},
+	{viotape_ReleaseError, EIO, "Internal error"},
+	{viotape_InvalidTape, EIO, "Invalid tape device"},
+	{viotape_InvalidOp, EIO, "Invalid operation"},
+	{viotape_TapeErr, EIO, "Tape error"},
+	{viotape_AllocTimedOut, EBUSY, "Allocate timed out"},
+	{viotape_BOTEnc, EIO, "Beginning of tape encountered"},
+	{viotape_BlankTape, EIO, "Blank tape"},
+	{viotape_BufferEmpty, EIO, "Buffer empty"},
+	{viotape_CleanCartFound, ENOMEDIUM, "Cleaning cartridge found"},
+	{viotape_CmdNotAllowed, EIO, "Command not allowed"},
+	{viotape_CmdNotSupported, EIO, "Command not supported"},
+	{viotape_DataCheck, EIO, "Data check"},
+	{viotape_DecompressErr, EIO, "Decompression error"},
+	{viotape_DeviceTimeout, EBUSY, "Device timeout"},
+	{viotape_DeviceUnavail, EIO, "Device unavailable"},
+	{viotape_DeviceBusy, EBUSY, "Device busy"},
+	{viotape_EndOfMedia, ENOSPC, "End of media"},
+	{viotape_EndOfTape, ENOSPC, "End of tape"},
+	{viotape_EquipCheck, EIO, "Equipment check"},
+	{viotape_InsufficientRs, EOVERFLOW, "Insufficient tape resources"},
+	{viotape_InvalidLogBlk, EIO, "Invalid logical block location"},
+	{viotape_LengthError, EOVERFLOW, "Length error"},
+	{viotape_LibDoorOpen, EBUSY, "Door open"},
+	{viotape_LoadFailure, ENOMEDIUM, "Load failure"},
+	{viotape_NotCapable, EIO, "Not capable"},
+	{viotape_NotOperational, EIO, "Not operational"},
+	{viotape_NotReady, EIO, "Not ready"},
+	{viotape_OpCancelled, EIO, "Operation cancelled"},
+	{viotape_PhyLinkErr, EIO, "Physical link error"},
+	{viotape_RdyNotBOT, EIO, "Ready but not beginning of tape"},
+	{viotape_TapeMark, EIO, "Tape mark"},
+	{viotape_WriteProt, EROFS, "Write protection error"},
+	{0, 0, NULL},
+};
+
+/* Maximum # tapes we support
+ */
+#define VIOTAPE_MAX_TAPE 8
+#define MAX_PARTITIONS 4
+
+/* defines for current tape state */
+#define VIOT_IDLE 0
+#define VIOT_READING 1
+#define VIOT_WRITING 2
+
+/* Our info on the tapes
+ */
+struct tape_descr {
+	char rsrcname[10];
+	char type[4];
+	char model[3];
+};
+
+static struct tape_descr *viotape_unitinfo = NULL;
+
+static const char *lasterr[VIOTAPE_MAX_TAPE];
+
+static struct mtget viomtget[VIOTAPE_MAX_TAPE];
+
+/* maintain the current state of each tape (and partition)
+   so that we know when to write EOF marks.
+*/
+static struct {
+	unsigned char cur_part;
+	devfs_handle_t dev_handle;
+	struct {
+		unsigned char rwi;
+	} part_stat[MAX_PARTITIONS];
+} state[VIOTAPE_MAX_TAPE];
+
+/* We single-thread
+ */
+static struct semaphore reqSem;
+
+/* When we send a request, we use this struct to get the response back
+ * from the interrupt handler
+ */
+struct opStruct {
+	void *buffer;
+	dma_addr_t dmaaddr;
+	size_t count;
+	int rc;
+	struct semaphore *sem;
+	struct opStruct *free;
+};
+
+static spinlock_t opStructListLock;
+static struct opStruct *opStructList;
+
+/* forward declaration to resolve interdependence */
+static int chg_state(int index, unsigned char new_state,
+		     struct file *file);
+
+/* Decode the kdev_t into its parts
+ */
+void getDevInfo(kdev_t dev, struct viot_devinfo_struct *devi)
+{
+	devi->major = MAJOR(dev);
+	devi->minor = MINOR(dev);
+	devi->devno = devi->minor & 0x1F;
+	devi->mode = (devi->minor & 0x60) >> 5;
+	/* if bit is set in the minor, do _not_ rewind automatically */
+	devi->rewind = !(devi->minor & 0x80);
+}
+
+
+/* Allocate an op structure from our pool
+ */
+static struct opStruct *getOpStruct(void)
+{
+	struct opStruct *newOpStruct;
+	spin_lock(&opStructListLock);
+
+	if (opStructList == NULL) {
+		newOpStruct = kmalloc(sizeof(struct opStruct), GFP_KERNEL);
+	} else {
+		newOpStruct = opStructList;
+		opStructList = opStructList->free;
+	}
+
+	if (newOpStruct)
+		memset(newOpStruct, 0x00, sizeof(struct opStruct));
+
+	spin_unlock(&opStructListLock);
+
+	return newOpStruct;
+}
+
+/* Return an op structure to our pool
+ */
+static void freeOpStruct(struct opStruct *opStruct)
+{
+	spin_lock(&opStructListLock);
+	opStruct->free = opStructList;
+	opStructList = opStruct;
+	spin_unlock(&opStructListLock);
+}
+
+/* Map our tape return codes to errno values
+ */
+int tapeRcToErrno(int tapeRc, char *operation, int tapeno)
+{
+	const struct vio_error_entry *err;
+	if(tapeRc == 0)
+		return 0;
+	err = vio_lookup_rc(viotape_err_table, tapeRc);
+
+	printk(KERN_WARNING_VIO "tape error 0x%04x on Device %d (%-10s): %s\n",
+	       tapeRc, tapeno, viotape_unitinfo[tapeno].rsrcname, err->msg);
+
+	lasterr[tapeno] = err->msg;
+
+	return -err->errno;
+}
+
+/* Handle reads from the proc file system.  
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+
+	len += sprintf(buf + len, "viotape driver version %d.%d\n",
+		       version_major, version_minor);
+
+	for (i = 0; i < viotape_numdev; i++) {
+
+		len +=
+		    sprintf(buf + len,
+			    "viotape device %d is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+			    i, viotape_unitinfo[i].rsrcname,
+			    viotape_unitinfo[i].type,
+			    viotape_unitinfo[i].model);
+		if (lasterr[i])
+			len +=
+			    sprintf(buf + len, "   last error: %s\n",
+				    lasterr[i]);
+	}
+
+	*eof = 1;
+	return len;
+}
+
+/* setup our proc file system entries
+ */
+void viotape_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent =
+	    create_proc_entry("viotape", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+}
+
+/* clean up our proc file system entries
+ */
+void viotape_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viotape", iSeries_proc);
+}
+
+
+/* Get info on all tapes from OS/400
+ */
+static void get_viotape_info(void)
+{
+	dma_addr_t dmaaddr;
+	HvLpEvent_Rc hvrc;
+	int i;
+	struct opStruct *op = getOpStruct();
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	if (op == NULL)
+		return;
+
+	if (viotape_unitinfo == NULL) {
+		viotape_unitinfo =
+		    kmalloc(sizeof(struct tape_descr) * VIOTAPE_MAX_TAPE,
+			    GFP_KERNEL);
+	}
+	memset(viotape_unitinfo, 0x00,
+	       sizeof(struct tape_descr) * VIOTAPE_MAX_TAPE);
+	memset(lasterr, 0x00, sizeof(lasterr));
+
+	op->sem = &Semaphore;
+
+	dmaaddr = pci_map_single(iSeries_vio_dev, viotape_unitinfo,
+				 sizeof(struct tape_descr) *
+				 VIOTAPE_MAX_TAPE, PCI_DMA_FROMDEVICE);
+	if (dmaaddr == 0xFFFFFFFF) {
+		printk(KERN_WARNING_VIO "viotape error allocating tce\n");
+		return;
+	}
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapegetinfo,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16, dmaaddr,
+					     sizeof(struct tape_descr) *
+					     VIOTAPE_MAX_TAPE, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk("viotape hv error on op %d\n", (int) hvrc);
+	}
+
+	down(&Semaphore);
+
+	freeOpStruct(op);
+
+
+	for (i = 0;
+	     ((i < VIOTAPE_MAX_TAPE) && (viotape_unitinfo[i].rsrcname[0]));
+	     i++) {
+		printk("found a tape %10.10s\n",
+		       viotape_unitinfo[i].rsrcname);
+		viotape_numdev++;
+	}
+}
+
+
+/* Write
+ */
+static ssize_t viotap_write(struct file *file, const char *buf,
+			    size_t count, loff_t * ppos)
+{
+	HvLpEvent_Rc hvrc;
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	unsigned short flags = file->f_flags;
+	struct opStruct *op = getOpStruct();
+	int noblock = ((flags & O_NONBLOCK) != 0);
+	int err;
+	struct viot_devinfo_struct devi;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	/* We need to make sure we can send a request.  We use
+	 * a semaphore to keep track of # requests in use.  If
+	 * we are non-blocking, make sure we don't block on the 
+	 * semaphore
+	 */
+	if (noblock) {
+		if (down_trylock(&reqSem)) {
+			freeOpStruct(op);
+			return -EWOULDBLOCK;
+		}
+	} else {
+		down(&reqSem);
+	}
+
+	/* Allocate a DMA buffer */
+	op->buffer = pci_alloc_consistent(iSeries_vio_dev, count, &op->dmaaddr);
+
+	if ((op->dmaaddr == 0xFFFFFFFF) || (op->buffer == NULL)) {
+		printk(KERN_WARNING_VIO 
+		       "tape error allocating dma buffer for len %ld\n",
+		       count);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	op->count = count;
+
+	/* Copy the data into the buffer */
+	err = copy_from_user(op->buffer, (const void *) buf, count);
+	if (err) {
+		printk(KERN_WARNING_VIO 
+		       "tape: error on copy from user\n");
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	if (noblock) {
+		op->sem = NULL;
+	} else {
+		op->sem = &Semaphore;
+	}
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapewrite,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.
+					      devno << 48) | op->dmaaddr,
+					     count, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk("viotape hv error on op %d\n", (int) hvrc);
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EIO;
+	}
+
+	if (noblock)
+		return count;
+
+	down(&Semaphore);
+
+	err = op->rc;
+
+	/* Free the buffer */
+	pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+
+	count = op->count;
+
+	freeOpStruct(op);
+	up(&reqSem);
+	if (err)
+		return tapeRcToErrno(err, "write", devi.devno);
+	else {
+		chg_state(devi.devno, VIOT_WRITING, file);
+		return count;
+	}
+}
+
+/* read
+ */
+static ssize_t viotap_read(struct file *file, char *buf, size_t count,
+			   loff_t * ptr)
+{
+	HvLpEvent_Rc hvrc;
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	unsigned short flags = file->f_flags;
+	struct opStruct *op = getOpStruct();
+	int noblock = ((flags & O_NONBLOCK) != 0);
+	int err;
+	struct viot_devinfo_struct devi;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	/* We need to make sure we can send a request.  We use
+	 * a semaphore to keep track of # requests in use.  If
+	 * we are non-blocking, make sure we don't block on the 
+	 * semaphore
+	 */
+	if (noblock) {
+		if (down_trylock(&reqSem)) {
+			freeOpStruct(op);
+			return -EWOULDBLOCK;
+		}
+	} else {
+		down(&reqSem);
+	}
+
+	chg_state(devi.devno, VIOT_READING, file);
+
+	/* Allocate a DMA buffer */
+	op->buffer = pci_alloc_consistent(iSeries_vio_dev, count, &op->dmaaddr);
+
+	if ((op->dmaaddr == 0xFFFFFFFF) || (op->buffer == NULL)) {
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EFAULT;
+	}
+
+	op->count = count;
+
+	op->sem = &Semaphore;
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotaperead,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.
+					      devno << 48) | op->dmaaddr,
+					     count, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO 
+		       "tape hv error on op %d\n", (int) hvrc);
+		pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+		freeOpStruct(op);
+		up(&reqSem);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (op->rc == 0) {
+		/* If we got data back        */
+		if (op->count) {
+			/* Copy the data into the buffer */
+			err = copy_to_user(buf, op->buffer, count);
+			if (err) {
+				printk("error on copy_to_user\n");
+				pci_free_consistent(iSeries_vio_dev, count,
+						    op->buffer,
+						    op->dmaaddr);
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EFAULT;
+			}
+		}
+	}
+
+	err = op->rc;
+
+	/* Free the buffer */
+	pci_free_consistent(iSeries_vio_dev, count, op->buffer, op->dmaaddr);
+	count = op->count;
+
+	freeOpStruct(op);
+	up(&reqSem);
+	if (err)
+		return tapeRcToErrno(err, "read", devi.devno);
+	else
+		return count;
+}
+
+/* read
+ */
+static int viotap_ioctl(struct inode *inode, struct file *file,
+			unsigned int cmd, unsigned long arg)
+{
+	HvLpEvent_Rc hvrc;
+	int err;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	struct opStruct *op = getOpStruct();
+	struct viot_devinfo_struct devi;
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+	down(&reqSem);
+
+	switch (cmd) {
+	case MTIOCTOP:{
+			struct mtop mtc;
+			u32 myOp;
+
+			/* inode is null if and only if we (the kernel) made the request */
+			if (inode == NULL)
+				memcpy(&mtc, (void *) arg,
+				       sizeof(struct mtop));
+			else if (copy_from_user
+				 ((char *) &mtc, (char *) arg,
+				  sizeof(struct mtop))) {
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EFAULT;
+			}
+
+			switch (mtc.mt_op) {
+			case MTRESET:
+				myOp = VIOTAPOP_RESET;
+				break;
+			case MTFSF:
+				myOp = VIOTAPOP_FSF;
+				break;
+			case MTBSF:
+				myOp = VIOTAPOP_BSF;
+				break;
+			case MTFSR:
+				myOp = VIOTAPOP_FSR;
+				break;
+			case MTBSR:
+				myOp = VIOTAPOP_BSR;
+				break;
+			case MTWEOF:
+				myOp = VIOTAPOP_WEOF;
+				break;
+			case MTREW:
+				myOp = VIOTAPOP_REW;
+				break;
+			case MTNOP:
+				myOp = VIOTAPOP_NOP;
+				break;
+			case MTEOM:
+				myOp = VIOTAPOP_EOM;
+				break;
+			case MTERASE:
+				myOp = VIOTAPOP_ERASE;
+				break;
+			case MTSETBLK:
+				myOp = VIOTAPOP_SETBLK;
+				break;
+			case MTSETDENSITY:
+				myOp = VIOTAPOP_SETDENSITY;
+				break;
+			case MTTELL:
+				myOp = VIOTAPOP_GETPOS;
+				break;
+			case MTSEEK:
+				myOp = VIOTAPOP_SETPOS;
+				break;
+			case MTSETPART:
+				myOp = VIOTAPOP_SETPART;
+				break;
+			default:
+				return -EIO;
+			}
+
+/* if we moved the head, we are no longer reading or writing */
+			switch (mtc.mt_op) {
+			case MTFSF:
+			case MTBSF:
+			case MTFSR:
+			case MTBSR:
+			case MTTELL:
+			case MTSEEK:
+			case MTREW:
+				chg_state(devi.devno, VIOT_IDLE, file);
+			}
+
+			op->sem = &Semaphore;
+			hvrc =
+			    HvCallEvent_signalLpEventFast(viopath_hostLp,
+							  HvLpEvent_Type_VirtualIo,
+							  viomajorsubtype_tape
+							  | viotapeop,
+							  HvLpEvent_AckInd_DoAck,
+							  HvLpEvent_AckType_ImmediateAck,
+							  viopath_sourceinst
+							  (viopath_hostLp),
+							  viopath_targetinst
+							  (viopath_hostLp),
+							  (u64) (unsigned
+								 long) op,
+							  VIOVERSION << 16,
+							  ((u64) devi.
+							   devno << 48), 0,
+							  (((u64) myOp) <<
+							   32) | mtc.
+							  mt_count, 0);
+			if (hvrc != HvLpEvent_Rc_Good) {
+				printk("viotape hv error on op %d\n",
+				       (int) hvrc);
+				freeOpStruct(op);
+				up(&reqSem);
+				return -EIO;
+			}
+			down(&Semaphore);
+			if (op->rc) {
+				freeOpStruct(op);
+				up(&reqSem);
+				return tapeRcToErrno(op->rc,
+						     "tape operation",
+						     devi.devno);
+			} else {
+				freeOpStruct(op);
+				up(&reqSem);
+				return 0;
+			}
+			break;
+		}
+
+	case MTIOCGET:
+		op->sem = &Semaphore;
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_tape |
+						     viotapegetstatus,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     op, VIOVERSION << 16,
+						     ((u64) devi.
+						      devno << 48), 0, 0,
+						     0);
+		if (hvrc != HvLpEvent_Rc_Good) {
+			printk("viotape hv error on op %d\n", (int) hvrc);
+			freeOpStruct(op);
+			up(&reqSem);
+			return -EIO;
+		}
+		down(&Semaphore);
+		up(&reqSem);
+		if (op->rc) {
+			freeOpStruct(op);
+			return tapeRcToErrno(op->rc, "get status",
+					     devi.devno);
+		} else {
+			freeOpStruct(op);
+			err =
+			    copy_to_user((void *) arg, &viomtget[dev],
+					 sizeof(viomtget[0]));
+			if (err) {
+				freeOpStruct(op);
+				return -EFAULT;
+			}
+			return 0;
+		}
+		break;
+	case MTIOCPOS:
+		printk("Got an MTIOCPOS\n");
+	default:
+		return -ENOSYS;
+	}
+	return 0;
+}
+
+/* Open
+ */
+static int viotap_open(struct inode *inode, struct file *file)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	HvLpEvent_Rc hvrc;
+	struct opStruct *op = getOpStruct();
+	struct viot_devinfo_struct devi;
+	if (op == NULL)
+		return -ENOMEM;
+
+	getDevInfo(dev, &devi);
+
+// Note: We currently only support one mode!
+	if ((devi.devno >= viotape_numdev) || (devi.mode)) {
+		freeOpStruct(op);
+		return -ENODEV;
+	}
+
+	op->sem = &Semaphore;
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapeopen,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.devno << 48), 0,
+					     0, 0);
+
+
+	if (hvrc != 0) {
+		printk("viotape bad rc on signalLpEvent %d\n", (int) hvrc);
+		freeOpStruct(op);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (op->rc) {
+		freeOpStruct(op);
+		return tapeRcToErrno(op->rc, "open", devi.devno);
+	} else {
+		freeOpStruct(op);
+		MOD_INC_USE_COUNT;
+		return 0;
+	}
+}
+
+
+/* Release
+ */
+static int viotap_release(struct inode *inode, struct file *file)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	kdev_t dev = file->f_dentry->d_inode->i_rdev;
+	HvLpEvent_Rc hvrc;
+	struct viot_devinfo_struct devi;
+	struct opStruct *op = getOpStruct();
+
+	if (op == NULL)
+		return -ENOMEM;
+	op->sem = &Semaphore;
+
+	getDevInfo(dev, &devi);
+
+	if (devi.devno >= viotape_numdev) {
+		freeOpStruct(op);
+		return -ENODEV;
+	}
+
+	chg_state(devi.devno, VIOT_IDLE, file);
+
+	if (devi.rewind) {
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+						     HvLpEvent_Type_VirtualIo,
+						     viomajorsubtype_tape |
+						     viotapeop,
+						     HvLpEvent_AckInd_DoAck,
+						     HvLpEvent_AckType_ImmediateAck,
+						     viopath_sourceinst
+						     (viopath_hostLp),
+						     viopath_targetinst
+						     (viopath_hostLp),
+						     (u64) (unsigned long)
+						     op, VIOVERSION << 16,
+						     ((u64) devi.
+						      devno << 48), 0,
+						     ((u64) VIOTAPOP_REW)
+						     << 32, 0);
+		down(&Semaphore);
+
+		if (op->rc) {
+			tapeRcToErrno(op->rc, "rewind", devi.devno);
+		}
+	}
+
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_tape |
+					     viotapeclose,
+					     HvLpEvent_AckInd_DoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst
+					     (viopath_hostLp),
+					     viopath_targetinst
+					     (viopath_hostLp),
+					     (u64) (unsigned long) op,
+					     VIOVERSION << 16,
+					     ((u64) devi.devno << 48), 0,
+					     0, 0);
+
+
+	if (hvrc != 0) {
+		printk("viotape: bad rc on signalLpEvent %d\n",
+		       (int) hvrc);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (op->rc) {
+		printk("viotape: close failed\n");
+	}
+	MOD_DEC_USE_COUNT;
+	return 0;
+}
+
+struct file_operations viotap_fops = {
+	owner:THIS_MODULE,
+	read:viotap_read,
+	write:viotap_write,
+	ioctl:viotap_ioctl,
+	open:viotap_open,
+	release:viotap_release,
+};
+
+/* Handle interrupt events for tape
+ */
+static void vioHandleTapeEvent(struct HvLpEvent *event)
+{
+	int tapeminor;
+	struct opStruct *op;
+	struct viotapelpevent *tevent = (struct viotapelpevent *) event;
+
+	if (event == NULL) {
+	  /* Notification that a partition went away! */
+	  if (!viopath_isactive(viopath_hostLp)) {
+	    /* TODO! Clean up */
+	  }
+	  return;
+	}
+
+	tapeminor = event->xSubtype & VIOMINOR_SUBTYPE_MASK;
+	switch (tapeminor) {
+	case viotapegetinfo:
+	case viotapeopen:
+	case viotapeclose:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		op->rc = tevent->mSubTypeRc;
+		up(op->sem);
+		break;
+	case viotaperead:
+	case viotapewrite:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		op->rc = tevent->mSubTypeRc;
+		op->count = tevent->mLen;
+
+		if (op->sem) {
+			up(op->sem);
+		} else {
+			freeOpStruct(op);
+			up(&reqSem);
+		}
+		break;
+	case viotapeop:
+	case viotapegetpos:
+	case viotapesetpos:
+	case viotapegetstatus:
+		op = (struct opStruct *) (unsigned long) event->
+		    xCorrelationToken;
+		if (op) {
+			op->count = tevent->u.tapeOp.mCount;
+			op->rc = tevent->mSubTypeRc;
+
+			if (op->sem) {
+				up(op->sem);
+			}
+		}
+		break;
+	default:
+		printk("viotape: wierd ack\n");
+	}
+}
+
+
+/* Do initialization
+ */
+int __init viotap_init(void)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int rc;
+	char tapename[32];
+	int i;
+
+	printk("viotape driver version %d.%d\n", version_major,
+	       version_minor);
+
+	sndMsgSeq = sndMsgAck = 0;
+	rcvMsgSeq = rcvMsgAck = 0;
+	opStructList = NULL;
+	spin_lock_init(&opStructListLock);
+
+	sema_init(&reqSem, VIOTAPE_MAXREQ);
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/*
+	 * Open to our hosting lp
+	 */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return -1;
+
+	printk("viotape: init - open path to hosting (%d)\n",
+	       viopath_hostLp);
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+	if (rc) {
+		printk("viotape: error on viopath_open to hostlp %d\n",
+		       rc);
+	}
+
+	vio_setHandler(viomajorsubtype_tape, vioHandleTapeEvent);
+
+	printk("viotape major is %d\n", viotape_major);
+
+	get_viotape_info();
+
+	if (devfs_register_chrdev(viotape_major, "viotape", &viotap_fops)) {
+		printk("Error registering viotape device\n");
+		return -1;
+	}
+
+	for (i = 0; i < viotape_numdev; i++) {
+		int j;
+		state[i].cur_part = 0;
+		for (j = 0; j < MAX_PARTITIONS; ++j)
+			state[i].part_stat[j].rwi = VIOT_IDLE;
+		sprintf(tapename, "viotape%d", i);
+		state[i].dev_handle =
+		    devfs_register(NULL, tapename, DEVFS_FL_DEFAULT,
+				   viotape_major, i,
+				   S_IFCHR | S_IRUSR | S_IWUSR | S_IRGRP |
+				   S_IWGRP, &viotap_fops, NULL);
+		printk
+		    ("viotape device %s is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+		     tapename, viotape_unitinfo[i].rsrcname,
+		     viotape_unitinfo[i].type, viotape_unitinfo[i].model);
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viotape_proc_init);
+
+	return 0;
+}
+
+/* Give a new state to the tape object
+ */
+static int chg_state(int index, unsigned char new_state, struct file *file)
+{
+	unsigned char *cur_state =
+	    &state[index].part_stat[state[index].cur_part].rwi;
+	int rc = 0;
+
+	/* if the same state, don't bother */
+	if (*cur_state == new_state)
+		return 0;
+
+	/* write an EOF if changing from writing to some other state */
+	if (*cur_state == VIOT_WRITING) {
+		struct mtop write_eof = { MTWEOF, 1 };
+		rc = viotap_ioctl(NULL, file, MTIOCTOP,
+				  (unsigned long) &write_eof);
+	}
+	*cur_state = new_state;
+	return rc;
+}
+
+/* Cleanup
+ */
+static void __exit viotap_exit(void)
+{
+	int i, ret;
+	for (i = 0; i < viotape_numdev; ++i)
+		devfs_unregister(state[i].dev_handle);
+	ret = devfs_unregister_chrdev(viotape_major, "viotape");
+	if (ret < 0)
+		printk("Error unregistering device: %d\n", ret);
+	iSeries_proc_callback(&viotape_proc_delete);
+	if (viotape_unitinfo != NULL) {
+		kfree(viotape_unitinfo);
+		viotape_unitinfo = NULL;
+	}
+	viopath_close(viopath_hostLp, viomajorsubtype_tape, VIOTAPE_MAXREQ + 2);
+	vio_clearHandler(viomajorsubtype_tape);
+}
+
+MODULE_LICENSE("GPL");
+module_init(viotap_init);
+module_exit(viotap_exit);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/Config.in linuxppc64_2_4/drivers/net/Config.in
--- linux-2.4.19/drivers/net/Config.in	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/net/Config.in	Fri Aug  9 12:35:58 2002
@@ -250,10 +250,6 @@
 
 endmenu
 
-if [ "$CONFIG_PPC_ISERIES" = "y" ]; then
-   dep_tristate 'iSeries Virtual Ethernet driver support' CONFIG_VETH $CONFIG_PPC_ISERIES
-fi
-
 bool 'FDDI driver support' CONFIG_FDDI
 if [ "$CONFIG_FDDI" = "y" ]; then
    if [ "$CONFIG_PCI" = "y" -o "$CONFIG_EISA" = "y" ]; then
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/Makefile linuxppc64_2_4/drivers/net/Makefile
--- linux-2.4.19/drivers/net/Makefile	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/net/Makefile	Tue Apr 23 09:37:28 2002
@@ -73,7 +73,6 @@
 obj-$(CONFIG_DM9102) += dmfe.o
 obj-$(CONFIG_YELLOWFIN) += yellowfin.o
 obj-$(CONFIG_ACENIC) += acenic.o
-obj-$(CONFIG_VETH) += veth.o
 obj-$(CONFIG_NATSEMI) += natsemi.o
 obj-$(CONFIG_NS83820) += ns83820.o
 obj-$(CONFIG_STNIC) += stnic.o 8390.o
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/acenic.c linuxppc64_2_4/drivers/net/acenic.c
--- linux-2.4.19/drivers/net/acenic.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/net/acenic.c	Fri Jun 21 14:49:42 2002
@@ -1933,6 +1933,7 @@
 	atomic_add(i, &ap->cur_rx_bufs);
 	ap->rx_std_skbprd = idx;
 
+        mb();  // DRENG
 	if (ACE_IS_TIGON_I(ap)) {
 		struct cmd cmd;
 		cmd.evt = C_SET_RX_PRD_IDX;
@@ -2305,7 +2306,7 @@
 		writel(idx, &regs->RxRetCsm);
 	}
 	ap->cur_rx = idx;
-
+        mb();  // DRENG
 	return;
  error:
 	idx = rxretprd;
@@ -2831,6 +2832,7 @@
 
  	wmb();
  	ap->tx_prd = idx;
+        mb();  // DRENG prd must be visible before telling HW to advance
  	ace_set_txprd(regs, ap, idx);
 
 	if (flagsize & BD_FLG_COAL_NOW) {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/pcnet32.c linuxppc64_2_4/drivers/net/pcnet32.c
--- linux-2.4.19/drivers/net/pcnet32.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/net/pcnet32.c	Tue Aug 13 20:46:11 2002
@@ -55,6 +55,8 @@
 #include <linux/skbuff.h>
 #include <linux/spinlock.h>
 
+#define DO_DXSUFLO
+
 /*
  * PCI device identifiers for "new style" Linux PCI Device Drivers
  */
@@ -221,8 +223,8 @@
  * That translates to 2 (4 == 2^^2) and 4 (16 == 2^^4).
  */
 #ifndef PCNET32_LOG_TX_BUFFERS
-#define PCNET32_LOG_TX_BUFFERS 4
-#define PCNET32_LOG_RX_BUFFERS 5
+#define PCNET32_LOG_TX_BUFFERS 6
+#define PCNET32_LOG_RX_BUFFERS 7
 #endif
 
 #define TX_RING_SIZE		(1 << (PCNET32_LOG_TX_BUFFERS))
@@ -233,7 +235,7 @@
 #define RX_RING_MOD_MASK	(RX_RING_SIZE - 1)
 #define RX_RING_LEN_BITS	((PCNET32_LOG_RX_BUFFERS) << 4)
 
-#define PKT_BUF_SZ		1544
+#define PKT_BUF_SZ             2048
 
 /* Offsets from base I/O address. */
 #define PCNET32_WIO_RDP		0x10
@@ -294,34 +296,34 @@
  */
 struct pcnet32_private {
     /* The Tx and Rx ring entries must be aligned on 16-byte boundaries in 32bit mode. */
-    struct pcnet32_rx_head    rx_ring[RX_RING_SIZE];
-    struct pcnet32_tx_head    tx_ring[TX_RING_SIZE];
-    struct pcnet32_init_block init_block;
-    dma_addr_t 		dma_addr;	/* DMA address of beginning of this object, 
-					   returned by pci_alloc_consistent */
-    struct pci_dev	*pci_dev;	/* Pointer to the associated pci device structure */
-    const char		*name;
+    struct pcnet32_rx_head   rx_ring[RX_RING_SIZE];
+    struct pcnet32_tx_head   tx_ring[TX_RING_SIZE];
+    struct pcnet32_init_block	init_block;
+    dma_addr_t dma_addr;		/* DMA address of beginning of this object, 
+                                    returned by pci_alloc_consistent */
+    struct pci_dev *pci_dev;		/* Pointer to the associated pci device structure */
+    const char *name;
     /* The saved address of a sent-in-place packet/buffer, for skfree(). */
-    struct sk_buff	*tx_skbuff[TX_RING_SIZE];
-    struct sk_buff	*rx_skbuff[RX_RING_SIZE];
-    dma_addr_t		tx_dma_addr[TX_RING_SIZE];
-    dma_addr_t		rx_dma_addr[RX_RING_SIZE];
+    struct sk_buff *tx_skbuff[TX_RING_SIZE];
+    struct sk_buff *rx_skbuff[RX_RING_SIZE];
+    dma_addr_t tx_dma_addr[TX_RING_SIZE];
+    dma_addr_t rx_dma_addr[RX_RING_SIZE];
     struct pcnet32_access a;
-    spinlock_t		lock;		/* Guard lock */
-    unsigned int	cur_rx, cur_tx;	/* The next free ring entry */
-    unsigned int	dirty_rx, dirty_tx; /* The ring entries to be free()ed. */
+    spinlock_t lock;					/* Guard lock */
+    unsigned int cur_rx, cur_tx;		/* The next free ring entry */
+    unsigned int dirty_rx, dirty_tx;	/* The ring entries to be free()ed. */
     struct net_device_stats stats;
-    char		tx_full;
-    int			options;
-    int	shared_irq:1,			/* shared irq possible */
+    char tx_full;
+    int	 options;
+    int	 shared_irq:1,			/* shared irq possible */
 	ltint:1,			/* enable TxDone-intr inhibitor */
-	dxsuflo:1,			/* disable transmit stop on uflo */
-	mii:1;				/* mii port available */
-    struct net_device	*next;
+	dxsuflo:1,                      /* disable transmit stop on uflo */
+	mii:1;                          /* mii port available */
+    struct net_device *next;
     struct mii_if_info mii_if;
 };
 
-static void pcnet32_probe_vlbus(void);
+static void  pcnet32_probe_vlbus(void);
 static int  pcnet32_probe_pci(struct pci_dev *, const struct pci_device_id *);
 static int  pcnet32_probe1(unsigned long, unsigned int, int, struct pci_dev *);
 static int  pcnet32_open(struct net_device *);
@@ -342,7 +344,6 @@
     PCI_ADDR0=0x10<<0, PCI_ADDR1=0x10<<1, PCI_ADDR2=0x10<<2, PCI_ADDR3=0x10<<3,
 };
 
-
 static u16 pcnet32_wio_read_csr (unsigned long addr, int index)
 {
     outw (index, addr+PCNET32_WIO_RAP);
@@ -444,12 +445,12 @@
 }
 
 static struct pcnet32_access pcnet32_dwio = {
-    read_csr:	pcnet32_dwio_read_csr,
-    write_csr:	pcnet32_dwio_write_csr,
-    read_bcr:	pcnet32_dwio_read_bcr,
-    write_bcr:	pcnet32_dwio_write_bcr,
-    read_rap:	pcnet32_dwio_read_rap,
-    write_rap:	pcnet32_dwio_write_rap,
+    read_csr: pcnet32_dwio_read_csr,
+    write_csr:  pcnet32_dwio_write_csr,
+    read_bcr: pcnet32_dwio_read_bcr,
+    write_bcr: pcnet32_dwio_write_bcr,
+    read_rap: pcnet32_dwio_read_rap,
+    write_rap: pcnet32_dwio_write_rap,
     reset:	pcnet32_dwio_reset
 };
 
@@ -467,13 +468,14 @@
     for (port = pcnet32_portlist; (ioaddr = *port); port++) {
 	if (!check_region(ioaddr, PCNET32_TOTAL_SIZE)) {
 	    /* check if there is really a pcnet chip on that ioaddr */
-	    if ((inb(ioaddr + 14) == 0x57) && (inb(ioaddr + 15) == 0x57))
+	    if ((inb(ioaddr + 14) == 0x57) && (inb(ioaddr + 15) == 0x57)) 
 		pcnet32_probe1(ioaddr, 0, 0, NULL);
 	}
     }
 }
 
 
+
 static int __devinit
 pcnet32_probe_pci(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
@@ -482,8 +484,8 @@
 
     err = pci_enable_device(pdev);
     if (err < 0) {
-	printk(KERN_ERR PFX "failed to enable device -- err=%d\n", err);
-	return err;
+      printk(KERN_ERR PFX "failed to enable device -- err=%d\n", err);
+      return err;
     }
     pci_set_master(pdev);
 
@@ -492,7 +494,7 @@
         printk (KERN_ERR PFX "card has no PCI IO resources, aborting\n");
         return -ENODEV;
     }
-    
+	
     if (!pci_dma_supported(pdev, PCNET32_DMA_MASK)) {
 	printk(KERN_ERR PFX "architecture does not support 32bit PCI busmaster DMA\n");
 	return -ENODEV;
@@ -508,12 +510,12 @@
  */
 static int __devinit
 pcnet32_probe1(unsigned long ioaddr, unsigned int irq_line, int shared,
-		struct pci_dev *pdev)
+              struct pci_dev *pdev)
 {
     struct pcnet32_private *lp;
     dma_addr_t lp_dma_addr;
-    int i, media;
-    int fdx, mii, fset, dxsuflo, ltint;
+    int i,media;
+    int fdx = 0, mii = 0, fset = 0, dxsuflo=0, ltint=0;
     int chip_version;
     char *chipname;
     struct net_device *dev;
@@ -521,25 +523,27 @@
     u8 promaddr[6];
 
     /* reset the chip */
+    pcnet32_dwio_reset(ioaddr);
+	udelay (100);
     pcnet32_wio_reset(ioaddr);
 
-    /* NOTE: 16-bit check is first, otherwise some older PCnet chips fail */
-    if (pcnet32_wio_read_csr(ioaddr, 0) == 4 && pcnet32_wio_check(ioaddr)) {
-	a = &pcnet32_wio;
+    /* Important to do the check for dwio mode first. */
+    if (pcnet32_dwio_read_csr(ioaddr, 0) == 4 && pcnet32_dwio_check(ioaddr)) {
+        a = &pcnet32_dwio;
     } else {
-	pcnet32_dwio_reset(ioaddr);
-	if (pcnet32_dwio_read_csr(ioaddr, 0) == 4 && pcnet32_dwio_check(ioaddr)) {
-	    a = &pcnet32_dwio;
+        if (pcnet32_wio_read_csr(ioaddr, 0) == 4 && 
+	    pcnet32_wio_check(ioaddr)) {
+	    a = &pcnet32_wio;
 	} else
 	    return -ENODEV;
     }
 
-    chip_version = a->read_csr(ioaddr, 88) | (a->read_csr(ioaddr,89) << 16);
+    chip_version = a->read_csr(ioaddr, 88) | (a->read_csr (ioaddr,89) << 16);
     if (pcnet32_debug > 2)
 	printk(KERN_INFO "  PCnet chip version is %#x.\n", chip_version);
     if ((chip_version & 0xfff) != 0x003)
 	return -ENODEV;
-    
+	
     /* initialize variables */
     fdx = mii = fset = dxsuflo = ltint = 0;
     chip_version = (chip_version >> 12) & 0xffff;
@@ -609,15 +613,29 @@
      *	one for latency - although on PCI this isnt a big loss. Older chips 
      *	have FIFO's smaller than a packet, so you can't do this.
      */
-	 
+    /*
+     * UPDATE
+     * Got to make sure that BCR18:MEMCMD, BCR18:BREADE, BCR18:BWRITE are
+     * set on a PCI
+     */
     if(fset)
     {
-	a->write_bcr(ioaddr, 18, (a->read_bcr(ioaddr, 18) | 0x0800));
-	a->write_csr(ioaddr, 80, (a->read_csr(ioaddr, 80) & 0x0C00) | 0x0c00);
-	dxsuflo = 1;
-	ltint = 1;
+    a->write_bcr(ioaddr, 18, (a->read_bcr(ioaddr, 18) | 0xA60));
+    a->write_csr(ioaddr, 3, 0x2eb7);
+    a->write_csr(ioaddr, 4, 0x32ea);
+    a->write_csr(ioaddr, 80, 0x3f00);
+	
+    dxsuflo = 1;
+    ltint = 1;
     }
     
+    if(ltint) {
+	    /* we need to enable timer interrupts to prevent skbuffs from 
+	     * being left on the tx ring for forever if no one else is tx'ing 
+	     * data - set the timer period to 122ms */
+	    a->write_bcr(ioaddr, 31, 0x253B);
+    }
+
     dev = alloc_etherdev(0);
     if(!dev)
 	return -ENOMEM;
@@ -631,6 +649,7 @@
      * they disagree with the CSRs.  Either way, we use the CSR values, and
      * double check that they are valid.
      */
+#ifndef CONFIG_PPC
     for (i = 0; i < 3; i++) {
 	unsigned int val;
 	val = a->read_csr(ioaddr, i+12) & 0x0ffff;
@@ -638,28 +657,29 @@
 	dev->dev_addr[2*i] = val & 0x0ff;
 	dev->dev_addr[2*i+1] = (val >> 8) & 0x0ff;
     }
+#endif
 
     /* read PROM address and compare with CSR address */
-    for (i = 0; i < 6; i++)
+    for (i = 0; i < 6; i++) 
 	promaddr[i] = inb(ioaddr + i);
-    
+
     if( memcmp( promaddr, dev->dev_addr, 6)
-	|| !is_valid_ether_addr(dev->dev_addr) ) {
-#ifndef __powerpc__
+      || !is_valid_ether_addr(dev->dev_addr) ) {
+#ifndef __powerpc__ 
 	if( is_valid_ether_addr(promaddr) ){
 #else
-	if( !is_valid_ether_addr(dev->dev_addr)
-	    && is_valid_ether_addr(promaddr)) {
+	if (!is_valid_ether_addr(dev->dev_addr)
+		&& is_valid_ether_addr(promaddr)) {
 #endif
-	    printk(" warning: CSR address invalid,\n");
-	    printk(KERN_INFO "    using instead PROM address of");
-	    memcpy(dev->dev_addr, promaddr, 6);
+		printk(" warning: CSR address invalid,\n");
+		printk(KERN_INFO " using instead PROM address of");
+		memcpy(dev->dev_addr, promaddr, 6);
 	}
-    }
+    }	    	    
 
     /* if the ethernet address is not valid, force to 00:00:00:00:00:00 */
     if( !is_valid_ether_addr(dev->dev_addr) )
-	memset(dev->dev_addr, 0, sizeof(dev->dev_addr));
+	memset(dev->dev_addr, 0, sizeof(dev->dev_addr));	
 
     for (i = 0; i < 6; i++)
 	printk(" %2.2x", dev->dev_addr[i] );
@@ -799,7 +819,6 @@
     return 0;
 }
 
-
 static int
 pcnet32_open(struct net_device *dev)
 {
@@ -892,7 +911,7 @@
     lp->init_block.filter[1] = 0x00000000;
     if (pcnet32_init_ring(dev))
 	return -ENOMEM;
-    
+
     /* Re-initialize the PCNET32, and start it when done. */
     lp->a.write_csr (ioaddr, 1, (lp->dma_addr + offsetof(struct pcnet32_private, init_block)) &0xffff);
     lp->a.write_csr (ioaddr, 2, (lp->dma_addr + offsetof(struct pcnet32_private, init_block)) >> 16);
@@ -900,6 +919,11 @@
     lp->a.write_csr (ioaddr, 4, 0x0915);
     lp->a.write_csr (ioaddr, 0, 0x0001);
 
+    if(lp->ltint) {
+	    /* start the software timer */
+	    lp->a.write_csr(ioaddr, 7, 0x0400); /* set STINTE */
+    }
+
     netif_start_queue(dev);
 
     i = 0;
@@ -974,7 +998,10 @@
 	    }
 	    skb_reserve (rx_skbuff, 2);
 	}
-        lp->rx_dma_addr[i] = pci_map_single(lp->pci_dev, rx_skbuff->tail, rx_skbuff->len, PCI_DMA_FROMDEVICE);
+
+	if (lp->rx_dma_addr[i] == NULL) 
+		lp->rx_dma_addr[i] = pci_map_single(lp->pci_dev, rx_skbuff->tail, PKT_BUF_SZ-2, PCI_DMA_FROMDEVICE);
+
 	lp->rx_ring[i].base = (u32)le32_to_cpu(lp->rx_dma_addr[i]);
 	lp->rx_ring[i].buf_length = le16_to_cpu(-PKT_BUF_SZ);
 	lp->rx_ring[i].status = le16_to_cpu(0x8000);
@@ -1007,9 +1034,11 @@
 	return;
     
     /* ReInit Ring */
+    /* pause for a short time, allow the prior write_csr time to finish. */
+    udelay(16);
     lp->a.write_csr (ioaddr, 0, 1);
     i = 0;
-    while (i++ < 100)
+    while (1)
 	if (lp->a.read_csr (ioaddr, 0) & 0x0100)
 	    break;
 
@@ -1023,10 +1052,10 @@
     struct pcnet32_private *lp = dev->priv;
     unsigned long ioaddr = dev->base_addr, flags;
 
-    spin_lock_irqsave(&lp->lock, flags);
+        spin_lock_irqsave(&lp->lock, flags);
     /* Transmitter timeout, serious problems. */
 	printk(KERN_ERR "%s: transmit timed out, status %4.4x, resetting.\n",
-	       dev->name, lp->a.read_csr(ioaddr, 0));
+	       dev->name, lp->a.read_csr (ioaddr, 0));
 	lp->a.write_csr (ioaddr, 0, 0x0004);
 	lp->stats.tx_errors++;
 	if (pcnet32_debug > 2) {
@@ -1049,7 +1078,7 @@
 	dev->trans_start = jiffies;
 	netif_start_queue(dev);
 
-	spin_unlock_irqrestore(&lp->lock, flags);
+        spin_unlock_irqrestore(&lp->lock, flags);
 }
 
 
@@ -1064,7 +1093,7 @@
 
     if (pcnet32_debug > 3) {
 	printk(KERN_DEBUG "%s: pcnet32_start_xmit() called, csr0 %4.4x.\n",
-	       dev->name, lp->a.read_csr(ioaddr, 0));
+	       dev->name, lp->a.read_csr (ioaddr, 0));
     }
 
     spin_lock_irqsave(&lp->lock, flags);
@@ -1127,7 +1156,7 @@
     struct net_device *dev = dev_id;
     struct pcnet32_private *lp;
     unsigned long ioaddr;
-    u16 csr0,rap;
+    u16 csr0,csr7,rap;
     int boguscnt =  max_interrupt_work;
     int must_restart;
 
@@ -1143,10 +1172,17 @@
     spin_lock(&lp->lock);
     
     rap = lp->a.read_rap(ioaddr);
-    while ((csr0 = lp->a.read_csr (ioaddr, 0)) & 0x8600 && --boguscnt >= 0) {
+    csr0 = lp->a.read_csr (ioaddr, 0);
+    csr7 = lp->ltint ? lp->a.read_csr(ioaddr, 7) : 0;
+    
+    while ((csr0 & 0x8600 || csr7 & 0x0800) && --boguscnt >= 0) {	    
 	/* Acknowledge all of the current interrupt sources ASAP. */
 	lp->a.write_csr (ioaddr, 0, csr0 & ~0x004f);
 
+	if(csr7 & 0x0800) {
+		lp->a.write_csr(ioaddr, 7, csr7);
+	}
+
 	must_restart = 0;
 
 	if (pcnet32_debug > 5)
@@ -1156,7 +1192,7 @@
 	if (csr0 & 0x0400)		/* Rx interrupt */
 	    pcnet32_rx(dev);
 
-	if (csr0 & 0x0200) {		/* Tx-done interrupt */
+	if (csr0 & 0x0200 || csr7 & 0x0800 ) { /* Tx-done or Timer interrupt */
 	    unsigned int dirty_tx = lp->dirty_tx;
 
 	    while (dirty_tx < lp->cur_tx) {
@@ -1255,6 +1291,9 @@
 	    lp->a.write_csr (ioaddr, 0, 0x0004);
 	    pcnet32_restart(dev, 0x0002);
 	}
+
+	csr0 = lp->a.read_csr (ioaddr, 0);
+	csr7 = lp->ltint ? lp->a.read_csr(ioaddr, 7) : 0;
     }
 
     /* Clear any other interrupt, and set interrupt enable. */
@@ -1309,12 +1348,12 @@
 		    if ((newskb = dev_alloc_skb (PKT_BUF_SZ))) {
 			skb_reserve (newskb, 2);
 			skb = lp->rx_skbuff[entry];
-			pci_unmap_single(lp->pci_dev, lp->rx_dma_addr[entry], skb->len, PCI_DMA_FROMDEVICE);
 			skb_put (skb, pkt_len);
 			lp->rx_skbuff[entry] = newskb;
 			newskb->dev = dev;
+			pci_unmap_single(lp->pci_dev, lp->rx_dma_addr[entry], PKT_BUF_SZ-2, PCI_DMA_FROMDEVICE);
                         lp->rx_dma_addr[entry] = 
-				pci_map_single(lp->pci_dev, newskb->tail,
+				pci_map_single(lp->pci_dev, newskb->tail, 
 					newskb->len, PCI_DMA_FROMDEVICE);
 			lp->rx_ring[entry].base = le32_to_cpu(lp->rx_dma_addr[entry]);
 			rx_in_place = 1;
@@ -1368,7 +1407,7 @@
 static int
 pcnet32_close(struct net_device *dev)
 {
-    unsigned long ioaddr = dev->base_addr;
+    unsigned long ioaddr = dev->base_addr, flags;
     struct pcnet32_private *lp = dev->priv;
     int i;
 
@@ -1383,19 +1422,35 @@
     /* We stop the PCNET32 here -- it occasionally polls memory if we don't. */
     lp->a.write_csr (ioaddr, 0, 0x0004);
 
+
+    /* Disable timer interrupts */
+    if(lp->ltint) {
+	    lp->a.write_csr (ioaddr, 7, 0x0000);
+    }
+
     /*
      * Switch back to 16bit mode to avoid problems with dumb 
      * DOS packet driver after a warm reboot
      */
     lp->a.write_bcr (ioaddr, 20, 4);
 
+    /*
+     *	FIXME: What happens if the bcr write is posted, the buffers are
+     *	freed and there is still incoming DMA traffic
+     */
+
+#warning "PCI posting bug"
+
     free_irq(dev->irq, dev);
-    
+   
+    /* Lock after free_irq to avoid deadlock with interrupt handler. */
+    spin_lock_irqsave(&lp->lock, flags);
+
     /* free all allocated skbuffs */
     for (i = 0; i < RX_RING_SIZE; i++) {
 	lp->rx_ring[i].status = 0;			    
 	if (lp->rx_skbuff[i]) {
-            pci_unmap_single(lp->pci_dev, lp->rx_dma_addr[i], lp->rx_skbuff[i]->len, PCI_DMA_FROMDEVICE);
+            pci_unmap_single(lp->pci_dev, lp->rx_dma_addr[i], PKT_BUF_SZ-2, PCI_DMA_FROMDEVICE);
 	    dev_kfree_skb(lp->rx_skbuff[i]);
         }
 	lp->rx_skbuff[i] = NULL;
@@ -1411,6 +1466,8 @@
         lp->tx_dma_addr[i] = 0;
     }
     
+    spin_unlock_irqrestore(&lp->lock, flags);
+
     MOD_DEC_USE_COUNT;
 
     return 0;
@@ -1504,13 +1561,13 @@
 
 	if (!lp->mii)
 		return 0;
-		
+
 	phyaddr = lp->a.read_bcr(ioaddr, 33);
 
-	lp->a.write_bcr(ioaddr, 33, ((phy_id & 0x1f) << 5) | (reg_num & 0x1f));
+	lp->a.write_bcr(ioaddr, 33, ((phy_id & 0x1f) << 5) | (reg_num & 0x1f)); 
 	val_out = lp->a.read_bcr(ioaddr, 34);
 	lp->a.write_bcr(ioaddr, 33, phyaddr);
-	
+
 	return val_out;
 }
 
@@ -1522,7 +1579,7 @@
 
 	if (!lp->mii)
 		return;
-		
+
 	phyaddr = lp->a.read_bcr(ioaddr, 33);
 
 	lp->a.write_bcr(ioaddr, 33, ((phy_id & 0x1f) << 5) | (reg_num & 0x1f));
@@ -1548,76 +1605,76 @@
 		return -EFAULT;
 
 	switch (ethcmd) {
-	case ETHTOOL_GDRVINFO: {
-		struct ethtool_drvinfo info = { ETHTOOL_GDRVINFO };
-		strcpy (info.driver, DRV_NAME);
-		strcpy (info.version, DRV_VERSION);
-		if (lp->pci_dev)
-			strcpy (info.bus_info, lp->pci_dev->slot_name);
-		else
-			sprintf(info.bus_info, "VLB 0x%lx", dev->base_addr);
-		if (copy_to_user (useraddr, &info, sizeof (info)))
-			return -EFAULT;
-		return 0;
-	}
-
-	/* get settings */
-	case ETHTOOL_GSET: {
-		struct ethtool_cmd ecmd = { ETHTOOL_GSET };
-		spin_lock_irq(&lp->lock);
-		mii_ethtool_gset(&lp->mii_if, &ecmd);
-		spin_unlock_irq(&lp->lock);
-		if (copy_to_user(useraddr, &ecmd, sizeof(ecmd)))
-			return -EFAULT;
-		return 0;
-	}
-	/* set settings */
-	case ETHTOOL_SSET: {
-		int r;
-		struct ethtool_cmd ecmd;
-		if (copy_from_user(&ecmd, useraddr, sizeof(ecmd)))
-			return -EFAULT;
-		spin_lock_irq(&lp->lock);
-		r = mii_ethtool_sset(&lp->mii_if, &ecmd);
-		spin_unlock_irq(&lp->lock);
-		return r;
-	}
-	/* restart autonegotiation */
-	case ETHTOOL_NWAY_RST: {
-		return mii_nway_restart(&lp->mii_if);
-	}
-	/* get link status */
-	case ETHTOOL_GLINK: {
-		struct ethtool_value edata = {ETHTOOL_GLINK};
-		edata.data = mii_link_ok(&lp->mii_if);
-		if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		case ETHTOOL_GDRVINFO: {
+			struct ethtool_drvinfo info = { ETHTOOL_GDRVINFO };
+			strcpy (info.driver, DRV_NAME);
+			strcpy (info.version, DRV_VERSION);
+			if (lp->pci_dev)
+				strcpy (info.bus_info, lp->pci_dev->slot_name);
+			else
+				sprintf(info.bus_info, "VLB 0x%lx", dev->base_addr);
+			if (copy_to_user (useraddr, &info, sizeof (info)))
+				return -EFAULT;
+			return 0;
+		}
+		/* get settings */
+		case ETHTOOL_GSET: {
+			struct ethtool_cmd ecmd = { ETHTOOL_GSET };
+			spin_lock_irq(&lp->lock);
+			mii_ethtool_gset(&lp->mii_if, &ecmd);
+			spin_unlock_irq(&lp->lock);
+			if (copy_to_user(useraddr, &ecmd, sizeof(ecmd)))
+				return -EFAULT;
+			return 0;
+		}
+		/* set settings */
+		case ETHTOOL_SSET: {
+			int r;
+			struct ethtool_cmd ecmd;
+			if (copy_from_user(&ecmd, useraddr, sizeof(ecmd)))
+				return -EFAULT;
+			spin_lock_irq(&lp->lock);
+			r = mii_ethtool_sset(&lp->mii_if, &ecmd);
+			spin_unlock_irq(&lp->lock);
+			return r;
+		}
+		/* restart autonegotiation */
+		case ETHTOOL_NWAY_RST: {
+			return mii_nway_restart(&lp->mii_if);
+		}
+		/* get link status */
+		case ETHTOOL_GLINK: {
+			struct ethtool_value edata = {ETHTOOL_GLINK};
+			edata.data = mii_link_ok(&lp->mii_if);
+			if (copy_to_user(useraddr, &edata, sizeof(edata)))
 			return -EFAULT;
-		return 0;
-	}
+			return 0;
+		}
 
-	/* get message-level */
-	case ETHTOOL_GMSGLVL: {
-		struct ethtool_value edata = {ETHTOOL_GMSGLVL};
-		edata.data = pcnet32_debug;
-		if (copy_to_user(useraddr, &edata, sizeof(edata)))
-			return -EFAULT;
-		return 0;
-	}
-	/* set message-level */
-	case ETHTOOL_SMSGLVL: {
-		struct ethtool_value edata;
-		if (copy_from_user(&edata, useraddr, sizeof(edata)))
-			return -EFAULT;
-		pcnet32_debug = edata.data;
-		return 0;
-	}
-	default:
-		break;
+		/* get message-level */
+		case ETHTOOL_GMSGLVL: {
+			struct ethtool_value edata = {ETHTOOL_GMSGLVL};
+			edata.data = pcnet32_debug;
+			if (copy_to_user(useraddr, &edata, sizeof(edata)))
+				return -EFAULT;
+			return 0;
+		}
+		/* set message-level */
+		case ETHTOOL_SMSGLVL: {
+			struct ethtool_value edata;
+			if (copy_from_user(&edata, useraddr, sizeof(edata)))
+				return -EFAULT;
+			pcnet32_debug = edata.data;
+			return 0;
+		}
+		default:
+			break;
 	}
 
-	return -EOPNOTSUPP;
+return -EOPNOTSUPP;
 }
 
+
 static int pcnet32_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 {
     unsigned long ioaddr = dev->base_addr;
@@ -1625,8 +1682,8 @@
     struct mii_ioctl_data *data = (struct mii_ioctl_data *)&rq->ifr_data;
     int phyaddr = lp->a.read_bcr (ioaddr, 33);
 
-    if (cmd == SIOCETHTOOL)
-	return pcnet32_ethtool_ioctl(dev, (void *) rq->ifr_data);
+	if (cmd == SIOCETHTOOL)
+		return pcnet32_ethtool_ioctl(dev, (void *) rq->ifr_data);
 
     if (lp->mii) {
 	switch(cmd) {
@@ -1645,6 +1702,7 @@
 	    lp->a.write_bcr (ioaddr, 34, data->val_in);
 	    lp->a.write_bcr (ioaddr, 33, phyaddr);
 	    return 0;
+
 	default:
 	    return -EOPNOTSUPP;
 	}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/starfire_firmware.pl linuxppc64_2_4/drivers/net/starfire_firmware.pl
--- linux-2.4.19/drivers/net/starfire_firmware.pl	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/starfire_firmware.pl	Mon Jun 18 18:18:54 2001
@@ -0,0 +1,31 @@
+#!/usr/bin/perl
+
+# This script can be used to generate a new starfire_firmware.h
+# from GFP_RX.DAT and GFP_TX.DAT, files included with the DDK
+# and also with the Novell drivers.
+
+open FW, "GFP_RX.DAT" || die;
+open FWH, ">starfire_firmware.h" || die;
+
+printf(FWH "static u32 firmware_rx[] = {\n");
+$counter = 0;
+while ($foo = <FW>) {
+  chomp;
+  printf(FWH "  0x%s, 0x0000%s,\n", substr($foo, 4, 8), substr($foo, 0, 4));
+  $counter++;
+}
+
+close FW;
+open FW, "GFP_TX.DAT" || die;
+
+printf(FWH "};\t/* %d Rx instructions */\n#define FIRMWARE_RX_SIZE %d\n\nstatic u32 firmware_tx[] = {\n", $counter, $counter);
+$counter = 0;
+while ($foo = <FW>) {
+  chomp;
+  printf(FWH "  0x%s, 0x0000%s,\n", substr($foo, 4, 8), substr($foo, 0, 4));
+  $counter++;
+}
+
+close FW;
+printf(FWH "};\t/* %d Tx instructions */\n#define FIRMWARE_TX_SIZE %d\n", $counter, $counter);
+close(FWH);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/tokenring/olympic.c linuxppc64_2_4/drivers/net/tokenring/olympic.c
--- linux-2.4.19/drivers/net/tokenring/olympic.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/net/tokenring/olympic.c	Wed Aug 21 13:06:50 2002
@@ -311,20 +311,20 @@
 	writel(readl(olympic_mmio+BCTL)|BCTL_MIMREB,olympic_mmio+BCTL);
 	
 	if (olympic_priv->olympic_ring_speed  == 0) { /* Autosense */
-		writel(readl(olympic_mmio+GPR)|GPR_AUTOSENSE,olympic_mmio+GPR);
+		writew(readw(olympic_mmio+GPR)|GPR_AUTOSENSE,olympic_mmio+GPR);
 		if (olympic_priv->olympic_message_level) 
 			printk(KERN_INFO "%s: Ringspeed autosense mode on\n",olympic_priv->olympic_card_name);
 	} else if (olympic_priv->olympic_ring_speed == 16) {
 		if (olympic_priv->olympic_message_level) 
 			printk(KERN_INFO "%s: Trying to open at 16 Mbps as requested\n", olympic_priv->olympic_card_name);
-		writel(GPR_16MBPS, olympic_mmio+GPR);
+		writew(GPR_16MBPS, olympic_mmio+GPR);
 	} else if (olympic_priv->olympic_ring_speed == 4) {
 		if (olympic_priv->olympic_message_level) 
 			printk(KERN_INFO "%s: Trying to open at 4 Mbps as requested\n", olympic_priv->olympic_card_name) ; 
-		writel(0, olympic_mmio+GPR);
+		writew(0, olympic_mmio+GPR);
 	} 
 	
-	writel(readl(olympic_mmio+GPR)|GPR_NEPTUNE_BF,olympic_mmio+GPR);
+	writew(readw(olympic_mmio+GPR)|GPR_NEPTUNE_BF,olympic_mmio+GPR);
 
 #if OLYMPIC_DEBUG
 	printk("GPR = %x\n",readw(olympic_mmio + GPR) ) ; 
@@ -431,15 +431,20 @@
 	printk("Before the open command \n");
 #endif	
 	do {
-		int i;
-
-		for(i=0;i<SRB_COMMAND_SIZE;i+=4)
-			writel(0,init_srb+i);
-		if(SRB_COMMAND_SIZE & 2)
-			writew(0,init_srb+(SRB_COMMAND_SIZE & ~3));
-		if(SRB_COMMAND_SIZE & 1)
-			writeb(0,init_srb+(SRB_COMMAND_SIZE & ~1));
+                /* Clear SRB Command                */
+		memset_io(init_srb,0,SRB_COMMAND_SIZE);
 
+		/****************************************************/
+		/* Replaced with memset_io macro.                   */ 
+		/*int i;                                            */
+		/*for(i=0;i<SRB_COMMAND_SIZE;i+=4)                  */
+		/*	writel(0,init_srb+i);                       */
+		/*if(SRB_COMMAND_SIZE & 2)                          */
+		/*	writew(0,init_srb+(SRB_COMMAND_SIZE & ~3)); */
+		/*if(SRB_COMMAND_SIZE & 1)                          */
+		/*	writeb(0,init_srb+(SRB_COMMAND_SIZE & ~1)); */
+		/****************************************************/
+ 
 		writeb(SRB_OPEN_ADAPTER,init_srb) ; 	/* open */
 		writeb(OLYMPIC_CLEAR_RET_CODE,init_srb+2);
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/8253xmac.c linuxppc64_2_4/drivers/net/wan/8253x/8253xmac.c
--- linux-2.4.19/drivers/net/wan/8253x/8253xmac.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/8253xmac.c	Mon May 13 16:39:14 2002
@@ -0,0 +1,205 @@
+/* 
+ * Copyright (C) 2001 By Joachim Martillo, Telford Tools, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ **/
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/ioctl.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include "ring.h"
+#include <linux/socket.h>
+#include <net/if.h>
+
+				/* This application sets a pseudo mac address that */
+				/* can be used when using the synchronous port in */
+				/* synchronous serial ethnernet emulation mode */
+
+int main(int argc, char **argv)
+{
+  int fd;
+  struct ifreq request;
+  PSEUDOMAC pmac;
+  char buffer[200];
+  int count;
+  unsigned int uppernib;
+  unsigned int lowernib;
+
+  
+  if(argc != 3)
+    {
+      fprintf(stderr, "Syntax: %s {ifname} {macaddr}.\n", *argv);
+      fflush(stdout);
+      exit(-1);
+    }
+  fd = socket(AF_INET, SOCK_DGRAM, 0);
+  if(fd < 0)
+    {
+      perror("socket failed.");
+      fflush(stdout);
+      exit(-2);
+    }
+
+  strcpy(request.ifr_ifrn.ifrn_name, argv[1]); /* requests go through the socket layer */
+
+  request.ifr_ifru.ifru_data = (char*) &pmac;
+
+  if(ioctl(fd, SAB8253XGETMAC, &request) < 0)
+    {
+      perror("ioctl failed.");
+      fflush(stdout);
+      exit(-3);
+    }
+  for(count = 0; count < 6; ++count)
+    {
+      buffer[2*count] = (pmac.addr[count] >> 4);
+      buffer[2*count] &= 0x0F;
+      if(buffer[2*count] < 10)
+	{
+	  buffer[2*count] += '0';
+	}
+      else
+	{
+	  buffer[2*count] += 'a';
+	}
+      buffer[(2*count)+1] = (pmac.addr[count] & 0x0F);
+      if(buffer[(2*count)+1] < 10)
+	{
+	  buffer[(2*count)+1] += '0';
+	}
+      else
+	{
+	  buffer[(2*count)+1] += 'a';
+	}
+    }
+  buffer[12] = 0;
+  printf("Old mac addres is %s.\n", buffer);
+  if(strlen(argv[2]) != 12)
+    {
+      printf("Bad size mac address %s.\n", argv[2]);
+      fflush(stdout);
+      exit(-1);
+    }
+  for(count = 0; count < 6; ++ count)
+    {
+      uppernib = argv[2][2*count];
+      lowernib = argv[2][(2*count)+1];
+      
+      switch(uppernib)
+	{
+	case '0':
+	case '1':
+	case '2':
+	case '3':
+	case '4':
+	case '5':
+	case '6':
+	case '7':
+	case '8':
+	case '9':
+	  uppernib -= '0';
+	  break;
+	case 'a':
+	case 'b':
+	case 'c':
+	case 'd':
+	case 'e':
+	case 'f':
+	  uppernib -= 'a';
+	  break;
+	case 'A':
+	case 'B':
+	case 'C':
+	case 'D':
+	case 'E':
+	case 'F':
+	  uppernib -= 'A';
+	  break;
+	default:
+	  uppernib = 0;
+	  break;
+	}
+      switch(lowernib)
+	{
+	case '0':
+	case '1':
+	case '2':
+	case '3':
+	case '4':
+	case '5':
+	case '6':
+	case '7':
+	case '8':
+	case '9':
+	  lowernib -= '0';
+	  break;
+	case 'a':
+	case 'b':
+	case 'c':
+	case 'd':
+	case 'e':
+	case 'f':
+	  lowernib -= 'a';
+	  break;
+	case 'A':
+	case 'B':
+	case 'C':
+	case 'D':
+	case 'E':
+	case 'F':
+	  lowernib -= 'A';
+	  break;
+	default:
+	  lowernib = 0;
+	  break;
+	}
+      pmac.addr[count] = ((uppernib << 4) | lowernib);
+    }
+    
+
+  if(ioctl(fd, SAB8253XSETMAC, &request) < 0) /* actually setting the mac address */
+    {
+      perror("ioctl failed.");
+      fflush(stdout);
+      exit(-2);
+    }
+
+  if(ioctl(fd, SAB8253XGETMAC, &request) < 0) /* getting it back so that value can be verified */
+    {
+      perror("ioctl failed.");
+      exit(-3);
+    }
+  for(count = 0; count < 6; ++count)
+    {
+      buffer[2*count] = (pmac.addr[count] >> 4);
+      buffer[2*count] &= 0x0F;
+      if(buffer[2*count] < 10)
+	{
+	  buffer[2*count] += '0';
+	}
+      else
+	{
+	  buffer[2*count] += 'a';
+	}
+      buffer[(2*count)+1] = (pmac.addr[count] & 0x0F);
+      if(buffer[(2*count)+1] < 10)
+	{
+	  buffer[(2*count)+1] += '0';
+	}
+      else
+	{
+	  buffer[(2*count)+1] += 'a';
+	}
+    }
+  buffer[12] = 0;
+  printf("New mac addres is %s.\n", buffer);
+  fflush(stdout);
+  exit(0);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/8253xmode.c linuxppc64_2_4/drivers/net/wan/8253x/8253xmode.c
--- linux-2.4.19/drivers/net/wan/8253x/8253xmode.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/8253xmode.c	Mon May 13 16:39:14 2002
@@ -0,0 +1,108 @@
+/* -*- linux-c -*- */
+/* 
+ * Copyright (C) 2001 By Joachim Martillo, Telford Tools, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ **/
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include "8253xioc.h"
+
+static char *signaling[] =
+{
+	"OFF",
+	"RS232",
+	"RS422",
+	"RS485",
+	"RS449",
+	"RS530",
+	"V.35"
+};
+
+				/* This application shows how to set sigmode
+				 * on those devices that support software
+				 * programmable signaling. */
+int main(int argc, char **argv)
+{
+	int fd;
+	unsigned int oldmode, newmode;
+	
+	if(argc != 3)
+	{
+		fprintf(stderr, "Syntax: %s {portname} {new mode}.\n", *argv);
+		fprintf(stderr, "{new mode} = off | 232 | 422 | 485 | 449 | 530 | v.35\n");
+		exit(-1);
+	}
+	fd = open(argv[1], O_RDWR);
+	if(fd < 0)
+	{
+		perror("open failed.");
+		exit(-2);
+	}
+	if(!strcmp("off", argv[2]))
+	{
+		newmode = SP502_OFF_MODE;
+	}
+	else if(!strcmp("232", argv[2]))
+	{
+		newmode = SP502_RS232_MODE;
+	}
+	else if(!strcmp("422", argv[2]))
+	{
+		newmode = SP502_RS422_MODE;
+	}
+	else if(!strcmp("485", argv[2]))
+	{
+		newmode = SP502_RS485_MODE;
+	}
+	else if(!strcmp("449", argv[2]))
+	{
+		newmode = SP502_RS449_MODE;
+	}
+	else if(!strcmp("530", argv[2]))
+	{
+		newmode = SP502_EIA530_MODE;
+	}
+	else if(!strcmp("v.35", argv[2]))
+	{
+		newmode = SP502_V35_MODE;
+	}
+	else
+	{
+		fprintf(stderr, "Unknown mode %s.\n", argv[2]);
+		fprintf(stderr, "Syntax: %s {portname} {new mode}.\n", *argv);
+		fprintf(stderr, "{new mode} = off | 232 | 422 | 485 | 449 | 530 | v.35\n");
+		exit(-1);
+	}
+	
+	/* get the current values */
+	if(ioctl(fd, ATIS_IOCGSIGMODE, &oldmode) < 0)
+	{
+		perror("ATIS_IOCGSIGMODE ioctl failed.");
+		exit(-3);
+	}
+	fprintf(stderr, "old mode = %s.\n", signaling[oldmode]);
+	
+	if(ioctl(fd, ATIS_IOCSSIGMODE, &newmode) < 0)
+	{
+		perror("ATIS_IOCSSIGMODE ioctl failed.");
+		exit(-3);
+	}
+
+	/* get the current values */
+	if(ioctl(fd, ATIS_IOCGSIGMODE, &oldmode) < 0)
+	{
+		perror("ATIS_IOCGSIGMODE ioctl failed.");
+		exit(-3);
+	}	
+	fprintf(stderr, "new mode = %s.\n", signaling[oldmode]);
+	fflush(stdout);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/8253xpeer.c linuxppc64_2_4/drivers/net/wan/8253x/8253xpeer.c
--- linux-2.4.19/drivers/net/wan/8253x/8253xpeer.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/8253xpeer.c	Mon May 13 16:39:14 2002
@@ -0,0 +1,113 @@
+/* -*- linux-c -*- */
+/* 
+ * Copyright (C) 2001 By Joachim Martillo, Telford Tools, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ **/
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include "8253xioc.h"
+#include <sys/poll.h>
+
+struct pollfd pollarray[2];
+
+/* This application sets up synchronous character mode and loosely
+ * emulates putmsg/getmsg use with read and write. */
+
+char buffer[8192];
+
+int main(int argc, char **argv)
+{
+	int fd;
+	int status;
+	int prompt = 1;
+	int count;
+	
+	if(argc != 2)
+	{
+		fprintf(stderr, "Syntax: %s {portname}\n", *argv);
+		exit(-1);
+	}
+	fd = open(argv[1], O_RDWR);
+	if(fd < 0)
+	{
+		perror("open failed.");
+		exit(-2);
+	}
+	do
+	{
+		if(prompt)
+		{
+			printf("Enter data: ");
+			fflush(stdout);
+			prompt = 0;
+		}
+		pollarray[0].fd = 0;
+		pollarray[0].events = POLLIN;
+		pollarray[0].revents = 0;
+		pollarray[1].fd = fd;
+		pollarray[1].events = POLLIN|POLLOUT;
+		pollarray[1].revents = 0;
+		status = poll(pollarray, 2, 10);
+		switch(status)
+		{
+		case 0:
+			break;
+			
+		case 1:
+		case 2:
+			if(pollarray[0].revents == POLLIN)
+			{
+				if(count = read(0, buffer, 150), count <= 0)
+				{
+					perror("unable to read stdio.\n");
+					exit(0);
+				}
+				buffer[count] = '\0';
+				if(count)
+				{
+					if(pollarray[1].revents & POLLOUT)
+					{
+						if(write(pollarray[1].fd, buffer, count) <= 0)
+						{
+							perror("unable to write protodevice.\n");
+							exit(-1);
+						}
+					}
+					else
+					{
+						printf("Write of protodevice would block.\n");
+						fflush(stdout);
+					}
+				}
+				prompt = 1;
+			}
+			if(pollarray[1].revents & POLLIN)
+			{
+				if(count = read(pollarray[1].fd, buffer, 8192), count <= 0)
+				{
+					perror("unable to read protodevice.\n");
+					exit(0);
+				}
+				buffer[count] = '\0';
+				printf("\nRead: %s", buffer);
+				fflush(stdout);
+				prompt = 1;
+			}
+			break;
+			
+		default:
+			break;
+		}
+	}
+	while(status >= 0);
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/8253xspeed.c linuxppc64_2_4/drivers/net/wan/8253x/8253xspeed.c
--- linux-2.4.19/drivers/net/wan/8253x/8253xspeed.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/8253xspeed.c	Mon May 13 16:39:14 2002
@@ -0,0 +1,91 @@
+/* -*- linux-c -*- */
+/* 
+ * Copyright (C) 2001 By Joachim Martillo, Telford Tools, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ **/
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include "8253xioc.h"
+
+
+/* This application shows how to set custom speeds/baudrates */
+
+int main(int argc, char **argv)
+{
+	int fd;
+	unsigned long oldspeed, newspeed;
+	char buffer[200];
+	int count;
+	long value;
+	int noprompt = 0;
+	int epromindex;
+	
+	if(argc < 2)
+	{
+		fprintf(stderr, "Syntax: %s {portname} [-n] {new speed}.\n", *argv);
+		exit(-1);
+	}
+	fd = open(argv[1], O_RDWR);
+	if(fd < 0)
+	{
+		perror("open failed.");
+		exit(-2);
+	}
+	
+	if((argc > 2) && !strcmp("-n", argv[2]))
+	{
+		noprompt = 1;
+	}
+	
+	/* get the current values */
+	if(ioctl(fd, ATIS_IOCGSPEED, &oldspeed) < 0)
+	{
+		perror("ioctl failed.");
+		exit(-3);
+	}
+	/* set up the existing values as defaults */
+	newspeed = oldspeed;
+	/* gather all new values from the command line */
+	/* or via tty input.*/
+	if(argc == (noprompt + 3))
+	{
+		newspeed = atoi(argv[count]);
+	}
+	
+	fprintf(stderr, "speed [%ld/%ld]: ", oldspeed, newspeed);
+	
+	if(!noprompt)
+	{
+		if(count = read(0, buffer, 150), count <= 0)
+		{
+			exit(0);
+		}
+		buffer[count] = '\0';
+		if(buffer[0] != '\n')
+		{
+			sscanf(buffer, "%ld", &newspeed);
+		}
+	}
+	else
+	{
+		fprintf(stderr, "\n");
+	}
+	
+	/* This ioctl does the actual register load. */
+	if(ioctl(fd, ATIS_IOCSSPEED, &newspeed) < 0)
+	{
+		perror("ioctl failed.");
+		exit(-3);
+	}
+	
+	fflush(stdout);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/clean linuxppc64_2_4/drivers/net/wan/8253x/clean
--- linux-2.4.19/drivers/net/wan/8253x/clean	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/clean	Mon May 13 16:39:14 2002
@@ -0,0 +1 @@
+rm -f 8253xcfg 8253xmac eprom9050 8253xspeed 8253xpeer eprom9050
\ No newline at end of file
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/net/wan/8253x/eprom9050.c linuxppc64_2_4/drivers/net/wan/8253x/eprom9050.c
--- linux-2.4.19/drivers/net/wan/8253x/eprom9050.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/net/wan/8253x/eprom9050.c	Mon May 13 16:39:14 2002
@@ -0,0 +1,100 @@
+/* -*- linux-c -*- */
+/* 
+ * Copyright (C) 2001 By Joachim Martillo, Telford Tools, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ **/
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include "8253xioc.h"
+#include "Reg9050.h"
+
+
+/* This application reprograms the eprom associated with the 9050 */
+
+int main(int argc, char **argv)
+{
+	int fd;
+	unsigned short oldeeprom[EPROM9050_SIZE], neweeprom[EPROM9050_SIZE];
+	char buffer[200];
+	int count;
+	int value;
+	unsigned short *pointer;
+	unsigned short *pointerold;
+	int noprompt = 0;
+	int epromindex;
+	
+	if(argc < 2)
+	{
+		fprintf(stderr, "Syntax: %s {portname} [-n] {prom values}.\n", *argv);
+		exit(-1);
+	}
+	fd = open(argv[1], O_RDWR);
+	if(fd < 0)
+	{
+		perror("open failed.");
+		exit(-2);
+	}
+	
+	if((argc > 2) && !strcmp("-n", argv[2]))
+	{
+		noprompt = 1;
+	}
+	
+	/* get the current values */
+	if(ioctl(fd, ATIS_IOCGSEP9050, &oldeeprom) < 0)
+	{
+		perror("ioctl failed.");
+		exit(-3);
+	}
+	/* set up the existing values as defaults */
+	memcpy(neweeprom, oldeeprom, sizeof(oldeeprom));
+	/* gather all new values from the command line */
+	/* or via tty input.*/
+	for(count = (2+noprompt), pointer = neweeprom; count < argc; ++count, ++pointer)
+	{
+		*pointer = atoi(argv[count]);
+	}
+	pointer = neweeprom;
+	pointerold = oldeeprom;
+	for(epromindex = 0; epromindex < EPROM9050_SIZE; ++epromindex)
+	{
+		fprintf(stderr, "LOCATION %i [%4.4x/%4.4x]: ", epromindex, *pointerold, *pointer);
+		
+		if(!noprompt)
+		{
+			if(count = read(0, buffer, 150), count <= 0)
+			{
+				exit(0);
+			}
+			buffer[count] = '\0';
+			if(buffer[0] != '\n')
+			{
+				sscanf(buffer, "%x", &value);
+				*pointer = (unsigned short) value;
+			}
+		}
+		else
+		{
+			fprintf(stderr, "\n");
+		}
+		++pointerold;
+		++pointer;
+	}
+	/* This ioctl does the actual register load. */
+	if(ioctl(fd, ATIS_IOCSSEP9050, neweeprom) < 0)
+	{
+		perror("ioctl failed.");
+		exit(-3);
+	}
+	
+	fflush(stdout);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/pci/pci.c linuxppc64_2_4/drivers/pci/pci.c
--- linux-2.4.19/drivers/pci/pci.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/pci/pci.c	Mon Apr 22 10:33:13 2002
@@ -1079,8 +1079,8 @@
 	res = child->resource[0];
 	pci_read_config_byte(dev, PCI_IO_BASE, &io_base_lo);
 	pci_read_config_byte(dev, PCI_IO_LIMIT, &io_limit_lo);
-	base = (io_base_lo & PCI_IO_RANGE_MASK) << 8;
-	limit = (io_limit_lo & PCI_IO_RANGE_MASK) << 8;
+	base = (unsigned long)(io_base_lo & PCI_IO_RANGE_MASK) << 8;
+	limit = (unsigned long)(io_limit_lo & PCI_IO_RANGE_MASK) << 8;
 
 	if ((io_base_lo & PCI_IO_RANGE_TYPE_MASK) == PCI_IO_RANGE_TYPE_32) {
 		u16 io_base_hi, io_limit_hi;
@@ -1107,8 +1107,8 @@
 	res = child->resource[1];
 	pci_read_config_word(dev, PCI_MEMORY_BASE, &mem_base_lo);
 	pci_read_config_word(dev, PCI_MEMORY_LIMIT, &mem_limit_lo);
-	base = (mem_base_lo & PCI_MEMORY_RANGE_MASK) << 16;
-	limit = (mem_limit_lo & PCI_MEMORY_RANGE_MASK) << 16;
+	base = (unsigned long)(mem_base_lo & PCI_MEMORY_RANGE_MASK) << 16;
+	limit = (unsigned long)(mem_limit_lo & PCI_MEMORY_RANGE_MASK) << 16;
 	if (base && base <= limit) {
 		res->flags = (mem_base_lo & PCI_MEMORY_RANGE_TYPE_MASK) | IORESOURCE_MEM;
 		res->start = base;
@@ -1123,16 +1123,16 @@
 	res = child->resource[2];
 	pci_read_config_word(dev, PCI_PREF_MEMORY_BASE, &mem_base_lo);
 	pci_read_config_word(dev, PCI_PREF_MEMORY_LIMIT, &mem_limit_lo);
-	base = (mem_base_lo & PCI_PREF_RANGE_MASK) << 16;
-	limit = (mem_limit_lo & PCI_PREF_RANGE_MASK) << 16;
+	base = (unsigned long)(mem_base_lo & PCI_PREF_RANGE_MASK) << 16;
+	limit = (unsigned long)(mem_limit_lo & PCI_PREF_RANGE_MASK) << 16;
 
 	if ((mem_base_lo & PCI_PREF_RANGE_TYPE_MASK) == PCI_PREF_RANGE_TYPE_64) {
 		u32 mem_base_hi, mem_limit_hi;
 		pci_read_config_dword(dev, PCI_PREF_BASE_UPPER32, &mem_base_hi);
 		pci_read_config_dword(dev, PCI_PREF_LIMIT_UPPER32, &mem_limit_hi);
 #if BITS_PER_LONG == 64
-		base |= ((long) mem_base_hi) << 32;
-		limit |= ((long) mem_limit_hi) << 32;
+		base |= ((unsigned long) mem_base_hi) << 32;
+		limit |= ((unsigned long) mem_limit_hi) << 32;
 #else
 		if (mem_base_hi || mem_limit_hi) {
 			printk(KERN_ERR "PCI: Unable to handle 64-bit address space for %s\n", child->name);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/pci/pci.ids linuxppc64_2_4/drivers/pci/pci.ids
--- linux-2.4.19/drivers/pci/pci.ids	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/pci/pci.ids	Tue Jul 30 07:57:19 2002
@@ -556,6 +556,7 @@
 	0022  IBM27-82351
 	002d  Python
 	002e  ServeRAID-3x
+	0031  Serial Adapter
 	0036  Miami
 	003a  CPU to PCI Bridge
 	003e  16/4 Token ring UTP/STP controller
@@ -596,6 +597,8 @@
 	0144  Yotta Video Compositor Output
 		1014 0145  Yotta Output Controller (ytout)
 	0156  405GP PLB to PCI Bridge
+	0180  Snipe chipset SCSI controller
+		1014 0241  iSeries 2757 DASD IOA
 	01a7  PCI-X to PCI-X Bridge
 	01bd  Netfinity ServeRAID controller
 	01be  ServeRAID-4M
@@ -1445,6 +1448,9 @@
 	0002  DAC960PD
 	0010  DAC960PX
 	0050  AcceleRAID 352/170/160 support Device
+	b166  Gemstone chipset SCSI controller
+		1014 0242  iSeries 2872 DASD IOA
+		1014 0266  PCI-X Ultra320 SCSI Adapter
 	ba55  eXtremeRAID 1100 support Device
 	ba56  eXtremeRAID 2000/3000 support Device
 106a  Aten Research Inc
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aha152x.h linuxppc64_2_4/drivers/scsi/aha152x.h
--- linux-2.4.19/drivers/scsi/aha152x.h	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aha152x.h	Sat Jun 23 01:38:25 2001
@@ -2,7 +2,7 @@
 #define _AHA152X_H
 
 /*
- * $Id: aha152x.h,v 2.5 2002/04/14 11:24:12 fischer Exp $
+ * $Id: aha152x.h,v 2.4 2000/12/16 12:48:48 fischer Exp $
  */
 
 #if defined(__KERNEL__)
@@ -27,7 +27,7 @@
    (unless we support more than 1 cmd_per_lun this should do) */
 #define AHA152X_MAXQUEUE 7
 
-#define AHA152X_REVID "Adaptec 152x SCSI driver; $Revision: 2.5 $"
+#define AHA152X_REVID "Adaptec 152x SCSI driver; $Revision: 2.4 $"
 
 /* Initial value of Scsi_Host entry */
 #define AHA152X { proc_name:			"aha152x",		\
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aic7770_osm.c linuxppc64_2_4/drivers/scsi/aic7xxx/aic7770_osm.c
--- linux-2.4.19/drivers/scsi/aic7xxx/aic7770_osm.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aic7770_osm.c	Mon Apr 22 13:53:41 2002
@@ -36,7 +36,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/linux/drivers/scsi/aic7xxx/aic7770_osm.c#11 $
+ * $Id: //depot/linux-aic7xxx-2.4.18_rc4/drivers/scsi/aic7xxx/aic7770_osm.c#1 $
  */
 
 #include "aic7xxx_osm.h"
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aic7xxx_host.h linuxppc64_2_4/drivers/scsi/aic7xxx/aic7xxx_host.h
--- linux-2.4.19/drivers/scsi/aic7xxx/aic7xxx_host.h	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aic7xxx_host.h	Tue Apr 23 09:37:29 2002
@@ -36,7 +36,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/linux/drivers/scsi/aic7xxx/aic7xxx_host.h#9 $
+ * $Id: //depot/linux-aic7xxx-2.4.18_rc4/drivers/scsi/aic7xxx/aic7xxx_host.h#3 $
  */
 
 #ifndef _AIC7XXX_HOST_H_
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c linuxppc64_2_4/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c
--- linux-2.4.19/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c	Thu Feb 21 20:56:23 2002
@@ -0,0 +1,407 @@
+/*
+ * Linux driver attachment glue for PCI based controllers.
+ *
+ * Copyright (c) 2000-2001 Adaptec Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification.
+ * 2. Redistributions in binary form must reproduce at minimum a disclaimer
+ *    substantially similar to the "NO WARRANTY" disclaimer below
+ *    ("Disclaimer") and any redistribution must be conditioned upon
+ *    including a substantially similar Disclaimer requirement for further
+ *    binary redistribution.
+ * 3. Neither the names of the above-listed copyright holders nor the names
+ *    of any contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * Alternatively, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") version 2 as published by the Free
+ * Software Foundation.
+ *
+ * NO WARRANTY
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * HOLDERS OR CONTRIBUTORS BE LIABLE FOR SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+ * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
+ * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGES.
+ *
+ * $Id: //depot/aic7xxx/linux/drivers/scsi/aic7xxx/aic7xxx_linux_pci.c#27 $
+ */
+
+#include "aic7xxx_osm.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+struct pci_device_id
+{
+};
+#endif
+
+static int	ahc_linux_pci_dev_probe(struct pci_dev *pdev,
+					const struct pci_device_id *ent);
+static int	ahc_linux_pci_reserve_io_region(struct ahc_softc *ahc,
+						u_long *base);
+#ifdef MMAPIO
+static int	ahc_linux_pci_reserve_mem_region(struct ahc_softc *ahc,
+						 u_long *bus_addr,
+						 uint8_t **maddr);
+#endif /* MMAPIO */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+static void	ahc_linux_pci_dev_remove(struct pci_dev *pdev);
+
+/* We do our own ID filtering.  So, grab all SCSI storage class devices. */
+static struct pci_device_id ahc_linux_pci_id_table[] = {
+	{
+		0x9004, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID,
+		PCI_CLASS_STORAGE_SCSI << 8, 0xFFFF00, 0
+	},
+	{
+		0x9005, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID,
+		PCI_CLASS_STORAGE_SCSI << 8, 0xFFFF00, 0
+	},
+	{ 0 }
+};
+MODULE_DEVICE_TABLE(pci, ahc_linux_pci_id_table);
+
+struct pci_driver aic7xxx_pci_driver = {
+	name:		"aic7xxx",
+	probe:		ahc_linux_pci_dev_probe,
+	remove:		ahc_linux_pci_dev_remove,
+	id_table:	ahc_linux_pci_id_table
+};
+
+static void
+ahc_linux_pci_dev_remove(struct pci_dev *pdev)
+{
+	struct ahc_softc *ahc;
+	struct ahc_softc *list_ahc;
+
+	/*
+	 * We should be able to just perform
+	 * the free directly, but check our
+	 * list for extra sanity.
+	 */
+	ahc = pci_get_drvdata(pdev);
+	TAILQ_FOREACH(list_ahc, &ahc_tailq, links) {
+		if (list_ahc == ahc) {
+			ahc_free(ahc);
+			break;
+		}
+	}
+}
+#endif /* !LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0) */
+
+static int
+ahc_linux_pci_dev_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	char		 buf[80];
+	struct		 ahc_softc *ahc;
+	ahc_dev_softc_t	 pci;
+	struct		 ahc_pci_identity *entry;
+	char		*name;
+	int		 error;
+
+	/*
+	 * Some BIOSen report the same device multiple times.
+	 */
+	TAILQ_FOREACH(ahc, &ahc_tailq, links) {
+		struct pci_dev *probed_pdev;
+
+		probed_pdev = ahc->dev_softc;
+		if (probed_pdev->bus->number == pdev->bus->number
+		 && probed_pdev->devfn == pdev->devfn)
+			break;
+	}
+	if (ahc != NULL) {
+		/* Skip duplicate. */
+		return (-ENODEV);
+	}
+
+	pci = pdev;
+	entry = ahc_find_pci_device(pci);
+	if (entry == NULL)
+		return (-ENODEV);
+
+	/*
+	 * Allocate a softc for this card and
+	 * set it up for attachment by our
+	 * common detect routine.
+	 */
+	sprintf(buf, "ahc_pci:%d:%d:%d",
+		ahc_get_pci_bus(pci),
+		ahc_get_pci_slot(pci),
+		ahc_get_pci_function(pci));
+	name = malloc(strlen(buf) + 1, M_DEVBUF, M_NOWAIT);
+	if (name == NULL)
+		return (-ENOMEM);
+	strcpy(name, buf);
+	ahc = ahc_alloc(NULL, name);
+	if (ahc == NULL)
+		return (-ENOMEM);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	if (pci_enable_device(pdev)) {
+		ahc_free(ahc);
+		return (-ENODEV);
+	}
+	pci_set_master(pdev);
+
+	if (sizeof(bus_addr_t) > 4
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,3)
+	 && ahc_linux_get_memsize() > 0x80000000
+	 && pci_set_dma_mask(pdev, 0x7FFFFFFFFFULL) == 0) {
+#else
+	 && ahc_linux_get_memsize() > 0x80000000) {
+
+		ahc->dev_softc->dma_mask = 
+		    (bus_addr_t)(0x7FFFFFFFFFULL & (bus_addr_t)~0);
+#endif
+		ahc->flags |= AHC_39BIT_ADDRESSING;
+		ahc->platform_data->hw_dma_mask =
+		    (bus_addr_t)(0x7FFFFFFFFFULL & (bus_addr_t)~0);
+	}
+#endif
+	ahc->dev_softc = pci;
+	ahc->platform_data->irq = pdev->irq;
+	error = ahc_pci_config(ahc, entry);
+	if (error != 0) {
+		ahc_free(ahc);
+		return (-error);
+	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	pci_set_drvdata(pdev, ahc);
+	if (aic7xxx_detect_complete)
+		ahc_linux_register_host(ahc, aic7xxx_driver_template);
+#endif
+	return (0);
+}
+
+int
+ahc_linux_pci_probe(Scsi_Host_Template *template)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	return (pci_module_init(&aic7xxx_pci_driver));
+#else
+	struct pci_dev *pdev;
+	u_int class;
+	int found;
+
+	/* If we don't have a PCI bus, we can't find any adapters. */
+	if (pci_present() == 0)
+		return (0);
+
+	found = 0;
+	pdev = NULL;
+	class = PCI_CLASS_STORAGE_SCSI << 8;
+	while ((pdev = pci_find_class(class, pdev)) != NULL) {
+		ahc_dev_softc_t pci;
+		int error;
+
+		pci = pdev;
+		error = ahc_linux_pci_dev_probe(pdev, /*pci_devid*/NULL);
+		if (error == 0)
+			found++;
+	}
+	return (found);
+#endif
+}
+
+static int
+ahc_linux_pci_reserve_io_region(struct ahc_softc *ahc, u_long *base)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	*base = pci_resource_start(ahc->dev_softc, 0);
+#else
+	*base = ahc_pci_read_config(ahc->dev_softc, PCIR_MAPS, 4);
+	*base &= PCI_BASE_ADDRESS_IO_MASK;
+#endif
+	if (base == 0)
+		return (ENOMEM);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,4,0)
+	if (check_region(*base, 256) != 0)
+		return (ENOMEM);
+	else
+		request_region(*base, 256, "aic7xxx");
+#else
+	if (request_region(*base, 256, "aic7xxx") == 0)
+		return (ENOMEM);
+#endif
+	return (0);
+}
+
+#ifdef MMAPIO
+static int
+ahc_linux_pci_reserve_mem_region(struct ahc_softc *ahc,
+				 u_long *bus_addr,
+				 uint8_t **maddr)
+{
+	u_long	start;
+	u_long	base_page;
+	u_long	base_offset;
+	int	error;
+
+	error = 0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,3,0)
+	start = pci_resource_start(ahc->dev_softc, 1);
+	base_page = start & PAGE_MASK;
+	base_offset = start - base_page;
+#else
+	start = ahc_pci_read_config(ahc->dev_softc, PCIR_MAPS+4, 4);
+	base_offset = start & PCI_BASE_ADDRESS_MEM_MASK;
+	base_page = base_offset & PAGE_MASK;
+	base_offset -= base_page;
+#endif
+	if (start != 0) {
+		*bus_addr = start;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+		if (request_mem_region(start, 0x1000, "aic7xxx") == 0)
+			error = ENOMEM;
+#endif
+		if (error == 0) {
+			*maddr = ioremap_nocache(base_page, base_offset + 256);
+			if (*maddr == NULL)
+				error = ENOMEM;
+			else
+				*maddr += base_offset;
+		}
+	} else
+		error = ENOMEM;
+	return (error);
+}
+#endif /* MMAPIO */
+
+int
+ahc_pci_map_registers(struct ahc_softc *ahc)
+{
+	uint32_t command;
+	u_long	 base;
+	uint8_t	*maddr;
+	int	 error;
+	int	 io_error;
+
+	/*
+	 * We always reserve both our register spaces to avoid
+	 * other devices claiming them.
+	 */
+	command = ahc_pci_read_config(ahc->dev_softc, PCIR_COMMAND, 4);
+	command &= ~(PCIM_CMD_PORTEN|PCIM_CMD_MEMEN);
+	base = 0;
+	maddr = NULL;
+#ifdef MMAPIO
+	error = ahc_linux_pci_reserve_mem_region(ahc, &base, &maddr);
+	if (error == 0) {
+		ahc->platform_data->mem_busaddr = base;
+		ahc->tag = BUS_SPACE_MEMIO;
+		ahc->bsh.maddr = maddr;
+		ahc_pci_write_config(ahc->dev_softc, PCIR_COMMAND,
+				     command | PCIM_CMD_MEMEN, 4);
+
+		/*
+		 * Do a quick test to see if memory mapped
+		 * I/O is functioning correctly.
+		 */
+		if (ahc_inb(ahc, HCNTRL) == 0xFF) {
+
+			printf("aic7xxx: PCI Device %d:%d:%d "
+			       "failed memory mapped test\n",
+			       ahc_get_pci_bus(ahc->dev_softc),
+			       ahc_get_pci_slot(ahc->dev_softc),
+			       ahc_get_pci_function(ahc->dev_softc));
+			maddr = NULL;
+		} else
+			command |= PCIM_CMD_MEMEN;
+	} else {
+		printf("aic7xxx: PCI%d:%d:%d MEM region 0x%lx "
+		       "unavailable. Cannot map device.\n",
+		       ahc_get_pci_bus(ahc->dev_softc),
+		       ahc_get_pci_slot(ahc->dev_softc),
+		       ahc_get_pci_function(ahc->dev_softc),
+		       base);
+	}
+#endif
+
+	/*
+	 * We always prefer memory mapped access.  Only
+	 * complain about our ioport conflicting with
+	 * another device if we are going to use it.
+	 */
+	io_error = ahc_linux_pci_reserve_io_region(ahc, &base);
+	if (maddr == NULL) {
+		error = io_error;
+		if (error != 0) {
+			printf("aic7xxx: PCI%d:%d:%d IO region 0x%lx[0..255] "
+			       "unavailable. Cannot map device.\n",
+			       ahc_get_pci_bus(ahc->dev_softc),
+			       ahc_get_pci_slot(ahc->dev_softc),
+			       ahc_get_pci_function(ahc->dev_softc),
+			       base);
+			base = 0;
+		} else {
+			ahc->tag = BUS_SPACE_PIO;
+			ahc->bsh.ioport = base;
+			command |= PCIM_CMD_PORTEN;
+		}
+	}
+	ahc_pci_write_config(ahc->dev_softc, PCIR_COMMAND, command, 4);
+	return (error);
+}
+
+int
+ahc_pci_map_int(struct ahc_softc *ahc)
+{
+	int error;
+
+	ahc->platform_data->irq = ahc->dev_softc->irq;
+	error = request_irq(ahc->platform_data->irq, ahc_linux_isr,
+			    SA_SHIRQ, "aic7xxx", ahc);
+	
+	return (-error);
+}
+
+void
+ahc_power_state_change(struct ahc_softc *ahc, ahc_power_state new_state)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,0)
+	pci_set_power_state(ahc->dev_softc, new_state);
+#else
+	uint32_t cap;
+	u_int cap_offset;
+
+	/*
+	 * Traverse the capability list looking for
+	 * the power management capability.
+	 */
+	cap = 0;
+	cap_offset = ahc_pci_read_config(ahc->dev_softc,
+					 PCIR_CAP_PTR, /*bytes*/1);
+	while (cap_offset != 0) {
+
+		cap = ahc_pci_read_config(ahc->dev_softc,
+					  cap_offset, /*bytes*/4);
+		if ((cap & 0xFF) == 1
+		 && ((cap >> 16) & 0x3) > 0) {
+			uint32_t pm_control;
+
+			pm_control = ahc_pci_read_config(ahc->dev_softc,
+							 cap_offset + 4,
+							 /*bytes*/4);
+			pm_control &= ~0x3;
+			pm_control |= new_state;
+			ahc_pci_write_config(ahc->dev_softc,
+					     cap_offset + 4,
+					     pm_control, /*bytes*/2);
+			break;
+		}
+		cap_offset = (cap >> 8) & 0xFF;
+	}
+#endif 
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm.h linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm.h
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm.h	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm.h	Mon Apr 22 10:33:26 2002
@@ -37,7 +37,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm.h#9 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm.h,v 1.11 2000/09/22 22:19:54 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_gram.y	Mon Apr 22 10:33:26 2002
@@ -38,7 +38,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm_gram.y#14 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_gram.y,v 1.12 2000/10/31 18:44:32 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_insformat.h	Mon Apr 22 10:33:26 2002
@@ -37,7 +37,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm_insformat.h#8 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_insformat.h,v 1.3 2000/09/22 22:19:54 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_scan.l	Mon Apr 22 10:33:26 2002
@@ -38,7 +38,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm_scan.l#10 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_scan.l,v 1.13.2.3 2001/07/28 18:46:44 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.c	Mon Apr 22 10:33:26 2002
@@ -36,7 +36,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm_symbol.c#13 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_symbol.c,v 1.11 2000/09/22 22:19:54 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h
--- linux-2.4.19/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h	Fri Aug  2 19:39:44 2002
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/aicasm/aicasm_symbol.h	Mon Apr 22 10:33:26 2002
@@ -36,7 +36,7 @@
  * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  * POSSIBILITY OF SUCH DAMAGES.
  *
- * $Id: //depot/aic7xxx/aic7xxx/aicasm/aicasm_symbol.h#10 $
+ * $Id$
  *
  * $FreeBSD: src/sys/dev/aic7xxx/aicasm/aicasm_symbol.h,v 1.11 2000/09/22 22:19:55 gibbs Exp $
  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/scsi/aic7xxx/cam.h linuxppc64_2_4/drivers/scsi/aic7xxx/cam.h
--- linux-2.4.19/drivers/scsi/aic7xxx/cam.h	Thu Oct 25 15:53:49 2001
+++ linuxppc64_2_4/drivers/scsi/aic7xxx/cam.h	Mon Apr 22 10:33:26 2002
@@ -29,7 +29,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $Id: //depot/aic7xxx/linux/drivers/scsi/aic7xxx/cam.h#11 $
+ * $Id$
  */
 
 #ifndef _AIC7XXX_CAM_H
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/drivers/sound/dmabuf.c linuxppc64_2_4/drivers/sound/dmabuf.c
--- linux-2.4.19/drivers/sound/dmabuf.c	Tue Aug  8 11:11:04 2000
+++ linuxppc64_2_4/drivers/sound/dmabuf.c	Mon Feb 25 08:44:33 2002
@@ -113,7 +113,7 @@
 		}
 	}
 	dmap->raw_buf = start_addr;
-	dmap->raw_buf_phys = virt_to_bus(start_addr);
+	dmap->raw_buf_phys = pci_map_single(NULL, start_addr, dmap->buffsize, PCI_DMA_BIDIRECTIONAL);
 
 	for (page = virt_to_page(start_addr); page <= virt_to_page(end_addr); page++)
 		mem_map_reserve(page);
@@ -134,6 +134,8 @@
 
 	start_addr = (unsigned long) dmap->raw_buf;
 	end_addr = start_addr + dmap->buffsize;
+
+	pci_unmap_single(NULL, dmap->raw_buf_phys, dmap->buffsize, PCI_DMA_BIDIRECTIONAL);
 
 	for (page = virt_to_page(start_addr); page <= virt_to_page(end_addr); page++)
 		mem_map_unreserve(page);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/Config.in linuxppc64_2_4/fs/Config.in
--- linux-2.4.19/fs/Config.in	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/fs/Config.in	Mon Apr 22 10:35:08 2002
@@ -52,6 +52,8 @@
 dep_mbool '  Transparent decompression extension' CONFIG_ZISOFS $CONFIG_ISO9660_FS
 
 tristate 'Minix fs support' CONFIG_MINIX_FS
+tristate 'JFS filesystem support' CONFIG_JFS_FS
+dep_mbool '  JFS debugging' CONFIG_JFS_DEBUG $CONFIG_JFS_FS
 
 tristate 'FreeVxFS file system support (VERITAS VxFS(TM) compatible)' CONFIG_VXFS_FS
 tristate 'NTFS file system support (read only)' CONFIG_NTFS_FS
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/Makefile linuxppc64_2_4/fs/Makefile
--- linux-2.4.19/fs/Makefile	Mon Feb 25 13:38:07 2002
+++ linuxppc64_2_4/fs/Makefile	Thu Feb 21 20:57:39 2002
@@ -67,6 +67,7 @@
 subdir-$(CONFIG_REISERFS_FS)	+= reiserfs
 subdir-$(CONFIG_DEVPTS_FS)	+= devpts
 subdir-$(CONFIG_SUN_OPENPROMFS)	+= openpromfs
+subdir-$(CONFIG_JFS_FS)     += jfs
 
 
 obj-$(CONFIG_BINFMT_AOUT)	+= binfmt_aout.o
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/binfmt_elf.c linuxppc64_2_4/fs/binfmt_elf.c
--- linux-2.4.19/fs/binfmt_elf.c	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/fs/binfmt_elf.c	Fri Jul 26 13:41:03 2002
@@ -440,6 +440,7 @@
 	unsigned int size;
 	unsigned long elf_entry, interp_load_addr = 0;
 	unsigned long start_code, end_code, start_data, end_data;
+	unsigned long reloc_func_desc = 0;
 	struct elfhdr elf_ex;
 	struct elfhdr interp_elf_ex;
   	struct exec interp_ex;
@@ -667,6 +668,7 @@
 				load_bias += error -
 				             ELF_PAGESTART(load_bias + vaddr);
 				load_addr += load_bias;
+				reloc_func_desc = load_addr;
 			}
 		}
 		k = elf_ppnt->p_vaddr;
@@ -713,6 +715,7 @@
 			send_sig(SIGSEGV, current, 0);
 			return 0;
 		}
+		reloc_func_desc = interp_load_addr;
 	}
 
 	kfree(elf_phdata);
@@ -775,10 +778,14 @@
 	/*
 	 * The ABI may specify that certain registers be set up in special
 	 * ways (on i386 %edx is the address of a DT_FINI function, for
-	 * example.  This macro performs whatever initialization to
-	 * the regs structure is required.
+	 * example.  In addition, it may also specify (eg, PowerPC64 ELF)
+	 * that the e_entry field is the address of the function descriptor
+	 * for the startup routine, rather than the address of the startup
+	 * routine itself.  This macro performs whatever initialization to
+	 * the regs structure is required as well as any relocations to the
+	 * function descriptor entries when executing dynamically links apps.
 	 */
-	ELF_PLAT_INIT(regs);
+	ELF_PLAT_INIT(regs, reloc_func_desc);
 #endif
 
 	start_thread(regs, elf_entry, bprm->p);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/iobuf.c linuxppc64_2_4/fs/iobuf.c
--- linux-2.4.19/fs/iobuf.c	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/fs/iobuf.c	Mon Jul  8 14:15:04 2002
@@ -18,7 +18,11 @@
 	if (atomic_dec_and_test(&kiobuf->io_count)) {
 		if (kiobuf->end_io)
 			kiobuf->end_io(kiobuf);
-		wake_up(&kiobuf->wait_queue);
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+			/* the end_io fn should take care of waiters too */
+		else
+#endif
+			wake_up(&kiobuf->wait_queue);
 	}
 }
 
@@ -124,7 +128,6 @@
 	iobuf->array_len = wanted;
 	return 0;
 }
-
 
 void kiobuf_wait_for_io(struct kiobuf *kiobuf)
 {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/Makefile linuxppc64_2_4/fs/jfs/Makefile
--- linux-2.4.19/fs/jfs/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/Makefile	Wed Nov 14 10:19:35 2001
@@ -0,0 +1,20 @@
+#
+# Makefile for the Linux JFS filesystem routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile.
+
+O_TARGET := jfs.o
+obj-y   := super.o file.o inode.o namei.o jfs_mount.o jfs_umount.o \
+	    jfs_xtree.o jfs_imap.o jfs_debug.o jfs_dmap.o \
+	    jfs_unicode.o jfs_dtree.o jfs_inode.o \
+	    jfs_extent.o symlink.o jfs_metapage.o \
+	    jfs_logmgr.o jfs_txnmgr.o jfs_uniupr.o
+obj-m   := $(O_TARGET)
+
+EXTRA_CFLAGS += -D_JFS_4K
+
+include $(TOPDIR)/Rules.make
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/dir.c linuxppc64_2_4/fs/jfs/dir.c
--- linux-2.4.19/fs/jfs/dir.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/dir.c	Wed Nov 14 10:19:35 2001
@@ -0,0 +1,112 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/fs.h>
+#include <linux/jfs_fs.h>
+#include <linux/jfs/jfs_filsys.h>
+#include <linux/jfs/jfs_lock.h>
+#include <linux/jfs/jfs_unicode.h>
+#include <linux/jfs/jfs_debug.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/locks.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+
+extern int jfs_create(struct inode *, struct dentry *, int);
+extern int jfs_mkdir(struct inode *, struct dentry *, int);
+extern int jfs_unlink(struct inode *, struct dentry *);
+extern int jfs_rmdir(struct inode *, struct dentry *);
+extern int jfs_link(struct dentry *, struct inode *, struct dentry *);
+extern int jfs_symlink(struct inode *, struct dentry *, const char *);
+extern int jfs_rename(struct inode *, struct dentry *, struct inode *,
+		      struct dentry *);
+extern int jfs_mknod(struct inode *, struct dentry *, int, int);
+extern int jfs_fsync_file(struct file *, struct dentry *, int);
+
+static ssize_t jfs_dir_read(struct file *filp,
+			    char *buf, size_t count, loff_t * ppos)
+{
+	return -EISDIR;
+}
+
+struct file_operations jfs_dir_operations = {
+	fsync:		jfs_fsync_file,
+	read:		jfs_dir_read,
+	readdir:	jfs_readdir,
+};
+
+static struct dentry *jfs_lookup(struct inode *dip, struct dentry *dentry)
+{
+	btstack_t btstack;
+	ino_t inum;
+	struct inode *ip;
+	component_t key;
+	const char *name = dentry->d_name.name;
+	int len = dentry->d_name.len;
+	int rc;
+
+	jFYI(1, ("jfs_lookup: name = %s\n", name));
+
+
+	if ((name[0] == '.') && (len == 1))
+		inum = dip->i_ino;
+	else if (strcmp(name, "..") == 0)
+		inum = PARENT(dip);
+	else {
+		if ((rc =
+		     get_UCSname(&key, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+			return ERR_PTR(-rc);
+		IREAD_LOCK(dip);
+		rc = dtSearch(dip, &key, &inum, &btstack, JFS_LOOKUP);
+		IREAD_UNLOCK(dip);
+		free_UCSname(&key);
+		if (rc == ENOENT) {
+			d_add(dentry, NULL);
+			return ERR_PTR(0);
+		} else if (rc) {
+			jERROR(1,
+			       ("jfs_lookup: dtSearch returned %d\n", rc));
+			return ERR_PTR(-rc);
+		}
+	}
+
+	ip = iget(dip->i_sb, inum);
+	if (ip == NULL) {
+		jERROR(1,
+		       ("jfs_lookup: iget failed on inum %d\n",
+			(uint) inum));
+		return ERR_PTR(-EACCES);
+	}
+
+	d_add(dentry, ip);
+
+	return ERR_PTR(0);
+}
+
+struct inode_operations jfs_dir_inode_operations = {
+	create:		jfs_create,
+	lookup:		jfs_lookup,
+	link:		jfs_link,
+	unlink:		jfs_unlink,
+	symlink:	jfs_symlink,
+	mkdir:		jfs_mkdir,
+	rmdir:		jfs_rmdir,
+	mknod:		jfs_mknod,
+	rename:		jfs_rename,
+};
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/endian24.h linuxppc64_2_4/fs/jfs/endian24.h
--- linux-2.4.19/fs/jfs/endian24.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/endian24.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,50 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#ifndef _H_ENDIAN24
+#define	_H_ENDIAN24
+
+/*
+ *	fs/jfs/endian24.h:
+ *
+ * Endian conversion for 24-byte data
+ *
+ */
+#define __swab24(x) \
+({ \
+	__u32 __x = (x); \
+	((__u32)( \
+		((__x & (__u32)0x000000ffUL) << 16) | \
+		 (__x & (__u32)0x0000ff00UL)        | \
+		((__x & (__u32)0x00ff0000UL) >> 16) )); \
+})
+
+#if (defined(__KERNEL__) && defined(__LITTLE_ENDIAN)) || (defined(__BYTE_ORDER) && (__BYTE_ORDER == __LITTLE_ENDIAN))
+	#define __cpu_to_le24(x) ((__u32)(x))
+	#define __le24_to_cpu(x) ((__u32)(x))
+#else
+	#define __cpu_to_le24(x) __swab24(x)
+	#define __le24_to_cpu(x) __swab24(x)
+#endif
+
+#ifdef __KERNEL__
+	#define cpu_to_le24 __cpu_to_le24
+	#define le24_to_cpu __le24_to_cpu
+#endif
+
+#endif				/* !_H_ENDIAN24 */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/file.c linuxppc64_2_4/fs/jfs/file.c
--- linux-2.4.19/fs/jfs/file.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/file.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,105 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_txnmgr.h"
+#include "jfs_debug.h"
+
+
+extern int generic_file_open(struct inode *, struct file *) __weak;
+extern loff_t generic_file_llseek(struct file *, loff_t, int origin) __weak;
+
+extern int jfs_commit_inode(struct inode *, int);
+
+int jfs_fsync(struct file *file, struct dentry *dentry, int datasync)
+{
+	struct inode *inode = dentry->d_inode;
+	int rc = 0;
+
+	rc = fsync_inode_data_buffers(inode);
+
+	if (!(inode->i_state & I_DIRTY))
+		return rc;
+	if (datasync || !(inode->i_state & I_DIRTY_DATASYNC))
+		return rc;
+
+	IWRITE_LOCK(inode);
+	rc |= jfs_commit_inode(inode, 1);
+	IWRITE_UNLOCK(inode);
+
+	return rc ? -EIO : 0;
+}
+
+struct file_operations jfs_file_operations = {
+	open:		generic_file_open,
+	llseek:		generic_file_llseek,
+	write:		generic_file_write,
+	read:		generic_file_read,
+	mmap:		generic_file_mmap,
+	fsync:		jfs_fsync,
+};
+
+/*
+ * Guts of jfs_truncate.  Called with locks already held.  Can be called
+ * with directory for truncating directory index table.
+ */
+void jfs_truncate_nolock(struct inode *ip, loff_t length)
+{
+	loff_t newsize;
+	tid_t tid;
+
+	ASSERT(length >= 0);
+
+	if (test_cflag(COMMIT_Nolink, ip)) {
+		xtTruncate(0, ip, length, COMMIT_WMAP);
+		return;
+	}
+
+	do {
+		tid = txBegin(ip->i_sb, 0);
+
+		newsize = xtTruncate(tid, ip, length,
+				     COMMIT_TRUNCATE | COMMIT_PWMAP);
+		if (newsize < 0) {
+			txEnd(tid);
+			break;
+		}
+
+		ip->i_mtime = ip->i_ctime = CURRENT_TIME;
+		mark_inode_dirty(ip);
+
+		txCommit(tid, 1, &ip, 0);
+		txEnd(tid);
+	} while (newsize > length);	/* Truncate isn't always atomic */
+}
+
+static void jfs_truncate(struct inode *ip)
+{
+	jFYI(1, ("jfs_truncate: size = 0x%lx\n", (ulong) ip->i_size));
+
+	IWRITE_LOCK(ip);
+	jfs_truncate_nolock(ip, ip->i_size);
+	IWRITE_UNLOCK(ip);
+}
+
+struct inode_operations jfs_file_inode_operations = {
+	truncate:	jfs_truncate,
+};
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/inode.c linuxppc64_2_4/fs/jfs/inode.c
--- linux-2.4.19/fs/jfs/inode.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/inode.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,329 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_imap.h"
+#include "jfs_extent.h"
+#include "jfs_unicode.h"
+#include "jfs_debug.h"
+
+
+extern struct inode_operations jfs_dir_inode_operations;
+extern struct inode_operations jfs_file_inode_operations;
+extern struct inode_operations jfs_symlink_inode_operations;
+extern struct file_operations jfs_dir_operations;
+extern struct file_operations jfs_file_operations;
+struct address_space_operations jfs_aops;
+extern int freeZeroLink(struct inode *);
+
+void jfs_put_inode(struct inode *inode)
+{
+	jFYI(1, ("In jfs_put_inode, inode = 0x%p\n", inode));
+}
+
+void jfs_read_inode(struct inode *inode)
+{
+	int rc;
+
+	rc = alloc_jfs_inode(inode);
+	if (rc) {
+		printk(__FUNCTION__ ": failed.");
+		goto bad_inode;
+	}
+	jFYI(1, ("In jfs_read_inode, inode = 0x%p\n", inode));
+
+	if (diRead(inode))
+		goto bad_inode_free;
+
+	if (S_ISREG(inode->i_mode)) {
+		inode->i_op = &jfs_file_inode_operations;
+		inode->i_fop = &jfs_file_operations;
+		inode->i_mapping->a_ops = &jfs_aops;
+	} else if (S_ISDIR(inode->i_mode)) {
+		inode->i_op = &jfs_dir_inode_operations;
+		inode->i_fop = &jfs_dir_operations;
+		inode->i_mapping->a_ops = &jfs_aops;
+		inode->i_mapping->gfp_mask = GFP_NOFS;
+	} else if (S_ISLNK(inode->i_mode)) {
+		if (inode->i_size > IDATASIZE) {
+			inode->i_op = &page_symlink_inode_operations;
+			inode->i_mapping->a_ops = &jfs_aops;
+		} else
+			inode->i_op = &jfs_symlink_inode_operations;
+	} else {
+		init_special_inode(inode, inode->i_mode,
+				   kdev_t_to_nr(inode->i_rdev));
+	}
+
+	return;
+
+      bad_inode_free:
+	free_jfs_inode(inode);
+      bad_inode:
+	make_bad_inode(inode);
+}
+
+/* This define is from fs/open.c */
+#define special_file(m) (S_ISCHR(m)||S_ISBLK(m)||S_ISFIFO(m)||S_ISSOCK(m))
+
+/*
+ * Workhorse of both fsync & write_inode
+ */
+int jfs_commit_inode(struct inode *inode, int wait)
+{
+	int rc = 0;
+	tid_t tid;
+	static int noisy = 5;
+
+	jFYI(1, ("In jfs_commit_inode, inode = 0x%p\n", inode));
+
+	/*
+	 * Don't commit if inode has been committed since last being
+	 * marked dirty, or if it has been deleted.
+	 */
+	if (test_cflag(COMMIT_Nolink, inode) ||
+	    !test_cflag(COMMIT_Dirty, inode))
+		return 0;
+
+	if (isReadOnly(inode)) {
+		/* kernel allows writes to devices on read-only
+		 * partitions and may think inode is dirty
+		 */
+		if (!special_file(inode->i_mode) && noisy) {
+			jERROR(1, ("jfs_commit_inode(0x%p) called on "
+				   "read-only volume\n", inode));
+			jERROR(1, ("Is remount racy?\n"));
+			noisy--;
+		}
+		return 0;
+	}
+
+	tid = txBegin(inode->i_sb, COMMIT_INODE);
+	rc = txCommit(tid, 1, &inode, wait ? COMMIT_SYNC : 0);
+	txEnd(tid);
+	return -rc;
+}
+
+void jfs_write_inode(struct inode *inode, int wait)
+{
+	/*
+	 * If COMMIT_DIRTY is not set, the inode isn't really dirty.
+	 * It has been committed since the last change, but was still
+	 * on the dirty inode list
+	 */
+	if (test_cflag(COMMIT_Nolink, inode) ||
+	    !test_cflag(COMMIT_Dirty, inode))
+		return;
+
+	IWRITE_LOCK(inode);
+
+	if (jfs_commit_inode(inode, wait)) {
+		jERROR(1, ("jfs_write_inode: jfs_commit_inode failed!\n"));
+	}
+
+	IWRITE_UNLOCK(inode);
+}
+
+void jfs_delete_inode(struct inode *inode)
+{
+	jFYI(1, ("In jfs_delete_inode, inode = 0x%p\n", inode));
+
+	IWRITE_LOCK(inode);
+	if (test_cflag(COMMIT_Freewmap, inode))
+		freeZeroLink(inode);
+
+	diFree(inode);
+	IWRITE_UNLOCK(inode);
+
+	clear_inode(inode);
+}
+
+void jfs_dirty_inode(struct inode *inode)
+{
+	static int noisy = 5;
+
+	if (isReadOnly(inode)) {
+		if (!special_file(inode->i_mode) && noisy) {
+			/* kernel allows writes to devices on read-only
+			 * partitions and may try to mark inode dirty
+			 */
+			jERROR(1, ("jfs_dirty_inode called on "
+				   "read-only volume\n"));
+			jERROR(1, ("Is remount racy?\n"));
+			noisy--;
+		}
+		return;
+	}
+
+	set_cflag(COMMIT_Dirty, inode);
+}
+
+static int jfs_get_block(struct inode *ip, long lblock,
+			 struct buffer_head *bh_result, int create)
+{
+	s64 lblock64 = lblock;
+	int no_size_check = 0;
+	int rc = 0;
+	int take_locks;
+	xad_t xad;
+	s64 xaddr;
+	int xflag;
+	s32 xlen;
+
+	/*
+	 * If this is a special inode (imap, dmap) or directory,
+	 * the lock should already be taken
+	 */
+	take_locks = ((JFS_IP(ip)->fileset != AGGREGATE_I) &&
+		      !S_ISDIR(ip->i_mode));
+	/*
+	 * Take appropriate lock on inode
+	 */
+	if (take_locks) {
+		if (create)
+			IWRITE_LOCK(ip);
+		else
+			IREAD_LOCK(ip);
+	}
+
+	/*
+	 * A directory's "data" is the inode index table, but i_size is the
+	 * size of the d-tree, so don't check the offset against i_size
+	 */
+	if (S_ISDIR(ip->i_mode))
+		no_size_check = 1;
+
+	if ((no_size_check ||
+	     ((lblock64 << ip->i_sb->s_blocksize_bits) < ip->i_size)) &&
+	    (xtLookup
+	     (ip, lblock64, 1, &xflag, &xaddr, &xlen, no_size_check)
+	     == 0) && xlen) {
+		if (xflag & XAD_NOTRECORDED) {
+			if (!create)
+				/*
+				 * Allocated but not recorded, read treats
+				 * this as a hole
+				 */
+				goto unlock;
+#ifdef _JFS_4K
+			XADoffset(&xad, lblock64);
+			XADlength(&xad, xlen);
+			XADaddress(&xad, xaddr);
+#else				/* _JFS_4K */
+			/*
+			 * As long as block size = 4K, this isn't a problem.
+			 * We should mark the whole page not ABNR, but how
+			 * will we know to mark the other blocks BH_New?
+			 */
+			BUG();
+#endif				/* _JFS_4K */
+			rc = extRecord(ip, &xad);
+			if (rc)
+				goto unlock;
+			bh_result->b_state |= (1UL << BH_New);
+		}
+
+		bh_result->b_dev = ip->i_dev;
+		bh_result->b_blocknr = xaddr;
+		bh_result->b_state |= (1UL << BH_Mapped);
+		goto unlock;
+	}
+	if (!create)
+		goto unlock;
+
+	/*
+	 * Allocate a new block
+	 */
+#ifdef _JFS_4K
+	if ((rc =
+	     extHint(ip, lblock64 << ip->i_sb->s_blocksize_bits, &xad)))
+		goto unlock;
+	rc = extAlloc(ip, 1, lblock64, &xad, FALSE);
+	if (rc)
+		goto unlock;
+
+	bh_result->b_dev = ip->i_dev;
+	bh_result->b_blocknr = addressXAD(&xad);
+	bh_result->b_state |= ((1UL << BH_Mapped) | (1UL << BH_New));
+
+#else				/* _JFS_4K */
+	/*
+	 * We need to do whatever it takes to keep all but the last buffers
+	 * in 4K pages - see jfs_write.c
+	 */
+	BUG();
+#endif				/* _JFS_4K */
+
+      unlock:
+	/*
+	 * Release lock on inode
+	 */
+	if (take_locks) {
+		if (create)
+			IWRITE_UNLOCK(ip);
+		else
+			IREAD_UNLOCK(ip);
+	}
+	return -rc;
+}
+
+static int jfs_writepage(struct page *page)
+{
+	return block_write_full_page(page, jfs_get_block);
+}
+
+static int jfs_readpage(struct file *file, struct page *page)
+{
+	return block_read_full_page(page, jfs_get_block);
+}
+
+static int jfs_prepare_write(struct file *file,
+			     struct page *page, unsigned from, unsigned to)
+{
+	return block_prepare_write(page, from, to, jfs_get_block);
+}
+
+static int jfs_bmap(struct address_space *mapping, long block)
+{
+	return generic_block_bmap(mapping, block, jfs_get_block);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,15))
+static int jfs_direct_IO(int rw, struct inode *inode, struct kiobuf *iobuf,
+			 unsigned long blocknr, int blocksize)
+{
+	return generic_direct_IO(rw, inode, iobuf, blocknr,
+				 blocksize, jfs_get_block);
+}
+#endif				/* Kernel >= 2.4.15 */
+
+struct address_space_operations jfs_aops = {
+	readpage:	jfs_readpage,
+	writepage:	jfs_writepage,
+	sync_page:	block_sync_page,
+	prepare_write:	jfs_prepare_write,
+	commit_write:	generic_commit_write,
+	bmap:		jfs_bmap,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,4,15))
+	direct_IO:	jfs_direct_IO,
+#endif
+};
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_btree.h linuxppc64_2_4/fs/jfs/jfs_btree.h
--- linux-2.4.19/fs/jfs/jfs_btree.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_btree.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,163 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef	_H_JFS_BTREE
+#define _H_JFS_BTREE
+/*
+ *	jfs_btree.h: B+-tree
+ *
+ * JFS B+-tree (dtree and xtree) common definitions
+ */
+
+/*
+ *	basic btree page - btpage_t
+ */
+typedef struct {
+	s64 next;		/* 8: right sibling bn */
+	s64 prev;		/* 8: left sibling bn */
+
+	u8 flag;		/* 1: */
+	u8 rsrvd[7];		/* 7: type specific */
+	s64 self;		/* 8: self address */
+
+	u8 entry[4064];		/* 4064: */
+} btpage_t;			/* (4096) */
+
+/* btpaget_t flag */
+#define BT_TYPE		0x07	/* B+-tree index */
+#define	BT_ROOT		0x01	/* root page */
+#define	BT_LEAF		0x02	/* leaf page */
+#define	BT_INTERNAL	0x04	/* internal page */
+#define	BT_RIGHTMOST	0x10	/* rightmost page */
+#define	BT_LEFTMOST	0x20	/* leftmost page */
+#define	BT_SWAPPED	0x80	/* used by fsck for endian swapping */
+
+/* btorder (in inode) */
+#define	BT_RANDOM		0x0000
+#define	BT_SEQUENTIAL		0x0001
+#define	BT_LOOKUP		0x0010
+#define	BT_INSERT		0x0020
+#define	BT_DELETE		0x0040
+
+/*
+ *	btree page buffer cache access
+ */
+#define BT_IS_ROOT(MP) (((MP)->xflag & COMMIT_PAGE) == 0)
+
+/* get page from buffer page */
+#define BT_PAGE(IP, MP, TYPE, ROOT)\
+	(BT_IS_ROOT(MP) ? (TYPE *)&JFS_IP(IP)->ROOT : (TYPE *)(MP)->data)
+
+/* get the page buffer and the page for specified block address */
+#define BT_GETPAGE(IP, BN, MP, TYPE, SIZE, P, RC, ROOT)\
+{\
+	if ((BN) == 0)\
+	{\
+		MP = (metapage_t *)&JFS_IP(IP)->bxflag;\
+		P = (TYPE *)&JFS_IP(IP)->ROOT;\
+		RC = 0;\
+		jEVENT(0,("%d BT_GETPAGE returning root\n", __LINE__));\
+	}\
+	else\
+	{\
+		jEVENT(0,("%d BT_GETPAGE reading block %d\n", __LINE__,\
+			 (int)BN));\
+		MP = read_metapage((IP), BN, SIZE, 1);\
+		if (MP) {\
+			RC = 0;\
+			P = (MP)->data;\
+		} else {\
+			P = NULL;\
+			jERROR(1,("bread failed!\n"));\
+			RC = EIO;\
+		}\
+	}\
+}
+
+#define BT_MARK_DIRTY(MP, IP)\
+{\
+	if (BT_IS_ROOT(MP))\
+		mark_inode_dirty(IP);\
+	else\
+		mark_metapage_dirty(MP);\
+}
+
+/* put the page buffer */
+#define BT_PUTPAGE(MP)\
+{\
+	if (! BT_IS_ROOT(MP)) \
+		release_metapage(MP); \
+}
+
+
+/*
+ *	btree traversal stack
+ *
+ * record the path traversed during the search;
+ * top frame record the leaf page/entry selected.
+ */
+#define	MAXTREEHEIGHT		8
+typedef struct btframe {	/* stack frame */
+	s64 bn;			/* 8: */
+	s16 index;		/* 2: */
+	s16 lastindex;		/* 2: */
+	struct metapage *mp;	/* 4: */
+} btframe_t;			/* (16) */
+
+typedef struct btstack {
+	btframe_t *top;		/* 4: */
+	int nsplit;		/* 4: */
+	btframe_t stack[MAXTREEHEIGHT];
+} btstack_t;
+
+#define BT_CLR(btstack)\
+	(btstack)->top = (btstack)->stack
+
+#define BT_PUSH(BTSTACK, BN, INDEX)\
+{\
+	(BTSTACK)->top->bn = BN;\
+	(BTSTACK)->top->index = INDEX;\
+	++(BTSTACK)->top;\
+	assert((BTSTACK)->top != &((BTSTACK)->stack[MAXTREEHEIGHT]));\
+}
+
+#define BT_POP(btstack)\
+	( (btstack)->top == (btstack)->stack ? NULL : --(btstack)->top )
+
+#define BT_STACK(btstack)\
+	( (btstack)->top == (btstack)->stack ? NULL : (btstack)->top )
+
+/* retrieve search results */
+#define BT_GETSEARCH(IP, LEAF, BN, MP, TYPE, P, INDEX, ROOT)\
+{\
+	BN = (LEAF)->bn;\
+	MP = (LEAF)->mp;\
+	if (BN)\
+		P = (TYPE *)MP->data;\
+	else\
+		P = (TYPE *)&JFS_IP(IP)->ROOT;\
+	INDEX = (LEAF)->index;\
+}
+
+/* put the page buffer of search */
+#define BT_PUTSEARCH(BTSTACK)\
+{\
+	if (! BT_IS_ROOT((BTSTACK)->top->mp))\
+		release_metapage((BTSTACK)->top->mp);\
+}
+#endif				/* _H_JFS_BTREE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_compat.h linuxppc64_2_4/fs/jfs/jfs_compat.h
--- linux-2.4.19/fs/jfs/jfs_compat.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_compat.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,87 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#ifndef _H_JFS_COMPAT
+#define	_H_JFS_COMPAT
+
+/*
+ *	jfs_compat.h:
+ *
+ * Definitions to allow JFS to build on older kernels.
+ *
+ * This file should be removed when JFS is merged with linux kernel
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/slab.h>
+
+#ifndef __weak
+#define __weak	__attribute__((weak));
+#endif
+
+#ifndef MODULE_LICENSE
+#define MODULE_LICENSE(x)
+#endif
+
+#ifndef GFP_NOFS
+#define GFP_NOFS GFP_BUFFER
+#endif
+
+#if !defined(KERNEL_HAS_O_DIRECT)
+#define fsync_inode_data_buffers fsync_inode_buffers
+#endif
+
+/*
+ * Linux 2.4.9 has broken min/max macros.
+ * Linux < 2.4.9 doesn't have min/max at all.
+ */
+#if (LINUX_VERSION_CODE == KERNEL_VERSION(2,4,9))
+#undef min
+#undef max
+#endif
+
+/*
+ * Completions are new in 2.4.7.
+ */
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,4,6))
+#define DECLARE_COMPLETION(c)	DECLARE_MUTEX_LOCKED(c)
+#define complete(c)		up(c)
+#define wait_for_completion(c)	down(c)
+/* must be last to not mess up the namespace */
+#define completion		semaphore
+#else
+#include <linux/completion.h>
+#endif
+
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,4,9))
+#define min(x,y) ({			\
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x < _y ? _x : _y; })
+
+#define max(x,y) ({			\
+	const typeof(x) _x = (x);	\
+	const typeof(y) _y = (y);	\
+	(void) (&_x == &_y);		\
+	_x > _y ? _x : _y; })
+#endif
+
+#endif				/* !_H_JFS_COMPAT */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_debug.c linuxppc64_2_4/fs/jfs/jfs_debug.c
--- linux-2.4.19/fs/jfs/jfs_debug.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_debug.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,145 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/fs.h>
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <asm/uaccess.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_debug.h"
+
+#ifdef CONFIG_JFS_DEBUG
+void dump_mem(char *label, void *data, int length)
+{
+	int i, j;
+	int *intptr = data;
+	char *charptr = data;
+	char buf[10], line[80];
+
+	printk("%s: dump of %d bytes of data at 0x%p\n\n", label, length,
+	       data);
+	for (i = 0; i < length; i += 16) {
+		line[0] = 0;
+		for (j = 0; (j < 4) && (i + j * 4 < length); j++) {
+			sprintf(buf, " %08x", intptr[i / 4 + j]);
+			strcat(line, buf);
+		}
+		buf[0] = ' ';
+		buf[2] = 0;
+		for (j = 0; (j < 16) && (i + j < length); j++) {
+			buf[1] =
+			    isprint(charptr[i + j]) ? charptr[i + j] : '.';
+			strcat(line, buf);
+		}
+		printk("%s\n", line);
+	}
+}
+
+#ifdef CONFIG_PROC_FS
+static int loglevel_read(char *page, char **start, off_t off,
+			 int count, int *eof, void *data)
+{
+	int len;
+
+	len = sprintf(page, "%d\n", jfsloglevel);
+
+	len -= off;
+	*start = page + off;
+
+	if (len > count)
+		len = count;
+	else
+		*eof = 1;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+static int loglevel_write(struct file *file, const char *buffer,
+			unsigned long count, void *data)
+{
+	char c;
+
+	if (get_user(c, buffer))
+		return -EFAULT;
+
+	/* yes, I know this is an ASCIIism.  --hch */
+	if (c < '0' || c > '9')
+		return -EINVAL;
+	jfsloglevel = c - '0';
+	return count;
+}
+
+
+extern read_proc_t jfs_txanchor_read;
+#ifdef CONFIG_JFS_STATISTICS
+extern read_proc_t jfs_lmstats_read;
+extern read_proc_t jfs_xtstat_read;
+extern read_proc_t jfs_mpstat_read;
+#endif
+static struct proc_dir_entry *base;
+
+static struct {
+	const char	*name;
+	read_proc_t	*read_fn;
+	write_proc_t	*write_fn;
+} Entries[] = {
+	{ "TxAnchor",	jfs_txanchor_read, },
+#ifdef CONFIG_JFS_STATISTICS
+	{ "lmstats",	jfs_lmstats_read, },
+	{ "xtstat",	jfs_xtstat_read, },
+	{ "mpstat",	jfs_mpstat_read, },
+#endif
+	{ "loglevel",	loglevel_read, loglevel_write }
+};
+#define NPROCENT	(sizeof(Entries)/sizeof(Entries[0]))
+
+void jfs_proc_init(void)
+{
+	int i;
+
+	if (!(base = proc_mkdir("jfs", proc_root_fs)))
+		return;
+	base->owner = THIS_MODULE;
+
+	for (i = 0; i < NPROCENT; i++) {
+		struct proc_dir_entry *p;
+		if ((p = create_proc_entry(Entries[i].name, 0, base))) {
+			p->read_proc = Entries[i].read_fn;
+			p->write_proc = Entries[i].write_fn;
+		}
+	}
+}
+
+void jfs_proc_clean(void)
+{
+	int i;
+
+	if (base) {
+		for (i = 0; i < NPROCENT; i++)
+			remove_proc_entry(Entries[i].name, base);
+		remove_proc_entry("jfs", base);
+	}
+}
+
+#endif /* CONFIG_PROC_FS */
+#endif /* CONFIG_JFS_DEBUG */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_debug.h linuxppc64_2_4/fs/jfs/jfs_debug.h
--- linux-2.4.19/fs/jfs/jfs_debug.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_debug.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,96 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/
+#ifndef _H_JFS_DEBUG
+#define _H_JFS_DEBUG
+
+/*
+ *	jfs_debug.h
+ *
+ * global debug message, data structure/macro definitions
+ * under control of CONFIG_JFS_DEBUG, CONFIG_JFS_STATISTICS;
+ */
+
+/*
+ *	assert with traditional printf/panic
+ */
+#ifdef CONFIG_KERNEL_ASSERTS
+/* kgdb stuff */
+#define assert(p) KERNEL_ASSERT(#p, p)
+#else
+#define assert(p) {\
+if (!(p))\
+	{\
+		printk("assert(%s)\n",#p);\
+		BUG();\
+	}\
+}
+#endif
+
+/*
+ *	debug ON
+ *	--------
+ */
+#ifdef CONFIG_JFS_DEBUG
+#define ASSERT(p) assert(p)
+
+/* dump memory contents */
+extern void dump_mem(char *label, void *data, int length);
+extern int jfsloglevel;
+
+/* information message: e.g., configuration, major event */
+#define jFYI(button, prspec) \
+	do { if (button && jfsloglevel > 1) printk prspec; } while (0)
+
+/* error event message: e.g., i/o error */
+extern int jfsERROR;
+#define jERROR(button, prspec) \
+	do { if (button && jfsloglevel > 0) { printk prspec; } } while (0)
+
+/* debug event message: */
+#define jEVENT(button,prspec) \
+	do { if (button) printk prspec; } while (0)
+
+/*
+ *	debug OFF
+ *	---------
+ */
+#else				/* CONFIG_JFS_DEBUG */
+#define dump_mem(label,data,length)
+#define ASSERT(p)
+#define jEVENT(button,prspec)
+#define jERROR(button,prspec)
+#define jFYI(button,prspec)
+#endif				/* CONFIG_JFS_DEBUG */
+
+/*
+ *	statistics
+ *	----------
+ */
+#ifdef	CONFIG_JFS_STATISTICS
+#define	INCREMENT(x)		((x)++)
+#define	DECREMENT(x)		((x)--)
+#define	HIGHWATERMARK(x,y)	((x) = max((x), (y)))
+#else
+#define	INCREMENT(x)
+#define	DECREMENT(x)
+#define	HIGHWATERMARK(x,y)
+#endif				/* CONFIG_JFS_STATISTICS */
+
+#endif				/* _H_JFS_DEBUG */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_defragfs.h linuxppc64_2_4/fs/jfs/jfs_defragfs.h
--- linux-2.4.19/fs/jfs/jfs_defragfs.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_defragfs.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,55 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef	_H_JFS_DEFRAGFS
+#define _H_JFS_DEFRAGFS
+
+/*
+ *	jfs_defragfs.h
+ */
+/*
+ *	defragfs parameter list
+ */
+typedef struct {
+	uint flag;		/* 4: */
+	u8 dev;			/* 1: */
+	u8 pad[3];		/* 3: */
+	s32 fileset;		/* 4: */
+	u32 inostamp;		/* 4: */
+	u32 ino;		/* 4: */
+	u32 gen;		/* 4: */
+	s64 xoff;		/* 8: */
+	s64 old_xaddr;		/* 8: */
+	s64 new_xaddr;		/* 8: */
+	s32 xlen;		/* 4: */
+} defragfs_t;			/* (52) */
+
+/* plist flag */
+#define DEFRAGFS_SYNC		0x80000000
+#define DEFRAGFS_COMMIT		0x40000000
+#define DEFRAGFS_RELOCATE	0x10000000
+
+#define	INODE_TYPE		0x0000F000	/* IFREG or IFDIR */
+
+#define EXTENT_TYPE		0x000000ff
+#define DTPAGE			0x00000001
+#define XTPAGE			0x00000002
+#define DATAEXT			0x00000004
+#define EAEXT			0x00000008
+
+#endif				/* _H_JFS_DEFRAGFS */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_dinode.h linuxppc64_2_4/fs/jfs/jfs_dinode.h
--- linux-2.4.19/fs/jfs/jfs_dinode.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_dinode.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,157 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+
+#ifndef _H_JFS_DINODE
+#define _H_JFS_DINODE
+
+/*
+ *      jfs_dinode.h: on-disk inode manager
+ *
+ */
+
+#define INODESLOTSIZE           128
+#define L2INODESLOTSIZE         7
+#define log2INODESIZE           9	/* log2(bytes per dinode) */
+
+
+/*
+ *      on-disk inode (dinode_t): 512 bytes
+ *
+ * note: align 64-bit fields on 8-byte boundary.
+ */
+struct dinode {
+	/*
+	 *      I. base area (128 bytes)
+	 *      ------------------------
+	 *
+	 * define generic/POSIX attributes
+	 */
+	u32 di_inostamp;	/* 4: stamp to show inode belongs to fileset */
+	s32 di_fileset;		/* 4: fileset number */
+	u32 di_number;		/* 4: inode number, aka file serial number */
+	u32 di_gen;		/* 4: inode generation number */
+
+	pxd_t di_ixpxd;		/* 8: inode extent descriptor */
+
+	s64 di_size;		/* 8: size */
+	s64 di_nblocks;		/* 8: number of blocks allocated */
+
+	u32 di_nlink;		/* 4: number of links to the object */
+
+	u32 di_uid;		/* 4: user id of owner */
+	u32 di_gid;		/* 4: group id of owner */
+
+	u32 di_mode;		/* 4: attribute, format and permission */
+
+	struct timestruc_t di_atime;	/* 8: time last data accessed */
+	struct timestruc_t di_ctime;	/* 8: time last status changed */
+	struct timestruc_t di_mtime;	/* 8: time last data modified */
+	struct timestruc_t di_otime;	/* 8: time created */
+
+	dxd_t di_acl;		/* 16: acl descriptor */
+
+	dxd_t di_ea;		/* 16: ea descriptor */
+
+	u32 di_next_index;	/* 4: Next available dir_table index */
+
+	s32 di_acltype;		/* 4: Type of ACL */
+
+	/*
+	 *      Extension Areas.
+	 *
+	 *      Historically, the inode was partitioned into 4 128-byte areas,
+	 *      the last 3 being defined as unions which could have multiple
+	 *      uses.  The first 96 bytes had been completely unused until
+	 *      an index table was added to the directory.  It is now more
+	 *      useful to describe the last 3/4 of the inode as a single
+	 *      union.  We would probably be better off redesigning the
+	 *      entire structure from scratch, but we don't want to break
+	 *      commonality with OS/2's JFS at this time.
+	 */
+	union {
+		struct {
+			/*
+			 * This table contains the information needed to
+			 * find a directory entry from a 32-bit index.
+			 * If the index is small enough, the table is inline,
+			 * otherwise, an x-tree root overlays this table
+			 */
+			dir_table_slot_t _table[12];	/* 96: inline */
+
+			dtroot_t _dtroot;		/* 288: dtree root */
+		} _dir;					/* (384) */
+#define di_dirtable	u._dir._table
+#define di_dtroot	u._dir._dtroot
+#define di_parent       di_dtroot.header.idotdot
+#define di_DASD		di_dtroot.header.DASD
+
+		struct {
+			union {
+				u8 _data[96];		/* 96: unused */
+				struct {
+					void *_imap;	/* 4: unused */
+					u32 _gengen;	/* 4: generator */
+				} _imap;
+			} _u1;				/* 96: */
+#define di_gengen	u._file._u1._imap._gengen
+
+			union {
+				xtpage_t _xtroot;
+				struct {
+					u8 unused[16];	/* 16: */
+					dxd_t _dxd;	/* 16: */
+					union {
+						u32 _rdev;	/* 4: */
+						u8 _fastsymlink[128];
+					} _u;
+					u8 _inlineea[128];
+				} _special;
+			} _u2;
+		} _file;
+#define di_xtroot	u._file._u2._xtroot
+#define di_dxd		u._file._u2._special._dxd
+#define di_btroot	di_xtroot
+#define di_inlinedata	u._file._u2._special._u
+#define di_rdev		u._file._u2._special._u._rdev
+#define di_fastsymlink	u._file._u2._special._u._fastsymlink
+#define di_inlineea     u._file._u2._special._inlineea
+	} u;
+};
+
+typedef struct dinode dinode_t;
+
+
+/* extended mode bits (on-disk inode di_mode) */
+#define IFJOURNAL       0x00010000	/* journalled file */
+#define ISPARSE         0x00020000	/* sparse file enabled */
+#define INLINEEA        0x00040000	/* inline EA area free */
+#define ISWAPFILE	0x00800000	/* file open for pager swap space */
+
+/* more extended mode bits: attributes for OS/2 */
+#define IREADONLY	0x02000000	/* no write access to file */
+#define IARCHIVE	0x40000000	/* file archive bit */
+#define ISYSTEM		0x08000000	/* system file */
+#define IHIDDEN		0x04000000	/* hidden file */
+#define IRASH		0x4E000000	/* mask for changeable attributes */
+#define INEWNAME	0x80000000	/* non-8.3 filename format */
+#define IDIRECTORY	0x20000000	/* directory (shadow of real bit) */
+#define ATTRSHIFT	25	/* bits to shift to move attribute
+				   specification to mode position */
+
+#endif /*_H_JFS_DINODE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_dmap.c linuxppc64_2_4/fs/jfs/jfs_dmap.c
--- linux-2.4.19/fs/jfs/jfs_dmap.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_dmap.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,4189 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ *
+ *   MODULE_NAME:		jfs_dmap.c
+ *
+ *   COMPONENT_NAME: 	sysjfs
+ *
+ *   FUNCTION:			block allocation map manager
+ *
+*/
+
+/*
+ * Change History :
+ *
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+#include "jfs_dmap.h"
+#include "jfs_imap.h"
+#include "jfs_lock.h"
+#include "jfs_metapage.h"
+#include "jfs_debug.h"
+
+/*
+ *	Debug code for double-checking block map
+ */
+/* #define	_JFS_DEBUG_DMAP	1 */
+
+#ifdef	_JFS_DEBUG_DMAP
+#define DBINITMAP(size,ipbmap,results) \
+	DBinitmap(size,ipbmap,results)
+#define DBALLOC(dbmap,mapsize,blkno,nblocks) \
+	DBAlloc(dbmap,mapsize,blkno,nblocks)
+#define DBFREE(dbmap,mapsize,blkno,nblocks) \
+	DBFree(dbmap,mapsize,blkno,nblocks)
+#define DBALLOCCK(dbmap,mapsize,blkno,nblocks) \
+	DBAllocCK(dbmap,mapsize,blkno,nblocks)
+#define DBFREECK(dbmap,mapsize,blkno,nblocks) \
+	DBFreeCK(dbmap,mapsize,blkno,nblocks)
+
+static void DBinitmap(s64, struct inode *, u32 **);
+static void DBAlloc(uint *, s64, s64, s64);
+static void DBFree(uint *, s64, s64, s64);
+static void DBAllocCK(uint *, s64, s64, s64);
+static void DBFreeCK(uint *, s64, s64, s64);
+#else
+#define DBINITMAP(size,ipbmap,results)
+#define DBALLOC(dbmap, mapsize, blkno, nblocks)
+#define DBFREE(dbmap, mapsize, blkno, nblocks)
+#define DBALLOCCK(dbmap, mapsize, blkno, nblocks)
+#define DBFREECK(dbmap, mapsize, blkno, nblocks)
+#endif				/* _JFS_DEBUG_DMAP */
+
+/*
+ *	SERIALIZATION of the Block Allocation Map.
+ *
+ *	the working state of the block allocation map is accessed in
+ *	two directions:
+ *	
+ *	1) allocation and free requests that start at the dmap
+ *	   level and move up through the dmap control pages (i.e.
+ *	   the vast majority of requests).
+ * 
+ * 	2) allocation requests that start at dmap control page
+ *	   level and work down towards the dmaps.
+ *	
+ *	the serialization scheme used here is as follows. 
+ *
+ *	requests which start at the bottom are serialized against each 
+ *	other through buffers and each requests holds onto its buffers 
+ *	as it works it way up from a single dmap to the required level 
+ *	of dmap control page.
+ *	requests that start at the top are serialized against each other
+ *	and request that start from the bottom by the multiple read/single
+ *	write inode lock of the bmap inode. requests starting at the top
+ *	take this lock in write mode while request starting at the bottom
+ *	take the lock in read mode.  a single top-down request may proceed
+ *	exclusively while multiple bottoms-up requests may proceed 
+ * 	simultaneously (under the protection of busy buffers).
+ *	
+ *	in addition to information found in dmaps and dmap control pages,
+ *	the working state of the block allocation map also includes read/
+ *	write information maintained in the bmap descriptor (i.e. total
+ *	free block count, allocation group level free block counts).
+ *	a single exclusive lock (BMAP_LOCK) is used to guard this information
+ *	in the face of multiple-bottoms up requests.
+ *	(lock ordering: IREAD_LOCK, BMAP_LOCK);
+ *	
+ *	accesses to the persistent state of the block allocation map (limited
+ *	to the persistent bitmaps in dmaps) is guarded by (busy) buffers.
+ */
+
+#define BMAP_LOCK_INIT(bmp)	init_MUTEX(&bmp->db_bmaplock)
+#define BMAP_LOCK(bmp)		down(&bmp->db_bmaplock)
+#define BMAP_UNLOCK(bmp)	up(&bmp->db_bmaplock)
+
+/*
+ * forward references
+ */
+static void dbAllocBits(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks);
+static void dbSplit(dmtree_t * tp, int leafno, int splitsz, int newval);
+static void dbBackSplit(dmtree_t * tp, int leafno);
+static void dbJoin(dmtree_t * tp, int leafno, int newval);
+static void dbAdjTree(dmtree_t * tp, int leafno, int newval);
+static int dbAdjCtl(bmap_t * bmp, s64 blkno, int newval, int alloc,
+		    int level);
+static int dbAllocAny(bmap_t * bmp, s64 nblocks, int l2nb, s64 * results);
+static int dbAllocNext(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks);
+static int dbAllocNear(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks,
+		       int l2nb, s64 * results);
+static int dbAllocDmap(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks);
+static int dbAllocDmapLev(bmap_t * bmp, dmap_t * dp, int nblocks, int l2nb,
+			  s64 * results);
+static int dbAllocAG(bmap_t * bmp, int agno, s64 nblocks, int l2nb,
+		     s64 * results);
+static int dbAllocCtl(bmap_t * bmp, s64 nblocks, int l2nb, s64 blkno,
+		      s64 * results);
+int dbExtend(struct inode *ip, s64 blkno, s64 nblocks, s64 addnblocks);
+static int dbFindBits(u32 word, int l2nb);
+static int dbFindCtl(bmap_t * bmp, int l2nb, int level, s64 * blkno);
+static int dbFindLeaf(dmtree_t * tp, int l2nb, int *leafidx);
+static void dbFreeBits(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks);
+static int dbFreeDmap(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks);
+static int dbMaxBud(u8 * cp);
+s64 dbMapFileSizeToMapSize(struct inode *ipbmap);
+int blkstol2(s64 nb);
+void fsDirty(void);
+
+int cntlz(u32 value);
+int cnttz(u32 word);
+
+static int dbAllocDmapBU(bmap_t * bmp, dmap_t * dp, s64 blkno,
+			 int nblocks);
+static int dbInitDmap(dmap_t * dp, s64 blkno, int nblocks);
+static int dbInitDmapTree(dmap_t * dp);
+static int dbInitTree(dmaptree_t * dtp);
+static int dbInitDmapCtl(dmapctl_t * dcp, int level, int i);
+static int dbGetL2AGSize(s64 nblocks);
+
+/*
+ *	buddy table
+ *
+ * table used for determining buddy sizes within characters of 
+ * dmap bitmap words.  the characters themselves serve as indexes
+ * into the table, with the table elements yielding the maximum
+ * binary buddy of free bits within the character.
+ */
+signed char budtab[256] = {
+	3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
+	2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, -1
+};
+
+
+/*
+ * NAME:    	dbMount()
+ *
+ * FUNCTION:	initializate the block allocation map.
+ *
+ *		memory is allocated for the in-core bmap descriptor and
+ *		the in-core descriptor is initialized from disk.
+ *
+ * PARAMETERS:
+ *      ipbmap	-  pointer to in-core inode for the block map.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOMEM	- insufficient memory
+ *      EIO	- i/o error
+ */
+int dbMount(struct inode *ipbmap)
+{
+	bmap_t *bmp;
+	dbmap_t *dbmp_le;
+	metapage_t *mp;
+	int i;
+
+	/*
+	 * allocate/initialize the in-memory bmap descriptor
+	 */
+	/* allocate memory for the in-memory bmap descriptor */
+	bmp = kmalloc(sizeof(bmap_t), GFP_KERNEL);
+	if (bmp == NULL)
+		return (ENOMEM);
+
+	/* read the on-disk bmap descriptor. */
+	mp = read_metapage(ipbmap,
+			   BMAPBLKNO << JFS_SBI(ipbmap->i_sb)->l2nbperpage,
+			   PSIZE, 0);
+	if (mp == NULL) {
+		kfree(bmp);
+		return (EIO);
+	}
+
+	/* copy the on-disk bmap descriptor to its in-memory version. */
+	dbmp_le = (dbmap_t *) mp->data;
+	bmp->db_mapsize = le64_to_cpu(dbmp_le->dn_mapsize);
+	bmp->db_nfree = le64_to_cpu(dbmp_le->dn_nfree);
+	bmp->db_l2nbperpage = le32_to_cpu(dbmp_le->dn_l2nbperpage);
+	bmp->db_numag = le32_to_cpu(dbmp_le->dn_numag);
+	bmp->db_maxlevel = le32_to_cpu(dbmp_le->dn_maxlevel);
+	bmp->db_maxag = le32_to_cpu(dbmp_le->dn_maxag);
+	bmp->db_agpref = le32_to_cpu(dbmp_le->dn_agpref);
+	bmp->db_aglevel = le32_to_cpu(dbmp_le->dn_aglevel);
+	bmp->db_agheigth = le32_to_cpu(dbmp_le->dn_agheigth);
+	bmp->db_agwidth = le32_to_cpu(dbmp_le->dn_agwidth);
+	bmp->db_agstart = le32_to_cpu(dbmp_le->dn_agstart);
+	bmp->db_agl2size = le32_to_cpu(dbmp_le->dn_agl2size);
+	for (i = 0; i < MAXAG; i++)
+		bmp->db_agfree[i] = le64_to_cpu(dbmp_le->dn_agfree[i]);
+	bmp->db_agsize = le64_to_cpu(dbmp_le->dn_agsize);
+	bmp->db_maxfreebud = dbmp_le->dn_maxfreebud;
+
+	/* release the buffer. */
+	release_metapage(mp);
+
+	/* bind the bmap inode and the bmap descriptor to each other. */
+	bmp->db_ipbmap = ipbmap;
+	JFS_SBI(ipbmap->i_sb)->bmap = bmp;
+
+	DBINITMAP(bmp->db_mapsize, ipbmap, &bmp->db_DBmap);
+
+	/*
+	 * allocate/initialize the bmap lock
+	 */
+	BMAP_LOCK_INIT(bmp);
+
+	return (0);
+}
+
+
+/*
+ * NAME:    	dbUnmount()
+ *
+ * FUNCTION:	terminate the block allocation map in preparation for
+ *		file system unmount.
+ *
+ * 		the in-core bmap descriptor is written to disk and
+ *		the memory for this descriptor is freed.
+ *
+ * PARAMETERS:
+ *      ipbmap	-  pointer to in-core inode for the block map.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ */
+int dbUnmount(struct inode *ipbmap, int mounterror)
+{
+	bmap_t *bmp = JFS_SBI(ipbmap->i_sb)->bmap;
+
+	if (!(mounterror || isReadOnly(ipbmap)))
+		dbSync(ipbmap);
+
+	/*
+	 * Invalidate the page cache buffers
+	 */
+	truncate_inode_pages(ipbmap->i_mapping, 0);
+
+	/* free the memory for the in-memory bmap. */
+	kfree(bmp);
+
+	return (0);
+}
+
+/*
+ *	dbSync()
+ */
+int dbSync(struct inode *ipbmap)
+{
+	dbmap_t *dbmp_le;
+	bmap_t *bmp = JFS_SBI(ipbmap->i_sb)->bmap;
+	metapage_t *mp;
+	int i;
+
+	/*
+	 * write bmap global control page
+	 */
+	/* get the buffer for the on-disk bmap descriptor. */
+	mp = read_metapage(ipbmap,
+			   BMAPBLKNO << JFS_SBI(ipbmap->i_sb)->l2nbperpage,
+			   PSIZE, 0);
+	if (mp == NULL) {
+		jERROR(1,("dbSync: read_metapage failed!\n"));
+		return (EIO);
+	}
+	/* copy the in-memory version of the bmap to the on-disk version */
+	dbmp_le = (dbmap_t *) mp->data;
+	dbmp_le->dn_mapsize = cpu_to_le64(bmp->db_mapsize);
+	dbmp_le->dn_nfree = cpu_to_le64(bmp->db_nfree);
+	dbmp_le->dn_l2nbperpage = cpu_to_le32(bmp->db_l2nbperpage);
+	dbmp_le->dn_numag = cpu_to_le32(bmp->db_numag);
+	dbmp_le->dn_maxlevel = cpu_to_le32(bmp->db_maxlevel);
+	dbmp_le->dn_maxag = cpu_to_le32(bmp->db_maxag);
+	dbmp_le->dn_agpref = cpu_to_le32(bmp->db_agpref);
+	dbmp_le->dn_aglevel = cpu_to_le32(bmp->db_aglevel);
+	dbmp_le->dn_agheigth = cpu_to_le32(bmp->db_agheigth);
+	dbmp_le->dn_agwidth = cpu_to_le32(bmp->db_agwidth);
+	dbmp_le->dn_agstart = cpu_to_le32(bmp->db_agstart);
+	dbmp_le->dn_agl2size = cpu_to_le32(bmp->db_agl2size);
+	for (i = 0; i < MAXAG; i++)
+		dbmp_le->dn_agfree[i] = cpu_to_le64(bmp->db_agfree[i]);
+	dbmp_le->dn_agsize = cpu_to_le64(bmp->db_agsize);
+	dbmp_le->dn_maxfreebud = bmp->db_maxfreebud;
+
+	/* write the buffer */
+	write_metapage(mp);
+
+	/*
+	 * write out dirty pages of bmap
+	 */
+	fsync_inode_data_buffers(ipbmap);
+
+	ipbmap->i_state |= I_DIRTY;
+	diWriteSpecial(ipbmap);
+
+	return (0);
+}
+
+
+/*
+ * NAME:    	dbFree()
+ *
+ * FUNCTION:	free the specified block range from the working block
+ *		allocation map.
+ *
+ *		the blocks will be free from the working map one dmap
+ *		at a time.
+ *
+ * PARAMETERS:
+ *      ip	-  pointer to in-core inode;
+ *      blkno	-  starting block number to be freed.
+ *      nblocks	-  number of blocks to be freed.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ */
+int dbFree(struct inode *ip, s64 blkno, s64 nblocks)
+{
+	metapage_t *mp;
+	dmap_t *dp;
+	int nb, rc;
+	s64 lblkno, rem;
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	bmap_t *bmp = JFS_SBI(ip->i_sb)->bmap;
+
+	IREAD_LOCK(ipbmap);
+
+	/* block to be freed better be within the mapsize. */
+	assert(blkno + nblocks <= bmp->db_mapsize);
+
+	/*
+	 * free the blocks a dmap at a time.
+	 */
+	mp = NULL;
+	for (rem = nblocks; rem > 0; rem -= nb, blkno += nb) {
+		/* release previous dmap if any */
+		if (mp) {
+			write_metapage(mp);
+		}
+
+		/* get the buffer for the current dmap. */
+		lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+		mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			IREAD_UNLOCK(ipbmap);
+			return (EIO);
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* determine the number of blocks to be freed from
+		 * this dmap.
+		 */
+		nb = min(rem, BPERDMAP - (blkno & (BPERDMAP - 1)));
+
+		DBALLOCCK(bmp->db_DBmap, bmp->db_mapsize, blkno, nb);
+
+		/* free the blocks. */
+		if ((rc = dbFreeDmap(bmp, dp, blkno, nb))) {
+			release_metapage(mp);
+			IREAD_UNLOCK(ipbmap);
+			return (rc);
+		}
+
+		DBFREE(bmp->db_DBmap, bmp->db_mapsize, blkno, nb);
+	}
+
+	/* write the last buffer. */
+	write_metapage(mp);
+
+	IREAD_UNLOCK(ipbmap);
+
+	return (0);
+}
+
+
+/*
+ * NAME:	dbUpdatePMap()
+ *
+ * FUNCTION:    update the allocation state (free or allocate) of the
+ *		specified block range in the persistent block allocation map.
+ *		
+ *		the blocks will be updated in the persistent map one
+ *		dmap at a time.
+ *
+ * PARAMETERS:
+ *      ipbmap	-  pointer to in-core inode for the block map.
+ *      free	- TRUE if block range is to be freed from the persistent
+ *		  map; FALSE if it is to   be allocated.
+ *      blkno	-  starting block number of the range.
+ *      nblocks	-  number of contiguous blocks in the range.
+ *      tblk	-  transaction block;
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ */
+int
+dbUpdatePMap(struct inode *ipbmap,
+	     int free, s64 blkno, s64 nblocks, tblock_t * tblk)
+{
+	int nblks, dbitno, wbitno, rbits;
+	int word, nbits, nwords;
+	bmap_t *bmp = JFS_SBI(ipbmap->i_sb)->bmap;
+	s64 lblkno, rem, lastlblkno;
+	u32 mask;
+	dmap_t *dp;
+	metapage_t *mp;
+	log_t *log;
+	int lsn, difft, diffp;
+
+	/* the blocks better be within the mapsize. */
+	assert(blkno + nblocks <= bmp->db_mapsize);
+
+	/* compute delta of transaction lsn from log syncpt */
+	lsn = tblk->lsn;
+	log = (log_t *) JFS_SBI(tblk->sb)->log;
+	logdiff(difft, lsn, log);
+
+	/*
+	 * update the block state a dmap at a time.
+	 */
+	mp = NULL;
+	lastlblkno = 0;
+	for (rem = nblocks; rem > 0; rem -= nblks, blkno += nblks) {
+		/* get the buffer for the current dmap. */
+		lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+		if (lblkno != lastlblkno) {
+			if (mp) {
+				write_metapage(mp);
+			}
+
+			mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE,
+					   0);
+			if (mp == NULL)
+				return (EIO);
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* determine the bit number and word within the dmap of
+		 * the starting block.  also determine how many blocks
+		 * are to be updated within this dmap.
+		 */
+		dbitno = blkno & (BPERDMAP - 1);
+		word = dbitno >> L2DBWORD;
+		nblks = min(rem, (s64)BPERDMAP - dbitno);
+
+		/* update the bits of the dmap words. the first and last
+		 * words may only have a subset of their bits updated. if
+		 * this is the case, we'll work against that word (i.e.
+		 * partial first and/or last) only in a single pass.  a 
+		 * single pass will also be used to update all words that
+		 * are to have all their bits updated.
+		 */
+		for (rbits = nblks; rbits > 0;
+		     rbits -= nbits, dbitno += nbits) {
+			/* determine the bit number within the word and
+			 * the number of bits within the word.
+			 */
+			wbitno = dbitno & (DBWORD - 1);
+			nbits = min(rbits, DBWORD - wbitno);
+
+			/* check if only part of the word is to be updated. */
+			if (nbits < DBWORD) {
+				/* update (free or allocate) the bits
+				 * in this word.
+				 */
+				mask =
+				    (ONES << (DBWORD - nbits) >> wbitno);
+				if (free)
+					dp->pmap[word] &=
+					    cpu_to_le32(~mask);
+				else
+					dp->pmap[word] |=
+					    cpu_to_le32(mask);
+
+				word += 1;
+			} else {
+				/* one or more words are to have all
+				 * their bits updated.  determine how
+				 * many words and how many bits.
+				 */
+				nwords = rbits >> L2DBWORD;
+				nbits = nwords << L2DBWORD;
+
+				/* update (free or allocate) the bits
+				 * in these words.
+				 */
+				if (free)
+					memset(&dp->pmap[word], 0,
+					       nwords * 4);
+				else
+					memset(&dp->pmap[word], (int) ONES,
+					       nwords * 4);
+
+				word += nwords;
+			}
+		}
+
+		/*
+		 * update dmap lsn
+		 */
+		if (lblkno == lastlblkno)
+			continue;
+
+		lastlblkno = lblkno;
+
+		if (mp->lsn != 0) {
+			/* inherit older/smaller lsn */
+			logdiff(diffp, mp->lsn, log);
+			if (difft < diffp) {
+				mp->lsn = lsn;
+
+				/* move bp after tblock in logsync list */
+				LOGSYNC_LOCK(log);
+				list_del(&mp->synclist);
+				list_add(&mp->synclist, &tblk->synclist);
+				LOGSYNC_UNLOCK(log);
+			}
+
+			/* inherit younger/larger clsn */
+			LOGSYNC_LOCK(log);
+			logdiff(difft, tblk->clsn, log);
+			logdiff(diffp, mp->clsn, log);
+			if (difft > diffp)
+				mp->clsn = tblk->clsn;
+			LOGSYNC_UNLOCK(log);
+		} else {
+			mp->log = log;
+			mp->lsn = lsn;
+
+			/* insert bp after tblock in logsync list */
+			LOGSYNC_LOCK(log);
+
+			log->count++;
+			list_add(&mp->synclist, &tblk->synclist);
+
+			mp->clsn = tblk->clsn;
+			LOGSYNC_UNLOCK(log);
+		}
+	}
+
+	/* write the last buffer. */
+	if (mp) {
+		write_metapage(mp);
+	}
+
+	return (0);
+}
+
+
+/*
+ * NAME:	dbNextAG()
+ *
+ * FUNCTION:    find the preferred allocation group for new allocations.
+ *
+ *		we try to keep the trailing (rightmost) allocation groups
+ *		free for large allocations.  we try to do this by targeting
+ *		new inode allocations towards the leftmost or 'active'
+ *		allocation groups while keeping the rightmost or 'inactive'
+ *		allocation groups free. once the active allocation groups
+ *		have dropped to a certain percentage of free space, we add
+ *		the leftmost inactive allocation group to the active set.
+ *
+ *		within the active allocation groups, we maintain a preferred
+ *		allocation group which consists of a group with at least
+ *		average free space over the active set. it is the preferred
+ *		group that we target new inode allocation towards.  the 
+ *		tie-in between inode allocation and block allocation occurs
+ *		as we allocate the first (data) block of an inode and specify
+ *		the inode (block) as the allocation hint for this block.
+ *
+ * PARAMETERS:
+ *      ipbmap	-  pointer to in-core inode for the block map.
+ *
+ * RETURN VALUES:
+ *      the preferred allocation group number.
+ *
+ * note: only called by dbAlloc();
+ */
+int dbNextAG(struct inode *ipbmap)
+{
+	s64 avgfree, inactfree, actfree, rem;
+	int actags, inactags, l2agsize;
+	bmap_t *bmp = JFS_SBI(ipbmap->i_sb)->bmap;
+
+	BMAP_LOCK(bmp);
+
+	/* determine the number of active allocation groups (i.e.
+	 * the number of allocation groups up to and including
+	 * the rightmost allocation group with blocks allocated
+	 * in it.
+	 */
+	actags = bmp->db_maxag + 1;
+	assert(actags <= bmp->db_numag);
+
+	/* get the number of inactive allocation groups (i.e. the
+	 * number of allocation group following the rightmost group
+	 * with allocation in it.
+	 */
+	inactags = bmp->db_numag - actags;
+
+	/* determine how many blocks are in the inactive allocation
+	 * groups. in doing this, we must account for the fact that
+	 * the rightmost group might be a partial group (i.e. file
+	 * system size is not a multiple of the group size).
+	 */
+	l2agsize = bmp->db_agl2size;
+	rem = bmp->db_mapsize & (bmp->db_agsize - 1);
+	inactfree = (inactags
+		     && rem) ? ((inactags - 1) << l2agsize) +
+	    rem : inactags << l2agsize;
+
+	/* now determine how many free blocks are in the active
+	 * allocation groups plus the average number of free blocks
+	 * within the active ags.
+	 */
+	actfree = bmp->db_nfree - inactfree;
+	avgfree = (u32) actfree / (u32) actags;
+
+	/* check if not all of the allocation groups are active.
+	 */
+	if (actags < bmp->db_numag) {
+		/* not all of the allocation groups are active.  determine
+		 * if we should extend the active set by 1 (i.e. add the
+		 * group following the current active set).  we do so if
+		 * the number of free blocks within the active set is less
+		 * than the allocation group set and average free within
+		 * the active set is less than 60%.  we activate a new group
+		 * by setting the allocation group preference to the new
+		 * group.
+		 */
+		if (actfree < bmp->db_agsize &&
+		    ((avgfree * 100) >> l2agsize) < 60)
+			bmp->db_agpref = actags;
+	} else {
+		/* all allocation groups are in the active set.  check if
+		 * the preferred allocation group has average free space.
+		 * if not, re-establish the preferred group as the leftmost
+		 * group with average free space.
+		 */
+		if (bmp->db_agfree[bmp->db_agpref] < avgfree) {
+			for (bmp->db_agpref = 0; bmp->db_agpref < actags;
+			     bmp->db_agpref++) {
+				if (bmp->db_agfree[bmp->db_agpref] <=
+				    avgfree)
+					break;
+			}
+			assert(bmp->db_agpref < bmp->db_numag);
+		}
+	}
+
+	BMAP_UNLOCK(bmp);
+
+	/* return the preferred group.
+	 */
+	return (bmp->db_agpref);
+}
+
+
+/*
+ * NAME:	dbAlloc()
+ *
+ * FUNCTION:    attempt to allocate a specified number of contiguous free
+ *		blocks from the working allocation block map.
+ *
+ *		the block allocation policy uses hints and a multi-step
+ *		approach.
+ *
+ *	  	for allocation requests smaller than the number of blocks
+ *		per dmap, we first try to allocate the new blocks
+ *		immediately following the hint.  if these blocks are not
+ *		available, we try to allocate blocks near the hint.  if
+ *		no blocks near the hint are available, we next try to 
+ *		allocate within the same dmap as contains the hint.
+ *
+ *		if no blocks are available in the dmap or the allocation
+ *		request is larger than the dmap size, we try to allocate
+ *		within the same allocation group as contains the hint. if
+ *		this does not succeed, we finally try to allocate anywhere
+ *		within the aggregate.
+ *
+ *		we also try to allocate anywhere within the aggregate for
+ *		for allocation requests larger than the allocation group
+ *		size or requests that specify no hint value.
+ *
+ * PARAMETERS:
+ *      ip	-  pointer to in-core inode;
+ *      hint	- allocation hint.
+ *      nblocks	- number of contiguous blocks in the range.
+ *      results	- on successful return, set to the starting block number
+ *		  of the newly allocated contiguous range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ */
+int dbAlloc(struct inode *ip, s64 hint, s64 nblocks, s64 * results)
+{
+	int rc, agno;
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	bmap_t *bmp;
+	metapage_t *mp;
+	s64 lblkno, blkno;
+	dmap_t *dp;
+	int l2nb;
+	s64 mapSize;
+
+	/* assert that nblocks is valid */
+	assert(nblocks > 0);
+
+#ifdef _STILL_TO_PORT
+	/* DASD limit check                                     F226941 */
+	if (OVER_LIMIT(ip, nblocks))
+		return ENOSPC;
+#endif				/* _STILL_TO_PORT */
+
+	/* get the log2 number of blocks to be allocated.
+	 * if the number of blocks is not a log2 multiple, 
+	 * it will be rounded up to the next log2 multiple.
+	 */
+	l2nb = BLKSTOL2(nblocks);
+
+	bmp = JFS_SBI(ip->i_sb)->bmap;
+
+//retry:        /* serialize w.r.t.extendfs() */
+	mapSize = bmp->db_mapsize;
+
+	/* the hint should be within the map */
+	assert(hint < mapSize);
+
+	/* if no hint was specified or the number of blocks to be
+	 * allocated is greater than the allocation group size, try
+	 * to allocate anywhere.
+	 */
+	if (hint == 0 || l2nb > bmp->db_agl2size) {
+		IWRITE_LOCK(ipbmap);
+
+		rc = dbAllocAny(bmp, nblocks, l2nb, results);
+		if (rc == 0) {
+			DBALLOC(bmp->db_DBmap, bmp->db_mapsize, *results,
+				nblocks);
+		}
+
+		IWRITE_UNLOCK(ipbmap);
+		return (rc);
+	}
+
+	/* we would like to allocate close to the hint.  adjust the
+	 * hint to the block following the hint since the allocators
+	 * will start looking for free space starting at this point.
+	 * if the hint was the last block of the file system, try to
+	 * allocate in the same allocation group as the hint.
+	 */
+	blkno = hint + 1;
+	if (blkno >= bmp->db_mapsize) {
+		blkno--;
+		goto tryag;
+	}
+
+	/* check if blkno crosses over into a new allocation group.
+	 * if so, check if we should allow allocations within this
+	 * allocation group.  we try to keep the trailing (rightmost)
+	 * allocation groups of the file system free for large
+	 * allocations and may want to prevent this allocation from
+	 * spilling over into this space.
+	 */
+	if ((blkno & (bmp->db_agsize - 1)) == 0) {
+		/* check if the AG is beyond the rightmost AG with
+		 * allocations in it.  if so, call dbNextAG() to
+		 * determine if the allocation should be allowed
+		 * to proceed within this AG or should be targeted
+		 * to another AG.
+		 */
+		agno = blkno >> bmp->db_agl2size;
+		if (agno > bmp->db_maxag) {
+			agno = dbNextAG(ipbmap);
+			blkno = (s64) agno << bmp->db_agl2size;
+			goto tryag;
+		}
+	}
+
+	/* check if the allocation request size can be satisfied from a
+	 * single dmap.  if so, try to allocate from the dmap containing
+	 * the hint using a tiered strategy.
+	 */
+	if (nblocks <= BPERDMAP) {
+		IREAD_LOCK(ipbmap);
+
+		/* get the buffer for the dmap containing the hint.
+		 */
+		lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+		mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			IREAD_UNLOCK(ipbmap);
+			return (EIO);
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* first, try to satisfy the allocation request with the
+		 * blocks beginning at the hint.
+		 */
+		if ((rc =
+		     dbAllocNext(bmp, dp, blkno,
+				 (int) nblocks)) != ENOSPC) {
+			if (rc == 0) {
+				*results = blkno;
+				DBALLOC(bmp->db_DBmap, bmp->db_mapsize,
+					*results, nblocks);
+				write_metapage(mp);
+			} else {
+				assert(rc == EIO);
+				release_metapage(mp);
+			}
+
+			IREAD_UNLOCK(ipbmap);
+			return (rc);
+		}
+
+		/* next, try to satisfy the allocation request with blocks
+		 * near the hint.
+		 */
+		if ((rc =
+		     dbAllocNear(bmp, dp, blkno, (int) nblocks, l2nb,
+				 results))
+		    != ENOSPC) {
+			if (rc == 0) {
+				DBALLOC(bmp->db_DBmap, bmp->db_mapsize,
+					*results, nblocks);
+				mark_metapage_dirty(mp);
+			}
+			release_metapage(mp);
+
+			IREAD_UNLOCK(ipbmap);
+			return (rc);
+		}
+
+		/* try to satisfy the allocation request with blocks within
+		 * the same allocation group as the hint.
+		 */
+		if ((rc =
+		     dbAllocDmapLev(bmp, dp, (int) nblocks, l2nb, results))
+		    != ENOSPC) {
+			if (rc == 0) {
+				DBALLOC(bmp->db_DBmap, bmp->db_mapsize,
+					*results, nblocks);
+				mark_metapage_dirty(mp);
+			}
+			release_metapage(mp);
+
+			IREAD_UNLOCK(ipbmap);
+			return (rc);
+		}
+
+		release_metapage(mp);
+		IREAD_UNLOCK(ipbmap);
+	}
+
+      tryag:
+	IWRITE_LOCK(ipbmap);
+
+	/* determine the allocation group number of the hint and try to
+	 * allocate within this allocation group.  if that fails, try to
+	 * allocate anywhere in the map.
+	 */
+	agno = blkno >> bmp->db_agl2size;
+	if ((rc = dbAllocAG(bmp, agno, nblocks, l2nb, results)) == ENOSPC)
+		rc = dbAllocAny(bmp, nblocks, l2nb, results);
+	if (rc == 0) {
+		DBALLOC(bmp->db_DBmap, bmp->db_mapsize, *results, nblocks);
+	}
+
+	IWRITE_UNLOCK(ipbmap);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbAllocExact()
+ *
+ * FUNCTION:    try to allocate the requested extent;
+ *
+ * PARAMETERS:
+ *      ip	- pointer to in-core inode;
+ *      blkno	- extent address;
+ *      nblocks	- extent length;
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ */
+int dbAllocExact(struct inode *ip, s64 blkno, int nblocks)
+{
+	int rc;
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	bmap_t *bmp = JFS_SBI(ip->i_sb)->bmap;
+	dmap_t *dp;
+	s64 lblkno;
+	metapage_t *mp;
+
+	IREAD_LOCK(ipbmap);
+
+	/*
+	 * validate extent request:
+	 *
+	 * note: defragfs policy:
+	 *  max 64 blocks will be moved.  
+	 *  allocation request size must be satisfied from a single dmap.
+	 */
+	if (nblocks <= 0 || nblocks > BPERDMAP || blkno >= bmp->db_mapsize) {
+		IREAD_UNLOCK(ipbmap);
+		return EINVAL;
+	}
+
+	if (nblocks > ((s64) 1 << bmp->db_maxfreebud)) {
+		/* the free space is no longer available */
+		IREAD_UNLOCK(ipbmap);
+		return ENOSPC;
+	}
+
+	/* read in the dmap covering the extent */
+	lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+	mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+	if (mp == NULL) {
+		IREAD_UNLOCK(ipbmap);
+		return (EIO);
+	}
+	dp = (dmap_t *) mp->data;
+
+	/* try to allocate the requested extent */
+	rc = dbAllocNext(bmp, dp, blkno, nblocks);
+
+	IREAD_UNLOCK(ipbmap);
+
+	if (rc == 0) {
+		DBALLOC(bmp->db_DBmap, bmp->db_mapsize, blkno, nblocks);
+		mark_metapage_dirty(mp);
+	}
+	release_metapage(mp);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbReAlloc()
+ *
+ * FUNCTION:    attempt to extend a current allocation by a specified
+ *		number of blocks.
+ *
+ *		this routine attempts to satisfy the allocation request
+ *		by first trying to extend the existing allocation in
+ *		place by allocating the additional blocks as the blocks
+ *		immediately following the current allocation.  if these
+ *		blocks are not available, this routine will attempt to
+ *		allocate a new set of contiguous blocks large enough
+ *		to cover the existing allocation plus the additional
+ *		number of blocks required.
+ *
+ * PARAMETERS:
+ *      ip	    -  pointer to in-core inode requiring allocation.
+ *      blkno	    -  starting block of the current allocation.
+ *      nblocks	    -  number of contiguous blocks within the current
+ *		       allocation.
+ *      addnblocks  -  number of blocks to add to the allocation.
+ *      results	-      on successful return, set to the starting block number
+ *		       of the existing allocation if the existing allocation
+ *		       was extended in place or to a newly allocated contiguous
+ *		       range if the existing allocation could not be extended
+ *		       in place.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ */
+int
+dbReAlloc(struct inode *ip,
+	  s64 blkno, s64 nblocks, s64 addnblocks, s64 * results)
+{
+	int rc;
+
+	/* try to extend the allocation in place.
+	 */
+	if ((rc = dbExtend(ip, blkno, nblocks, addnblocks)) == 0) {
+		*results = blkno;
+		return (0);
+	} else {
+		if (rc != ENOSPC)
+			return (rc);
+	}
+
+	/* could not extend the allocation in place, so allocate a
+	 * new set of blocks for the entire request (i.e. try to get
+	 * a range of contiguous blocks large enough to cover the
+	 * existing allocation plus the additional blocks.)
+	 */
+	return (dbAlloc
+		(ip, blkno + nblocks - 1, addnblocks + nblocks, results));
+}
+
+
+/*
+ * NAME:	dbExtend()
+ *
+ * FUNCTION:    attempt to extend a current allocation by a specified
+ *		number of blocks.
+ *
+ *		this routine attempts to satisfy the allocation request
+ *		by first trying to extend the existing allocation in
+ *		place by allocating the additional blocks as the blocks
+ *		immediately following the current allocation.
+ *
+ * PARAMETERS:
+ *      ip	    -  pointer to in-core inode requiring allocation.
+ *      blkno	    -  starting block of the current allocation.
+ *      nblocks	    -  number of contiguous blocks within the current
+ *		       allocation.
+ *      addnblocks  -  number of blocks to add to the allocation.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ */
+int dbExtend(struct inode *ip, s64 blkno, s64 nblocks, s64 addnblocks)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	s64 lblkno, lastblkno, extblkno;
+	uint rel_block;
+	metapage_t *mp;
+	dmap_t *dp;
+	int rc;
+	struct inode *ipbmap = sbi->ipbmap;
+	bmap_t *bmp;
+
+	/*
+	 * We don't want a non-aligned extent to cross a page boundary
+	 */
+	if (((rel_block = blkno & (sbi->nbperpage - 1))) &&
+	    (rel_block + nblocks + addnblocks > sbi->nbperpage))
+		return (ENOSPC);
+
+	/* get the last block of the current allocation */
+	lastblkno = blkno + nblocks - 1;
+
+	/* determine the block number of the block following
+	 * the existing allocation.
+	 */
+	extblkno = lastblkno + 1;
+
+	IREAD_LOCK(ipbmap);
+
+	/* better be within the file system */
+	bmp = sbi->bmap;
+	assert(lastblkno >= 0 && lastblkno < bmp->db_mapsize);
+
+	/* we'll attempt to extend the current allocation in place by
+	 * allocating the additional blocks as the blocks immediately
+	 * following the current allocation.  we only try to extend the
+	 * current allocation in place if the number of additional blocks
+	 * can fit into a dmap, the last block of the current allocation
+	 * is not the last block of the file system, and the start of the
+	 * inplace extension is not on an allocation group boundry.
+	 */
+	if (addnblocks > BPERDMAP || extblkno >= bmp->db_mapsize ||
+	    (extblkno & (bmp->db_agsize - 1)) == 0) {
+		IREAD_UNLOCK(ipbmap);
+		return (ENOSPC);
+	}
+
+	/* get the buffer for the dmap containing the first block
+	 * of the extension.
+	 */
+	lblkno = BLKTODMAP(extblkno, bmp->db_l2nbperpage);
+	mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+	if (mp == NULL) {
+		IREAD_UNLOCK(ipbmap);
+		return (EIO);
+	}
+
+	DBALLOCCK(bmp->db_DBmap, bmp->db_mapsize, blkno, nblocks);
+	dp = (dmap_t *) mp->data;
+
+	/* try to allocate the blocks immediately following the
+	 * current allocation.
+	 */
+	rc = dbAllocNext(bmp, dp, extblkno, (int) addnblocks);
+
+	IREAD_UNLOCK(ipbmap);
+
+	/* were we successful ? */
+	if (rc == 0) {
+		DBALLOC(bmp->db_DBmap, bmp->db_mapsize, extblkno,
+			addnblocks);
+		write_metapage(mp);
+	} else {
+		/* we were not successful */
+		release_metapage(mp);
+		assert(rc == ENOSPC || rc == EIO);
+	}
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbAllocNext()
+ *
+ * FUNCTION:    attempt to allocate the blocks of the specified block
+ *		range within a dmap.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap.
+ *      blkno	-  starting block number of the range.
+ *      nblocks	-  number of contiguous free blocks of the range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap) held on entry/exit;
+ */
+static int dbAllocNext(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	int dbitno, word, rembits, nb, nwords, wbitno, nw;
+	int l2size;
+	s8 *leaf;
+	u32 mask;
+
+	/* pick up a pointer to the leaves of the dmap tree.
+	 */
+	leaf = dp->tree.stree + le32_to_cpu(dp->tree.leafidx);
+
+	/* determine the bit number and word within the dmap of the
+	 * starting block.
+	 */
+	dbitno = blkno & (BPERDMAP - 1);
+	word = dbitno >> L2DBWORD;
+
+	/* check if the specified block range is contained within
+	 * this dmap.
+	 */
+	if (dbitno + nblocks > BPERDMAP)
+		return (ENOSPC);
+
+	/* check if the starting leaf indicates that anything
+	 * is free.
+	 */
+	if (leaf[word] == NOFREE)
+		return (ENOSPC);
+
+	/* check the dmaps words corresponding to block range to see
+	 * if the block range is free.  not all bits of the first and
+	 * last words may be contained within the block range.  if this
+	 * is the case, we'll work against those words (i.e. partial first
+	 * and/or last) on an individual basis (a single pass) and examine
+	 * the actual bits to determine if they are free.  a single pass
+	 * will be used for all dmap words fully contained within the
+	 * specified range.  within this pass, the leaves of the dmap
+	 * tree will be examined to determine if the blocks are free. a
+	 * single leaf may describe the free space of multiple dmap
+	 * words, so we may visit only a subset of the actual leaves
+	 * corresponding to the dmap words of the block range.
+	 */
+	for (rembits = nblocks; rembits > 0; rembits -= nb, dbitno += nb) {
+		/* determine the bit number within the word and
+		 * the number of bits within the word.
+		 */
+		wbitno = dbitno & (DBWORD - 1);
+		nb = min(rembits, DBWORD - wbitno);
+
+		/* check if only part of the word is to be examined.
+		 */
+		if (nb < DBWORD) {
+			/* check if the bits are free.
+			 */
+			mask = (ONES << (DBWORD - nb) >> wbitno);
+			if ((mask & ~le32_to_cpu(dp->wmap[word])) != mask)
+				return (ENOSPC);
+
+			word += 1;
+		} else {
+			/* one or more dmap words are fully contained
+			 * within the block range.  determine how many
+			 * words and how many bits.
+			 */
+			nwords = rembits >> L2DBWORD;
+			nb = nwords << L2DBWORD;
+
+			/* now examine the appropriate leaves to determine
+			 * if the blocks are free.
+			 */
+			while (nwords > 0) {
+				/* does the leaf describe any free space ?
+				 */
+				if (leaf[word] < BUDMIN)
+					return (ENOSPC);
+
+				/* determine the l2 number of bits provided
+				 * by this leaf.
+				 */
+				l2size =
+				    min((int)leaf[word], NLSTOL2BSZ(nwords));
+
+				/* determine how many words were handled.
+				 */
+				nw = BUDSIZE(l2size, BUDMIN);
+
+				nwords -= nw;
+				word += nw;
+			}
+		}
+	}
+
+	/* allocate the blocks.
+	 */
+	return (dbAllocDmap(bmp, dp, blkno, nblocks));
+}
+
+
+/*
+ * NAME:	dbAllocNear()
+ *
+ * FUNCTION:    attempt to allocate a number of contiguous free blocks near
+ *		a specified block (hint) within a dmap.
+ *
+ *		starting with the dmap leaf that covers the hint, we'll
+ *		check the next four contiguous leaves for sufficient free
+ *		space.  if sufficient free space is found, we'll allocate
+ *		the desired free space.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap.
+ *      blkno	-  block number to allocate near.
+ *      nblocks	-  actual number of contiguous free blocks desired.
+ *      l2nb	-  log2 number of contiguous free blocks desired.
+ *      results	-  on successful return, set to the starting block number
+ *		   of the newly allocated range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap) held on entry/exit;
+ */
+static int
+dbAllocNear(bmap_t * bmp,
+	    dmap_t * dp, s64 blkno, int nblocks, int l2nb, s64 * results)
+{
+	int word, lword, rc;
+	s8 *leaf = dp->tree.stree + le32_to_cpu(dp->tree.leafidx);
+
+	/* determine the word within the dmap that holds the hint
+	 * (i.e. blkno).  also, determine the last word in the dmap
+	 * that we'll include in our examination.
+	 */
+	word = (blkno & (BPERDMAP - 1)) >> L2DBWORD;
+	lword = min(word + 4, LPERDMAP);
+
+	/* examine the leaves for sufficient free space.
+	 */
+	for (; word < lword; word++) {
+		/* does the leaf describe sufficient free space ?
+		 */
+		if (leaf[word] < l2nb)
+			continue;
+
+		/* determine the block number within the file system
+		 * of the first block described by this dmap word.
+		 */
+		blkno = le64_to_cpu(dp->start) + (word << L2DBWORD);
+
+		/* if not all bits of the dmap word are free, get the
+		 * starting bit number within the dmap word of the required
+		 * string of free bits and adjust the block number with the
+		 * value.
+		 */
+		if (leaf[word] < BUDMIN)
+			blkno +=
+			    dbFindBits(le32_to_cpu(dp->wmap[word]), l2nb);
+
+		/* allocate the blocks.
+		 */
+		if ((rc = dbAllocDmap(bmp, dp, blkno, nblocks)) == 0)
+			*results = blkno;
+
+		return (rc);
+	}
+
+	return (ENOSPC);
+}
+
+
+/*
+ * NAME:	dbAllocAG()
+ *
+ * FUNCTION:    attempt to allocate the specified number of contiguous
+ *		free blocks within the specified allocation group.
+ *
+ *		unless the allocation group size is equal to the number
+ *		of blocks per dmap, the dmap control pages will be used to
+ *		find the required free space, if available.  we start the
+ *		search at the highest dmap control page level which
+ *		distinctly describes the allocation group's free space
+ *		(i.e. the highest level at which the allocation group's
+ *		free space is not mixed in with that of any other group).
+ *		in addition, we start the search within this level at a
+ *		height of the dmapctl dmtree at which the nodes distinctly
+ *		describe the allocation group's free space.  at this height,
+ *		the allocation group's free space may be represented by 1
+ *		or two sub-trees, depending on the allocation group size.
+ *		we search the top nodes of these subtrees left to right for
+ *		sufficient free space.  if sufficient free space is found,
+ *		the subtree is searched to find the leftmost leaf that 
+ *		has free space.  once we have made it to the leaf, we
+ *		move the search to the next lower level dmap control page
+ *		corresponding to this leaf.  we continue down the dmap control
+ *		pages until we find the dmap that contains or starts the
+ *		sufficient free space and we allocate at this dmap.
+ *
+ *		if the allocation group size is equal to the dmap size,
+ *		we'll start at the dmap corresponding to the allocation
+ *		group and attempt the allocation at this level.
+ *
+ *		the dmap control page search is also not performed if the
+ *		allocation group is completely free and we go to the first
+ *		dmap of the allocation group to do the allocation.  this is
+ *		done because the allocation group may be part (not the first
+ *		part) of a larger binary buddy system, causing the dmap
+ *		control pages to indicate no free space (NOFREE) within
+ *		the allocation group.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *	agno	- allocation group number.
+ *      nblocks	-  actual number of contiguous free blocks desired.
+ *      l2nb	-  log2 number of contiguous free blocks desired.
+ *      results	-  on successful return, set to the starting block number
+ *		   of the newly allocated range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * note: IWRITE_LOCK(ipmap) held on entry/exit;
+ */
+static int
+dbAllocAG(bmap_t * bmp, int agno, s64 nblocks, int l2nb, s64 * results)
+{
+	metapage_t *mp;
+	dmapctl_t *dcp;
+	int rc, ti, i, k, m, n, agperlev;
+	s64 blkno, lblkno;
+	int budmin;
+
+	/* allocation request should not be for more than the
+	 * allocation group size.
+	 */
+	assert(l2nb <= bmp->db_agl2size);
+
+	/* determine the starting block number of the allocation
+	 * group.
+	 */
+	blkno = (s64) agno << bmp->db_agl2size;
+
+	/* check if the allocation group size is the minimum allocation
+	 * group size or if the allocation group is completely free. if
+	 * the allocation group size is the minimum size of BPERDMAP (i.e.
+	 * 1 dmap), there is no need to search the dmap control page (below)
+	 * that fully describes the allocation group since the allocation
+	 * group is already fully described by a dmap.  in this case, we
+	 * just call dbAllocCtl() to search the dmap tree and allocate the
+	 * required space if available.  
+	 *
+	 * if the allocation group is completely free, dbAllocCtl() is
+	 * also called to allocate the required space.  this is done for
+	 * two reasons.  first, it makes no sense searching the dmap control
+	 * pages for free space when we know that free space exists.  second,
+	 * the dmap control pages may indicate that the allocation group
+	 * has no free space if the allocation group is part (not the first
+	 * part) of a larger binary buddy system.
+	 */
+	if (bmp->db_agsize == BPERDMAP
+	    || bmp->db_agfree[agno] == bmp->db_agsize) {
+		rc = dbAllocCtl(bmp, nblocks, l2nb, blkno, results);
+		/* assert(!(rc == ENOSPC && bmp->db_agfree[agno] == bmp->db_agsize)); */
+		if ((rc == ENOSPC) &&
+		    (bmp->db_agfree[agno] == bmp->db_agsize)) {
+			jERROR(1,
+			       ("dbAllocAG: removed assert, but still need to debug here\nblkno = 0x%Lx, nblocks = 0x%Lx\n",
+				(unsigned long long) blkno,
+				(unsigned long long) nblocks));
+		}
+		return (rc);
+	}
+
+	/* the buffer for the dmap control page that fully describes the
+	 * allocation group.
+	 */
+	lblkno = BLKTOCTL(blkno, bmp->db_l2nbperpage, bmp->db_aglevel);
+	mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+	if (mp == NULL)
+		return (EIO);
+	dcp = (dmapctl_t *) mp->data;
+	budmin = dcp->budmin;
+
+	/* search the subtree(s) of the dmap control page that describes
+	 * the allocation group, looking for sufficient free space.  to begin,
+	 * determine how many allocation groups are represented in a dmap
+	 * control page at the control page level (i.e. L0, L1, L2) that
+	 * fully describes an allocation group. next, determine the starting
+	 * tree index of this allocation group within the control page.
+	 */
+	agperlev =
+	    (1 << (L2LPERCTL - (bmp->db_agheigth << 1))) / bmp->db_agwidth;
+	ti = bmp->db_agstart + bmp->db_agwidth * (agno & (agperlev - 1));
+
+	/* dmap control page trees fan-out by 4 and a single allocation 
+	 * group may be described by 1 or 2 subtrees within the ag level
+	 * dmap control page, depending upon the ag size. examine the ag's
+	 * subtrees for sufficient free space, starting with the leftmost
+	 * subtree.
+	 */
+	for (i = 0; i < bmp->db_agwidth; i++, ti++) {
+		/* is there sufficient free space ?
+		 */
+		if (l2nb > dcp->stree[ti])
+			continue;
+
+		/* sufficient free space found in a subtree. now search down
+		 * the subtree to find the leftmost leaf that describes this
+		 * free space.
+		 */
+		for (k = bmp->db_agheigth; k > 0; k--) {
+			for (n = 0, m = (ti << 2) + 1; n < 4; n++) {
+				if (l2nb <= dcp->stree[m + n]) {
+					ti = m + n;
+					break;
+				}
+			}
+			assert(n < 4);
+		}
+
+		/* determine the block number within the file system
+		 * that corresponds to this leaf.
+		 */
+		if (bmp->db_aglevel == 2)
+			blkno = 0;
+		else if (bmp->db_aglevel == 1)
+			blkno &= ~(MAXL1SIZE - 1);
+		else		/* bmp->db_aglevel == 0 */
+			blkno &= ~(MAXL0SIZE - 1);
+
+		blkno +=
+		    ((s64) (ti - le32_to_cpu(dcp->leafidx))) << budmin;
+
+		/* release the buffer in preparation for going down
+		 * the next level of dmap control pages.
+		 */
+		release_metapage(mp);
+
+		/* check if we need to continue to search down the lower
+		 * level dmap control pages.  we need to if the number of
+		 * blocks required is less than maximum number of blocks
+		 * described at the next lower level.
+		 */
+		if (l2nb < budmin) {
+
+			/* search the lower level dmap control pages to get
+			 * the starting block number of the the dmap that
+			 * contains or starts off the free space.
+			 */
+			if ((rc =
+			     dbFindCtl(bmp, l2nb, bmp->db_aglevel - 1,
+				       &blkno))) {
+				assert(rc != ENOSPC);
+				return (rc);
+			}
+		}
+
+		/* allocate the blocks.
+		 */
+		rc = dbAllocCtl(bmp, nblocks, l2nb, blkno, results);
+		assert(rc != ENOSPC);
+		return (rc);
+	}
+
+	/* no space in the allocation group.  release the buffer and
+	 * return ENOSPC.
+	 */
+	release_metapage(mp);
+
+	return (ENOSPC);
+}
+
+
+/*
+ * NAME:	dbAllocAny()
+ *
+ * FUNCTION:    attempt to allocate the specified number of contiguous
+ *		free blocks anywhere in the file system.
+ *
+ *		dbAllocAny() attempts to find the sufficient free space by
+ *		searching down the dmap control pages, starting with the
+ *		highest level (i.e. L0, L1, L2) control page.  if free space
+ *		large enough to satisfy the desired free space is found, the
+ *		desired free space is allocated.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      nblocks	 -  actual number of contiguous free blocks desired.
+ *      l2nb	 -  log2 number of contiguous free blocks desired.
+ *      results	-  on successful return, set to the starting block number
+ *		   of the newly allocated range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int dbAllocAny(bmap_t * bmp, s64 nblocks, int l2nb, s64 * results)
+{
+	int rc;
+	s64 blkno = 0;
+
+	/* starting with the top level dmap control page, search
+	 * down the dmap control levels for sufficient free space.
+	 * if free space is found, dbFindCtl() returns the starting
+	 * block number of the dmap that contains or starts off the
+	 * range of free space.
+	 */
+	if ((rc = dbFindCtl(bmp, l2nb, bmp->db_maxlevel, &blkno)))
+		return (rc);
+
+	/* allocate the blocks.
+	 */
+	rc = dbAllocCtl(bmp, nblocks, l2nb, blkno, results);
+	assert(rc != ENOSPC);
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbFindCtl()
+ *
+ * FUNCTION:    starting at a specified dmap control page level and block
+ *		number, search down the dmap control levels for a range of
+ *	        contiguous free blocks large enough to satisfy an allocation
+ *		request for the specified number of free blocks.
+ *
+ *		if sufficient contiguous free blocks are found, this routine
+ *		returns the starting block number within a dmap page that
+ *		contains or starts a range of contiqious free blocks that
+ *		is sufficient in size.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      level	-  starting dmap control page level.
+ *      l2nb	-  log2 number of contiguous free blocks desired.
+ *      *blkno	-  on entry, starting block number for conducting the search.
+ *		   on successful return, the first block within a dmap page
+ *		   that contains or starts a range of contiguous free blocks.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int dbFindCtl(bmap_t * bmp, int l2nb, int level, s64 * blkno)
+{
+	int rc, leafidx, lev;
+	s64 b, lblkno;
+	dmapctl_t *dcp;
+	int budmin;
+	metapage_t *mp;
+
+	/* starting at the specified dmap control page level and block
+	 * number, search down the dmap control levels for the starting
+	 * block number of a dmap page that contains or starts off 
+	 * sufficient free blocks.
+	 */
+	for (lev = level, b = *blkno; lev >= 0; lev--) {
+		/* get the buffer of the dmap control page for the block
+		 * number and level (i.e. L0, L1, L2).
+		 */
+		lblkno = BLKTOCTL(b, bmp->db_l2nbperpage, lev);
+		mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL)
+			return (EIO);
+		dcp = (dmapctl_t *) mp->data;
+		budmin = dcp->budmin;
+
+		/* search the tree within the dmap control page for
+		 * sufficent free space.  if sufficient free space is found,
+		 * dbFindLeaf() returns the index of the leaf at which
+		 * free space was found.
+		 */
+		rc = dbFindLeaf((dmtree_t *) dcp, l2nb, &leafidx);
+
+		/* release the buffer.
+		 */
+		release_metapage(mp);
+
+		/* space found ?
+		 */
+		if (rc) {
+			assert(lev == level);
+			return (ENOSPC);
+		}
+
+		/* adjust the block number to reflect the location within
+		 * the dmap control page (i.e. the leaf) at which free 
+		 * space was found.
+		 */
+		b += (((s64) leafidx) << budmin);
+
+		/* we stop the search at this dmap control page level if
+		 * the number of blocks required is greater than or equal
+		 * to the maximum number of blocks described at the next
+		 * (lower) level.
+		 */
+		if (l2nb >= budmin)
+			break;
+	}
+
+	*blkno = b;
+	return (0);
+}
+
+
+/*
+ * NAME:	dbAllocCtl()
+ *
+ * FUNCTION:    attempt to allocate a specified number of contiguous
+ *		blocks starting within a specific dmap.  
+ *		
+ *		this routine is called by higher level routines that search
+ *		the dmap control pages above the actual dmaps for contiguous
+ *		free space.  the result of successful searches by these
+ * 		routines are the starting block numbers within dmaps, with
+ *		the dmaps themselves containing the desired contiguous free
+ *		space or starting a contiguous free space of desired size
+ *		that is made up of the blocks of one or more dmaps. these
+ *		calls should not fail due to insufficent resources.
+ *
+ *		this routine is called in some cases where it is not known
+ *		whether it will fail due to insufficient resources.  more
+ *		specifically, this occurs when allocating from an allocation
+ *		group whose size is equal to the number of blocks per dmap.
+ *		in this case, the dmap control pages are not examined prior
+ *		to calling this routine (to save pathlength) and the call
+ *		might fail.
+ *
+ *		for a request size that fits within a dmap, this routine relies
+ *		upon the dmap's dmtree to find the requested contiguous free
+ *		space.  for request sizes that are larger than a dmap, the
+ *		requested free space will start at the first block of the
+ *		first dmap (i.e. blkno).
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      nblocks	 -  actual number of contiguous free blocks to allocate.
+ *      l2nb	 -  log2 number of contiguous free blocks to allocate.
+ *      blkno	 -  starting block number of the dmap to start the allocation
+ *		    from.
+ *      results	-  on successful return, set to the starting block number
+ *		   of the newly allocated range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int
+dbAllocCtl(bmap_t * bmp, s64 nblocks, int l2nb, s64 blkno, s64 * results)
+{
+	int rc, nb;
+	s64 b, lblkno, n;
+	metapage_t *mp;
+	dmap_t *dp;
+
+	/* check if the allocation request is confined to a single dmap.
+	 */
+	if (l2nb <= L2BPERDMAP) {
+		/* get the buffer for the dmap.
+		 */
+		lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+		mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL)
+			return (EIO);
+		dp = (dmap_t *) mp->data;
+
+		/* try to allocate the blocks.
+		 */
+		rc = dbAllocDmapLev(bmp, dp, (int) nblocks, l2nb, results);
+		if (rc == 0)
+			mark_metapage_dirty(mp);
+
+		release_metapage(mp);
+
+		return (rc);
+	}
+
+	/* allocation request involving multiple dmaps. it must start on
+	 * a dmap boundary.
+	 */
+	assert((blkno & (BPERDMAP - 1)) == 0);
+
+	/* allocate the blocks dmap by dmap.
+	 */
+	for (n = nblocks, b = blkno; n > 0; n -= nb, b += nb) {
+		/* get the buffer for the dmap.
+		 */
+		lblkno = BLKTODMAP(b, bmp->db_l2nbperpage);
+		mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			rc = EIO;
+			goto backout;
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* the dmap better be all free.
+		 */
+		assert(dp->tree.stree[ROOT] == L2BPERDMAP);
+
+		/* determine how many blocks to allocate from this dmap.
+		 */
+		nb = min(n, (s64)BPERDMAP);
+
+		/* allocate the blocks from the dmap.
+		 */
+		if ((rc = dbAllocDmap(bmp, dp, b, nb))) {
+			release_metapage(mp);
+			goto backout;
+		}
+
+		/* write the buffer.
+		 */
+		write_metapage(mp);
+	}
+
+	/* set the results (starting block number) and return.
+	 */
+	*results = blkno;
+	return (0);
+
+	/* something failed in handling an allocation request involving
+	 * multiple dmaps.  we'll try to clean up by backing out any
+	 * allocation that has already happened for this request.  if
+	 * we fail in backing out the allocation, we'll mark the file
+	 * system to indicate that blocks have been leaked.
+	 */
+      backout:
+
+	/* try to backout the allocations dmap by dmap.
+	 */
+	for (n = nblocks - n, b = blkno; n > 0;
+	     n -= BPERDMAP, b += BPERDMAP) {
+		/* get the buffer for this dmap.
+		 */
+		lblkno = BLKTODMAP(b, bmp->db_l2nbperpage);
+		mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			/* could not back out.  mark the file system
+			 * to indicate that we have leaked blocks.
+			 */
+			fsDirty();	/* !!! */
+			jERROR(1,
+			       ("dbAllocCtl: I/O Error: Block Leakage.\n"));
+			continue;
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* free the blocks is this dmap.
+		 */
+		if (dbFreeDmap(bmp, dp, b, BPERDMAP)) {
+			/* could not back out.  mark the file system
+			 * to indicate that we have leaked blocks.
+			 */
+			release_metapage(mp);
+			fsDirty();	/* !!! */
+			jERROR(1, ("dbAllocCtl: Block Leakage.\n"));
+			continue;
+		}
+
+		/* write the buffer.
+		 */
+		write_metapage(mp);
+	}
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbAllocDmapLev()
+ *
+ * FUNCTION:    attempt to allocate a specified number of contiguous blocks
+ *		from a specified dmap.
+ *		
+ *		this routine checks if the contiguous blocks are available.
+ *		if so, nblocks of blocks are allocated; otherwise, ENOSPC is
+ *		returned.
+ *
+ * PARAMETERS:
+ *      mp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap to attempt to allocate blocks from. 
+ *      l2nb	-  log2 number of contiguous block desired.
+ *      nblocks	-  actual number of contiguous block desired.
+ *      results	-  on successful return, set to the starting block number
+ *		   of the newly allocated range.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient disk resources
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap), e.g., from dbAlloc(), or 
+ *	IWRITE_LOCK(ipbmap), e.g., dbAllocCtl(), held on entry/exit;
+ */
+static int
+dbAllocDmapLev(bmap_t * bmp,
+	       dmap_t * dp, int nblocks, int l2nb, s64 * results)
+{
+	s64 blkno;
+	int leafidx, rc;
+
+	/* can't be more than a dmaps worth of blocks */
+	assert(l2nb <= L2BPERDMAP);
+
+	/* search the tree within the dmap page for sufficient
+	 * free space.  if sufficient free space is found, dbFindLeaf()
+	 * returns the index of the leaf at which free space was found.
+	 */
+	if (dbFindLeaf((dmtree_t *) & dp->tree, l2nb, &leafidx))
+		return (ENOSPC);
+
+	/* determine the block number within the file system corresponding
+	 * to the leaf at which free space was found.
+	 */
+	blkno = le64_to_cpu(dp->start) + (leafidx << L2DBWORD);
+
+	/* if not all bits of the dmap word are free, get the starting
+	 * bit number within the dmap word of the required string of free
+	 * bits and adjust the block number with this value.
+	 */
+	if (dp->tree.stree[leafidx + LEAFIND] < BUDMIN)
+		blkno += dbFindBits(le32_to_cpu(dp->wmap[leafidx]), l2nb);
+
+	/* allocate the blocks */
+	if ((rc = dbAllocDmap(bmp, dp, blkno, nblocks)) == 0)
+		*results = blkno;
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbAllocDmap()
+ *
+ * FUNCTION:    adjust the disk allocation map to reflect the allocation
+ *		of a specified block range within a dmap.
+ *
+ *		this routine allocates the specified blocks from the dmap
+ *		through a call to dbAllocBits(). if the allocation of the
+ *		block range causes the maximum string of free blocks within
+ *		the dmap to change (i.e. the value of the root of the dmap's
+ *		dmtree), this routine will cause this change to be reflected
+ *		up through the appropriate levels of the dmap control pages
+ *		by a call to dbAdjCtl() for the L0 dmap control page that
+ *		covers this dmap.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap to allocate the block range from.
+ *      blkno	-  starting block number of the block to be allocated.
+ *      nblocks	-  number of blocks to be allocated.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int dbAllocDmap(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	s8 oldroot;
+	int rc;
+
+	/* save the current value of the root (i.e. maximum free string)
+	 * of the dmap tree.
+	 */
+	oldroot = dp->tree.stree[ROOT];
+
+	/* allocate the specified (blocks) bits */
+	dbAllocBits(bmp, dp, blkno, nblocks);
+
+	/* if the root has not changed, done. */
+	if (dp->tree.stree[ROOT] == oldroot)
+		return (0);
+
+	/* root changed. bubble the change up to the dmap control pages.
+	 * if the adjustment of the upper level control pages fails,
+	 * backout the bit allocation (thus making everything consistent).
+	 */
+	if ((rc = dbAdjCtl(bmp, blkno, dp->tree.stree[ROOT], 1, 0)))
+		dbFreeBits(bmp, dp, blkno, nblocks);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbFreeDmap()
+ *
+ * FUNCTION:    adjust the disk allocation map to reflect the allocation
+ *		of a specified block range within a dmap.
+ *
+ *		this routine frees the specified blocks from the dmap through
+ *		a call to dbFreeBits(). if the deallocation of the block range
+ *		causes the maximum string of free blocks within the dmap to
+ *		change (i.e. the value of the root of the dmap's dmtree), this
+ *		routine will cause this change to be reflected up through the
+ *	        appropriate levels of the dmap control pages by a call to
+ *		dbAdjCtl() for the L0 dmap control page that covers this dmap.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap to free the block range from.
+ *      blkno	-  starting block number of the block to be freed.
+ *      nblocks	-  number of blocks to be freed.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int dbFreeDmap(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	s8 oldroot;
+	int rc, word;
+
+	/* save the current value of the root (i.e. maximum free string)
+	 * of the dmap tree.
+	 */
+	oldroot = dp->tree.stree[ROOT];
+
+	/* free the specified (blocks) bits */
+	dbFreeBits(bmp, dp, blkno, nblocks);
+
+	/* if the root has not changed, done. */
+	if (dp->tree.stree[ROOT] == oldroot)
+		return (0);
+
+	/* root changed. bubble the change up to the dmap control pages.
+	 * if the adjustment of the upper level control pages fails,
+	 * backout the deallocation. 
+	 */
+	if ((rc = dbAdjCtl(bmp, blkno, dp->tree.stree[ROOT], 0, 0))) {
+		word = (blkno & (BPERDMAP - 1)) >> L2DBWORD;
+
+		/* as part of backing out the deallocation, we will have
+		 * to back split the dmap tree if the deallocation caused
+		 * the freed blocks to become part of a larger binary buddy
+		 * system.
+		 */
+		if (dp->tree.stree[word] == NOFREE)
+			dbBackSplit((dmtree_t *) & dp->tree, word);
+
+		dbAllocBits(bmp, dp, blkno, nblocks);
+	}
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbAllocBits()
+ *
+ * FUNCTION:    allocate a specified block range from a dmap.
+ *
+ *		this routine updates the dmap to reflect the working
+ *		state allocation of the specified block range. it directly
+ *		updates the bits of the working map and causes the adjustment
+ *		of the binary buddy system described by the dmap's dmtree
+ *		leaves to reflect the bits allocated.  it also causes the
+ *		dmap's dmtree, as a whole, to reflect the allocated range.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap to allocate bits from.
+ *      blkno	-  starting block number of the bits to be allocated.
+ *      nblocks	-  number of bits to be allocated.
+ *
+ * RETURN VALUES: none
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static void dbAllocBits(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	int dbitno, word, rembits, nb, nwords, wbitno, nw, agno;
+	dmtree_t *tp = (dmtree_t *) & dp->tree;
+	int size;
+	s8 *leaf;
+
+	/* pick up a pointer to the leaves of the dmap tree */
+	leaf = dp->tree.stree + LEAFIND;
+
+	/* determine the bit number and word within the dmap of the
+	 * starting block.
+	 */
+	dbitno = blkno & (BPERDMAP - 1);
+	word = dbitno >> L2DBWORD;
+
+	/* block range better be within the dmap */
+	assert(dbitno + nblocks <= BPERDMAP);
+
+	/* allocate the bits of the dmap's words corresponding to the block
+	 * range. not all bits of the first and last words may be contained
+	 * within the block range.  if this is the case, we'll work against
+	 * those words (i.e. partial first and/or last) on an individual basis
+	 * (a single pass), allocating the bits of interest by hand and
+	 * updating the leaf corresponding to the dmap word. a single pass
+	 * will be used for all dmap words fully contained within the
+	 * specified range.  within this pass, the bits of all fully contained
+	 * dmap words will be marked as free in a single shot and the leaves
+	 * will be updated. a single leaf may describe the free space of
+	 * multiple dmap words, so we may update only a subset of the actual
+	 * leaves corresponding to the dmap words of the block range.
+	 */
+	for (rembits = nblocks; rembits > 0; rembits -= nb, dbitno += nb) {
+		/* determine the bit number within the word and
+		 * the number of bits within the word.
+		 */
+		wbitno = dbitno & (DBWORD - 1);
+		nb = min(rembits, DBWORD - wbitno);
+
+		/* check if only part of a word is to be allocated.
+		 */
+		if (nb < DBWORD) {
+			/* allocate (set to 1) the appropriate bits within
+			 * this dmap word.
+			 */
+			dp->wmap[word] |= cpu_to_le32(ONES << (DBWORD - nb)
+						      >> wbitno);
+
+			/* update the leaf for this dmap word. in addition
+			 * to setting the leaf value to the binary buddy max
+			 * of the updated dmap word, dbSplit() will split
+			 * the binary system of the leaves if need be.
+			 */
+			dbSplit(tp, word, BUDMIN,
+				dbMaxBud((u8 *) & dp->wmap[word]));
+
+			word += 1;
+		} else {
+			/* one or more dmap words are fully contained
+			 * within the block range.  determine how many
+			 * words and allocate (set to 1) the bits of these
+			 * words.
+			 */
+			nwords = rembits >> L2DBWORD;
+			memset(&dp->wmap[word], (int) ONES, nwords * 4);
+
+			/* determine how many bits.
+			 */
+			nb = nwords << L2DBWORD;
+
+			/* now update the appropriate leaves to reflect
+			 * the allocated words.
+			 */
+			for (; nwords > 0; nwords -= nw) {
+				assert(leaf[word] >= BUDMIN);
+
+				/* determine what the leaf value should be
+				 * updated to as the minimum of the l2 number
+				 * of bits being allocated and the l2 number
+				 * of bits currently described by this leaf.
+				 */
+				size = min((int)leaf[word], NLSTOL2BSZ(nwords));
+
+				/* update the leaf to reflect the allocation.
+				 * in addition to setting the leaf value to
+				 * NOFREE, dbSplit() will split the binary
+				 * system of the leaves to reflect the current
+				 * allocation (size).
+				 */
+				dbSplit(tp, word, size, NOFREE);
+
+				/* get the number of dmap words handled */
+				nw = BUDSIZE(size, BUDMIN);
+				word += nw;
+			}
+		}
+	}
+
+	/* update the free count for this dmap */
+	dp->nfree = cpu_to_le32(le32_to_cpu(dp->nfree) - nblocks);
+
+	BMAP_LOCK(bmp);
+
+	/* if this allocation group is completely free,
+	 * update the maximum allocation group number if this allocation
+	 * group is the new max.
+	 */
+	agno = blkno >> bmp->db_agl2size;
+	if (agno > bmp->db_maxag)
+		bmp->db_maxag = agno;
+
+	/* update the free count for the allocation group and map */
+	bmp->db_agfree[agno] -= nblocks;
+	bmp->db_nfree -= nblocks;
+
+	BMAP_UNLOCK(bmp);
+}
+
+
+/*
+ * NAME:	dbFreeBits()
+ *
+ * FUNCTION:    free a specified block range from a dmap.
+ *
+ *		this routine updates the dmap to reflect the working
+ *		state allocation of the specified block range. it directly
+ *		updates the bits of the working map and causes the adjustment
+ *		of the binary buddy system described by the dmap's dmtree
+ *		leaves to reflect the bits freed.  it also causes the dmap's
+ *		dmtree, as a whole, to reflect the deallocated range.
+ *
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      dp	-  pointer to dmap to free bits from.
+ *      blkno	-  starting block number of the bits to be freed.
+ *      nblocks	-  number of bits to be freed.
+ *
+ * RETURN VALUES: none
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static void dbFreeBits(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	int dbitno, word, rembits, nb, nwords, wbitno, nw, agno;
+	dmtree_t *tp = (dmtree_t *) & dp->tree;
+	int size;
+
+	/* determine the bit number and word within the dmap of the
+	 * starting block.
+	 */
+	dbitno = blkno & (BPERDMAP - 1);
+	word = dbitno >> L2DBWORD;
+
+	/* block range better be within the dmap.
+	 */
+	assert(dbitno + nblocks <= BPERDMAP);
+
+	/* free the bits of the dmaps words corresponding to the block range.
+	 * not all bits of the first and last words may be contained within
+	 * the block range.  if this is the case, we'll work against those
+	 * words (i.e. partial first and/or last) on an individual basis
+	 * (a single pass), freeing the bits of interest by hand and updating
+	 * the leaf corresponding to the dmap word. a single pass will be used
+	 * for all dmap words fully contained within the specified range.  
+	 * within this pass, the bits of all fully contained dmap words will
+	 * be marked as free in a single shot and the leaves will be updated. a
+	 * single leaf may describe the free space of multiple dmap words,
+	 * so we may update only a subset of the actual leaves corresponding
+	 * to the dmap words of the block range.
+	 *
+	 * dbJoin() is used to update leaf values and will join the binary
+	 * buddy system of the leaves if the new leaf values indicate this
+	 * should be done.
+	 */
+	for (rembits = nblocks; rembits > 0; rembits -= nb, dbitno += nb) {
+		/* determine the bit number within the word and
+		 * the number of bits within the word.
+		 */
+		wbitno = dbitno & (DBWORD - 1);
+		nb = min(rembits, DBWORD - wbitno);
+
+		/* check if only part of a word is to be freed.
+		 */
+		if (nb < DBWORD) {
+			/* free (zero) the appropriate bits within this
+			 * dmap word. 
+			 */
+			dp->wmap[word] &=
+			    cpu_to_le32(~(ONES << (DBWORD - nb)
+					  >> wbitno));
+
+			/* update the leaf for this dmap word.
+			 */
+			dbJoin(tp, word,
+			       dbMaxBud((u8 *) & dp->wmap[word]));
+
+			word += 1;
+		} else {
+			/* one or more dmap words are fully contained
+			 * within the block range.  determine how many
+			 * words and free (zero) the bits of these words.
+			 */
+			nwords = rembits >> L2DBWORD;
+			memset(&dp->wmap[word], 0, nwords * 4);
+
+			/* determine how many bits.
+			 */
+			nb = nwords << L2DBWORD;
+
+			/* now update the appropriate leaves to reflect
+			 * the freed words.
+			 */
+			for (; nwords > 0; nwords -= nw) {
+				/* determine what the leaf value should be
+				 * updated to as the minimum of the l2 number
+				 * of bits being freed and the l2 (max) number
+				 * of bits that can be described by this leaf.
+				 */
+				size =
+				    min(LITOL2BSZ
+					(word, L2LPERDMAP, BUDMIN),
+					NLSTOL2BSZ(nwords));
+
+				/* update the leaf.
+				 */
+				dbJoin(tp, word, size);
+
+				/* get the number of dmap words handled.
+				 */
+				nw = BUDSIZE(size, BUDMIN);
+				word += nw;
+			}
+		}
+	}
+
+	/* update the free count for this dmap.
+	 */
+	dp->nfree = cpu_to_le32(le32_to_cpu(dp->nfree) + nblocks);
+
+	BMAP_LOCK(bmp);
+
+	/* update the free count for the allocation group and 
+	 * map.
+	 */
+	agno = blkno >> bmp->db_agl2size;
+	bmp->db_nfree += nblocks;
+	bmp->db_agfree[agno] += nblocks;
+
+	/* check if this allocation group is not completely free and
+	 * if it is currently the maximum (rightmost) allocation group.
+	 * if so, establish the new maximum allocation group number by
+	 * searching left for the first allocation group with allocation.
+	 */
+	if ((bmp->db_agfree[agno] == bmp->db_agsize
+	     && agno == bmp->db_maxag) || (agno == bmp->db_numag - 1
+					   && bmp->db_agfree[agno] ==
+					   (bmp-> db_mapsize &
+					    (BPERDMAP - 1)))) {
+		while (bmp->db_maxag > 0) {
+			bmp->db_maxag -= 1;
+			if (bmp->db_agfree[bmp->db_maxag] !=
+			    bmp->db_agsize)
+				break;
+		}
+
+		/* re-establish the allocation group preference if the
+		 * current preference is right of the maximum allocation
+		 * group.
+		 */
+		if (bmp->db_agpref > bmp->db_maxag)
+			bmp->db_agpref = bmp->db_maxag;
+	}
+
+	BMAP_UNLOCK(bmp);
+}
+
+
+/*
+ * NAME:	dbAdjCtl()
+ *
+ * FUNCTION:	adjust a dmap control page at a specified level to reflect
+ *		the change in a lower level dmap or dmap control page's
+ *		maximum string of free blocks (i.e. a change in the root
+ *		of the lower level object's dmtree) due to the allocation
+ *		or deallocation of a range of blocks with a single dmap.
+ *
+ *		on entry, this routine is provided with the new value of
+ *		the lower level dmap or dmap control page root and the
+ *		starting block number of the block range whose allocation
+ *		or deallocation resulted in the root change.  this range
+ *		is respresented by a single leaf of the current dmapctl
+ *		and the leaf will be updated with this value, possibly
+ *		causing a binary buddy system within the leaves to be 
+ *		split or joined.  the update may also cause the dmapctl's
+ *		dmtree to be updated.
+ *
+ *		if the adjustment of the dmap control page, itself, causes its
+ *		root to change, this change will be bubbled up to the next dmap
+ *		control level by a recursive call to this routine, specifying
+ *		the new root value and the next dmap control page level to
+ *		be adjusted.
+ * PARAMETERS:
+ *      bmp	-  pointer to bmap descriptor
+ *      blkno	-  the first block of a block range within a dmap.  it is
+ *		   the allocation or deallocation of this block range that
+ *		   requires the dmap control page to be adjusted.
+ *      newval	-  the new value of the lower level dmap or dmap control
+ *		   page root.
+ *      alloc	-  TRUE if adjustment is due to an allocation.
+ *      level	-  current level of dmap control page (i.e. L0, L1, L2) to
+ *		   be adjusted.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static int
+dbAdjCtl(bmap_t * bmp, s64 blkno, int newval, int alloc, int level)
+{
+	metapage_t *mp;
+	s8 oldroot;
+	int oldval;
+	s64 lblkno;
+	dmapctl_t *dcp;
+	int rc, leafno, ti;
+
+	/* get the buffer for the dmap control page for the specified
+	 * block number and control page level.
+	 */
+	lblkno = BLKTOCTL(blkno, bmp->db_l2nbperpage, level);
+	mp = read_metapage(bmp->db_ipbmap, lblkno, PSIZE, 0);
+	if (mp == NULL)
+		return (EIO);
+	dcp = (dmapctl_t *) mp->data;
+
+	/* determine the leaf number corresponding to the block and
+	 * the index within the dmap control tree.
+	 */
+	leafno = BLKTOCTLLEAF(blkno, dcp->budmin);
+	ti = leafno + le32_to_cpu(dcp->leafidx);
+
+	/* save the current leaf value and the current root level (i.e.
+	 * maximum l2 free string described by this dmapctl).
+	 */
+	oldval = dcp->stree[ti];
+	oldroot = dcp->stree[ROOT];
+
+	/* check if this is a control page update for an allocation.
+	 * if so, update the leaf to reflect the new leaf value using
+	 * dbSplit(); otherwise (deallocation), use dbJoin() to udpate
+	 * the leaf with the new value.  in addition to updating the
+	 * leaf, dbSplit() will also split the binary buddy system of
+	 * the leaves, if required, and bubble new values within the
+	 * dmapctl tree, if required.  similarly, dbJoin() will join
+	 * the binary buddy system of leaves and bubble new values up
+	 * the dmapctl tree as required by the new leaf value.
+	 */
+	if (alloc) {
+		/* check if we are in the middle of a binary buddy
+		 * system.  this happens when we are performing the
+		 * first allocation out of an allocation group that
+		 * is part (not the first part) of a larger binary
+		 * buddy system.  if we are in the middle, back split
+		 * the system prior to calling dbSplit() which assumes
+		 * that it is at the front of a binary buddy system.
+		 */
+		if (oldval == NOFREE) {
+			dbBackSplit((dmtree_t *) dcp, leafno);
+			oldval = dcp->stree[ti];
+		}
+		dbSplit((dmtree_t *) dcp, leafno, dcp->budmin, newval);
+	} else {
+		dbJoin((dmtree_t *) dcp, leafno, newval);
+	}
+
+	/* check if the root of the current dmap control page changed due
+	 * to the update and if the current dmap control page is not at
+	 * the current top level (i.e. L0, L1, L2) of the map.  if so (i.e.
+	 * root changed and this is not the top level), call this routine
+	 * again (recursion) for the next higher level of the mapping to
+	 * reflect the change in root for the current dmap control page.
+	 */
+	if (dcp->stree[ROOT] != oldroot) {
+		/* are we below the top level of the map.  if so,
+		 * bubble the root up to the next higher level.
+		 */
+		if (level < bmp->db_maxlevel) {
+			/* bubble up the new root of this dmap control page to
+			 * the next level.
+			 */
+			if ((rc =
+			     dbAdjCtl(bmp, blkno, dcp->stree[ROOT], alloc,
+				      level + 1))) {
+				/* something went wrong in bubbling up the new
+				 * root value, so backout the changes to the
+				 * current dmap control page.
+				 */
+				if (alloc) {
+					dbJoin((dmtree_t *) dcp, leafno,
+					       oldval);
+				} else {
+					/* the dbJoin() above might have
+					 * caused a larger binary buddy system
+					 * to form and we may now be in the
+					 * middle of it.  if this is the case,
+					 * back split the buddies.
+					 */
+					if (dcp->stree[ti] == NOFREE)
+						dbBackSplit((dmtree_t *)
+							    dcp, leafno);
+					dbSplit((dmtree_t *) dcp, leafno,
+						dcp->budmin, oldval);
+				}
+
+				/* release the buffer and return the error.
+				 */
+				release_metapage(mp);
+				return (rc);
+			}
+		} else {
+			/* we're at the top level of the map. update
+			 * the bmap control page to reflect the size
+			 * of the maximum free buddy system.
+			 */
+			assert(level == bmp->db_maxlevel);
+			assert(bmp->db_maxfreebud == oldroot);
+			bmp->db_maxfreebud = dcp->stree[ROOT];
+		}
+	}
+
+	/* write the buffer.
+	 */
+	write_metapage(mp);
+
+	return (0);
+}
+
+
+/*
+ * NAME:	dbSplit()
+ *
+ * FUNCTION:    update the leaf of a dmtree with a new value, splitting
+ *		the leaf from the binary buddy system of the dmtree's
+ *		leaves, as required.
+ *
+ * PARAMETERS:
+ *      tp	- pointer to the tree containing the leaf.
+ *      leafno	- the number of the leaf to be updated.
+ *      splitsz	- the size the binary buddy system starting at the leaf
+ *		  must be split to, specified as the log2 number of blocks.
+ *      newval	- the new value for the leaf.
+ *
+ * RETURN VALUES: none
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static void dbSplit(dmtree_t * tp, int leafno, int splitsz, int newval)
+{
+	int budsz;
+	int cursz;
+	s8 *leaf = tp->dmt_stree + le32_to_cpu(tp->dmt_leafidx);
+
+	/* check if the leaf needs to be split.
+	 */
+	if (leaf[leafno] > tp->dmt_budmin) {
+		/* the split occurs by cutting the buddy system in half
+		 * at the specified leaf until we reach the specified
+		 * size.  pick up the starting split size (current size
+		 * - 1 in l2) and the corresponding buddy size.
+		 */
+		cursz = leaf[leafno] - 1;
+		budsz = BUDSIZE(cursz, tp->dmt_budmin);
+
+		/* split until we reach the specified size.
+		 */
+		while (cursz >= splitsz) {
+			/* update the buddy's leaf with its new value.
+			 */
+			dbAdjTree(tp, leafno ^ budsz, cursz);
+
+			/* on to the next size and buddy.
+			 */
+			cursz -= 1;
+			budsz >>= 1;
+		}
+	}
+
+	/* adjust the dmap tree to reflect the specified leaf's new 
+	 * value.
+	 */
+	dbAdjTree(tp, leafno, newval);
+}
+
+
+/*
+ * NAME:	dbBackSplit()
+ *
+ * FUNCTION:    back split the binary buddy system of dmtree leaves
+ *		that hold a specified leaf until the specified leaf
+ *		starts its own binary buddy system.
+ *
+ *		the allocators typically perform allocations at the start
+ *		of binary buddy systems and dbSplit() is used to accomplish
+ *		any required splits.  in some cases, however, allocation
+ *		may occur in the middle of a binary system and requires a
+ *		back split, with the split proceeding out from the middle of
+ *		the system (less efficient) rather than the start of the
+ *		system (more efficient).  the cases in which a back split
+ *		is required are rare and are limited to the first allocation
+ *		within an allocation group which is a part (not first part)
+ *		of a larger binary buddy system and a few exception cases
+ *		in which a previous join operation must be backed out.
+ *
+ * PARAMETERS:
+ *      tp	- pointer to the tree containing the leaf.
+ *      leafno	- the number of the leaf to be updated.
+ *
+ * RETURN VALUES: none
+ *
+ * serialization: IREAD_LOCK(ipbmap) or IWRITE_LOCK(ipbmap) held on entry/exit;
+ */
+static void dbBackSplit(dmtree_t * tp, int leafno)
+{
+	int budsz, bud, w, bsz, size;
+	int cursz;
+	s8 *leaf = tp->dmt_stree + le32_to_cpu(tp->dmt_leafidx);
+
+	/* leaf should be part (not first part) of a binary
+	 * buddy system.
+	 */
+	assert(leaf[leafno] == NOFREE);
+
+	/* the back split is accomplished by iteratively finding the leaf
+	 * that starts the buddy system that contains the specified leaf and
+	 * splitting that system in two.  this iteration continues until
+	 * the specified leaf becomes the start of a buddy system. 
+	 *
+	 * determine maximum possible l2 size for the specified leaf.
+	 */
+	size =
+	    LITOL2BSZ(leafno, le32_to_cpu(tp->dmt_l2nleafs),
+		      tp->dmt_budmin);
+
+	/* determine the number of leaves covered by this size.  this
+	 * is the buddy size that we will start with as we search for
+	 * the buddy system that contains the specified leaf.
+	 */
+	budsz = BUDSIZE(size, tp->dmt_budmin);
+
+	/* back split.
+	 */
+	while (leaf[leafno] == NOFREE) {
+		/* find the leftmost buddy leaf.
+		 */
+		for (w = leafno, bsz = budsz;; bsz <<= 1,
+		     w = (w < bud) ? w : bud) {
+			assert(bsz < le32_to_cpu(tp->dmt_nleafs));
+
+			/* determine the buddy.
+			 */
+			bud = w ^ bsz;
+
+			/* check if this buddy is the start of the system.
+			 */
+			if (leaf[bud] != NOFREE) {
+				/* split the leaf at the start of the
+				 * system in two.
+				 */
+				cursz = leaf[bud] - 1;
+				dbSplit(tp, bud, cursz, cursz);
+				break;
+			}
+		}
+	}
+
+	assert(leaf[leafno] == size);
+}
+
+
+/*
+ * NAME:	dbJoin()
+ *
+ * FUNCTION:    update the leaf of a dmtree with a new value, joining
+ *		the leaf with other leaves of the dmtree into a multi-leaf
+ *		binary buddy system, as required.
+ *
+ * PARAMETERS:
+ *      tp	- pointer to the tree containing the leaf.
+ *      leafno	- the number of the leaf to be updated.
+ *      newval	- the new value for the leaf.
+ *
+ * RETURN VALUES: none
+ */
+static void dbJoin(dmtree_t * tp, int leafno, int newval)
+{
+	int budsz, buddy;
+	s8 *leaf;
+
+	/* can the new leaf value require a join with other leaves ?
+	 */
+	if (newval >= tp->dmt_budmin) {
+		/* pickup a pointer to the leaves of the tree.
+		 */
+		leaf = tp->dmt_stree + le32_to_cpu(tp->dmt_leafidx);
+
+		/* try to join the specified leaf into a large binary
+		 * buddy system.  the join proceeds by attempting to join
+		 * the specified leafno with its buddy (leaf) at new value.
+		 * if the join occurs, we attempt to join the left leaf
+		 * of the joined buddies with its buddy at new value + 1.
+		 * we continue to join until we find a buddy that cannot be
+		 * joined (does not have a value equal to the size of the
+		 * last join) or until all leaves have been joined into a
+		 * single system.
+		 *
+		 * get the buddy size (number of words covered) of
+		 * the new value.
+		 */
+		budsz = BUDSIZE(newval, tp->dmt_budmin);
+
+		/* try to join.
+		 */
+		while (budsz < le32_to_cpu(tp->dmt_nleafs)) {
+			/* get the buddy leaf.
+			 */
+			buddy = leafno ^ budsz;
+
+			/* if the leaf's new value is greater than its
+			 * buddy's value, we join no more.
+			 */
+			if (newval > leaf[buddy])
+				break;
+
+			assert(newval == leaf[buddy]);
+
+			/* check which (leafno or buddy) is the left buddy.
+			 * the left buddy gets to claim the blocks resulting
+			 * from the join while the right gets to claim none.
+			 * the left buddy is also eligable to participate in
+			 * a join at the next higher level while the right
+			 * is not.
+			 *
+			 */
+			if (leafno < buddy) {
+				/* leafno is the left buddy.
+				 */
+				dbAdjTree(tp, buddy, NOFREE);
+			} else {
+				/* buddy is the left buddy and becomes
+				 * leafno.
+				 */
+				dbAdjTree(tp, leafno, NOFREE);
+				leafno = buddy;
+			}
+
+			/* on to try the next join.
+			 */
+			newval += 1;
+			budsz <<= 1;
+		}
+	}
+
+	/* update the leaf value.
+	 */
+	dbAdjTree(tp, leafno, newval);
+}
+
+
+/*
+ * NAME:	dbAdjTree()
+ *
+ * FUNCTION:    update a leaf of a dmtree with a new value, adjusting
+ *		the dmtree, as required, to reflect the new leaf value.
+ *		the combination of any buddies must already be done before
+ *		this is called.
+ *
+ * PARAMETERS:
+ *      tp	- pointer to the tree to be adjusted.
+ *      leafno	- the number of the leaf to be updated.
+ *      newval	- the new value for the leaf.
+ *
+ * RETURN VALUES: none
+ */
+static void dbAdjTree(dmtree_t * tp, int leafno, int newval)
+{
+	int lp, pp, k;
+	int max;
+
+	/* pick up the index of the leaf for this leafno.
+	 */
+	lp = leafno + le32_to_cpu(tp->dmt_leafidx);
+
+	/* is the current value the same as the old value ?  if so,
+	 * there is nothing to do.
+	 */
+	if (tp->dmt_stree[lp] == newval)
+		return;
+
+	/* set the new value.
+	 */
+	tp->dmt_stree[lp] = newval;
+
+	/* bubble the new value up the tree as required.
+	 */
+	for (k = 0; k < le32_to_cpu(tp->dmt_height); k++) {
+		/* get the index of the first leaf of the 4 leaf
+		 * group containing the specified leaf (leafno).
+		 */
+		lp = ((lp - 1) & ~0x03) + 1;
+
+		/* get the index of the parent of this 4 leaf group.
+		 */
+		pp = (lp - 1) >> 2;
+
+		/* determine the maximum of the 4 leaves.
+		 */
+		max = TREEMAX(&tp->dmt_stree[lp]);
+
+		/* if the maximum of the 4 is the same as the
+		 * parent's value, we're done.
+		 */
+		if (tp->dmt_stree[pp] == max)
+			break;
+
+		/* parent gets new value.
+		 */
+		tp->dmt_stree[pp] = max;
+
+		/* parent becomes leaf for next go-round.
+		 */
+		lp = pp;
+	}
+}
+
+
+/*
+ * NAME:	dbFindLeaf()
+ *
+ * FUNCTION:    search a dmtree_t for sufficient free blocks, returning
+ *		the index of a leaf describing the free blocks if 
+ *		sufficient free blocks are found.
+ *
+ *		the search starts at the top of the dmtree_t tree and
+ *		proceeds down the tree to the leftmost leaf with sufficient
+ *		free space.
+ *
+ * PARAMETERS:
+ *      tp	- pointer to the tree to be searched.
+ *      l2nb	- log2 number of free blocks to search for.
+ *	leafidx	- return pointer to be set to the index of the leaf
+ *		  describing at least l2nb free blocks if sufficient
+ *		  free blocks are found.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      ENOSPC	- insufficient free blocks. 
+ */
+static int dbFindLeaf(dmtree_t * tp, int l2nb, int *leafidx)
+{
+	int ti, n = 0, k, x = 0;
+
+	/* first check the root of the tree to see if there is
+	 * sufficient free space.
+	 */
+	if (l2nb > tp->dmt_stree[ROOT])
+		return (ENOSPC);
+
+	/* sufficient free space available. now search down the tree
+	 * starting at the next level for the leftmost leaf that
+	 * describes sufficient free space.
+	 */
+	for (k = le32_to_cpu(tp->dmt_height), ti = 1;
+	     k > 0; k--, ti = ((ti + n) << 2) + 1) {
+		/* search the four nodes at this level, starting from
+		 * the left.
+		 */
+		for (x = ti, n = 0; n < 4; n++) {
+			/* sufficient free space found.  move to the next
+			 * level (or quit if this is the last level).
+			 */
+			if (l2nb <= tp->dmt_stree[x + n])
+				break;
+		}
+
+		/* better have found something since the higher
+		 * levels of the tree said it was here.
+		 */
+		assert(n < 4);
+	}
+
+	/* set the return to the leftmost leaf describing sufficient
+	 * free space.
+	 */
+	*leafidx = x + n - le32_to_cpu(tp->dmt_leafidx);
+
+	return (0);
+}
+
+
+/*
+ * NAME:	dbFindBits()
+ *
+ * FUNCTION:    find a specified number of binary buddy free bits within a
+ *		dmap bitmap word value.
+ *
+ *		this routine searches the bitmap value for (1 << l2nb) free
+ *		bits at (1 << l2nb) alignments within the value.
+ *
+ * PARAMETERS:
+ *      word	-  dmap bitmap word value.
+ *      l2nb	-  number of free bits specified as a log2 number.
+ *
+ * RETURN VALUES:
+ *      starting bit number of free bits.
+ */
+static int dbFindBits(u32 word, int l2nb)
+{
+	int bitno, nb;
+	u32 mask;
+
+	/* get the number of bits.
+	 */
+	nb = 1 << l2nb;
+	assert(nb <= DBWORD);
+
+	/* complement the word so we can use a mask (i.e. 0s represent
+	 * free bits) and compute the mask.
+	 */
+	word = ~word;
+	mask = ONES << (DBWORD - nb);
+
+	/* scan the word for nb free bits at nb alignments.
+	 */
+	for (bitno = 0; mask != 0; bitno += nb, mask >>= nb) {
+		if ((mask & word) == mask)
+			break;
+	}
+
+	ASSERT(bitno < 32);
+
+	/* return the bit number.
+	 */
+	return (bitno);
+}
+
+
+/*
+ * NAME:	dbMaxBud(u8 *cp)
+ *
+ * FUNCTION:    determine the largest binary buddy string of free
+ *		bits within 32-bits of the map.
+ *
+ * PARAMETERS:
+ *      cp	-  pointer to the 32-bit value.
+ *
+ * RETURN VALUES:
+ *      largest binary buddy of free bits within a dmap word.
+ */
+static int dbMaxBud(u8 * cp)
+{
+	signed char tmp1, tmp2;
+
+	/* check if the wmap word is all free. if so, the
+	 * free buddy size is BUDMIN.
+	 */
+	if (*((uint *) cp) == 0)
+		return (BUDMIN);
+
+	/* check if the wmap word is half free. if so, the
+	 * free buddy size is BUDMIN-1.
+	 */
+	if (*((u16 *) cp) == 0 || *((u16 *) cp + 1) == 0)
+		return (BUDMIN - 1);
+
+	/* not all free or half free. determine the free buddy
+	 * size thru table lookup using quarters of the wmap word.
+	 */
+	tmp1 = max(budtab[cp[2]], budtab[cp[3]]);
+	tmp2 = max(budtab[cp[0]], budtab[cp[1]]);
+	return (max(tmp1, tmp2));
+}
+
+
+/*
+ * NAME:	cnttz(uint word)
+ *
+ * FUNCTION:    determine the number of trailing zeros within a 32-bit
+ *		value.
+ *
+ * PARAMETERS:
+ *      value	-  32-bit value to be examined.
+ *
+ * RETURN VALUES:
+ *      count of trailing zeros
+ */
+int cnttz(u32 word)
+{
+	int n;
+
+	for (n = 0; n < 32; n++, word >>= 1) {
+		if (word & 0x01)
+			break;
+	}
+
+	return (n);
+}
+
+
+/*
+ * NAME:	cntlz(u32 value)
+ *
+ * FUNCTION:    determine the number of leading zeros within a 32-bit
+ *		value.
+ *
+ * PARAMETERS:
+ *      value	-  32-bit value to be examined.
+ *
+ * RETURN VALUES:
+ *      count of leading zeros
+ */
+int cntlz(u32 value)
+{
+	int n;
+
+	for (n = 0; n < 32; n++, value <<= 1) {
+		if (value & HIGHORDER)
+			break;
+	}
+	return (n);
+}
+
+
+/*
+ * NAME:	blkstol2(s64 nb)
+ *
+ * FUNCTION:	convert a block count to its log2 value. if the block
+ *	        count is not a l2 multiple, it is rounded up to the next
+ *		larger l2 multiple.
+ *
+ * PARAMETERS:
+ *      nb	-  number of blocks
+ *
+ * RETURN VALUES:
+ *      log2 number of blocks
+ */
+int blkstol2(s64 nb)
+{
+	int l2nb;
+	s64 mask;		/* meant to be signed */
+
+	mask = (s64) 1 << (64 - 1);
+
+	/* count the leading bits.
+	 */
+	for (l2nb = 0; l2nb < 64; l2nb++, mask >>= 1) {
+		/* leading bit found.
+		 */
+		if (nb & mask) {
+			/* determine the l2 value.
+			 */
+			l2nb = (64 - 1) - l2nb;
+
+			/* check if we need to round up.
+			 */
+			if (~mask & nb)
+				l2nb++;
+
+			return (l2nb);
+		}
+	}
+	assert(0);
+	return 0;		/* fix compiler warning */
+}
+
+
+/*
+ * NAME:	fsDirty()
+ *
+ * FUNCTION:    xxx
+ *
+ * PARAMETERS:
+ *      ipmnt	- mount inode
+ *
+ * RETURN VALUES:
+ *      none
+ */
+void fsDirty()
+{
+	printk("fsDirty(): bye-bye\n");
+	assert(0);
+}
+
+
+/*
+ * NAME:    	dbAllocBottomUp()
+ *
+ * FUNCTION:	alloc the specified block range from the working block
+ *		allocation map.
+ *
+ *		the blocks will be alloc from the working map one dmap
+ *		at a time.
+ *
+ * PARAMETERS:
+ *      ip	-  pointer to in-core inode;
+ *      blkno	-  starting block number to be freed.
+ *      nblocks	-  number of blocks to be freed.
+ *
+ * RETURN VALUES:
+ *      0	- success
+ *      EIO	- i/o error
+ */
+int dbAllocBottomUp(struct inode *ip, s64 blkno, s64 nblocks)
+{
+	metapage_t *mp;
+	dmap_t *dp;
+	int nb, rc;
+	s64 lblkno, rem;
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	bmap_t *bmp = JFS_SBI(ip->i_sb)->bmap;
+
+	IREAD_LOCK(ipbmap);
+
+	/* block to be allocated better be within the mapsize. */
+	ASSERT(nblocks <= bmp->db_mapsize - blkno);
+
+	/*
+	 * allocate the blocks a dmap at a time.
+	 */
+	mp = NULL;
+	for (rem = nblocks; rem > 0; rem -= nb, blkno += nb) {
+		/* release previous dmap if any */
+		if (mp) {
+			write_metapage(mp);
+		}
+
+		/* get the buffer for the current dmap. */
+		lblkno = BLKTODMAP(blkno, bmp->db_l2nbperpage);
+		mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			IREAD_UNLOCK(ipbmap);
+			return (EIO);
+		}
+		dp = (dmap_t *) mp->data;
+
+		/* determine the number of blocks to be allocated from
+		 * this dmap.
+		 */
+		nb = min(rem, BPERDMAP - (blkno & (BPERDMAP - 1)));
+
+		DBFREECK(bmp->db_DBmap, bmp->db_mapsize, blkno, nb);
+
+		/* allocate the blocks. */
+		if ((rc = dbAllocDmapBU(bmp, dp, blkno, nb))) {
+			release_metapage(mp);
+			IREAD_UNLOCK(ipbmap);
+			return (rc);
+		}
+
+		DBALLOC(bmp->db_DBmap, bmp->db_mapsize, blkno, nb);
+	}
+
+	/* write the last buffer. */
+	write_metapage(mp);
+
+	IREAD_UNLOCK(ipbmap);
+
+	return (0);
+}
+
+
+static int dbAllocDmapBU(bmap_t * bmp, dmap_t * dp, s64 blkno, int nblocks)
+{
+	int rc;
+	int dbitno, word, rembits, nb, nwords, wbitno, agno;
+	s8 oldroot, *leaf;
+	dmaptree_t *tp = (dmaptree_t *) & dp->tree;
+
+	/* save the current value of the root (i.e. maximum free string)
+	 * of the dmap tree.
+	 */
+	oldroot = tp->stree[ROOT];
+
+	/* pick up a pointer to the leaves of the dmap tree */
+	leaf = tp->stree + LEAFIND;
+
+	/* determine the bit number and word within the dmap of the
+	 * starting block.
+	 */
+	dbitno = blkno & (BPERDMAP - 1);
+	word = dbitno >> L2DBWORD;
+
+	/* block range better be within the dmap */
+	assert(dbitno + nblocks <= BPERDMAP);
+
+	/* allocate the bits of the dmap's words corresponding to the block
+	 * range. not all bits of the first and last words may be contained
+	 * within the block range.  if this is the case, we'll work against
+	 * those words (i.e. partial first and/or last) on an individual basis
+	 * (a single pass), allocating the bits of interest by hand and
+	 * updating the leaf corresponding to the dmap word. a single pass
+	 * will be used for all dmap words fully contained within the
+	 * specified range.  within this pass, the bits of all fully contained
+	 * dmap words will be marked as free in a single shot and the leaves
+	 * will be updated. a single leaf may describe the free space of
+	 * multiple dmap words, so we may update only a subset of the actual
+	 * leaves corresponding to the dmap words of the block range.
+	 */
+	for (rembits = nblocks; rembits > 0; rembits -= nb, dbitno += nb) {
+		/* determine the bit number within the word and
+		 * the number of bits within the word.
+		 */
+		wbitno = dbitno & (DBWORD - 1);
+		nb = min(rembits, DBWORD - wbitno);
+
+		/* check if only part of a word is to be allocated.
+		 */
+		if (nb < DBWORD) {
+			/* allocate (set to 1) the appropriate bits within
+			 * this dmap word.
+			 */
+			dp->wmap[word] |= cpu_to_le32(ONES << (DBWORD - nb)
+						      >> wbitno);
+
+			word += 1;
+		} else {
+			/* one or more dmap words are fully contained
+			 * within the block range.  determine how many
+			 * words and allocate (set to 1) the bits of these
+			 * words.
+			 */
+			nwords = rembits >> L2DBWORD;
+			memset(&dp->wmap[word], (int) ONES, nwords * 4);
+
+			/* determine how many bits */
+			nb = nwords << L2DBWORD;
+		}
+	}
+
+	/* update the free count for this dmap */
+	dp->nfree = cpu_to_le32(le32_to_cpu(dp->nfree) - nblocks);
+
+	/* reconstruct summary tree */
+	dbInitDmapTree(dp);
+
+	BMAP_LOCK(bmp);
+
+	/* if this allocation group is completely free,
+	 * update the highest active allocation group number 
+	 * if this allocation group is the new max.
+	 */
+	agno = blkno >> bmp->db_agl2size;
+	if (agno > bmp->db_maxag)
+		bmp->db_maxag = agno;
+
+	/* update the free count for the allocation group and map */
+	bmp->db_agfree[agno] -= nblocks;
+	bmp->db_nfree -= nblocks;
+
+	BMAP_UNLOCK(bmp);
+
+	/* if the root has not changed, done. */
+	if (tp->stree[ROOT] == oldroot)
+		return (0);
+
+	/* root changed. bubble the change up to the dmap control pages.
+	 * if the adjustment of the upper level control pages fails,
+	 * backout the bit allocation (thus making everything consistent).
+	 */
+	if ((rc = dbAdjCtl(bmp, blkno, tp->stree[ROOT], 1, 0)))
+		dbFreeBits(bmp, dp, blkno, nblocks);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:	dbExtendFS()
+ *
+ * FUNCTION:	extend bmap from blkno for nblocks;
+ * 		dbExtendFS() updates bmap ready for dbAllocBottomUp();
+ *
+ * L2
+ *  |
+ *   L1---------------------------------L1
+ *    |                                  |
+ *     L0---------L0---------L0           L0---------L0---------L0
+ *      |          |          |            |          |          |
+ *       d0,...,dn  d0,...,dn  d0,...,dn    d0,...,dn  d0,...,dn  d0,.,dm;
+ * L2L1L0d0,...,dnL0d0,...,dnL0d0,...,dnL1L0d0,...,dnL0d0,...,dnL0d0,..dm
+ *
+ * <---old---><----------------------------extend----------------------->   
+ */
+int dbExtendFS(struct inode *ipbmap, s64 blkno,	s64 nblocks)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ipbmap->i_sb);
+	int nbperpage = sbi->nbperpage;
+	int i, i0 = TRUE, j, j0 = TRUE, k, n;
+	s64 newsize;
+	s64 p;
+	metapage_t *mp, *l2mp, *l1mp, *l0mp;
+	dmapctl_t *l2dcp, *l1dcp, *l0dcp;
+	dmap_t *dp;
+	s8 *l0leaf, *l1leaf, *l2leaf;
+	bmap_t *bmp = sbi->bmap;
+	int agno, l2agsize, oldl2agsize;
+	s64 ag_rem;
+
+	newsize = blkno + nblocks;
+
+	jEVENT(0, ("dbExtendFS: blkno:%Ld nblocks:%Ld newsize:%Ld\n",
+		   (long long) blkno, (long long) nblocks,
+		   (long long) newsize));
+
+	/*
+	 *      initialize bmap control page.
+	 *
+	 * all the data in bmap control page should exclude
+	 * the mkfs hidden dmap page.
+	 */
+
+	/* update mapsize */
+	bmp->db_mapsize = newsize;
+	bmp->db_maxlevel = BMAPSZTOLEV(bmp->db_mapsize);
+
+	/* compute new AG size */
+	l2agsize = dbGetL2AGSize(newsize);
+	oldl2agsize = bmp->db_agl2size;
+
+	bmp->db_agl2size = l2agsize;
+	bmp->db_agsize = 1 << l2agsize;
+
+	/* compute new number of AG */
+	agno = bmp->db_numag;
+	bmp->db_numag = newsize >> l2agsize;
+	bmp->db_numag += ((u32) newsize % (u32) bmp->db_agsize) ? 1 : 0;
+
+	/*
+	 *      reconfigure db_agfree[] 
+	 * from old AG configuration to new AG configuration;
+	 *
+	 * coalesce contiguous k (newAGSize/oldAGSize) AGs;
+	 * i.e., (AGi, ..., AGj) where i = k*n and j = k*(n+1) - 1 to AGn;
+	 * note: new AG size = old AG size * (2**x).
+	 */
+	if (l2agsize == oldl2agsize)
+		goto extend;
+	k = 1 << (l2agsize - oldl2agsize);
+	ag_rem = bmp->db_agfree[0];	/* save agfree[0] */
+	for (i = 0, n = 0; i < agno; n++) {
+		bmp->db_agfree[n] = 0;	/* init collection point */
+
+		/* coalesce cotiguous k AGs; */
+		for (j = 0; j < k && i < agno; j++, i++) {
+			/* merge AGi to AGn */
+			bmp->db_agfree[n] += bmp->db_agfree[i];
+		}
+	}
+	bmp->db_agfree[0] += ag_rem;	/* restore agfree[0] */
+
+	for (; n < MAXAG; n++)
+		bmp->db_agfree[n] = 0;
+
+	/*
+	 * update highest active ag number
+	 */
+
+	bmp->db_maxag = bmp->db_maxag / k;
+
+	/*
+	 *      extend bmap
+	 *
+	 * update bit maps and corresponding level control pages;
+	 * global control page db_nfree, db_agfree[agno], db_maxfreebud;
+	 */
+      extend:
+	/* get L2 page */
+	p = BMAPBLKNO + nbperpage;	/* L2 page */
+	l2mp = read_metapage(ipbmap, p, PSIZE, 0);
+	assert(l2mp);
+	l2dcp = (dmapctl_t *) l2mp->data;
+
+	/* compute start L1 */
+	k = blkno >> L2MAXL1SIZE;
+	l2leaf = l2dcp->stree + CTLLEAFIND + k;
+	p = BLKTOL1(blkno, sbi->l2nbperpage);	/* L1 page */
+
+	/*
+	 * extend each L1 in L2
+	 */
+	for (; k < LPERCTL; k++, p += nbperpage) {
+		/* get L1 page */
+		if (j0) {
+			/* read in L1 page: (blkno & (MAXL1SIZE - 1)) */
+			l1mp = read_metapage(ipbmap, p, PSIZE, 0);
+			if (l1mp == NULL)
+				goto errout;
+			l1dcp = (dmapctl_t *) l1mp->data;
+
+			/* compute start L0 */
+			j = (blkno & (MAXL1SIZE - 1)) >> L2MAXL0SIZE;
+			l1leaf = l1dcp->stree + CTLLEAFIND + j;
+			p = BLKTOL0(blkno, sbi->l2nbperpage);
+			j0 = FALSE;
+		} else {
+			/* assign/init L1 page */
+			l1mp = get_metapage(ipbmap, p, PSIZE, 0);
+			if (l1mp == NULL)
+				goto errout;
+
+			l1dcp = (dmapctl_t *) l1mp->data;
+
+			/* compute start L0 */
+			j = 0;
+			l1leaf = l1dcp->stree + CTLLEAFIND;
+			p += nbperpage;	/* 1st L0 of L1.k  */
+		}
+
+		/*
+		 * extend each L0 in L1
+		 */
+		for (; j < LPERCTL; j++) {
+			/* get L0 page */
+			if (i0) {
+				/* read in L0 page: (blkno & (MAXL0SIZE - 1)) */
+
+				l0mp = read_metapage(ipbmap, p, PSIZE, 0);
+				if (l0mp == NULL)
+					goto errout;
+				l0dcp = (dmapctl_t *) l0mp->data;
+
+				/* compute start dmap */
+				i = (blkno & (MAXL0SIZE - 1)) >>
+				    L2BPERDMAP;
+				l0leaf = l0dcp->stree + CTLLEAFIND + i;
+				p = BLKTODMAP(blkno,
+					      sbi->l2nbperpage);
+				i0 = FALSE;
+			} else {
+				/* assign/init L0 page */
+				l0mp = get_metapage(ipbmap, p, PSIZE, 0);
+				if (l0mp == NULL)
+					goto errout;
+
+				l0dcp = (dmapctl_t *) l0mp->data;
+
+				/* compute start dmap */
+				i = 0;
+				l0leaf = l0dcp->stree + CTLLEAFIND;
+				p += nbperpage;	/* 1st dmap of L0.j */
+			}
+
+			/*
+			 * extend each dmap in L0
+			 */
+			for (; i < LPERCTL; i++) {
+				/*
+				 * reconstruct the dmap page, and
+				 * initialize corresponding parent L0 leaf
+				 */
+				if ((n = blkno & (BPERDMAP - 1))) {
+					/* read in dmap page: */
+					mp = read_metapage(ipbmap, p,
+							   PSIZE, 0);
+					if (mp == NULL)
+						goto errout;
+					n = min(nblocks, (s64)BPERDMAP - n);
+				} else {
+					/* assign/init dmap page */
+					mp = read_metapage(ipbmap, p,
+							   PSIZE, 0);
+					if (mp == NULL)
+						goto errout;
+
+					n = min(nblocks, (s64)BPERDMAP);
+				}
+
+				dp = (dmap_t *) mp->data;
+				*l0leaf = dbInitDmap(dp, blkno, n);
+
+				bmp->db_nfree += n;
+				agno = le64_to_cpu(dp->start) >> l2agsize;
+				bmp->db_agfree[agno] += n;
+
+				write_metapage(mp);
+
+				l0leaf++;
+				p += nbperpage;
+
+				blkno += n;
+				nblocks -= n;
+				if (nblocks == 0)
+					break;
+			}	/* for each dmap in a L0 */
+
+			/*
+			 * build current L0 page from its leaves, and 
+			 * initialize corresponding parent L1 leaf
+			 */
+			*l1leaf = dbInitDmapCtl(l0dcp, 0, ++i);
+			write_metapage(l0mp);
+
+			if (nblocks)
+				l1leaf++;	/* continue for next L0 */
+			else {
+				/* more than 1 L0 ? */
+				if (j > 0)
+					break;	/* build L1 page */
+				else {
+					/* summarize in global bmap page */
+					bmp->db_maxfreebud = *l1leaf;
+					release_metapage(l1mp);
+					release_metapage(l2mp);
+					goto finalize;
+				}
+			}
+		}		/* for each L0 in a L1 */
+
+		/*
+		 * build current L1 page from its leaves, and 
+		 * initialize corresponding parent L2 leaf
+		 */
+		*l2leaf = dbInitDmapCtl(l1dcp, 1, ++j);
+		write_metapage(l1mp);
+
+		if (nblocks)
+			l2leaf++;	/* continue for next L1 */
+		else {
+			/* more than 1 L1 ? */
+			if (k > 0)
+				break;	/* build L2 page */
+			else {
+				/* summarize in global bmap page */
+				bmp->db_maxfreebud = *l2leaf;
+				release_metapage(l2mp);
+				goto finalize;
+			}
+		}
+	}			/* for each L1 in a L2 */
+
+	assert(0);
+
+	/*
+	 *      finalize bmap control page
+	 */
+      finalize:
+
+	return 0;
+
+      errout:
+	return EIO;
+}
+
+
+/*
+ *	dbFinalizeBmap()
+ */
+void dbFinalizeBmap(struct inode *ipbmap)
+{
+	bmap_t *bmp = JFS_SBI(ipbmap->i_sb)->bmap;
+	int actags, inactags, l2nl;
+	s64 ag_rem, actfree, inactfree, avgfree;
+	int i, n;
+
+	/*
+	 *      finalize bmap control page
+	 */
+//finalize:
+	/* 
+	 * compute db_agpref: preferred ag to allocate from
+	 * (the leftmost ag with average free space in it);
+	 */
+//agpref:
+	/* get the number of active ags and inacitve ags */
+	actags = bmp->db_maxag + 1;
+	inactags = bmp->db_numag - actags;
+	ag_rem = bmp->db_mapsize & (bmp->db_agsize - 1);	/* ??? */
+
+	/* determine how many blocks are in the inactive allocation
+	 * groups. in doing this, we must account for the fact that
+	 * the rightmost group might be a partial group (i.e. file
+	 * system size is not a multiple of the group size).
+	 */
+	inactfree = (inactags && ag_rem) ?
+	    ((inactags - 1) << bmp->db_agl2size) + ag_rem
+	    : inactags << bmp->db_agl2size;
+
+	/* determine how many free blocks are in the active
+	 * allocation groups plus the average number of free blocks
+	 * within the active ags.
+	 */
+	actfree = bmp->db_nfree - inactfree;
+	avgfree = (u32) actfree / (u32) actags;
+
+	/* if the preferred allocation group has not average free space.
+	 * re-establish the preferred group as the leftmost
+	 * group with average free space.
+	 */
+	if (bmp->db_agfree[bmp->db_agpref] < avgfree) {
+		for (bmp->db_agpref = 0; bmp->db_agpref < actags;
+		     bmp->db_agpref++) {
+			if (bmp->db_agfree[bmp->db_agpref] >= avgfree)
+				break;
+		}
+		assert(bmp->db_agpref < bmp->db_numag);
+	}
+
+	/*
+	 * compute db_aglevel, db_agheigth, db_width, db_agstart:
+	 * an ag is covered in aglevel dmapctl summary tree, 
+	 * at agheight level height (from leaf) with agwidth number of nodes 
+	 * each, which starts at agstart index node of the smmary tree node 
+	 * array;
+	 */
+	bmp->db_aglevel = BMAPSZTOLEV(bmp->db_agsize);
+	l2nl =
+	    bmp->db_agl2size - (L2BPERDMAP + bmp->db_aglevel * L2LPERCTL);
+	bmp->db_agheigth = l2nl >> 1;
+	bmp->db_agwidth = 1 << (l2nl - (bmp->db_agheigth << 1));
+	for (i = 5 - bmp->db_agheigth, bmp->db_agstart = 0, n = 1; i > 0;
+	     i--) {
+		bmp->db_agstart += n;
+		n <<= 2;
+	}
+
+/*
+printk("bmap: agpref:%d aglevel:%d agheigth:%d agwidth:%d\n",
+	bmp->db_agpref, bmp->db_aglevel, bmp->db_agheigth, bmp->db_agwidth);
+*/
+}
+
+
+/*
+ * NAME:	dbInitDmap()/ujfs_idmap_page()
+ *                                                                    
+ * FUNCTION:	initialize working/persistent bitmap of the dmap page
+ *		for the specified number of blocks:
+ *                                                                    
+ *		at entry, the bitmaps had been initialized as free (ZEROS);
+ *		The number of blocks will only account for the actually 
+ *		existing blocks. Blocks which don't actually exist in 
+ *		the aggregate will be marked as allocated (ONES);
+ *
+ * PARAMETERS:
+ *	dp	- pointer to page of map
+ *	nblocks	- number of blocks this page
+ *
+ * RETURNS: NONE
+ */
+static int dbInitDmap(dmap_t * dp, s64 Blkno, int nblocks)
+{
+	int blkno, w, b, r, nw, nb, i;
+/*
+printk("sbh_dmap:  in dbInitDmap blkno:%Ld nblocks:%ld\n", Blkno, nblocks); 
+*/
+
+	/* starting block number within the dmap */
+	blkno = Blkno & (BPERDMAP - 1);
+
+	if (blkno == 0) {
+		dp->nblocks = dp->nfree = cpu_to_le32(nblocks);
+		dp->start = cpu_to_le64(Blkno);
+
+		if (nblocks == BPERDMAP) {
+			memset(&dp->wmap[0], 0, LPERDMAP * 4);
+			memset(&dp->pmap[0], 0, LPERDMAP * 4);
+			goto initTree;
+		}
+	} else {
+		dp->nblocks =
+		    cpu_to_le32(le32_to_cpu(dp->nblocks) + nblocks);
+		dp->nfree = cpu_to_le32(le32_to_cpu(dp->nfree) + nblocks);
+	}
+
+	/* word number containing start block number */
+	w = blkno >> L2DBWORD;
+
+	/*
+	 * free the bits corresponding to the block range (ZEROS):
+	 * note: not all bits of the first and last words may be contained 
+	 * within the block range.
+	 */
+	for (r = nblocks; r > 0; r -= nb, blkno += nb) {
+		/* number of bits preceding range to be freed in the word */
+		b = blkno & (DBWORD - 1);
+		/* number of bits to free in the word */
+		nb = min(r, DBWORD - b);
+
+		/* is partial word to be freed ? */
+		if (nb < DBWORD) {
+			/* free (set to 0) from the bitmap word */
+			dp->wmap[w] &= cpu_to_le32(~(ONES << (DBWORD - nb)
+						     >> b));
+			dp->pmap[w] &= cpu_to_le32(~(ONES << (DBWORD - nb)
+						     >> b));
+
+			/* skip the word freed */
+			w++;
+		} else {
+			/* free (set to 0) contiguous bitmap words */
+			nw = r >> L2DBWORD;
+			memset(&dp->wmap[w], 0, nw * 4);
+			memset(&dp->pmap[w], 0, nw * 4);
+
+			/* skip the words freed */
+			nb = nw << L2DBWORD;
+			w += nw;
+		}
+	}
+
+	/*
+	 * mark bits following the range to be freed (non-existing 
+	 * blocks) as allocated (ONES)
+	 */
+/*
+printk("sbh_dmap:  in dbInitDmap, preparing to mark unbacked, blkno:%ld nblocks:%ld\n",
+		blkno, nblocks); 
+*/
+
+	if (blkno == BPERDMAP)
+		goto initTree;
+
+	/* the first word beyond the end of existing blocks */
+	w = blkno >> L2DBWORD;
+
+	/* does nblocks fall on a 32-bit boundary ? */
+	b = blkno & (DBWORD - 1);
+/*
+printk("sbh_dmap:  in dbInitDmap, b:%ld w:%ld mask: %lx\n", b, w, (ONES>>b)); 
+*/
+	if (b) {
+		/* mark a partial word allocated */
+		dp->wmap[w] = dp->pmap[w] = cpu_to_le32(ONES >> b);
+		w++;
+	}
+
+	/* set the rest of the words in the page to allocated (ONES) */
+	for (i = w; i < LPERDMAP; i++)
+		dp->pmap[i] = dp->wmap[i] = ONES;
+
+	/*
+	 * init tree
+	 */
+      initTree:
+	return (dbInitDmapTree(dp));
+}
+
+
+/*
+ * NAME:	dbInitDmapTree()/ujfs_complete_dmap()
+ *                                                                    
+ * FUNCTION:	initialize summary tree of the specified dmap:
+ *
+ *		at entry, bitmap of the dmap has been initialized;
+ *                                                                    
+ * PARAMETERS:
+ *	dp	- dmap to complete
+ *	blkno	- starting block number for this dmap
+ *	treemax	- will be filled in with max free for this dmap
+ *
+ * RETURNS:	max free string at the root of the tree
+ */
+static int dbInitDmapTree(dmap_t * dp)
+{
+	dmaptree_t *tp;
+	s8 *cp;
+	int i;
+
+	/* init fixed info of tree */
+	tp = &dp->tree;
+	tp->nleafs = cpu_to_le32(LPERDMAP);
+	tp->l2nleafs = cpu_to_le32(L2LPERDMAP);
+	tp->leafidx = cpu_to_le32(LEAFIND);
+	tp->height = cpu_to_le32(4);
+	tp->budmin = BUDMIN;
+
+	/* init each leaf from corresponding wmap word:
+	 * note: leaf is set to NOFREE(-1) if all blocks of corresponding
+	 * bitmap word are allocated. 
+	 */
+	cp = tp->stree + le32_to_cpu(tp->leafidx);
+	for (i = 0; i < LPERDMAP; i++)
+		*cp++ = dbMaxBud((u8 *) & dp->wmap[i]);
+
+	/* build the dmap's binary buddy summary tree */
+	return (dbInitTree(tp));
+}
+
+
+/*
+ * NAME:	dbInitTree()/ujfs_adjtree()
+ *                                                                    
+ * FUNCTION:	initialize binary buddy summary tree of a dmap or dmapctl.
+ *
+ *		at entry, the leaves of the tree has been initialized 
+ *		from corresponding bitmap word or root of summary tree
+ *		of the child control page;
+ *		configure binary buddy system at the leaf level, then
+ *		bubble up the values of the leaf nodes up the tree.
+ *
+ * PARAMETERS:
+ *	cp	- Pointer to the root of the tree
+ *	l2leaves- Number of leaf nodes as a power of 2
+ *	l2min	- Number of blocks that can be covered by a leaf
+ *		  as a power of 2
+ *
+ * RETURNS: max free string at the root of the tree
+ */
+static int dbInitTree(dmaptree_t * dtp)
+{
+	int l2max, l2free, bsize, nextb, i;
+	int child, parent, nparent;
+	s8 *tp, *cp, *cp1;
+
+	tp = dtp->stree;
+
+	/* Determine the maximum free string possible for the leaves */
+	l2max = le32_to_cpu(dtp->l2nleafs) + dtp->budmin;
+
+	/*
+	 * configure the leaf levevl into binary buddy system
+	 *
+	 * Try to combine buddies starting with a buddy size of 1 
+	 * (i.e. two leaves). At a buddy size of 1 two buddy leaves 
+	 * can be combined if both buddies have a maximum free of l2min; 
+	 * the combination will result in the left-most buddy leaf having 
+	 * a maximum free of l2min+1.  
+	 * After processing all buddies for a given size, process buddies 
+	 * at the next higher buddy size (i.e. current size * 2) and 
+	 * the next maximum free (current free + 1).  
+	 * This continues until the maximum possible buddy combination 
+	 * yields maximum free.
+	 */
+	for (l2free = dtp->budmin, bsize = 1; l2free < l2max;
+	     l2free++, bsize = nextb) {
+		/* get next buddy size == current buddy pair size */
+		nextb = bsize << 1;
+
+		/* scan each adjacent buddy pair at current buddy size */
+		for (i = 0, cp = tp + le32_to_cpu(dtp->leafidx);
+		     i < le32_to_cpu(dtp->nleafs);
+		     i += nextb, cp += nextb) {
+			/* coalesce if both adjacent buddies are max free */
+			if (*cp == l2free && *(cp + bsize) == l2free) {
+				*cp = l2free + 1;	/* left take right */
+				*(cp + bsize) = -1;	/* right give left */
+			}
+		}
+	}
+
+	/*
+	 * bubble summary information of leaves up the tree.
+	 *
+	 * Starting at the leaf node level, the four nodes described by
+	 * the higher level parent node are compared for a maximum free and 
+	 * this maximum becomes the value of the parent node.  
+	 * when all lower level nodes are processed in this fashion then 
+	 * move up to the next level (parent becomes a lower level node) and 
+	 * continue the process for that level.
+	 */
+	for (child = le32_to_cpu(dtp->leafidx),
+	     nparent = le32_to_cpu(dtp->nleafs) >> 2;
+	     nparent > 0; nparent >>= 2, child = parent) {
+		/* get index of 1st node of parent level */
+		parent = (child - 1) >> 2;
+
+		/* set the value of the parent node as the maximum 
+		 * of the four nodes of the current level.
+		 */
+		for (i = 0, cp = tp + child, cp1 = tp + parent;
+		     i < nparent; i++, cp += 4, cp1++)
+			*cp1 = TREEMAX(cp);
+	}
+
+	return (*tp);
+}
+
+
+/*
+ *	dbInitDmapCtl()
+ *
+ * function: initialize dmapctl page
+ */
+static int dbInitDmapCtl(dmapctl_t * dcp, int level, int i)
+{				/* start leaf index not covered by range */
+	s8 *cp;
+
+	dcp->nleafs = cpu_to_le32(LPERCTL);
+	dcp->l2nleafs = cpu_to_le32(L2LPERCTL);
+	dcp->leafidx = cpu_to_le32(CTLLEAFIND);
+	dcp->height = cpu_to_le32(5);
+	dcp->budmin = L2BPERDMAP + L2LPERCTL * level;
+
+	/*
+	 * initialize the leaves of current level that were not covered 
+	 * by the specified input block range (i.e. the leaves have no 
+	 * low level dmapctl or dmap).
+	 */
+	cp = &dcp->stree[CTLLEAFIND + i];
+	for (; i < LPERCTL; i++)
+		*cp++ = NOFREE;
+
+	/* build the dmap's binary buddy summary tree */
+	return (dbInitTree((dmaptree_t *) dcp));
+}
+
+
+/*
+ * NAME:	dbGetL2AGSize()/ujfs_getagl2size()
+ *                                                                    
+ * FUNCTION:	Determine log2(allocation group size) from aggregate size
+ *                                                                    
+ * PARAMETERS:
+ *	nblocks	- Number of blocks in aggregate
+ *
+ * RETURNS: log2(allocation group size) in aggregate blocks
+ */
+static int dbGetL2AGSize(s64 nblocks)
+{
+	s64 sz;
+	s64 m;
+	int l2sz;
+
+	if (nblocks < BPERDMAP * MAXAG)
+		return (L2BPERDMAP);
+
+	/* round up aggregate size to power of 2 */
+	m = ((u64) 1 << (64 - 1));
+	for (l2sz = 64; l2sz >= 0; l2sz--, m >>= 1) {
+		if (m & nblocks)
+			break;
+	}
+
+	sz = (s64) 1 << l2sz;
+	if (sz < nblocks)
+		l2sz += 1;
+
+	/* agsize = roundupSize/max_number_of_ag */
+	return (l2sz - L2MAXAG);
+}
+
+
+/*
+ * NAME:	dbMapFileSizeToMapSize()
+ *                                                                    
+ * FUNCTION:	compute number of blocks the block allocation map file 
+ *		can cover from the map file size;
+ *
+ * RETURNS:	Number of blocks which can be covered by this block map file;
+ */
+
+/*
+ * maximum number of map pages at each level including control pages
+ */
+#define MAXL0PAGES	(1 + LPERCTL)
+#define MAXL1PAGES	(1 + LPERCTL * MAXL0PAGES)
+#define MAXL2PAGES	(1 + LPERCTL * MAXL1PAGES)
+
+/*
+ * convert number of map pages to the zero origin top dmapctl level
+ */
+#define BMAPPGTOLEV(npages)	\
+	(((npages) <= 3 + MAXL0PAGES) ? 0 \
+       : ((npages) <= 2 + MAXL1PAGES) ? 1 : 2)
+
+s64 dbMapFileSizeToMapSize(struct inode * ipbmap)
+{
+	struct super_block *sb = ipbmap->i_sb;
+	s64 nblocks;
+	s64 npages, ndmaps;
+	int level, i;
+	int complete, factor;
+
+	nblocks = ipbmap->i_size >> JFS_SBI(sb)->l2bsize;
+	npages = nblocks >> JFS_SBI(sb)->l2nbperpage;
+	level = BMAPPGTOLEV(npages);
+
+	/* At each level, accumulate the number of dmap pages covered by 
+	 * the number of full child levels below it;
+	 * repeat for the last incomplete child level.
+	 */
+	ndmaps = 0;
+	npages--;		/* skip the first global control page */
+	/* skip higher level control pages above top level covered by map */
+	npages -= (2 - level);
+	npages--;		/* skip top level's control page */
+	for (i = level; i >= 0; i--) {
+		factor =
+		    (i == 2) ? MAXL1PAGES : ((i == 1) ? MAXL0PAGES : 1);
+		complete = (u32) npages / factor;
+		ndmaps += complete * ((i == 2) ? LPERCTL * LPERCTL
+				      : ((i == 1) ? LPERCTL : 1));
+
+		/* pages in last/incomplete child */
+		npages = (u32) npages % factor;
+		/* skip incomplete child's level control page */
+		npages--;
+	}
+
+	/* convert the number of dmaps into the number of blocks 
+	 * which can be covered by the dmaps;
+	 */
+	nblocks = ndmaps << L2BPERDMAP;
+
+	return (nblocks);
+}
+
+
+#ifdef	_JFS_DEBUG_DMAP
+/*
+ *	DBinitmap()
+ */
+static void DBinitmap(s64 size, struct inode *ipbmap, u32 ** results)
+{
+	int npages;
+	u32 *dbmap, *d;
+	int n;
+	s64 lblkno, cur_block;
+	dmap_t *dp;
+	metapage_t *mp;
+
+	npages = size / 32768;
+	npages += (size % 32768) ? 1 : 0;
+
+	dbmap = (u32 *) xmalloc(npages * 4096, L2PSIZE, kernel_heap);
+	if (dbmap == NULL)
+		assert(0);
+
+	for (n = 0, d = dbmap; n < npages; n++, d += 1024)
+		bzero(d, 4096);
+
+	/* Need to initialize from disk map pages
+	 */
+	for (d = dbmap, cur_block = 0; cur_block < size;
+	     cur_block += BPERDMAP, d += LPERDMAP) {
+		lblkno = BLKTODMAP(cur_block,
+				   JFS_SBI(ipbmap->i_sb)->bmap->
+				   db_l2nbperpage);
+		mp = read_metapage(ipbmap, lblkno, PSIZE, 0);
+		if (mp == NULL) {
+			assert(0);
+		}
+		dp = (dmap_t *) mp->data;
+
+		for (n = 0; n < LPERDMAP; n++)
+			d[n] = le32_to_cpu(dp->wmap[n]);
+
+		release_metapage(mp);
+	}
+
+	*results = dbmap;
+}
+
+
+/*
+ *	DBAlloc()
+ */
+void DBAlloc(uint * dbmap, s64 mapsize, s64 blkno, s64 nblocks)
+{
+	int word, nb, bitno;
+	u32 mask;
+
+	assert(blkno > 0 && blkno < mapsize);
+	assert(nblocks > 0 && nblocks <= mapsize);
+
+	assert(blkno + nblocks <= mapsize);
+
+	dbmap += (blkno / 32);
+	while (nblocks > 0) {
+		bitno = blkno & (32 - 1);
+		nb = min(nblocks, 32 - bitno);
+
+		mask = (0xffffffff << (32 - nb) >> bitno);
+		assert((mask & *dbmap) == 0);
+		*dbmap |= mask;
+
+		dbmap++;
+		blkno += nb;
+		nblocks -= nb;
+	}
+}
+
+
+/*
+ *	DBFree()
+ */
+static void DBFree(uint * dbmap, s64 mapsize, s64 blkno, s64 nblocks)
+{
+	int word, nb, bitno;
+	u32 mask;
+
+	assert(blkno > 0 && blkno < mapsize);
+	assert(nblocks > 0 && nblocks <= mapsize);
+
+	assert(blkno + nblocks <= mapsize);
+
+	dbmap += (blkno / 32);
+	while (nblocks > 0) {
+		bitno = blkno & (32 - 1);
+		nb = min(nblocks, 32 - bitno);
+
+		mask = (0xffffffff << (32 - nb) >> bitno);
+		assert((mask & *dbmap) == mask);
+		*dbmap &= ~mask;
+
+		dbmap++;
+		blkno += nb;
+		nblocks -= nb;
+	}
+}
+
+
+/*
+ *	DBAllocCK()
+ */
+static void DBAllocCK(uint * dbmap, s64 mapsize, s64 blkno, s64 nblocks)
+{
+	int word, nb, bitno;
+	u32 mask;
+
+	assert(blkno > 0 && blkno < mapsize);
+	assert(nblocks > 0 && nblocks <= mapsize);
+
+	assert(blkno + nblocks <= mapsize);
+
+	dbmap += (blkno / 32);
+	while (nblocks > 0) {
+		bitno = blkno & (32 - 1);
+		nb = min(nblocks, 32 - bitno);
+
+		mask = (0xffffffff << (32 - nb) >> bitno);
+		assert((mask & *dbmap) == mask);
+
+		dbmap++;
+		blkno += nb;
+		nblocks -= nb;
+	}
+}
+
+
+/*
+ *	DBFreeCK()
+ */
+static void DBFreeCK(uint * dbmap, s64 mapsize, s64 blkno, s64 nblocks)
+{
+	int word, nb, bitno;
+	u32 mask;
+
+	assert(blkno > 0 && blkno < mapsize);
+	assert(nblocks > 0 && nblocks <= mapsize);
+
+	assert(blkno + nblocks <= mapsize);
+
+	dbmap += (blkno / 32);
+	while (nblocks > 0) {
+		bitno = blkno & (32 - 1);
+		nb = min(nblocks, 32 - bitno);
+
+		mask = (0xffffffff << (32 - nb) >> bitno);
+		assert((mask & *dbmap) == 0);
+
+		dbmap++;
+		blkno += nb;
+		nblocks -= nb;
+	}
+}
+
+
+/*
+ *	dbPrtMap()
+ */
+static void dbPrtMap(bmap_t * bmp)
+{
+	printk("   mapsize:   %d%d\n", bmp->db_mapsize);
+	printk("   nfree:     %d%d\n", bmp->db_nfree);
+	printk("   numag:     %d\n", bmp->db_numag);
+	printk("   agsize:    %d%d\n", bmp->db_agsize);
+	printk("   agl2size:  %d\n", bmp->db_agl2size);
+	printk("   agwidth:   %d\n", bmp->db_agwidth);
+	printk("   agstart:   %d\n", bmp->db_agstart);
+	printk("   agheigth:  %d\n", bmp->db_agheigth);
+	printk("   aglevel:   %d\n", bmp->db_aglevel);
+	printk("   maxlevel:  %d\n", bmp->db_maxlevel);
+	printk("   maxag:     %d\n", bmp->db_maxag);
+	printk("   agpref:    %d\n", bmp->db_agpref);
+	printk("   l2nbppg:   %d\n", bmp->db_l2nbperpage);
+}
+
+
+/*
+ *	dbPrtCtl()
+ */
+static void dbPrtCtl(dmapctl_t * dcp)
+{
+	int i, j, n;
+
+	printk("   height:    %08x\n", le32_to_cpu(dcp->height));
+	printk("   leafidx:   %08x\n", le32_to_cpu(dcp->leafidx));
+	printk("   budmin:    %08x\n", dcp->budmin);
+	printk("   nleafs:    %08x\n", le32_to_cpu(dcp->nleafs));
+	printk("   l2nleafs:  %08x\n", le32_to_cpu(dcp->l2nleafs));
+
+	printk("\n Tree:\n");
+	for (i = 0; i < CTLLEAFIND; i += 8) {
+		n = min(8, CTLLEAFIND - i);
+
+		for (j = 0; j < n; j++)
+			printf("  [%03x]: %02x", i + j,
+			       (char) dcp->stree[i + j]);
+		printf("\n");
+	}
+
+	printk("\n Tree Leaves:\n");
+	for (i = 0; i < LPERCTL; i += 8) {
+		n = min(8, LPERCTL - i);
+
+		for (j = 0; j < n; j++)
+			printf("  [%03x]: %02x",
+			       i + j,
+			       (char) dcp->stree[i + j + CTLLEAFIND]);
+		printf("\n");
+	}
+}
+#endif				/* _JFS_DEBUG_DMAP */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_dmap.h linuxppc64_2_4/fs/jfs/jfs_dmap.h
--- linux-2.4.19/fs/jfs/jfs_dmap.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_dmap.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,301 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ *	jfs_dmap.h: block allocation map manager
+ */
+
+#ifndef	_H_JFS_DMAP
+#define _H_JFS_DMAP
+
+#include "jfs_txnmgr.h"
+
+#define BMAPVERSION	1	/* version number */
+#define	TREESIZE	(256+64+16+4+1)	/* size of a dmap tree */
+#define	LEAFIND		(64+16+4+1)	/* index of 1st leaf of a dmap tree */
+#define LPERDMAP	256	/* num leaves per dmap tree */
+#define L2LPERDMAP	8	/* l2 number of leaves per dmap tree */
+#define	DBWORD		32	/* # of blks covered by a map word */
+#define	L2DBWORD	5	/* l2 # of blks covered by a mword */
+#define BUDMIN  	L2DBWORD	/* max free string in a map word */
+#define BPERDMAP	(LPERDMAP * DBWORD)	/* num of blks per dmap */
+#define L2BPERDMAP	13	/* l2 num of blks per dmap */
+#define CTLTREESIZE	(1024+256+64+16+4+1)	/* size of a dmapctl tree */
+#define CTLLEAFIND	(256+64+16+4+1)	/* idx of 1st leaf of a dmapctl tree */
+#define LPERCTL		1024	/* num of leaves per dmapctl tree */
+#define L2LPERCTL	10	/* l2 num of leaves per dmapctl tree */
+#define	ROOT		0	/* index of the root of a tree */
+#define	NOFREE		((s8) -1)	/* no blocks free */
+#define	MAXAG		128	/* max number of allocation groups */
+#define L2MAXAG		7	/* l2 max num of AG */
+#define L2MINAGSZ	25	/* l2 of minimum AG size in bytes */
+#define	BMAPBLKNO	0	/* lblkno of bmap within the map */
+
+/*
+ * maximum l2 number of disk blocks at the various dmapctl levels.
+ */
+#define	L2MAXL0SIZE	(L2BPERDMAP + 1 * L2LPERCTL)
+#define	L2MAXL1SIZE	(L2BPERDMAP + 2 * L2LPERCTL)
+#define	L2MAXL2SIZE	(L2BPERDMAP + 3 * L2LPERCTL)
+
+/*
+ * maximum number of disk blocks at the various dmapctl levels.
+ */
+#define	MAXL0SIZE	((s64)1 << L2MAXL0SIZE)
+#define	MAXL1SIZE	((s64)1 << L2MAXL1SIZE)
+#define	MAXL2SIZE	((s64)1 << L2MAXL2SIZE)
+
+#define	MAXMAPSIZE	MAXL2SIZE	/* maximum aggregate map size */
+
+/* 
+ * determine the maximum free string for four (lower level) nodes
+ * of the tree.
+ */
+static __inline signed char TREEMAX(signed char *cp)
+{
+	signed char tmp1, tmp2;
+
+	tmp1 = max(*(cp+2), *(cp+3));
+	tmp2 = max(*(cp), *(cp+1));
+
+	return max(tmp1, tmp2);
+}
+
+/*
+ * convert disk block number to the logical block number of the dmap
+ * describing the disk block.  s is the log2(number of logical blocks per page)
+ *
+ * The calculation figures out how many logical pages are in front of the dmap.
+ *	- the number of dmaps preceding it
+ *	- the number of L0 pages preceding its L0 page
+ *	- the number of L1 pages preceding its L1 page
+ *	- 3 is added to account for the L2, L1, and L0 page for this dmap
+ *	- 1 is added to account for the control page of the map.
+ */
+#define BLKTODMAP(b,s)    \
+        ((((b) >> 13) + ((b) >> 23) + ((b) >> 33) + 3 + 1) << (s))
+
+/*
+ * convert disk block number to the logical block number of the LEVEL 0
+ * dmapctl describing the disk block.  s is the log2(number of logical blocks
+ * per page)
+ *
+ * The calculation figures out how many logical pages are in front of the L0.
+ *	- the number of dmap pages preceding it
+ *	- the number of L0 pages preceding it
+ *	- the number of L1 pages preceding its L1 page
+ *	- 2 is added to account for the L2, and L1 page for this L0
+ *	- 1 is added to account for the control page of the map.
+ */
+#define BLKTOL0(b,s)      \
+        (((((b) >> 23) << 10) + ((b) >> 23) + ((b) >> 33) + 2 + 1) << (s))
+
+/*
+ * convert disk block number to the logical block number of the LEVEL 1
+ * dmapctl describing the disk block.  s is the log2(number of logical blocks
+ * per page)
+ *
+ * The calculation figures out how many logical pages are in front of the L1.
+ *	- the number of dmap pages preceding it
+ *	- the number of L0 pages preceding it
+ *	- the number of L1 pages preceding it
+ *	- 1 is added to account for the L2 page
+ *	- 1 is added to account for the control page of the map.
+ */
+#define BLKTOL1(b,s)      \
+     (((((b) >> 33) << 20) + (((b) >> 33) << 10) + ((b) >> 33) + 1 + 1) << (s))
+
+/*
+ * convert disk block number to the logical block number of the dmapctl
+ * at the specified level which describes the disk block.
+ */
+#define BLKTOCTL(b,s,l)   \
+        (((l) == 2) ? 1 : ((l) == 1) ? BLKTOL1((b),(s)) : BLKTOL0((b),(s)))
+
+/* 
+ * convert aggregate map size to the zero origin dmapctl level of the
+ * top dmapctl.
+ */
+#define	BMAPSZTOLEV(size)	\
+	(((size) <= MAXL0SIZE) ? 0 : ((size) <= MAXL1SIZE) ? 1 : 2)
+
+/* convert disk block number to allocation group number.
+ */
+#define BLKTOAG(b,sbi)	((b) >> ((sbi)->bmap->db_agl2size))
+
+/* convert allocation group number to starting disk block
+ * number.
+ */
+#define AGTOBLK(a,ip)	\
+	((s64)(a) << (JFS_SBI((ip)->i_sb)->bmap->db_agl2size))
+
+/*
+ *	dmap summary tree
+ *
+ * dmaptree_t must be consistent with dmapctl_t.
+ */
+typedef struct {
+	s32 nleafs;		/* 4: number of tree leafs      */
+	s32 l2nleafs;		/* 4: l2 number of tree leafs   */
+	s32 leafidx;		/* 4: index of first tree leaf  */
+	s32 height;		/* 4: height of the tree        */
+	s8 budmin;		/* 1: min l2 tree leaf value to combine */
+	s8 stree[TREESIZE];	/* TREESIZE: tree               */
+	u8 pad[2];		/* 2: pad to word boundary      */
+} dmaptree_t;			/* - 360 -                      */
+
+/*
+ *	dmap page per 8K blocks bitmap
+ */
+typedef struct {
+	s32 nblocks;		/* 4: num blks covered by this dmap     */
+	s32 nfree;		/* 4: num of free blks in this dmap     */
+	s64 start;		/* 8: starting blkno for this dmap      */
+	dmaptree_t tree;	/* 360: dmap tree                       */
+	u8 pad[1672];		/* 1672: pad to 2048 bytes              */
+	u32 wmap[LPERDMAP];	/* 1024: bits of the working map        */
+	u32 pmap[LPERDMAP];	/* 1024: bits of the persistent map     */
+} dmap_t;			/* - 4096 -                             */
+
+/*
+ *	disk map control page per level.
+ *
+ * dmapctl_t must be consistent with dmaptree_t.
+ */
+typedef struct {
+	s32 nleafs;		/* 4: number of tree leafs      */
+	s32 l2nleafs;		/* 4: l2 number of tree leafs   */
+	s32 leafidx;		/* 4: index of the first tree leaf      */
+	s32 height;		/* 4: height of tree            */
+	s8 budmin;		/* 1: minimum l2 tree leaf value        */
+	s8 stree[CTLTREESIZE];	/* CTLTREESIZE: dmapctl tree    */
+	u8 pad[2714];		/* 2714: pad to 4096            */
+} dmapctl_t;			/* - 4096 -                     */
+
+/*
+ *	common definition for dmaptree_t within dmap and dmapctl
+ */
+typedef union {
+	dmaptree_t t1;
+	dmapctl_t t2;
+} dmtree_t;
+
+/* macros for accessing fields within dmtree_t */
+#define	dmt_nleafs	t1.nleafs
+#define	dmt_l2nleafs 	t1.l2nleafs
+#define	dmt_leafidx 	t1.leafidx
+#define	dmt_height 	t1.height
+#define	dmt_budmin 	t1.budmin
+#define	dmt_stree 	t1.stree
+
+/* 
+ *	on-disk aggregate disk allocation map descriptor.
+ */
+typedef struct {
+	s64 dn_mapsize;		/* 8: number of blocks in aggregate     */
+	s64 dn_nfree;		/* 8: num free blks in aggregate map    */
+	s32 dn_l2nbperpage;	/* 4: number of blks per page           */
+	s32 dn_numag;		/* 4: total number of ags               */
+	s32 dn_maxlevel;	/* 4: number of active ags              */
+	s32 dn_maxag;		/* 4: max active alloc group number     */
+	s32 dn_agpref;		/* 4: preferred alloc group (hint)      */
+	s32 dn_aglevel;		/* 4: dmapctl level holding the AG      */
+	s32 dn_agheigth;	/* 4: height in dmapctl of the AG       */
+	s32 dn_agwidth;		/* 4: width in dmapctl of the AG        */
+	s32 dn_agstart;		/* 4: start tree index at AG height     */
+	s32 dn_agl2size;	/* 4: l2 num of blks per alloc group    */
+	s64 dn_agfree[MAXAG];	/* 8*MAXAG: per AG free count           */
+	s64 dn_agsize;		/* 8: num of blks per alloc group       */
+	s8 dn_maxfreebud;	/* 1: max free buddy system             */
+	u8 pad[3007];		/* 3007: pad to 4096                    */
+} dbmap_t;			/* - 4096 -                             */
+
+/* 
+ *	in-memory aggregate disk allocation map descriptor.
+ */
+typedef struct bmap {
+	dbmap_t db_bmap;	/* on-disk aggregate map descriptor */
+	struct inode *db_ipbmap;	/* ptr to aggregate map incore inode */
+	struct semaphore db_bmaplock;	/* aggregate map lock */
+	u32 *db_DBmap;
+} bmap_t;
+
+/* macros for accessing fields within in-memory aggregate map descriptor */
+#define	db_mapsize	db_bmap.dn_mapsize
+#define	db_nfree	db_bmap.dn_nfree
+#define	db_agfree	db_bmap.dn_agfree
+#define	db_agsize	db_bmap.dn_agsize
+#define	db_agl2size	db_bmap.dn_agl2size
+#define	db_agwidth	db_bmap.dn_agwidth
+#define	db_agheigth	db_bmap.dn_agheigth
+#define	db_agstart	db_bmap.dn_agstart
+#define	db_numag	db_bmap.dn_numag
+#define	db_maxlevel	db_bmap.dn_maxlevel
+#define	db_aglevel	db_bmap.dn_aglevel
+#define	db_agpref	db_bmap.dn_agpref
+#define	db_maxag	db_bmap.dn_maxag
+#define	db_maxfreebud	db_bmap.dn_maxfreebud
+#define	db_l2nbperpage	db_bmap.dn_l2nbperpage
+
+/*
+ * macros for various conversions needed by the allocators.
+ * blkstol2(), cntlz(), and cnttz() are operating system dependent functions.
+ */
+/* convert number of blocks to log2 number of blocks, rounding up to
+ * the next log2 value if blocks is not a l2 multiple.
+ */
+#define	BLKSTOL2(d)		(blkstol2(d))
+
+/* convert number of leafs to log2 leaf value */
+#define	NLSTOL2BSZ(n)		(31 - cntlz((n)) + BUDMIN)
+
+/* convert leaf index to log2 leaf value */
+#define	LITOL2BSZ(n,m,b)	((((n) == 0) ? (m) : cnttz((n))) + (b))
+
+/* convert a block number to a dmap control leaf index */
+#define BLKTOCTLLEAF(b,m)	\
+	(((b) & (((s64)1 << ((m) + L2LPERCTL)) - 1)) >> (m))
+
+/* convert log2 leaf value to buddy size */
+#define	BUDSIZE(s,m)		(1 << ((s) - (m)))
+
+/*
+ *	external references.
+ */
+extern int dbMount(struct inode *ipbmap);
+
+extern int dbUnmount(struct inode *ipbmap, int mounterror);
+
+extern int dbFree(struct inode *ipbmap, s64 blkno, s64 nblocks);
+
+extern int dbUpdatePMap(struct inode *ipbmap,
+			int free, s64 blkno, s64 nblocks, tblock_t * tblk);
+
+extern int dbNextAG(struct inode *ipbmap);
+
+extern int dbAlloc(struct inode *ipbmap, s64 hint, s64 nblocks, s64 * results);
+
+extern int dbAllocExact(struct inode *ip, s64 blkno, int nblocks);
+
+extern int dbReAlloc(struct inode *ipbmap,
+		     s64 blkno, s64 nblocks, s64 addnblocks, s64 * results);
+
+extern int dbSync(struct inode *ipbmap);
+extern int dbAllocBottomUp(struct inode *ip, s64 blkno, s64 nblocks);
+extern int dbExtendFS(struct inode *ipbmap, s64 blkno, s64 nblocks);
+extern void dbFinalizeBmap(struct inode *ipbmap);
+extern s64 dbMapFileSizeToMapSize(struct inode *ipbmap);
+#endif				/* _H_JFS_DMAP */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_dtree.c linuxppc64_2_4/fs/jfs/jfs_dtree.c
--- linux-2.4.19/fs/jfs/jfs_dtree.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_dtree.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,4525 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * 
+*/
+
+/*
+ *	jfs_dtree.c: directory B+-tree manager
+ *
+ * B+-tree with variable length key directory:
+ *
+ * each directory page is structured as an array of 32-byte
+ * directory entry slots initialized as a freelist
+ * to avoid search/compaction of free space at insertion.
+ * when an entry is inserted, a number of slots are allocated
+ * from the freelist as required to store variable length data
+ * of the entry; when the entry is deleted, slots of the entry
+ * are returned to freelist.
+ *
+ * leaf entry stores full name as key and file serial number
+ * (aka inode number) as data.
+ * internal/router entry stores sufffix compressed name
+ * as key and simple extent descriptor as data.
+ *
+ * each directory page maintains a sorted entry index table
+ * which stores the start slot index of sorted entries
+ * to allow binary search on the table.
+ *
+ * directory starts as a root/leaf page in on-disk inode
+ * inline data area.
+ * when it becomes full, it starts a leaf of a external extent
+ * of length of 1 block. each time the first leaf becomes full,
+ * it is extended rather than split (its size is doubled),
+ * until its length becoms 4 KBytes, from then the extent is split
+ * with new 4 Kbyte extent when it becomes full
+ * to reduce external fragmentation of small directories.
+ *
+ * blah, blah, blah, for linear scan of directory in pieces by
+ * readdir().
+ *
+ *
+ *	case-insensitive directory file system
+ *
+ * names are stored in case-sensitive way in leaf entry.
+ * but stored, searched and compared in case-insensitive (uppercase) order
+ * (i.e., both search key and entry key are folded for search/compare):
+ * (note that case-sensitive order is BROKEN in storage, e.g.,
+ *  sensitive: Ad, aB, aC, aD -> insensitive: aB, aC, aD, Ad
+ *
+ *  entries which folds to the same key makes up a equivalent class
+ *  whose members are stored as contiguous cluster (may cross page boundary)
+ *  but whose order is arbitrary and acts as duplicate, e.g.,
+ *  abc, Abc, aBc, abC)
+ *
+ * once match is found at leaf, requires scan forward/backward
+ * either for, in case-insensitive search, duplicate
+ * or for, in case-sensitive search, for exact match
+ *
+ * router entry must be created/stored in case-insensitive way
+ * in internal entry:
+ * (right most key of left page and left most key of right page
+ * are folded, and its suffix compression is propagated as router
+ * key in parent)
+ * (e.g., if split occurs <abc> and <aBd>, <ABD> trather than <aB>
+ * should be made the router key for the split)
+ *
+ * case-insensitive search:
+ *
+ * 	fold search key;
+ *
+ *	case-insensitive search of B-tree:
+ *	for internal entry, router key is already folded;
+ *	for leaf entry, fold the entry key before comparison.
+ *
+ *	if (leaf entry case-insensitive match found)
+ *		if (next entry satisfies case-insensitive match)
+ *			return EDUPLICATE;
+ *		if (prev entry satisfies case-insensitive match)
+ *			return EDUPLICATE;
+ *		return match;
+ *	else
+ *		return no match;
+ *
+ * 	serialization:
+ * target directory inode lock is being held on entry/exit
+ * of all main directory service routines.
+ *
+ *	log based recovery:
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_superblock.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_dmap.h"
+#include "jfs_unicode.h"
+#include "jfs_debug.h"
+
+/* dtree split parameter */
+typedef struct {
+	metapage_t *mp;
+	s16 index;
+	s16 nslot;
+	component_t *key;
+	ddata_t *data;
+	pxdlist_t *pxdlist;
+} dtsplit_t;
+
+#define DT_PAGE(IP, MP) BT_PAGE(IP, MP, dtpage_t, i_dtroot)
+
+/* get page buffer for specified block address */
+#define DT_GETPAGE(IP, BN, MP, SIZE, P, RC)\
+{\
+	BT_GETPAGE(IP, BN, MP, dtpage_t, SIZE, P, RC, i_dtroot)\
+	if (!(RC))\
+	{\
+		if (((P)->header.nextindex > (((BN)==0)?DTROOTMAXSLOT:(P)->header.maxslot)) ||\
+		    ((BN) && ((P)->header.maxslot > DTPAGEMAXSLOT)))\
+		{\
+			jERROR(1,("DT_GETPAGE: dtree page corrupt\n"));\
+			BT_PUTPAGE(MP);\
+			updateSuper((IP)->i_sb, FM_DIRTY);\
+			MP = NULL;\
+			RC = EIO;\
+		}\
+	}\
+}
+
+/* for consistency */
+#define DT_PUTPAGE(MP) BT_PUTPAGE(MP)
+
+#define DT_GETSEARCH(IP, LEAF, BN, MP, P, INDEX) \
+	BT_GETSEARCH(IP, LEAF, BN, MP, dtpage_t, P, INDEX, i_dtroot)
+
+/*
+ * forward references
+ */
+static int dtSplitUp(tid_t tid, struct inode *ip,
+		     dtsplit_t * split, btstack_t * btstack);
+
+static int dtSplitPage(tid_t tid, struct inode *ip, dtsplit_t * split,
+		       metapage_t ** rmpp, dtpage_t ** rpp, pxd_t * rxdp);
+
+static int dtExtendPage(tid_t tid, struct inode *ip,
+			dtsplit_t * split, btstack_t * btstack);
+
+static int dtSplitRoot(tid_t tid, struct inode *ip,
+		       dtsplit_t * split, metapage_t ** rmpp);
+
+static int dtDeleteUp(tid_t tid, struct inode *ip, metapage_t * fmp,
+		      dtpage_t * fp, btstack_t * btstack);
+
+static int dtSearchNode(struct inode *ip,
+			s64 lmxaddr, pxd_t * kpxd, btstack_t * btstack);
+
+static int dtRelink(tid_t tid, struct inode *ip, dtpage_t * p);
+
+static int dtReadFirst(struct inode *ip, btstack_t * btstack);
+
+static int dtReadNext(struct inode *ip,
+		      loff_t * offset, btstack_t * btstack);
+
+static int dtCompare(component_t * key, dtpage_t * p, int si);
+
+static int ciCompare(component_t * key, dtpage_t * p, int si, int flag);
+
+static void dtGetKey(dtpage_t * p, int i, component_t * key, int flag);
+
+static void ciGetLeafPrefixKey(dtpage_t * lp, int li, dtpage_t * rp,
+			       int ri, component_t * key, int flag);
+
+static void dtInsertEntry(dtpage_t * p, int index, component_t * key,
+			  ddata_t * data, dtlock_t ** dtlock);
+
+static void dtMoveEntry(dtpage_t * sp, int si, dtpage_t * dp,
+			dtlock_t ** sdtlock, dtlock_t ** ddtlock,
+			int do_index);
+
+static void dtDeleteEntry(dtpage_t * p, int fi, dtlock_t ** dtlock);
+
+static void dtTruncateEntry(dtpage_t * p, int ti, dtlock_t ** dtlock);
+
+static void dtLinelockFreelist(dtpage_t * p, int m, dtlock_t ** dtlock);
+
+#define ciToUpper(c)	UniStrupr((c)->name)
+
+/*
+ *	find_index()
+ *
+ *	Returns dtree page containing directory table entry for specified
+ *	index and pointer to its entry.
+ *
+ *	mp must be released by caller.
+ */
+static dir_table_slot_t *find_index(struct inode *ip, u32 index,
+				    metapage_t ** mp)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	s64 blkno;
+	s64 offset;
+	int page_offset;
+	dir_table_slot_t *slot;
+	static int maxWarnings = 10;
+
+	if (index < 2) {
+		if (maxWarnings) {
+			jERROR(1, ("find_entry called with index = %d\n",
+				   index));
+			maxWarnings--;
+		}
+		return 0;
+	}
+
+	if (index >= jfs_ip->next_index) {
+		jFYI(1, ("find_entry called with index >= next_index\n"));
+		return 0;
+	}
+
+	if (jfs_ip->next_index <= (MAX_INLINE_DIRTABLE_ENTRY + 1)) {
+		/*
+		 * Inline directory table
+		 */
+		*mp = 0;
+		slot = &jfs_ip->i_dirtable[index - 2];
+	} else {
+		offset = (index - 2) * sizeof(dir_table_slot_t);
+		page_offset = offset & (PSIZE - 1);
+		blkno = ((offset + 1) >> L2PSIZE) <<
+		    JFS_SBI(ip->i_sb)->l2nbperpage;
+
+		if (*mp && ((*mp)->index != blkno)) {
+			release_metapage(*mp);
+			*mp = 0;
+		}
+		if (*mp == 0)
+			*mp = read_metapage(ip, blkno, PSIZE, 0);
+		if (*mp == 0) {
+			jERROR(1,
+			       ("free_index: error reading directory table\n"));
+			return 0;
+		}
+
+		slot =
+		    (dir_table_slot_t *) ((char *) (*mp)->data +
+					  page_offset);
+	}
+	return slot;
+}
+
+static inline void lock_index(tid_t tid, struct inode *ip, metapage_t * mp,
+			      u32 index)
+{
+	tlock_t *tlck;
+	linelock_t *llck;
+	lv_t *lv;
+
+	tlck = txLock(tid, ip, mp, tlckDATA);
+	llck = (linelock_t *) tlck->lock;
+
+	if (llck->index >= llck->maxcnt)
+		llck = txLinelock(llck);
+	lv = &llck->lv[llck->index];
+
+	/*
+	 *      Linelock slot size is twice the size of directory table
+	 *      slot size.  512 entries per page.
+	 */
+	lv->offset = ((index - 2) & 511) >> 1;
+	lv->length = 1;
+	llck->index++;
+}
+
+/*
+ *	add_index()
+ *
+ *	Adds an entry to the directory index table.  This is used to provide
+ *	each directory entry with a persistent index in which to resume
+ *	directory traversals
+ */
+static u32 add_index(tid_t tid, struct inode *ip, s64 bn, int slot)
+{
+	struct super_block *sb = ip->i_sb;
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	u64 blkno;
+	dir_table_slot_t *dirtab_slot;
+	u32 index;
+	linelock_t *llck;
+	lv_t *lv;
+	metapage_t *mp;
+	s64 offset;
+	uint page_offset;
+	int rc;
+	tlock_t *tlck;
+	s64 xaddr;
+
+	ASSERT(DO_INDEX(ip));
+
+	if (jfs_ip->next_index < 2) {
+		jERROR(1, ("next_index = %d.  Please fix this!\n",
+			   jfs_ip->next_index));
+		jfs_ip->next_index = 2;
+	}
+
+	index = jfs_ip->next_index++;
+
+	if (index <= MAX_INLINE_DIRTABLE_ENTRY) {
+		/*
+		 * i_size reflects size of index table, or 8 bytes per entry.
+		 */
+		ip->i_size = (loff_t) (index - 1) << 3;
+
+		/*
+		 * dir table fits inline within inode
+		 */
+		dirtab_slot = &jfs_ip->i_dirtable[index-2];
+		dirtab_slot->flag = DIR_INDEX_VALID;
+		dirtab_slot->slot = slot;
+		DTSaddress(dirtab_slot, bn);
+
+		set_cflag(COMMIT_Dirtable, ip);
+
+		return index;
+	}
+	if (index == (MAX_INLINE_DIRTABLE_ENTRY + 1)) {
+		/*
+		 * It's time to move the inline table to an external
+		 * page and begin to build the xtree
+		 */
+
+		/*
+		 * Save the table, we're going to overwrite it with the
+		 * xtree root
+		 */
+		dir_table_slot_t temp_table[12];
+		memcpy(temp_table, &jfs_ip->i_dirtable, sizeof(temp_table));
+
+		/*
+		 * Initialize empty x-tree
+		 */
+		xtInitRoot(tid, ip);
+
+		/*
+		 * Allocate the first block & add it to the xtree
+		 */
+		xaddr = 0;
+		if ((rc =
+		     xtInsert(tid, ip, 0, 0, sbi->nbperpage,
+			      &xaddr, 0))) {
+			jFYI(1, ("add_index: xtInsert failed!\n"));
+			return -1;
+		}
+		ip->i_size = PSIZE;
+		ip->i_blocks += LBLK2PBLK(sb, sbi->nbperpage);
+
+		if ((mp = get_metapage(ip, 0, ip->i_blksize, 0)) == 0) {
+			jERROR(1, ("add_index: get_metapage failed!\n"));
+			xtTruncate(tid, ip, 0, COMMIT_PWMAP);
+			return -1;
+		}
+		tlck = txLock(tid, ip, mp, tlckDATA);
+		llck = (linelock_t *) & tlck->lock;
+		ASSERT(llck->index == 0);
+		lv = &llck->lv[0];
+
+		lv->offset = 0;
+		lv->length = 6;	/* tlckDATA slot size is 16 bytes */
+		llck->index++;
+
+		memcpy(mp->data, temp_table, sizeof(temp_table));
+
+		mark_metapage_dirty(mp);
+		release_metapage(mp);
+
+		/*
+		 * Logging is now directed by xtree tlocks
+		 */
+		clear_cflag(COMMIT_Dirtable, ip);
+	}
+
+	offset = (index - 2) * sizeof(dir_table_slot_t);
+	page_offset = offset & (PSIZE - 1);
+	blkno = ((offset + 1) >> L2PSIZE) << sbi->l2nbperpage;
+	if (page_offset == 0) {
+		/*
+		 * This will be the beginning of a new page
+		 */
+		xaddr = 0;
+		if ((rc =
+		     xtInsert(tid, ip, 0, blkno, sbi->nbperpage,
+			      &xaddr, 0))) {
+			jFYI(1, ("add_index: xtInsert failed!\n"));
+			jfs_ip->next_index--;
+			return -1;
+		}
+		ip->i_size += PSIZE;
+		ip->i_blocks += LBLK2PBLK(sb, sbi->nbperpage);
+
+		if ((mp = get_metapage(ip, blkno, PSIZE, 0)))
+			memset(mp->data, 0, PSIZE);	/* Just looks better */
+		else
+			xtTruncate(tid, ip, offset, COMMIT_PWMAP);
+	} else
+		mp = read_metapage(ip, blkno, PSIZE, 0);
+
+	if (mp == 0) {
+		jERROR(1, ("add_index: get/read_metapage failed!\n"));
+		return -1;
+	}
+
+	lock_index(tid, ip, mp, index);
+
+	dirtab_slot =
+	    (dir_table_slot_t *) ((char *) mp->data + page_offset);
+	dirtab_slot->flag = DIR_INDEX_VALID;
+	dirtab_slot->slot = slot;
+	DTSaddress(dirtab_slot, bn);
+
+	mark_metapage_dirty(mp);
+	release_metapage(mp);
+
+	return index;
+}
+
+/*
+ *	free_index()
+ *
+ *	Marks an entry to the directory index table as free.
+ */
+static void free_index(tid_t tid, struct inode *ip, u32 index, u32 next)
+{
+	dir_table_slot_t *dirtab_slot;
+	metapage_t *mp = 0;
+
+	dirtab_slot = find_index(ip, index, &mp);
+
+	if (dirtab_slot == 0)
+		return;
+
+	dirtab_slot->flag = DIR_INDEX_FREE;
+	dirtab_slot->slot = dirtab_slot->addr1 = 0;
+	dirtab_slot->addr2 = cpu_to_le32(next);
+
+	if (mp) {
+		lock_index(tid, ip, mp, index);
+		mark_metapage_dirty(mp);
+		release_metapage(mp);
+	} else
+		set_cflag(COMMIT_Dirtable, ip);
+}
+
+/*
+ *	modify_index()
+ *
+ *	Changes an entry in the directory index table
+ */
+static void modify_index(tid_t tid, struct inode *ip, u32 index, s64 bn,
+			 int slot, metapage_t ** mp)
+{
+	dir_table_slot_t *dirtab_slot;
+
+	dirtab_slot = find_index(ip, index, mp);
+
+	if (dirtab_slot == 0)
+		return;
+
+	DTSaddress(dirtab_slot, bn);
+	dirtab_slot->slot = slot;
+
+	if (*mp) {
+		lock_index(tid, ip, *mp, index);
+		mark_metapage_dirty(*mp);
+	} else
+		set_cflag(COMMIT_Dirtable, ip);
+}
+
+/*
+ *	get_index()
+ *
+ *	reads a directory table slot
+ */
+static int get_index(struct inode *ip, u32 index,
+		     dir_table_slot_t * dirtab_slot)
+{
+	metapage_t *mp = 0;
+	dir_table_slot_t *slot;
+
+	slot = find_index(ip, index, &mp);
+	if (slot == 0) {
+		return -EIO;
+	}
+
+	memcpy(dirtab_slot, slot, sizeof(dir_table_slot_t));
+
+	if (mp)
+		release_metapage(mp);
+
+	return 0;
+}
+
+/*
+ *	dtSearch()
+ *
+ * function:
+ *	Search for the entry with specified key
+ *
+ * parameter:
+ *
+ * return: 0 - search result on stack, leaf page pinned;
+ *	   errno - I/O error
+ */
+int dtSearch(struct inode *ip,
+	 component_t * key, ino_t * data, btstack_t * btstack, int flag)
+{
+	int rc = 0;
+	int cmp = 1;		/* init for empty page */
+	s64 bn;
+	metapage_t *mp;
+	dtpage_t *p;
+	s8 *stbl;
+	int base, index, lim;
+	btframe_t *btsp;
+	pxd_t *pxd;
+	int psize = 288;	/* initial in-line directory */
+	ino_t inumber;
+	component_t ciKey;
+	struct super_block *sb = ip->i_sb;
+
+	ciKey.name =
+	    (wchar_t *) kmalloc((JFS_NAME_MAX + 1) * sizeof(wchar_t),
+				GFP_NOFS);
+	if (ciKey.name == 0) {
+		rc = ENOMEM;
+		goto dtSearch_Exit2;
+	}
+
+
+	/* uppercase search key for c-i directory */
+	UniStrcpy(ciKey.name, key->name);
+	ciKey.namlen = key->namlen;
+
+	/* only uppercase if case-insensitive support is on */
+	if ((JFS_SBI(sb)->mntflag & JFS_OS2) == JFS_OS2) {
+		ciToUpper(&ciKey);
+	}
+	BT_CLR(btstack);	/* reset stack */
+
+	/* init level count for max pages to split */
+	btstack->nsplit = 1;
+
+	/*
+	 *      search down tree from root:
+	 *
+	 * between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
+	 * internal page, child page Pi contains entry with k, Ki <= K < Kj.
+	 *
+	 * if entry with search key K is not found
+	 * internal page search find the entry with largest key Ki
+	 * less than K which point to the child page to search;
+	 * leaf page search find the entry with smallest key Kj
+	 * greater than K so that the returned index is the position of
+	 * the entry to be shifted right for insertion of new entry.
+	 * for empty tree, search key is greater than any key of the tree.
+	 *
+	 * by convention, root bn = 0.
+	 */
+	for (bn = 0;;) {
+		/* get/pin the page to search */
+		DT_GETPAGE(ip, bn, mp, psize, p, rc);
+		if (rc)
+			goto dtSearch_Exit1;
+
+		/* get sorted entry table of the page */
+		stbl = DT_GETSTBL(p);
+
+		/*
+		 * binary search with search key K on the current page.
+		 */
+		for (base = 0, lim = p->header.nextindex; lim; lim >>= 1) {
+			index = base + (lim >> 1);
+
+			if (p->header.flag & BT_LEAF) {
+				/* uppercase leaf name to compare */
+				cmp =
+				    ciCompare(&ciKey, p, stbl[index],
+					      JFS_SBI(sb)->mntflag);
+			} else {
+				/* router key is in uppercase */
+
+				cmp = dtCompare(&ciKey, p, stbl[index]);
+
+
+			}
+			if (cmp == 0) {
+				/*
+				 *      search hit
+				 */
+				/* search hit - leaf page:
+				 * return the entry found
+				 */
+				if (p->header.flag & BT_LEAF) {
+					inumber = le32_to_cpu(
+			((ldtentry_t *) & p->slot[stbl[index]])->inumber);
+
+					/*
+					 * search for JFS_LOOKUP
+					 */
+					if (flag == JFS_LOOKUP) {
+						*data = inumber;
+						rc = 0;
+						goto out;
+					}
+
+					/*
+					 * search for JFS_CREATE
+					 */
+					if (flag == JFS_CREATE) {
+						*data = inumber;
+						rc = EEXIST;
+						goto out;
+					}
+
+					/*
+					 * search for JFS_REMOVE or JFS_RENAME
+					 */
+					if ((flag == JFS_REMOVE ||
+					     flag == JFS_RENAME) &&
+					    *data != inumber) {
+						rc = ESTALE;
+						goto out;
+					}
+
+					/*
+					 * JFS_REMOVE|JFS_FINDDIR|JFS_RENAME
+					 */
+					/* save search result */
+					*data = inumber;
+					btsp = btstack->top;
+					btsp->bn = bn;
+					btsp->index = index;
+					btsp->mp = mp;
+
+					rc = 0;
+					goto dtSearch_Exit1;
+				}
+
+				/* search hit - internal page:
+				 * descend/search its child page
+				 */
+				goto getChild;
+			}
+
+			if (cmp > 0) {
+				base = index + 1;
+				--lim;
+			}
+		}
+
+		/*
+		 *      search miss
+		 *
+		 * base is the smallest index with key (Kj) greater than
+		 * search key (K) and may be zero or (maxindex + 1) index.
+		 */
+		/*
+		 * search miss - leaf page
+		 *
+		 * return location of entry (base) where new entry with
+		 * search key K is to be inserted.
+		 */
+		if (p->header.flag & BT_LEAF) {
+			/*
+			 * search for JFS_LOOKUP, JFS_REMOVE, or JFS_RENAME
+			 */
+			if (flag == JFS_LOOKUP || flag == JFS_REMOVE ||
+			    flag == JFS_RENAME) {
+				rc = ENOENT;
+				goto out;
+			}
+
+			/*
+			 * search for JFS_CREATE|JFS_FINDDIR:
+			 *
+			 * save search result
+			 */
+			*data = 0;
+			btsp = btstack->top;
+			btsp->bn = bn;
+			btsp->index = base;
+			btsp->mp = mp;
+
+			rc = 0;
+			goto dtSearch_Exit1;
+		}
+
+		/*
+		 * search miss - internal page
+		 *
+		 * if base is non-zero, decrement base by one to get the parent
+		 * entry of the child page to search.
+		 */
+		index = base ? base - 1 : base;
+
+		/*
+		 * go down to child page
+		 */
+	      getChild:
+		/* update max. number of pages to split */
+		if (btstack->nsplit >= 8) {
+			/* Something's corrupted, mark filesytem dirty so
+			 * chkdsk will fix it.
+			 */
+			jERROR(1, ("stack overrun in dtSearch!\n"));
+			updateSuper(sb, FM_DIRTY);
+			rc = EIO;
+			goto out;
+		}
+		btstack->nsplit++;
+
+		/* push (bn, index) of the parent page/entry */
+		BT_PUSH(btstack, bn, index);
+
+		/* get the child page block number */
+		pxd = (pxd_t *) & p->slot[stbl[index]];
+		bn = addressPXD(pxd);
+		psize = lengthPXD(pxd) << JFS_SBI(ip->i_sb)->l2bsize;
+
+		/* unpin the parent page */
+		DT_PUTPAGE(mp);
+	}
+
+      out:
+	DT_PUTPAGE(mp);
+
+      dtSearch_Exit1:
+
+	kfree(ciKey.name);
+
+      dtSearch_Exit2:
+
+	return rc;
+}
+
+
+/*
+ *	dtInsert()
+ *
+ * function: insert an entry to directory tree
+ *
+ * parameter:
+ *
+ * return: 0 - success;
+ *	   errno - failure;
+ */
+int dtInsert(tid_t tid, struct inode *ip,
+	 component_t * name, ino_t * fsn, btstack_t * btstack)
+{
+	int rc = 0;
+	metapage_t *mp;		/* meta-page buffer */
+	dtpage_t *p;		/* base B+-tree index page */
+	s64 bn;
+	int index;
+	dtsplit_t split;	/* split information */
+	ddata_t data;
+	dtlock_t *dtlck;
+	int n;
+	tlock_t *tlck;
+	lv_t *lv;
+
+	/*
+	 *      retrieve search result
+	 *
+	 * dtSearch() returns (leaf page pinned, index at which to insert).
+	 * n.b. dtSearch() may return index of (maxindex + 1) of
+	 * the full page.
+	 */
+	DT_GETSEARCH(ip, btstack->top, bn, mp, p, index);
+
+	/*
+	 *      insert entry for new key
+	 */
+	if (DO_INDEX(ip)) {
+		if (JFS_IP(ip)->next_index == DIREND) {
+			DT_PUTPAGE(mp);
+			return EMLINK;
+		}
+		n = NDTLEAF(name->namlen);
+		data.leaf.tid = tid;
+		data.leaf.ip = ip;
+	} else {
+		n = NDTLEAF_LEGACY(name->namlen);
+		data.leaf.ip = 0;	/* signifies legacy directory format */
+	}
+	data.leaf.ino = cpu_to_le32(*fsn);
+
+	/*
+	 *      leaf page does not have enough room for new entry:
+	 *
+	 *      extend/split the leaf page;
+	 *
+	 * dtSplitUp() will insert the entry and unpin the leaf page.
+	 */
+	if (n > p->header.freecnt) {
+		split.mp = mp;
+		split.index = index;
+		split.nslot = n;
+		split.key = name;
+		split.data = &data;
+		rc = dtSplitUp(tid, ip, &split, btstack);
+		return rc;
+	}
+
+	/*
+	 *      leaf page does have enough room for new entry:
+	 *
+	 *      insert the new data entry into the leaf page;
+	 */
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire a transaction lock on the leaf page
+	 */
+	tlck = txLock(tid, ip, mp, tlckDTREE | tlckENTRY);
+	dtlck = (dtlock_t *) & tlck->lock;
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+
+	/* linelock header */
+	lv->offset = 0;
+	lv->length = 1;
+	dtlck->index++;
+
+	dtInsertEntry(p, index, name, &data, &dtlck);
+
+	/* linelock stbl of non-root leaf page */
+	if (!(p->header.flag & BT_ROOT)) {
+		if (dtlck->index >= dtlck->maxcnt)
+			dtlck = (dtlock_t *) txLinelock(dtlck);
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		n = index >> L2DTSLOTSIZE;
+		lv->offset = p->header.stblindex + n;
+		lv->length =
+		    ((p->header.nextindex - 1) >> L2DTSLOTSIZE) - n + 1;
+		dtlck->index++;
+	}
+
+	/* unpin the leaf page */
+	DT_PUTPAGE(mp);
+
+	return 0;
+}
+
+
+/*
+ *	dtSplitUp()
+ *
+ * function: propagate insertion bottom up;
+ *
+ * parameter:
+ *
+ * return: 0 - success;
+ *	   errno - failure;
+ * 	leaf page unpinned;
+ */
+static int dtSplitUp(tid_t tid,
+	  struct inode *ip, dtsplit_t * split, btstack_t * btstack)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	int rc = 0;
+	metapage_t *smp;
+	dtpage_t *sp;		/* split page */
+	metapage_t *rmp;
+	dtpage_t *rp;		/* new right page split from sp */
+	pxd_t rpxd;		/* new right page extent descriptor */
+	metapage_t *lmp;
+	dtpage_t *lp;		/* left child page */
+	int skip;		/* index of entry of insertion */
+	btframe_t *parent;	/* parent page entry on traverse stack */
+	s64 xaddr, nxaddr;
+	int xlen, xsize;
+	pxdlist_t pxdlist;
+	pxd_t *pxd;
+	component_t key = { 0, 0 };
+	ddata_t *data = split->data;
+	int n;
+	dtlock_t *dtlck;
+	tlock_t *tlck;
+	lv_t *lv;
+
+	/* get split page */
+	smp = split->mp;
+	sp = DT_PAGE(ip, smp);
+
+	key.name =
+	    (wchar_t *) kmalloc((JFS_NAME_MAX + 2) * sizeof(wchar_t),
+				GFP_NOFS);
+	if (key.name == 0) {
+		DT_PUTPAGE(smp);
+		rc = ENOMEM;
+		goto dtSplitUp_Exit;
+	}
+
+	/*
+	 *      split leaf page
+	 *
+	 * The split routines insert the new entry, and
+	 * acquire txLock as appropriate.
+	 */
+	/*
+	 *      split root leaf page:
+	 */
+	if (sp->header.flag & BT_ROOT) {
+		/*
+		 * allocate a single extent child page
+		 */
+		xlen = 1;
+		n = sbi->bsize >> L2DTSLOTSIZE;
+		n -= (n + 31) >> L2DTSLOTSIZE;	/* stbl size */
+		n -= DTROOTMAXSLOT - sp->header.freecnt; /* header + entries */
+		if (n <= split->nslot)
+			xlen++;
+		if ((rc = dbAlloc(ip, 0, (s64) xlen, &xaddr)))
+			goto freeKeyName;
+
+		pxdlist.maxnpxd = 1;
+		pxdlist.npxd = 0;
+		pxd = &pxdlist.pxd[0];
+		PXDaddress(pxd, xaddr);
+		PXDlength(pxd, xlen);
+		split->pxdlist = &pxdlist;
+		rc = dtSplitRoot(tid, ip, split, &rmp);
+
+		DT_PUTPAGE(rmp);
+		DT_PUTPAGE(smp);
+
+		goto freeKeyName;
+	}
+
+	/*
+	 *      extend first leaf page
+	 *
+	 * extend the 1st extent if less than buffer page size
+	 * (dtExtendPage() reurns leaf page unpinned)
+	 */
+	pxd = &sp->header.self;
+	xlen = lengthPXD(pxd);
+	xsize = xlen << sbi->l2bsize;
+	if (xsize < PSIZE) {
+		xaddr = addressPXD(pxd);
+		n = xsize >> L2DTSLOTSIZE;
+		n -= (n + 31) >> L2DTSLOTSIZE;	/* stbl size */
+		if ((n + sp->header.freecnt) <= split->nslot)
+			n = xlen + (xlen << 1);
+		else
+			n = xlen;
+		if ((rc = dbReAlloc(sbi->ipbmap, xaddr, (s64) xlen,
+				    (s64) n, &nxaddr)))
+			goto extendOut;
+
+		pxdlist.maxnpxd = 1;
+		pxdlist.npxd = 0;
+		pxd = &pxdlist.pxd[0];
+		PXDaddress(pxd, nxaddr)
+		    PXDlength(pxd, xlen + n);
+		split->pxdlist = &pxdlist;
+		if ((rc = dtExtendPage(tid, ip, split, btstack))) {
+			nxaddr = addressPXD(pxd);
+			if (xaddr != nxaddr) {
+				/* free relocated extent */
+				xlen = lengthPXD(pxd);
+				dbFree(ip, nxaddr, (s64) xlen);
+			} else {
+				/* free extended delta */
+				xlen = lengthPXD(pxd) - n;
+				xaddr = addressPXD(pxd) + xlen;
+				dbFree(ip, xaddr, (s64) n);
+			}
+		}
+
+	      extendOut:
+		DT_PUTPAGE(smp);
+		goto freeKeyName;
+	}
+
+	/*
+	 *      split leaf page <sp> into <sp> and a new right page <rp>.
+	 *
+	 * return <rp> pinned and its extent descriptor <rpxd>
+	 */
+	/*
+	 * allocate new directory page extent and
+	 * new index page(s) to cover page split(s)
+	 *
+	 * allocation hint: ?
+	 */
+	n = btstack->nsplit;
+	pxdlist.maxnpxd = pxdlist.npxd = 0;
+	xlen = sbi->nbperpage;
+	for (pxd = pxdlist.pxd; n > 0; n--, pxd++) {
+		if ((rc = dbAlloc(ip, 0, (s64) xlen, &xaddr)) == 0) {
+			PXDaddress(pxd, xaddr);
+			PXDlength(pxd, xlen);
+			pxdlist.maxnpxd++;
+			continue;
+		}
+
+		DT_PUTPAGE(smp);
+
+		/* undo allocation */
+		goto splitOut;
+	}
+
+	split->pxdlist = &pxdlist;
+	if ((rc = dtSplitPage(tid, ip, split, &rmp, &rp, &rpxd))) {
+		DT_PUTPAGE(smp);
+
+		/* undo allocation */
+		goto splitOut;
+	}
+
+	/*
+	 * propagate up the router entry for the leaf page just split
+	 *
+	 * insert a router entry for the new page into the parent page,
+	 * propagate the insert/split up the tree by walking back the stack
+	 * of (bn of parent page, index of child page entry in parent page)
+	 * that were traversed during the search for the page that split.
+	 *
+	 * the propagation of insert/split up the tree stops if the root
+	 * splits or the page inserted into doesn't have to split to hold
+	 * the new entry.
+	 *
+	 * the parent entry for the split page remains the same, and
+	 * a new entry is inserted at its right with the first key and
+	 * block number of the new right page.
+	 *
+	 * There are a maximum of 4 pages pinned at any time:
+	 * two children, left parent and right parent (when the parent splits).
+	 * keep the child pages pinned while working on the parent.
+	 * make sure that all pins are released at exit.
+	 */
+	while ((parent = BT_POP(btstack)) != NULL) {
+		/* parent page specified by stack frame <parent> */
+
+		/* keep current child pages (<lp>, <rp>) pinned */
+		lmp = smp;
+		lp = sp;
+
+		/*
+		 * insert router entry in parent for new right child page <rp>
+		 */
+		/* get the parent page <sp> */
+		DT_GETPAGE(ip, parent->bn, smp, PSIZE, sp, rc);
+		if (rc) {
+			DT_PUTPAGE(lmp);
+			DT_PUTPAGE(rmp);
+			goto splitOut;
+		}
+
+		/*
+		 * The new key entry goes ONE AFTER the index of parent entry,
+		 * because the split was to the right.
+		 */
+		skip = parent->index + 1;
+
+		/*
+		 * compute the key for the router entry
+		 *
+		 * key suffix compression:
+		 * for internal pages that have leaf pages as children,
+		 * retain only what's needed to distinguish between
+		 * the new entry and the entry on the page to its left.
+		 * If the keys compare equal, retain the entire key.
+		 *
+		 * note that compression is performed only at computing
+		 * router key at the lowest internal level.
+		 * further compression of the key between pairs of higher
+		 * level internal pages loses too much information and
+		 * the search may fail.
+		 * (e.g., two adjacent leaf pages of {a, ..., x} {xx, ...,}
+		 * results in two adjacent parent entries (a)(xx).
+		 * if split occurs between these two entries, and
+		 * if compression is applied, the router key of parent entry
+		 * of right page (x) will divert search for x into right
+		 * subtree and miss x in the left subtree.)
+		 *
+		 * the entire key must be retained for the next-to-leftmost
+		 * internal key at any level of the tree, or search may fail
+		 * (e.g., ?)
+		 */
+		switch (rp->header.flag & BT_TYPE) {
+		case BT_LEAF:
+			/*
+			 * compute the length of prefix for suffix compression
+			 * between last entry of left page and first entry
+			 * of right page
+			 */
+			if ((sp->header.flag & BT_ROOT && skip > 1) ||
+			    sp->header.prev != 0 || skip > 1) {
+				/* compute uppercase router prefix key */
+				ciGetLeafPrefixKey(lp,
+						   lp->header.nextindex - 1,
+						   rp, 0, &key, sbi->mntflag);
+			} else {
+				/* next to leftmost entry of
+				   lowest internal level */
+
+				/* compute uppercase router key */
+				dtGetKey(rp, 0, &key, sbi->mntflag);
+				key.name[key.namlen] = 0;
+
+				if ((sbi->mntflag & JFS_OS2) == JFS_OS2)
+					ciToUpper(&key);
+			}
+
+			n = NDTINTERNAL(key.namlen);
+			break;
+
+		case BT_INTERNAL:
+			dtGetKey(rp, 0, &key, sbi->mntflag);
+			n = NDTINTERNAL(key.namlen);
+			break;
+
+		default:
+			jERROR(2, ("dtSplitUp(): UFO!\n"));
+			break;
+		}
+
+		/* unpin left child page */
+		DT_PUTPAGE(lmp);
+
+		/*
+		 * compute the data for the router entry
+		 */
+		data->xd = rpxd;	/* child page xd */
+
+		/*
+		 * parent page is full - split the parent page
+		 */
+		if (n > sp->header.freecnt) {
+			/* init for parent page split */
+			split->mp = smp;
+			split->index = skip;	/* index at insert */
+			split->nslot = n;
+			split->key = &key;
+			/* split->data = data; */
+
+			/* unpin right child page */
+			DT_PUTPAGE(rmp);
+
+			/* The split routines insert the new entry,
+			 * acquire txLock as appropriate.
+			 * return <rp> pinned and its block number <rbn>.
+			 */
+			rc = (sp->header.flag & BT_ROOT) ?
+			    dtSplitRoot(tid, ip, split, &rmp) :
+			    dtSplitPage(tid, ip, split, &rmp, &rp, &rpxd);
+			if (rc) {
+				DT_PUTPAGE(smp);
+				goto splitOut;
+			}
+
+			/* smp and rmp are pinned */
+		}
+		/*
+		 * parent page is not full - insert router entry in parent page
+		 */
+		else {
+			BT_MARK_DIRTY(smp, ip);
+			/*
+			 * acquire a transaction lock on the parent page
+			 */
+			tlck = txLock(tid, ip, smp, tlckDTREE | tlckENTRY);
+			dtlck = (dtlock_t *) & tlck->lock;
+			ASSERT(dtlck->index == 0);
+			lv = (lv_t *) & dtlck->lv[0];
+
+			/* linelock header */
+			lv->offset = 0;
+			lv->length = 1;
+			dtlck->index++;
+
+			/* linelock stbl of non-root parent page */
+			if (!(sp->header.flag & BT_ROOT)) {
+				lv++;
+				n = skip >> L2DTSLOTSIZE;
+				lv->offset = sp->header.stblindex + n;
+				lv->length =
+				    ((sp->header.nextindex -
+				      1) >> L2DTSLOTSIZE) - n + 1;
+				dtlck->index++;
+			}
+
+			dtInsertEntry(sp, skip, &key, data, &dtlck);
+
+			/* exit propagate up */
+			break;
+		}
+	}
+
+	/* unpin current split and its right page */
+	DT_PUTPAGE(smp);
+	DT_PUTPAGE(rmp);
+
+	/*
+	 * free remaining extents allocated for split
+	 */
+      splitOut:
+	n = pxdlist.npxd;
+	pxd = &pxdlist.pxd[n];
+	for (; n < pxdlist.maxnpxd; n++, pxd++)
+		dbFree(ip, addressPXD(pxd), (s64) lengthPXD(pxd));
+
+      freeKeyName:
+	kfree(key.name);
+
+      dtSplitUp_Exit:
+
+	return rc;
+}
+
+
+/*
+ *	dtSplitPage()
+ *
+ * function: Split a non-root page of a btree.
+ *
+ * parameter:
+ *
+ * return: 0 - success;
+ *	   errno - failure;
+ *	return split and new page pinned;
+ */
+static int dtSplitPage(tid_t tid, struct inode *ip, dtsplit_t * split,
+	    metapage_t ** rmpp, dtpage_t ** rpp, pxd_t * rpxdp)
+{
+	struct super_block *sb = ip->i_sb;
+	int rc = 0;
+	metapage_t *smp;
+	dtpage_t *sp;
+	metapage_t *rmp;
+	dtpage_t *rp;		/* new right page allocated */
+	s64 rbn;		/* new right page block number */
+	metapage_t *mp;
+	dtpage_t *p;
+	s64 nextbn;
+	pxdlist_t *pxdlist;
+	pxd_t *pxd;
+	int skip, nextindex, half, left, nxt, off, si;
+	ldtentry_t *ldtentry;
+	idtentry_t *idtentry;
+	u8 *stbl;
+	dtslot_t *f;
+	int fsi, stblsize;
+	int n;
+	dtlock_t *sdtlck, *rdtlck;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	lv_t *slv, *rlv, *lv;
+
+	/* get split page */
+	smp = split->mp;
+	sp = DT_PAGE(ip, smp);
+
+	/*
+	 * allocate the new right page for the split
+	 */
+	pxdlist = split->pxdlist;
+	pxd = &pxdlist->pxd[pxdlist->npxd];
+	pxdlist->npxd++;
+	rbn = addressPXD(pxd);
+	rmp = get_metapage(ip, rbn, PSIZE, 1);
+	if (rmp == NULL)
+		return EIO;
+
+	jEVENT(0,
+	       ("dtSplitPage: ip:0x%p smp:0x%p rmp:0x%p\n", ip, smp, rmp));
+
+	BT_MARK_DIRTY(rmp, ip);
+	/*
+	 * acquire a transaction lock on the new right page
+	 */
+	tlck = txLock(tid, ip, rmp, tlckDTREE | tlckNEW);
+	rdtlck = (dtlock_t *) & tlck->lock;
+
+	rp = (dtpage_t *) rmp->data;
+	*rpp = rp;
+	rp->header.self = *pxd;
+
+	BT_MARK_DIRTY(smp, ip);
+	/*
+	 * acquire a transaction lock on the split page
+	 *
+	 * action:
+	 */
+	tlck = txLock(tid, ip, smp, tlckDTREE | tlckENTRY);
+	sdtlck = (dtlock_t *) & tlck->lock;
+
+	/* linelock header of split page */
+	ASSERT(sdtlck->index == 0);
+	slv = (lv_t *) & sdtlck->lv[0];
+	slv->offset = 0;
+	slv->length = 1;
+	sdtlck->index++;
+
+	/*
+	 * initialize/update sibling pointers between sp and rp
+	 */
+	nextbn = le64_to_cpu(sp->header.next);
+	rp->header.next = cpu_to_le64(nextbn);
+	rp->header.prev = cpu_to_le64(addressPXD(&sp->header.self));
+	sp->header.next = cpu_to_le64(rbn);
+
+	/*
+	 * initialize new right page
+	 */
+	rp->header.flag = sp->header.flag;
+
+	/* compute sorted entry table at start of extent data area */
+	rp->header.nextindex = 0;
+	rp->header.stblindex = 1;
+
+	n = PSIZE >> L2DTSLOTSIZE;
+	rp->header.maxslot = n;
+	stblsize = (n + 31) >> L2DTSLOTSIZE;	/* in unit of slot */
+
+	/* init freelist */
+	fsi = rp->header.stblindex + stblsize;
+	rp->header.freelist = fsi;
+	rp->header.freecnt = rp->header.maxslot - fsi;
+
+	/*
+	 *      sequential append at tail: append without split
+	 *
+	 * If splitting the last page on a level because of appending
+	 * a entry to it (skip is maxentry), it's likely that the access is
+	 * sequential. Adding an empty page on the side of the level is less
+	 * work and can push the fill factor much higher than normal.
+	 * If we're wrong it's no big deal, we'll just do the split the right
+	 * way next time.
+	 * (It may look like it's equally easy to do a similar hack for
+	 * reverse sorted data, that is, split the tree left,
+	 * but it's not. Be my guest.)
+	 */
+	if (nextbn == 0 && split->index == sp->header.nextindex) {
+		/* linelock header + stbl (first slot) of new page */
+		rlv = (lv_t *) & rdtlck->lv[rdtlck->index];
+		rlv->offset = 0;
+		rlv->length = 2;
+		rdtlck->index++;
+
+		/*
+		 * initialize freelist of new right page
+		 */
+		f = &rp->slot[fsi];
+		for (fsi++; fsi < rp->header.maxslot; f++, fsi++)
+			f->next = fsi;
+		f->next = -1;
+
+		/* insert entry at the first entry of the new right page */
+		dtInsertEntry(rp, 0, split->key, split->data, &rdtlck);
+
+		goto out;
+	}
+
+	/*
+	 *      non-sequential insert (at possibly middle page)
+	 */
+
+	/*
+	 * update prev pointer of previous right sibling page;
+	 */
+	if (nextbn != 0) {
+		DT_GETPAGE(ip, nextbn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the next page
+		 */
+		tlck = txLock(tid, ip, mp, tlckDTREE | tlckRELINK);
+		jEVENT(0,
+		       ("dtSplitPage: tlck = 0x%p, ip = 0x%p, mp=0x%p\n",
+			tlck, ip, mp));
+		dtlck = (dtlock_t *) & tlck->lock;
+
+		/* linelock header of previous right sibling page */
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		p->header.prev = cpu_to_le64(rbn);
+
+		DT_PUTPAGE(mp);
+	}
+
+	/*
+	 * split the data between the split and right pages.
+	 */
+	skip = split->index;
+	half = (PSIZE >> L2DTSLOTSIZE) >> 1;	/* swag */
+	left = 0;
+
+	/*
+	 *      compute fill factor for split pages
+	 *
+	 * <nxt> traces the next entry to move to rp
+	 * <off> traces the next entry to stay in sp
+	 */
+	stbl = (u8 *) & sp->slot[sp->header.stblindex];
+	nextindex = sp->header.nextindex;
+	for (nxt = off = 0; nxt < nextindex; ++off) {
+		if (off == skip)
+			/* check for fill factor with new entry size */
+			n = split->nslot;
+		else {
+			si = stbl[nxt];
+			switch (sp->header.flag & BT_TYPE) {
+			case BT_LEAF:
+				ldtentry = (ldtentry_t *) & sp->slot[si];
+				if (DO_INDEX(ip))
+					n = NDTLEAF(ldtentry->namlen);
+				else
+					n = NDTLEAF_LEGACY(ldtentry->
+							   namlen);
+				break;
+
+			case BT_INTERNAL:
+				idtentry = (idtentry_t *) & sp->slot[si];
+				n = NDTINTERNAL(idtentry->namlen);
+				break;
+
+			default:
+				break;
+			}
+
+			++nxt;	/* advance to next entry to move in sp */
+		}
+
+		left += n;
+		if (left >= half)
+			break;
+	}
+
+	/* <nxt> poins to the 1st entry to move */
+
+	/*
+	 *      move entries to right page
+	 *
+	 * dtMoveEntry() initializes rp and reserves entry for insertion
+	 *
+	 * split page moved out entries are linelocked;
+	 * new/right page moved in entries are linelocked;
+	 */
+	/* linelock header + stbl of new right page */
+	rlv = (lv_t *) & rdtlck->lv[rdtlck->index];
+	rlv->offset = 0;
+	rlv->length = 5;
+	rdtlck->index++;
+
+	dtMoveEntry(sp, nxt, rp, &sdtlck, &rdtlck, DO_INDEX(ip));
+
+	sp->header.nextindex = nxt;
+
+	/*
+	 * finalize freelist of new right page
+	 */
+	fsi = rp->header.freelist;
+	f = &rp->slot[fsi];
+	for (fsi++; fsi < rp->header.maxslot; f++, fsi++)
+		f->next = fsi;
+	f->next = -1;
+
+	/*
+	 * Update directory index table for entries now in right page
+	 */
+	if ((rp->header.flag & BT_LEAF) && DO_INDEX(ip)) {
+		mp = 0;
+		stbl = DT_GETSTBL(rp);
+		for (n = 0; n < rp->header.nextindex; n++) {
+			ldtentry = (ldtentry_t *) & rp->slot[stbl[n]];
+			modify_index(tid, ip, le32_to_cpu(ldtentry->index),
+				     rbn, n, &mp);
+		}
+		if (mp)
+			release_metapage(mp);
+	}
+
+	/*
+	 * the skipped index was on the left page,
+	 */
+	if (skip <= off) {
+		/* insert the new entry in the split page */
+		dtInsertEntry(sp, skip, split->key, split->data, &sdtlck);
+
+		/* linelock stbl of split page */
+		if (sdtlck->index >= sdtlck->maxcnt)
+			sdtlck = (dtlock_t *) txLinelock(sdtlck);
+		slv = (lv_t *) & sdtlck->lv[sdtlck->index];
+		n = skip >> L2DTSLOTSIZE;
+		slv->offset = sp->header.stblindex + n;
+		slv->length =
+		    ((sp->header.nextindex - 1) >> L2DTSLOTSIZE) - n + 1;
+		sdtlck->index++;
+	}
+	/*
+	 * the skipped index was on the right page,
+	 */
+	else {
+		/* adjust the skip index to reflect the new position */
+		skip -= nxt;
+
+		/* insert the new entry in the right page */
+		dtInsertEntry(rp, skip, split->key, split->data, &rdtlck);
+	}
+
+      out:
+	*rmpp = rmp;
+	*rpxdp = *pxd;
+
+	ip->i_blocks += LBLK2PBLK(sb, lengthPXD(pxd));
+
+	jEVENT(0, ("dtSplitPage: ip:0x%p sp:0x%p rp:0x%p\n", ip, sp, rp));
+	return 0;
+}
+
+
+/*
+ *	dtExtendPage()
+ *
+ * function: extend 1st/only directory leaf page
+ *
+ * parameter:
+ *
+ * return: 0 - success;
+ *	   errno - failure;
+ *	return extended page pinned;
+ */
+static int dtExtendPage(tid_t tid,
+	     struct inode *ip, dtsplit_t * split, btstack_t * btstack)
+{
+	struct super_block *sb = ip->i_sb;
+	int rc;
+	metapage_t *smp, *pmp, *mp;
+	dtpage_t *sp, *pp;
+	pxdlist_t *pxdlist;
+	pxd_t *pxd, *tpxd;
+	int xlen, xsize;
+	int newstblindex, newstblsize;
+	int oldstblindex, oldstblsize;
+	int fsi, last;
+	dtslot_t *f;
+	btframe_t *parent;
+	int n;
+	dtlock_t *dtlck;
+	s64 xaddr, txaddr;
+	tlock_t *tlck;
+	pxdlock_t *pxdlock;
+	lv_t *lv;
+	uint type;
+	ldtentry_t *ldtentry;
+	u8 *stbl;
+
+	/* get page to extend */
+	smp = split->mp;
+	sp = DT_PAGE(ip, smp);
+
+	/* get parent/root page */
+	parent = BT_POP(btstack);
+	DT_GETPAGE(ip, parent->bn, pmp, PSIZE, pp, rc);
+	if (rc)
+		return (rc);
+
+	/*
+	 *      extend the extent
+	 */
+	pxdlist = split->pxdlist;
+	pxd = &pxdlist->pxd[pxdlist->npxd];
+	pxdlist->npxd++;
+
+	xaddr = addressPXD(pxd);
+	tpxd = &sp->header.self;
+	txaddr = addressPXD(tpxd);
+	/* in-place extension */
+	if (xaddr == txaddr) {
+		type = tlckEXTEND;
+	}
+	/* relocation */
+	else {
+		type = tlckNEW;
+
+		/* save moved extent descriptor for later free */
+		tlck = txMaplock(tid, ip, tlckDTREE | tlckRELOCATE);
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		pxdlock->flag = mlckFREEPXD;
+		pxdlock->pxd = sp->header.self;
+		pxdlock->index = 1;
+
+		/*
+		 * Update directory index table to reflect new page address
+		 */
+		if (DO_INDEX(ip)) {
+			mp = 0;
+			stbl = DT_GETSTBL(sp);
+			for (n = 0; n < sp->header.nextindex; n++) {
+				ldtentry =
+				    (ldtentry_t *) & sp->slot[stbl[n]];
+				modify_index(tid, ip,
+					     le32_to_cpu(ldtentry->index),
+					     xaddr, n, &mp);
+			}
+			if (mp)
+				release_metapage(mp);
+		}
+	}
+
+	/*
+	 *      extend the page
+	 */
+	sp->header.self = *pxd;
+
+	jEVENT(0,
+	       ("dtExtendPage: ip:0x%p smp:0x%p sp:0x%p\n", ip, smp, sp));
+
+	BT_MARK_DIRTY(smp, ip);
+	/*
+	 * acquire a transaction lock on the extended/leaf page
+	 */
+	tlck = txLock(tid, ip, smp, tlckDTREE | type);
+	dtlck = (dtlock_t *) & tlck->lock;
+	lv = (lv_t *) & dtlck->lv[0];
+
+	/* update buffer extent descriptor of extended page */
+	xlen = lengthPXD(pxd);
+	xsize = xlen << JFS_SBI(sb)->l2bsize;
+#ifdef _STILL_TO_PORT
+	bmSetXD(smp, xaddr, xsize);
+#endif				/*  _STILL_TO_PORT */
+
+	/*
+	 * copy old stbl to new stbl at start of extended area
+	 */
+	oldstblindex = sp->header.stblindex;
+	oldstblsize = (sp->header.maxslot + 31) >> L2DTSLOTSIZE;
+	newstblindex = sp->header.maxslot;
+	n = xsize >> L2DTSLOTSIZE;
+	newstblsize = (n + 31) >> L2DTSLOTSIZE;
+	memcpy(&sp->slot[newstblindex], &sp->slot[oldstblindex],
+	       sp->header.nextindex);
+
+	/*
+	 * in-line extension: linelock old area of extended page
+	 */
+	if (type == tlckEXTEND) {
+		/* linelock header */
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+		lv++;
+
+		/* linelock new stbl of extended page */
+		lv->offset = newstblindex;
+		lv->length = newstblsize;
+	}
+	/*
+	 * relocation: linelock whole relocated area
+	 */
+	else {
+		lv->offset = 0;
+		lv->length = sp->header.maxslot + newstblsize;
+	}
+
+	dtlck->index++;
+
+	sp->header.maxslot = n;
+	sp->header.stblindex = newstblindex;
+	/* sp->header.nextindex remains the same */
+
+	/*
+	 * add old stbl region at head of freelist
+	 */
+	fsi = oldstblindex;
+	f = &sp->slot[fsi];
+	last = sp->header.freelist;
+	for (n = 0; n < oldstblsize; n++, fsi++, f++) {
+		f->next = last;
+		last = fsi;
+	}
+	sp->header.freelist = last;
+	sp->header.freecnt += oldstblsize;
+
+	/*
+	 * append free region of newly extended area at tail of freelist
+	 */
+	/* init free region of newly extended area */
+	fsi = n = newstblindex + newstblsize;
+	f = &sp->slot[fsi];
+	for (fsi++; fsi < sp->header.maxslot; f++, fsi++)
+		f->next = fsi;
+	f->next = -1;
+
+	/* append new free region at tail of old freelist */
+	fsi = sp->header.freelist;
+	if (fsi == -1)
+		sp->header.freelist = n;
+	else {
+		do {
+			f = &sp->slot[fsi];
+			fsi = f->next;
+		} while (fsi != -1);
+
+		f->next = n;
+	}
+
+	sp->header.freecnt += sp->header.maxslot - n;
+
+	/*
+	 * insert the new entry
+	 */
+	dtInsertEntry(sp, split->index, split->key, split->data, &dtlck);
+
+	BT_MARK_DIRTY(pmp, ip);
+	/*
+	 * linelock any freeslots residing in old extent
+	 */
+	if (type == tlckEXTEND) {
+		n = sp->header.maxslot >> 2;
+		if (sp->header.freelist < n)
+			dtLinelockFreelist(sp, n, &dtlck);
+	}
+
+	/*
+	 *      update parent entry on the parent/root page
+	 */
+	/*
+	 * acquire a transaction lock on the parent/root page
+	 */
+	tlck = txLock(tid, ip, pmp, tlckDTREE | tlckENTRY);
+	dtlck = (dtlock_t *) & tlck->lock;
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+
+	/* linelock parent entry - 1st slot */
+	lv->offset = 1;
+	lv->length = 1;
+	dtlck->index++;
+
+	/* update the parent pxd for page extension */
+	tpxd = (pxd_t *) & pp->slot[1];
+	*tpxd = *pxd;
+
+	/* Since the directory might have an EA and/or ACL associated with it
+	 * we need to make sure we take that into account when setting the
+	 * i_nblocks
+	 */
+	ip->i_blocks = LBLK2PBLK(ip->i_sb, xlen +
+				 ((JFS_IP(ip)->ea.flag & DXD_EXTENT) ?
+				  lengthDXD(&JFS_IP(ip)->ea) : 0) +
+				 ((JFS_IP(ip)->acl.flag & DXD_EXTENT) ?
+				  lengthDXD(&JFS_IP(ip)->acl) : 0));
+
+	jEVENT(0,
+	       ("dtExtendPage: ip:0x%p smp:0x%p sp:0x%p\n", ip, smp, sp));
+
+
+	DT_PUTPAGE(pmp);
+	return 0;
+}
+
+
+/*
+ *	dtSplitRoot()
+ *
+ * function:
+ *	split the full root page into
+ *	original/root/split page and new right page
+ *	i.e., root remains fixed in tree anchor (inode) and
+ *	the root is copied to a single new right child page
+ *	since root page << non-root page, and
+ *	the split root page contains a single entry for the
+ *	new right child page.
+ *
+ * parameter:
+ *
+ * return: 0 - success;
+ *	   errno - failure;
+ *	return new page pinned;
+ */
+static int dtSplitRoot(tid_t tid,
+	    struct inode *ip, dtsplit_t * split, metapage_t ** rmpp)
+{
+	struct super_block *sb = ip->i_sb;
+	metapage_t *smp;
+	dtroot_t *sp;
+	metapage_t *rmp;
+	dtpage_t *rp;
+	s64 rbn;
+	int xlen;
+	int xsize;
+	dtslot_t *f;
+	s8 *stbl;
+	int fsi, stblsize, n;
+	idtentry_t *s;
+	pxd_t *ppxd;
+	pxdlist_t *pxdlist;
+	pxd_t *pxd;
+	dtlock_t *dtlck;
+	tlock_t *tlck;
+	lv_t *lv;
+
+	/* get split root page */
+	smp = split->mp;
+	sp = &JFS_IP(ip)->i_dtroot;
+
+	/*
+	 *      allocate/initialize a single (right) child page
+	 *
+	 * N.B. at first split, a one (or two) block to fit new entry
+	 * is allocated; at subsequent split, a full page is allocated;
+	 */
+	pxdlist = split->pxdlist;
+	pxd = &pxdlist->pxd[pxdlist->npxd];
+	pxdlist->npxd++;
+	rbn = addressPXD(pxd);
+	xlen = lengthPXD(pxd);
+	xsize = xlen << JFS_SBI(sb)->l2bsize;
+	rmp = get_metapage(ip, rbn, xsize, 1);
+	rp = rmp->data;
+
+	BT_MARK_DIRTY(rmp, ip);
+	/*
+	 * acquire a transaction lock on the new right page
+	 */
+	tlck = txLock(tid, ip, rmp, tlckDTREE | tlckNEW);
+	dtlck = (dtlock_t *) & tlck->lock;
+
+	rp->header.flag =
+	    (sp->header.flag & BT_LEAF) ? BT_LEAF : BT_INTERNAL;
+	rp->header.self = *pxd;
+
+	/* initialize sibling pointers */
+	rp->header.next = 0;
+	rp->header.prev = 0;
+
+	/*
+	 *      move in-line root page into new right page extent
+	 */
+	/* linelock header + copied entries + new stbl (1st slot) in new page */
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+	lv->offset = 0;
+	lv->length = 10;	/* 1 + 8 + 1 */
+	dtlck->index++;
+
+	n = xsize >> L2DTSLOTSIZE;
+	rp->header.maxslot = n;
+	stblsize = (n + 31) >> L2DTSLOTSIZE;
+
+	/* copy old stbl to new stbl at start of extended area */
+	rp->header.stblindex = DTROOTMAXSLOT;
+	stbl = (s8 *) & rp->slot[DTROOTMAXSLOT];
+	memcpy(stbl, sp->header.stbl, sp->header.nextindex);
+	rp->header.nextindex = sp->header.nextindex;
+
+	/* copy old data area to start of new data area */
+	memcpy(&rp->slot[1], &sp->slot[1], IDATASIZE);
+
+	/*
+	 * append free region of newly extended area at tail of freelist
+	 */
+	/* init free region of newly extended area */
+	fsi = n = DTROOTMAXSLOT + stblsize;
+	f = &rp->slot[fsi];
+	for (fsi++; fsi < rp->header.maxslot; f++, fsi++)
+		f->next = fsi;
+	f->next = -1;
+
+	/* append new free region at tail of old freelist */
+	fsi = sp->header.freelist;
+	if (fsi == -1)
+		rp->header.freelist = n;
+	else {
+		rp->header.freelist = fsi;
+
+		do {
+			f = &rp->slot[fsi];
+			fsi = f->next;
+		} while (fsi != -1);
+
+		f->next = n;
+	}
+
+	rp->header.freecnt = sp->header.freecnt + rp->header.maxslot - n;
+
+	/*
+	 * Update directory index table for entries now in right page
+	 */
+	if ((rp->header.flag & BT_LEAF) && DO_INDEX(ip)) {
+		metapage_t *mp = 0;
+		ldtentry_t *ldtentry;
+
+		stbl = DT_GETSTBL(rp);
+		for (n = 0; n < rp->header.nextindex; n++) {
+			ldtentry = (ldtentry_t *) & rp->slot[stbl[n]];
+			modify_index(tid, ip, le32_to_cpu(ldtentry->index),
+				     rbn, n, &mp);
+		}
+		if (mp)
+			release_metapage(mp);
+	}
+	/*
+	 * insert the new entry into the new right/child page
+	 * (skip index in the new right page will not change)
+	 */
+	dtInsertEntry(rp, split->index, split->key, split->data, &dtlck);
+
+	/*
+	 *      reset parent/root page
+	 *
+	 * set the 1st entry offset to 0, which force the left-most key
+	 * at any level of the tree to be less than any search key.
+	 *
+	 * The btree comparison code guarantees that the left-most key on any
+	 * level of the tree is never used, so it doesn't need to be filled in.
+	 */
+	BT_MARK_DIRTY(smp, ip);
+	/*
+	 * acquire a transaction lock on the root page (in-memory inode)
+	 */
+	tlck = txLock(tid, ip, smp, tlckDTREE | tlckNEW | tlckBTROOT);
+	dtlck = (dtlock_t *) & tlck->lock;
+
+	/* linelock root */
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+	lv->offset = 0;
+	lv->length = DTROOTMAXSLOT;
+	dtlck->index++;
+
+	/* update page header of root */
+	if (sp->header.flag & BT_LEAF) {
+		sp->header.flag &= ~BT_LEAF;
+		sp->header.flag |= BT_INTERNAL;
+	}
+
+	/* init the first entry */
+	s = (idtentry_t *) & sp->slot[DTENTRYSTART];
+	ppxd = (pxd_t *) s;
+	*ppxd = *pxd;
+	s->next = -1;
+	s->namlen = 0;
+
+	stbl = sp->header.stbl;
+	stbl[0] = DTENTRYSTART;
+	sp->header.nextindex = 1;
+
+	/* init freelist */
+	fsi = DTENTRYSTART + 1;
+	f = &sp->slot[fsi];
+
+	/* init free region of remaining area */
+	for (fsi++; fsi < DTROOTMAXSLOT; f++, fsi++)
+		f->next = fsi;
+	f->next = -1;
+
+	sp->header.freelist = DTENTRYSTART + 1;
+	sp->header.freecnt = DTROOTMAXSLOT - (DTENTRYSTART + 1);
+
+	*rmpp = rmp;
+
+	ip->i_blocks += LBLK2PBLK(ip->i_sb, lengthPXD(pxd));
+	return 0;
+}
+
+
+/*
+ *	dtDelete()
+ *
+ * function: delete the entry(s) referenced by a key.
+ *
+ * parameter:
+ *
+ * return:
+ */
+int dtDelete(tid_t tid,
+	 struct inode *ip, component_t * key, ino_t * ino, int flag)
+{
+	int rc = 0;
+	s64 bn;
+	metapage_t *mp, *imp;
+	dtpage_t *p;
+	int index;
+	btstack_t btstack;
+	dtlock_t *dtlck;
+	tlock_t *tlck;
+	lv_t *lv;
+	int i;
+	ldtentry_t *ldtentry;
+	u8 *stbl;
+	u32 table_index, next_index;
+	metapage_t *nmp;
+	dtpage_t *np;
+
+	/*
+	 *      search for the entry to delete:
+	 *
+	 * dtSearch() returns (leaf page pinned, index at which to delete).
+	 */
+	if ((rc = dtSearch(ip, key, ino, &btstack, flag)))
+		return rc;
+
+	/* retrieve search result */
+	DT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/*
+	 * We need to find put the index of the next entry into the
+	 * directory index table in order to resume a readdir from this
+	 * entry.
+	 */
+	if (DO_INDEX(ip)) {
+		stbl = DT_GETSTBL(p);
+		ldtentry = (ldtentry_t *) & p->slot[stbl[index]];
+		table_index = le32_to_cpu(ldtentry->index);
+		if (index == (p->header.nextindex - 1)) {
+			/*
+			 * Last entry in this leaf page
+			 */
+			if ((p->header.flag & BT_ROOT)
+			    || (p->header.next == 0))
+				next_index = -1;
+			else {
+				/* Read next leaf page */
+				DT_GETPAGE(ip, le64_to_cpu(p->header.next),
+					   nmp, PSIZE, np, rc);
+				if (rc)
+					next_index = -1;
+				else {
+					stbl = DT_GETSTBL(np);
+					ldtentry =
+					    (ldtentry_t *) & np->
+					    slot[stbl[0]];
+					next_index =
+					    le32_to_cpu(ldtentry->index);
+					DT_PUTPAGE(nmp);
+				}
+			}
+		} else {
+			ldtentry =
+			    (ldtentry_t *) & p->slot[stbl[index + 1]];
+			next_index = le32_to_cpu(ldtentry->index);
+		}
+		free_index(tid, ip, table_index, next_index);
+	}
+	/*
+	 * the leaf page becomes empty, delete the page
+	 */
+	if (p->header.nextindex == 1) {
+		/* delete empty page */
+		rc = dtDeleteUp(tid, ip, mp, p, &btstack);
+	}
+	/*
+	 * the leaf page has other entries remaining:
+	 *
+	 * delete the entry from the leaf page.
+	 */
+	else {
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the leaf page
+		 */
+		tlck = txLock(tid, ip, mp, tlckDTREE | tlckENTRY);
+		dtlck = (dtlock_t *) & tlck->lock;
+
+		/*
+		 * Do not assume that dtlck->index will be zero.  During a
+		 * rename within a directory, this transaction may have
+		 * modified this page already when adding the new entry.
+		 */
+
+		/* linelock header */
+		if (dtlck->index >= dtlck->maxcnt)
+			dtlck = (dtlock_t *) txLinelock(dtlck);
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		/* linelock stbl of non-root leaf page */
+		if (!(p->header.flag & BT_ROOT)) {
+			if (dtlck->index >= dtlck->maxcnt)
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+			lv = (lv_t *) & dtlck->lv[dtlck->index];
+			i = index >> L2DTSLOTSIZE;
+			lv->offset = p->header.stblindex + i;
+			lv->length =
+			    ((p->header.nextindex - 1) >> L2DTSLOTSIZE) -
+			    i + 1;
+			dtlck->index++;
+		}
+
+		/* free the leaf entry */
+		dtDeleteEntry(p, index, &dtlck);
+
+		/*
+		 * Update directory index table for entries moved in stbl
+		 */
+		if (DO_INDEX(ip) && index < p->header.nextindex) {
+			imp = 0;
+			stbl = DT_GETSTBL(p);
+			for (i = index; i < p->header.nextindex; i++) {
+				ldtentry =
+				    (ldtentry_t *) & p->slot[stbl[i]];
+				modify_index(tid, ip,
+					     le32_to_cpu(ldtentry->index),
+					     bn, i, &imp);
+			}
+			if (imp)
+				release_metapage(imp);
+		}
+
+		DT_PUTPAGE(mp);
+	}
+
+	return rc;
+}
+
+
+/*
+ *	dtDeleteUp()
+ *
+ * function:
+ *	free empty pages as propagating deletion up the tree
+ *
+ * parameter:
+ *
+ * return:
+ */
+static int dtDeleteUp(tid_t tid, struct inode *ip,
+	   metapage_t * fmp, dtpage_t * fp, btstack_t * btstack)
+{
+	int rc = 0;
+	metapage_t *mp;
+	dtpage_t *p;
+	int index, nextindex;
+	int xlen;
+	btframe_t *parent;
+	dtlock_t *dtlck;
+	tlock_t *tlck;
+	lv_t *lv;
+	pxdlock_t *pxdlock;
+	int i;
+
+	/*
+	 *      keep the root leaf page which has become empty
+	 */
+	if (BT_IS_ROOT(fmp)) {
+		/*
+		 * reset the root
+		 *
+		 * dtInitRoot() acquires txlock on the root
+		 */
+		dtInitRoot(tid, ip, PARENT(ip));
+
+		DT_PUTPAGE(fmp);
+
+		return 0;
+	}
+
+	/*
+	 *      free the non-root leaf page
+	 */
+	/*
+	 * acquire a transaction lock on the page
+	 *
+	 * write FREEXTENT|NOREDOPAGE log record
+	 * N.B. linelock is overlaid as freed extent descriptor, and
+	 * the buffer page is freed;
+	 */
+	tlck = txMaplock(tid, ip, tlckDTREE | tlckFREE);
+	pxdlock = (pxdlock_t *) & tlck->lock;
+	pxdlock->flag = mlckFREEPXD;
+	pxdlock->pxd = fp->header.self;
+	pxdlock->index = 1;
+
+	/* update sibling pointers */
+	if ((rc = dtRelink(tid, ip, fp)))
+		return rc;
+
+	xlen = lengthPXD(&fp->header.self);
+	ip->i_blocks -= LBLK2PBLK(ip->i_sb, xlen);
+
+	/* free/invalidate its buffer page */
+	discard_metapage(fmp);
+
+	/*
+	 *      propagate page deletion up the directory tree
+	 *
+	 * If the delete from the parent page makes it empty,
+	 * continue all the way up the tree.
+	 * stop if the root page is reached (which is never deleted) or
+	 * if the entry deletion does not empty the page.
+	 */
+	while ((parent = BT_POP(btstack)) != NULL) {
+		/* pin the parent page <sp> */
+		DT_GETPAGE(ip, parent->bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/*
+		 * free the extent of the child page deleted
+		 */
+		index = parent->index;
+
+		/*
+		 * delete the entry for the child page from parent
+		 */
+		nextindex = p->header.nextindex;
+
+		/*
+		 * the parent has the single entry being deleted:
+		 *
+		 * free the parent page which has become empty.
+		 */
+		if (nextindex == 1) {
+			/*
+			 * keep the root internal page which has become empty
+			 */
+			if (p->header.flag & BT_ROOT) {
+				/*
+				 * reset the root
+				 *
+				 * dtInitRoot() acquires txlock on the root
+				 */
+				dtInitRoot(tid, ip, PARENT(ip));
+
+				DT_PUTPAGE(mp);
+
+				return 0;
+			}
+			/*
+			 * free the parent page
+			 */
+			else {
+				/*
+				 * acquire a transaction lock on the page
+				 *
+				 * write FREEXTENT|NOREDOPAGE log record
+				 */
+				tlck =
+				    txMaplock(tid, ip,
+					      tlckDTREE | tlckFREE);
+				pxdlock = (pxdlock_t *) & tlck->lock;
+				pxdlock->flag = mlckFREEPXD;
+				pxdlock->pxd = p->header.self;
+				pxdlock->index = 1;
+
+				/* update sibling pointers */
+				if ((rc = dtRelink(tid, ip, p)))
+					return rc;
+
+				xlen = lengthPXD(&p->header.self);
+				ip->i_blocks -= LBLK2PBLK(ip->i_sb, xlen);
+
+				/* free/invalidate its buffer page */
+				discard_metapage(mp);
+
+				/* propagate up */
+				continue;
+			}
+		}
+
+		/*
+		 * the parent has other entries remaining:
+		 *
+		 * delete the router entry from the parent page.
+		 */
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the page
+		 *
+		 * action: router entry deletion
+		 */
+		tlck = txLock(tid, ip, mp, tlckDTREE | tlckENTRY);
+		dtlck = (dtlock_t *) & tlck->lock;
+
+		/* linelock header */
+		if (dtlck->index >= dtlck->maxcnt)
+			dtlck = (dtlock_t *) txLinelock(dtlck);
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		/* linelock stbl of non-root leaf page */
+		if (!(p->header.flag & BT_ROOT)) {
+			if (dtlck->index < dtlck->maxcnt)
+				lv++;
+			else {
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+				lv = (lv_t *) & dtlck->lv[0];
+			}
+			i = index >> L2DTSLOTSIZE;
+			lv->offset = p->header.stblindex + i;
+			lv->length =
+			    ((p->header.nextindex - 1) >> L2DTSLOTSIZE) -
+			    i + 1;
+			dtlck->index++;
+		}
+
+		/* free the router entry */
+		dtDeleteEntry(p, index, &dtlck);
+
+		/* reset key of new leftmost entry of level (for consistency) */
+		if (index == 0 &&
+		    ((p->header.flag & BT_ROOT) || p->header.prev == 0))
+			dtTruncateEntry(p, 0, &dtlck);
+
+		/* unpin the parent page */
+		DT_PUTPAGE(mp);
+
+		/* exit propagation up */
+		break;
+	}
+
+	return 0;
+}
+
+
+/*
+ * NAME:        dtRelocate()
+ *
+ * FUNCTION:    relocate dtpage (internal or leaf) of directory;
+ *              This function is mainly used by defragfs utility.
+ */
+int dtRelocate(tid_t tid, struct inode *ip, s64 lmxaddr, pxd_t * opxd,
+	       s64 nxaddr)
+{
+	int rc = 0;
+	metapage_t *mp, *pmp, *lmp, *rmp;
+	dtpage_t *p, *pp, *rp = 0, *lp= 0;
+	s64 bn;
+	int index;
+	btstack_t btstack;
+	pxd_t *pxd;
+	s64 oxaddr, nextbn, prevbn;
+	int xlen, xsize;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	pxdlock_t *pxdlock;
+	s8 *stbl;
+	lv_t *lv;
+
+	oxaddr = addressPXD(opxd);
+	xlen = lengthPXD(opxd);
+
+	jEVENT(0, ("dtRelocate: lmxaddr:%Ld xaddr:%Ld:%Ld xlen:%d\n",
+		   lmxaddr, oxaddr, nxaddr, xlen));
+
+	/*
+	 *      1. get the internal parent dtpage covering
+	 *      router entry for the tartget page to be relocated;
+	 */
+	rc = dtSearchNode(ip, lmxaddr, opxd, &btstack);
+	if (rc)
+		return rc;
+
+	/* retrieve search result */
+	DT_GETSEARCH(ip, btstack.top, bn, pmp, pp, index);
+	jEVENT(0, ("dtRelocate: parent router entry validated.\n"));
+
+	/*
+	 *      2. relocate the target dtpage
+	 */
+	/* read in the target page from src extent */
+	DT_GETPAGE(ip, oxaddr, mp, PSIZE, p, rc);
+	if (rc) {
+		/* release the pinned parent page */
+		DT_PUTPAGE(pmp);
+		return rc;
+	}
+
+	/*
+	 * read in sibling pages if any to update sibling pointers;
+	 */
+	rmp = NULL;
+	if (p->header.next) {
+		nextbn = le64_to_cpu(p->header.next);
+		DT_GETPAGE(ip, nextbn, rmp, PSIZE, rp, rc);
+		if (rc) {
+			DT_PUTPAGE(mp);
+			DT_PUTPAGE(pmp);
+			return (rc);
+		}
+	}
+
+	lmp = NULL;
+	if (p->header.prev) {
+		prevbn = le64_to_cpu(p->header.prev);
+		DT_GETPAGE(ip, prevbn, lmp, PSIZE, lp, rc);
+		if (rc) {
+			DT_PUTPAGE(mp);
+			DT_PUTPAGE(pmp);
+			if (rmp)
+				DT_PUTPAGE(rmp);
+			return (rc);
+		}
+	}
+
+	/* at this point, all xtpages to be updated are in memory */
+
+	/*
+	 * update sibling pointers of sibling dtpages if any;
+	 */
+	if (lmp) {
+		tlck = txLock(tid, ip, lmp, tlckDTREE | tlckRELINK);
+		dtlck = (dtlock_t *) & tlck->lock;
+		/* linelock header */
+		ASSERT(dtlck->index == 0);
+		lv = (lv_t *) & dtlck->lv[0];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		lp->header.next = cpu_to_le64(nxaddr);
+		DT_PUTPAGE(lmp);
+	}
+
+	if (rmp) {
+		tlck = txLock(tid, ip, rmp, tlckDTREE | tlckRELINK);
+		dtlck = (dtlock_t *) & tlck->lock;
+		/* linelock header */
+		ASSERT(dtlck->index == 0);
+		lv = (lv_t *) & dtlck->lv[0];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		rp->header.prev = cpu_to_le64(nxaddr);
+		DT_PUTPAGE(rmp);
+	}
+
+	/*
+	 * update the target dtpage to be relocated
+	 *
+	 * write LOG_REDOPAGE of LOG_NEW type for dst page
+	 * for the whole target page (logredo() will apply
+	 * after image and update bmap for allocation of the
+	 * dst extent), and update bmap for allocation of
+	 * the dst extent;
+	 */
+	tlck = txLock(tid, ip, mp, tlckDTREE | tlckNEW);
+	dtlck = (dtlock_t *) & tlck->lock;
+	/* linelock header */
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+
+	/* update the self address in the dtpage header */
+	pxd = &p->header.self;
+	PXDaddress(pxd, nxaddr);
+
+	/* the dst page is the same as the src page, i.e.,
+	 * linelock for afterimage of the whole page;
+	 */
+	lv->offset = 0;
+	lv->length = p->header.maxslot;
+	dtlck->index++;
+
+	/* update the buffer extent descriptor of the dtpage */
+	xsize = xlen << JFS_SBI(ip->i_sb)->l2bsize;
+#ifdef _STILL_TO_PORT
+	bmSetXD(mp, nxaddr, xsize);
+#endif /* _STILL_TO_PORT */
+	/* unpin the relocated page */
+	DT_PUTPAGE(mp);
+	jEVENT(0, ("dtRelocate: target dtpage relocated.\n"));
+
+	/* the moved extent is dtpage, then a LOG_NOREDOPAGE log rec
+	 * needs to be written (in logredo(), the LOG_NOREDOPAGE log rec
+	 * will also force a bmap update ).
+	 */
+
+	/*
+	 *      3. acquire maplock for the source extent to be freed;
+	 */
+	/* for dtpage relocation, write a LOG_NOREDOPAGE record
+	 * for the source dtpage (logredo() will init NoRedoPage
+	 * filter and will also update bmap for free of the source
+	 * dtpage), and upadte bmap for free of the source dtpage;
+	 */
+	tlck = txMaplock(tid, ip, tlckDTREE | tlckFREE);
+	pxdlock = (pxdlock_t *) & tlck->lock;
+	pxdlock->flag = mlckFREEPXD;
+	PXDaddress(&pxdlock->pxd, oxaddr);
+	PXDlength(&pxdlock->pxd, xlen);
+	pxdlock->index = 1;
+
+	/*
+	 *      4. update the parent router entry for relocation;
+	 *
+	 * acquire tlck for the parent entry covering the target dtpage;
+	 * write LOG_REDOPAGE to apply after image only;
+	 */
+	jEVENT(0, ("dtRelocate: update parent router entry.\n"));
+	tlck = txLock(tid, ip, pmp, tlckDTREE | tlckENTRY);
+	dtlck = (dtlock_t *) & tlck->lock;
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+
+	/* update the PXD with the new address */
+	stbl = DT_GETSTBL(pp);
+	pxd = (pxd_t *) & pp->slot[stbl[index]];
+	PXDaddress(pxd, nxaddr);
+	lv->offset = stbl[index];
+	lv->length = 1;
+	dtlck->index++;
+
+	/* unpin the parent dtpage */
+	DT_PUTPAGE(pmp);
+
+	return rc;
+}
+
+
+/*
+ * NAME:	dtSearchNode()
+ *
+ * FUNCTION:	Search for an dtpage containing a specified address
+ *              This function is mainly used by defragfs utility.
+ *
+ * NOTE:	Search result on stack, the found page is pinned at exit.
+ *		The result page must be an internal dtpage.
+ *		lmxaddr give the address of the left most page of the
+ *		dtree level, in which the required dtpage resides.
+ */
+static int dtSearchNode(struct inode *ip, s64 lmxaddr, pxd_t * kpxd,
+			btstack_t * btstack)
+{
+	int rc = 0;
+	s64 bn;
+	metapage_t *mp;
+	dtpage_t *p;
+	int psize = 288;	/* initial in-line directory */
+	s8 *stbl;
+	int i;
+	pxd_t *pxd;
+	btframe_t *btsp;
+
+	BT_CLR(btstack);	/* reset stack */
+
+	/*
+	 *      descend tree to the level with specified leftmost page
+	 *
+	 *  by convention, root bn = 0.
+	 */
+	for (bn = 0;;) {
+		/* get/pin the page to search */
+		DT_GETPAGE(ip, bn, mp, psize, p, rc);
+		if (rc)
+			return rc;
+
+		/* does the xaddr of leftmost page of the levevl
+		 * matches levevl search key ?
+		 */
+		if (p->header.flag & BT_ROOT) {
+			if (lmxaddr == 0)
+				break;
+		} else if (addressPXD(&p->header.self) == lmxaddr)
+			break;
+
+		/*
+		 * descend down to leftmost child page
+		 */
+		if (p->header.flag & BT_LEAF)
+			return ESTALE;
+
+		/* get the leftmost entry */
+		stbl = DT_GETSTBL(p);
+		pxd = (pxd_t *) & p->slot[stbl[0]];
+
+		/* get the child page block address */
+		bn = addressPXD(pxd);
+		psize = lengthPXD(pxd) << JFS_SBI(ip->i_sb)->l2bsize;
+		/* unpin the parent page */
+		DT_PUTPAGE(mp);
+	}
+
+	/*
+	 *      search each page at the current levevl
+	 */
+      loop:
+	stbl = DT_GETSTBL(p);
+	for (i = 0; i < p->header.nextindex; i++) {
+		pxd = (pxd_t *) & p->slot[stbl[i]];
+
+		/* found the specified router entry */
+		if (addressPXD(pxd) == addressPXD(kpxd) &&
+		    lengthPXD(pxd) == lengthPXD(kpxd)) {
+			btsp = btstack->top;
+			btsp->bn = bn;
+			btsp->index = i;
+			btsp->mp = mp;
+
+			return 0;
+		}
+	}
+
+	/* get the right sibling page if any */
+	if (p->header.next)
+		bn = le64_to_cpu(p->header.next);
+	else {
+		DT_PUTPAGE(mp);
+		return ESTALE;
+	}
+
+	/* unpin current page */
+	DT_PUTPAGE(mp);
+
+	/* get the right sibling page */
+	DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	goto loop;
+}
+
+
+/*
+ *	dtRelink()
+ *
+ * function:
+ *	link around a freed page.
+ *
+ * parameter:
+ *	fp:	page to be freed
+ *
+ * return:
+ */
+static int dtRelink(tid_t tid, struct inode *ip, dtpage_t * p)
+{
+	int rc;
+	metapage_t *mp;
+	s64 nextbn, prevbn;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	lv_t *lv;
+
+	nextbn = le64_to_cpu(p->header.next);
+	prevbn = le64_to_cpu(p->header.prev);
+
+	/* update prev pointer of the next page */
+	if (nextbn != 0) {
+		DT_GETPAGE(ip, nextbn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the next page
+		 *
+		 * action: update prev pointer;
+		 */
+		tlck = txLock(tid, ip, mp, tlckDTREE | tlckRELINK);
+		jEVENT(0,
+		       ("dtRelink nextbn: tlck = 0x%p, ip = 0x%p, mp=0x%p\n",
+			tlck, ip, mp));
+		dtlck = (dtlock_t *) & tlck->lock;
+
+		/* linelock header */
+		if (dtlck->index >= dtlck->maxcnt)
+			dtlck = (dtlock_t *) txLinelock(dtlck);
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		p->header.prev = cpu_to_le64(prevbn);
+		DT_PUTPAGE(mp);
+	}
+
+	/* update next pointer of the previous page */
+	if (prevbn != 0) {
+		DT_GETPAGE(ip, prevbn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the prev page
+		 *
+		 * action: update next pointer;
+		 */
+		tlck = txLock(tid, ip, mp, tlckDTREE | tlckRELINK);
+		jEVENT(0,
+		       ("dtRelink prevbn: tlck = 0x%p, ip = 0x%p, mp=0x%p\n",
+			tlck, ip, mp));
+		dtlck = (dtlock_t *) & tlck->lock;
+
+		/* linelock header */
+		if (dtlck->index >= dtlck->maxcnt)
+			dtlck = (dtlock_t *) txLinelock(dtlck);
+		lv = (lv_t *) & dtlck->lv[dtlck->index];
+		lv->offset = 0;
+		lv->length = 1;
+		dtlck->index++;
+
+		p->header.next = cpu_to_le64(nextbn);
+		DT_PUTPAGE(mp);
+	}
+
+	return 0;
+}
+
+
+/*
+ *	dtInitRoot()
+ *
+ * initialize directory root (inline in inode)
+ */
+void dtInitRoot(tid_t tid, struct inode *ip, u32 idotdot)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	dtroot_t *p;
+	int fsi;
+	dtslot_t *f;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	lv_t *lv;
+	u16 xflag_save;
+
+	/*
+	 * If this was previously an non-empty directory, we need to remove
+	 * the old directory table.
+	 */
+	if (DO_INDEX(ip)) {
+		if (jfs_ip->next_index > (MAX_INLINE_DIRTABLE_ENTRY + 1)) {
+			tblock_t *tblk = tid_to_tblock(tid);
+			/*
+			 * We're playing games with the tid's xflag.  If
+			 * we're removing a regular file, the file's xtree
+			 * is committed with COMMIT_PMAP, but we always
+			 * commit the directories xtree with COMMIT_PWMAP.
+			 */
+			xflag_save = tblk->xflag;
+			tblk->xflag = 0;
+			/*
+			 * xtTruncate isn't guaranteed to fully truncate
+			 * the xtree.  The caller needs to check i_size
+			 * after committing the transaction to see if
+			 * additional truncation is needed.  The
+			 * COMMIT_Stale flag tells caller that we
+			 * initiated the truncation.
+			 */
+			xtTruncate(tid, ip, 0, COMMIT_PWMAP);
+			set_cflag(COMMIT_Stale, ip);
+
+			tblk->xflag = xflag_save;
+			/*
+			 * Tells jfs_metapage code that the metadata pages
+			 * for the index table are no longer useful, and
+			 * remove them from page cache.
+			 */
+			invalidate_inode_metapages(ip);
+		} else
+			ip->i_size = 1;
+
+		jfs_ip->next_index = 2;
+	} else
+		ip->i_size = IDATASIZE;
+
+	/*
+	 * acquire a transaction lock on the root
+	 *
+	 * action: directory initialization;
+	 */
+	tlck = txLock(tid, ip, (metapage_t *) & jfs_ip->bxflag,
+		      tlckDTREE | tlckENTRY | tlckBTROOT);
+	dtlck = (dtlock_t *) & tlck->lock;
+
+	/* linelock root */
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+	lv->offset = 0;
+	lv->length = DTROOTMAXSLOT;
+	dtlck->index++;
+
+	p = &jfs_ip->i_dtroot;
+
+	p->header.flag = DXD_INDEX | BT_ROOT | BT_LEAF;
+
+	p->header.nextindex = 0;
+
+	/* init freelist */
+	fsi = 1;
+	f = &p->slot[fsi];
+
+	/* init data area of root */
+	for (fsi++; fsi < DTROOTMAXSLOT; f++, fsi++)
+		f->next = fsi;
+	f->next = -1;
+
+	p->header.freelist = 1;
+	p->header.freecnt = 8;
+
+	/* init '..' entry */
+	p->header.idotdot = cpu_to_le32(idotdot);
+
+#if 0
+	ip->i_blocks = LBLK2PBLK(ip->i_sb,
+				 ((jfs_ip->ea.flag & DXD_EXTENT) ?
+				  lengthDXD(&jfs_ip->ea) : 0) +
+				 ((jfs_ip->acl.flag & DXD_EXTENT) ?
+				  lengthDXD(&jfs_ip->acl) : 0));
+#endif
+
+	return;
+}
+
+/*
+ *	jfs_readdir()
+ *
+ * function: read directory entries sequentially
+ *	from the specified entry offset
+ *
+ * parameter:
+ *
+ * return: offset = (pn, index) of start entry
+ *	of next jfs_readdir()/dtRead()
+ */
+int jfs_readdir(struct file *filp, void *dirent, filldir_t filldir)
+{
+	struct inode *ip = filp->f_dentry->d_inode;
+	struct nls_table *codepage = JFS_SBI(ip->i_sb)->nls_tab;
+	int rc = 0;
+	struct dtoffset {
+		s16 pn;
+		s16 index;
+		s32 unused;
+	} *dtoffset = (struct dtoffset *) &filp->f_pos;
+	s64 bn;
+	metapage_t *mp;
+	dtpage_t *p;
+	int index;
+	s8 *stbl;
+	btstack_t btstack;
+	int i, next;
+	ldtentry_t *d;
+	dtslot_t *t;
+	int d_namleft, d_namlen, len, outlen;
+	char *d_name, *name_ptr;
+	int dtlhdrdatalen;
+	u32 dir_index;
+	int do_index = 0;
+	uint loop_count = 0;
+
+	if (filp->f_pos == DIREND)
+		return 0;
+
+	if (DO_INDEX(ip)) {
+		/*
+		 * persistent index is stored in directory entries.
+		 * Special cases:        0 = .
+		 *                       1 = ..
+		 *                      -1 = End of directory
+		 */
+		do_index = 1;
+		dtlhdrdatalen = DTLHDRDATALEN;
+
+		dir_index = (u32) filp->f_pos;
+
+		if (dir_index > 1) {
+			dir_table_slot_t dirtab_slot;
+
+			if (dtEmpty(ip)) {
+				filp->f_pos = DIREND;
+				return 0;
+			}
+		      repeat:
+			rc = get_index(ip, dir_index, &dirtab_slot);
+			if (rc) {
+				filp->f_pos = DIREND;
+				return rc;
+			}
+			if (dirtab_slot.flag == DIR_INDEX_FREE) {
+				if (loop_count++ > JFS_IP(ip)->next_index) {
+					jERROR(1, ("jfs_readdir detected "
+						   "infinite loop!\n"));
+					filp->f_pos = DIREND;
+					return 0;
+				}
+				dir_index = le32_to_cpu(dirtab_slot.addr2);
+				if (dir_index == -1) {
+					filp->f_pos = DIREND;
+					return 0;
+				}
+				goto repeat;
+			}
+			bn = addressDTS(&dirtab_slot);
+			index = dirtab_slot.slot;
+			DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+			if (rc) {
+				filp->f_pos = DIREND;
+				return 0;
+			}
+			if (p->header.flag & BT_INTERNAL) {
+				jERROR(1,("jfs_readdir: bad index table\n"));
+				DT_PUTPAGE(mp);
+				filp->f_pos = -1;
+				return 0;
+			}
+		} else {
+			if (dir_index == 0) {
+				/*
+				 * self "."
+				 */
+				filp->f_pos = 0;
+				if (filldir(dirent, ".", 1, 0, ip->i_ino,
+					    DT_DIR))
+					return 0;
+			}
+			/*
+			 * parent ".."
+			 */
+			filp->f_pos = 1;
+			if (filldir
+			    (dirent, "..", 2, 1, PARENT(ip), DT_DIR))
+				return 0;
+
+			/*
+			 * Find first entry of left-most leaf
+			 */
+			if (dtEmpty(ip)) {
+				filp->f_pos = DIREND;
+				return 0;
+			}
+
+			if ((rc = dtReadFirst(ip, &btstack)))
+				return -rc;
+
+			DT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+		}
+	} else {
+		/*
+		 * Legacy filesystem - OS/2 & Linux JFS < 0.3.6
+		 *
+		 * pn = index = 0:      First entry "."
+		 * pn = 0; index = 1:   Second entry ".."
+		 * pn > 0:              Real entries, pn=1 -> leftmost page
+		 * pn = index = -1:     No more entries
+		 */
+		dtlhdrdatalen = DTLHDRDATALEN_LEGACY;
+
+		if (filp->f_pos == 0) {
+			/* build "." entry */
+
+			if (filldir(dirent, ".", 1, filp->f_pos, ip->i_ino,
+				    DT_DIR))
+				return 0;
+			dtoffset->index = 1;
+		}
+
+		if (dtoffset->pn == 0) {
+			if (dtoffset->index == 1) {
+				/* build ".." entry */
+
+				if (filldir(dirent, "..", 2, filp->f_pos,
+					    PARENT(ip), DT_DIR))
+					return 0;
+			} else {
+				jERROR(1,
+				       ("jfs_readdir called with invalid offset!\n"));
+			}
+			dtoffset->pn = 1;
+			dtoffset->index = 0;
+		}
+
+		if (dtEmpty(ip)) {
+			filp->f_pos = DIREND;
+			return 0;
+		}
+
+		if ((rc = dtReadNext(ip, &filp->f_pos, &btstack))) {
+			jERROR(1,
+			       ("jfs_readdir: unexpected rc = %d from dtReadNext\n",
+				rc));
+			filp->f_pos = DIREND;
+			return 0;
+		}
+		/* get start leaf page and index */
+		DT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+		/* offset beyond directory eof ? */
+		if (bn < 0) {
+			filp->f_pos = DIREND;
+			return 0;
+		}
+	}
+
+	d_name = kmalloc((JFS_NAME_MAX + 1) * sizeof(wchar_t), GFP_NOFS);
+	if (d_name == NULL) {
+		DT_PUTPAGE(mp);
+		jERROR(1, ("jfs_readdir: kmalloc failed!\n"));
+		filp->f_pos = DIREND;
+		return 0;
+	}
+	while (1) {
+		stbl = DT_GETSTBL(p);
+
+		for (i = index; i < p->header.nextindex; i++) {
+			d = (ldtentry_t *) & p->slot[stbl[i]];
+
+			d_namleft = d->namlen;
+			name_ptr = d_name;
+
+			if (do_index) {
+				filp->f_pos = le32_to_cpu(d->index);
+				len = min(d_namleft, DTLHDRDATALEN);
+			} else
+				len = min(d_namleft, DTLHDRDATALEN_LEGACY);
+
+			/* copy the name of head/only segment */
+			outlen = jfs_strfromUCS_le(name_ptr, d->name, len,
+						   codepage);
+			d_namlen = outlen;
+
+			/* copy name in the additional segment(s) */
+			next = d->next;
+			while (next >= 0) {
+				t = (dtslot_t *) & p->slot[next];
+				name_ptr += outlen;
+				d_namleft -= len;
+				/* Sanity Check */
+				if (d_namleft == 0) {
+					jERROR(1,("JFS:Dtree error: "
+					  "ino = %ld, bn=%Ld, index = %d\n",
+						  ip->i_ino, bn, i));
+					updateSuper(ip->i_sb, FM_DIRTY);
+					goto skip_one;
+				}
+				len = min(d_namleft, DTSLOTDATALEN);
+				outlen = jfs_strfromUCS_le(name_ptr, t->name,
+							   len, codepage);
+				d_namlen+= outlen;
+
+				next = t->next;
+			}
+
+			if (filldir(dirent, d_name, d_namlen, filp->f_pos,
+				    le32_to_cpu(d->inumber), DT_UNKNOWN))
+				goto out;
+skip_one:
+			if (!do_index)
+				dtoffset->index++;
+		}
+
+		/*
+		 * get next leaf page
+		 */
+
+		if (p->header.flag & BT_ROOT) {
+			filp->f_pos = DIREND;
+			break;
+		}
+
+		bn = le64_to_cpu(p->header.next);
+		if (bn == 0) {
+			filp->f_pos = DIREND;
+			break;
+		}
+
+		/* unpin previous leaf page */
+		DT_PUTPAGE(mp);
+
+		/* get next leaf page */
+		DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc) {
+			kfree(d_name);
+			return -rc;
+		}
+
+		/* update offset (pn:index) for new page */
+		index = 0;
+		if (!do_index) {
+			dtoffset->pn++;
+			dtoffset->index = 0;
+		}
+
+	}
+
+      out:
+	kfree(d_name);
+	DT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *	dtReadFirst()
+ *
+ * function: get the leftmost page of the directory
+ */
+static int dtReadFirst(struct inode *ip, btstack_t * btstack)
+{
+	int rc = 0;
+	s64 bn;
+	int psize = 288;	/* initial in-line directory */
+	metapage_t *mp;
+	dtpage_t *p;
+	s8 *stbl;
+	btframe_t *btsp;
+	pxd_t *xd;
+
+	BT_CLR(btstack);	/* reset stack */
+
+	/*
+	 *      descend leftmost path of the tree
+	 *
+	 * by convention, root bn = 0.
+	 */
+	for (bn = 0;;) {
+		DT_GETPAGE(ip, bn, mp, psize, p, rc);
+		if (rc)
+			return rc;
+
+		/*
+		 * leftmost leaf page
+		 */
+		if (p->header.flag & BT_LEAF) {
+			/* return leftmost entry */
+			btsp = btstack->top;
+			btsp->bn = bn;
+			btsp->index = 0;
+			btsp->mp = mp;
+
+			return 0;
+		}
+
+		/*
+		 * descend down to leftmost child page
+		 */
+		/* push (bn, index) of the parent page/entry */
+		BT_PUSH(btstack, bn, 0);
+
+		/* get the leftmost entry */
+		stbl = DT_GETSTBL(p);
+		xd = (pxd_t *) & p->slot[stbl[0]];
+
+		/* get the child page block address */
+		bn = addressPXD(xd);
+		psize = lengthPXD(xd) << JFS_SBI(ip->i_sb)->l2bsize;
+
+		/* unpin the parent page */
+		DT_PUTPAGE(mp);
+	}
+}
+
+
+/*
+ *	dtReadNext()
+ *
+ * function: get the page of the specified offset (pn:index)
+ *
+ * return: if (offset > eof), bn = -1;
+ *
+ * note: if index > nextindex of the target leaf page,
+ * start with 1st entry of next leaf page;
+ */
+static int dtReadNext(struct inode *ip, loff_t * offset, btstack_t * btstack)
+{
+	int rc = 0;
+	struct dtoffset {
+		s16 pn;
+		s16 index;
+		s32 unused;
+	} *dtoffset = (struct dtoffset *) offset;
+	s64 bn;
+	metapage_t *mp;
+	dtpage_t *p;
+	int index;
+	int pn;
+	s8 *stbl;
+	btframe_t *btsp, *parent;
+	pxd_t *xd;
+
+	/*
+	 * get leftmost leaf page pinned
+	 */
+	if ((rc = dtReadFirst(ip, btstack)))
+		return rc;
+
+	/* get leaf page */
+	DT_GETSEARCH(ip, btstack->top, bn, mp, p, index);
+
+	/* get the start offset (pn:index) */
+	pn = dtoffset->pn - 1;	/* Now pn = 0 represents leftmost leaf */
+	index = dtoffset->index;
+
+	/* start at leftmost page ? */
+	if (pn == 0) {
+		/* offset beyond eof ? */
+		if (index < p->header.nextindex)
+			goto out;
+
+		if (p->header.flag & BT_ROOT) {
+			bn = -1;
+			goto out;
+		}
+
+		/* start with 1st entry of next leaf page */
+		dtoffset->pn++;
+		dtoffset->index = index = 0;
+		goto a;
+	}
+
+	/* start at non-leftmost page: scan parent pages for large pn */
+	if (p->header.flag & BT_ROOT) {
+		bn = -1;
+		goto out;
+	}
+
+	/* start after next leaf page ? */
+	if (pn > 1)
+		goto b;
+
+	/* get leaf page pn = 1 */
+      a:
+	bn = le64_to_cpu(p->header.next);
+
+	/* unpin leaf page */
+	DT_PUTPAGE(mp);
+
+	/* offset beyond eof ? */
+	if (bn == 0) {
+		bn = -1;
+		goto out;
+	}
+
+	goto c;
+
+	/*
+	 * scan last internal page level to get target leaf page
+	 */
+      b:
+	/* unpin leftmost leaf page */
+	DT_PUTPAGE(mp);
+
+	/* get left most parent page */
+	btsp = btstack->top;
+	parent = btsp - 1;
+	bn = parent->bn;
+	DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/* scan parent pages at last internal page level */
+	while (pn >= p->header.nextindex) {
+		pn -= p->header.nextindex;
+
+		/* get next parent page address */
+		bn = le64_to_cpu(p->header.next);
+
+		/* unpin current parent page */
+		DT_PUTPAGE(mp);
+
+		/* offset beyond eof ? */
+		if (bn == 0) {
+			bn = -1;
+			goto out;
+		}
+
+		/* get next parent page */
+		DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/* update parent page stack frame */
+		parent->bn = bn;
+	}
+
+	/* get leaf page address */
+	stbl = DT_GETSTBL(p);
+	xd = (pxd_t *) & p->slot[stbl[pn]];
+	bn = addressPXD(xd);
+
+	/* unpin parent page */
+	DT_PUTPAGE(mp);
+
+	/*
+	 * get target leaf page
+	 */
+      c:
+	DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/*
+	 * leaf page has been completed:
+	 * start with 1st entry of next leaf page
+	 */
+	if (index >= p->header.nextindex) {
+		bn = le64_to_cpu(p->header.next);
+
+		/* unpin leaf page */
+		DT_PUTPAGE(mp);
+
+		/* offset beyond eof ? */
+		if (bn == 0) {
+			bn = -1;
+			goto out;
+		}
+
+		/* get next leaf page */
+		DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/* start with 1st entry of next leaf page */
+		dtoffset->pn++;
+		dtoffset->index = 0;
+	}
+
+      out:
+	/* return target leaf page pinned */
+	btsp = btstack->top;
+	btsp->bn = bn;
+	btsp->index = dtoffset->index;
+	btsp->mp = mp;
+
+	return 0;
+}
+
+
+/*
+ *	dtCompare()
+ *
+ * function: compare search key with an internal entry
+ *
+ * return:
+ *	< 0 if k is < record
+ *	= 0 if k is = record
+ *	> 0 if k is > record
+ */
+static int dtCompare(component_t * key,	/* search key */
+		     dtpage_t * p,	/* directory page */
+		     int si)
+{				/* entry slot index */
+	register int rc;
+	register wchar_t *kname, *name;
+	register int klen, namlen, len;
+	idtentry_t *ih;
+	dtslot_t *t;
+
+	/*
+	 * force the left-most key on internal pages, at any level of
+	 * the tree, to be less than any search key.
+	 * this obviates having to update the leftmost key on an internal
+	 * page when the user inserts a new key in the tree smaller than
+	 * anything that has been stored.
+	 *
+	 * (? if/when dtSearch() narrows down to 1st entry (index = 0),
+	 * at any internal page at any level of the tree,
+	 * it descends to child of the entry anyway -
+	 * ? make the entry as min size dummy entry)
+	 *
+	 * if (e->index == 0 && h->prevpg == P_INVALID && !(h->flags & BT_LEAF))
+	 * return (1);
+	 */
+
+	kname = key->name;
+	klen = key->namlen;
+
+	ih = (idtentry_t *) & p->slot[si];
+	si = ih->next;
+	name = ih->name;
+	namlen = ih->namlen;
+	len = min(namlen, DTIHDRDATALEN);
+
+	/* compare with head/only segment */
+	len = min(klen, len);
+	if ((rc = UniStrncmp_le(kname, name, len)))
+		return rc;
+
+	klen -= len;
+	namlen -= len;
+
+	/* compare with additional segment(s) */
+	kname += len;
+	while (klen > 0 && namlen > 0) {
+		/* compare with next name segment */
+		t = (dtslot_t *) & p->slot[si];
+		len = min(namlen, DTSLOTDATALEN);
+		len = min(klen, len);
+		name = t->name;
+		if ((rc = UniStrncmp_le(kname, name, len)))
+			return rc;
+
+		klen -= len;
+		namlen -= len;
+		kname += len;
+		si = t->next;
+	}
+
+	return (klen - namlen);
+}
+
+
+
+
+/*
+ *	ciCompare()
+ *
+ * function: compare search key with an (leaf/internal) entry
+ *
+ * return:
+ *	< 0 if k is < record
+ *	= 0 if k is = record
+ *	> 0 if k is > record
+ */
+static int ciCompare(component_t * key,	/* search key */
+		     dtpage_t * p,	/* directory page */
+		     int si,	/* entry slot index */
+		     int flag)
+{
+	register int rc;
+	register wchar_t *kname, *name, x;
+	register int klen, namlen, len;
+	ldtentry_t *lh;
+	idtentry_t *ih;
+	dtslot_t *t;
+	int i;
+
+	/*
+	 * force the left-most key on internal pages, at any level of
+	 * the tree, to be less than any search key.
+	 * this obviates having to update the leftmost key on an internal
+	 * page when the user inserts a new key in the tree smaller than
+	 * anything that has been stored.
+	 *
+	 * (? if/when dtSearch() narrows down to 1st entry (index = 0),
+	 * at any internal page at any level of the tree,
+	 * it descends to child of the entry anyway -
+	 * ? make the entry as min size dummy entry)
+	 *
+	 * if (e->index == 0 && h->prevpg == P_INVALID && !(h->flags & BT_LEAF))
+	 * return (1);
+	 */
+
+	kname = key->name;
+	klen = key->namlen;
+
+	/*
+	 * leaf page entry
+	 */
+	if (p->header.flag & BT_LEAF) {
+		lh = (ldtentry_t *) & p->slot[si];
+		si = lh->next;
+		name = lh->name;
+		namlen = lh->namlen;
+		if (flag & JFS_DIR_INDEX)
+			len = min(namlen, DTLHDRDATALEN);
+		else
+			len = min(namlen, DTLHDRDATALEN_LEGACY);
+	}
+	/*
+	 * internal page entry
+	 */
+	else {
+		ih = (idtentry_t *) & p->slot[si];
+		si = ih->next;
+		name = ih->name;
+		namlen = ih->namlen;
+		len = min(namlen, DTIHDRDATALEN);
+	}
+
+	/* compare with head/only segment */
+	len = min(klen, len);
+	for (i = 0; i < len; i++, kname++, name++) {
+		/* only uppercase if case-insensitive support is on */
+		if ((flag & JFS_OS2) == JFS_OS2)
+			x = UniToupper(le16_to_cpu(*name));
+		else
+			x = le16_to_cpu(*name);
+		if ((rc = *kname - x))
+			return rc;
+	}
+
+	klen -= len;
+	namlen -= len;
+
+	/* compare with additional segment(s) */
+	while (klen > 0 && namlen > 0) {
+		/* compare with next name segment */
+		t = (dtslot_t *) & p->slot[si];
+		len = min(namlen, DTSLOTDATALEN);
+		len = min(klen, len);
+		name = t->name;
+		for (i = 0; i < len; i++, kname++, name++) {
+			/* only uppercase if case-insensitive support is on */
+			if ((flag & JFS_OS2) == JFS_OS2)
+				x = UniToupper(le16_to_cpu(*name));
+			else
+				x = le16_to_cpu(*name);
+
+			if ((rc = *kname - x))
+				return rc;
+		}
+
+		klen -= len;
+		namlen -= len;
+		si = t->next;
+	}
+
+	return (klen - namlen);
+}
+
+
+/*
+ *	ciGetLeafPrefixKey()
+ *
+ * function: compute prefix of suffix compression
+ *	     from two adjacent leaf entries
+ *	     across page boundary
+ *
+ * return:
+ *	Number of prefix bytes needed to distinguish b from a.
+ */
+static void ciGetLeafPrefixKey(dtpage_t * lp, int li, dtpage_t * rp,
+			       int ri, component_t * key, int flag)
+{
+	register int klen, namlen;
+	register wchar_t *pl, *pr, *kname;
+	wchar_t lname[JFS_NAME_MAX + 1];
+	component_t lkey = { 0, lname };
+	wchar_t rname[JFS_NAME_MAX + 1];
+	component_t rkey = { 0, rname };
+
+	/* get left and right key */
+	dtGetKey(lp, li, &lkey, flag);
+	lkey.name[lkey.namlen] = 0;
+
+	if ((flag & JFS_OS2) == JFS_OS2)
+		ciToUpper(&lkey);
+
+	dtGetKey(rp, ri, &rkey, flag);
+	rkey.name[rkey.namlen] = 0;
+
+
+	if ((flag & JFS_OS2) == JFS_OS2)
+		ciToUpper(&rkey);
+
+	/* compute prefix */
+	klen = 0;
+	kname = key->name;
+	namlen = min(lkey.namlen, rkey.namlen);
+	for (pl = lkey.name, pr = rkey.name;
+	     namlen; pl++, pr++, namlen--, klen++, kname++) {
+		*kname = *pr;
+		if (*pl != *pr) {
+			key->namlen = klen + 1;
+			return;
+		}
+	}
+
+	/* l->namlen <= r->namlen since l <= r */
+	if (lkey.namlen < rkey.namlen) {
+		*kname = *pr;
+		key->namlen = klen + 1;
+	} else			/* l->namelen == r->namelen */
+		key->namlen = klen;
+
+	return;
+}
+
+
+
+/*
+ *	dtGetKey()
+ *
+ * function: get key of the entry
+ */
+static void dtGetKey(dtpage_t * p, int i,	/* entry index */
+		     component_t * key, int flag)
+{
+	int si;
+	s8 *stbl;
+	ldtentry_t *lh;
+	idtentry_t *ih;
+	dtslot_t *t;
+	int namlen, len;
+	wchar_t *name, *kname;
+
+	/* get entry */
+	stbl = DT_GETSTBL(p);
+	si = stbl[i];
+	if (p->header.flag & BT_LEAF) {
+		lh = (ldtentry_t *) & p->slot[si];
+		si = lh->next;
+		namlen = lh->namlen;
+		name = lh->name;
+		if (flag & JFS_DIR_INDEX)
+			len = min(namlen, DTLHDRDATALEN);
+		else
+			len = min(namlen, DTLHDRDATALEN_LEGACY);
+	} else {
+		ih = (idtentry_t *) & p->slot[si];
+		si = ih->next;
+		namlen = ih->namlen;
+		name = ih->name;
+		len = min(namlen, DTIHDRDATALEN);
+	}
+
+	key->namlen = namlen;
+	kname = key->name;
+
+	/*
+	 * move head/only segment
+	 */
+	UniStrncpy_le(kname, name, len);
+
+	/*
+	 * move additional segment(s)
+	 */
+	while (si >= 0) {
+		/* get next segment */
+		t = &p->slot[si];
+		kname += len;
+		namlen -= len;
+		len = min(namlen, DTSLOTDATALEN);
+		UniStrncpy_le(kname, t->name, len);
+
+		si = t->next;
+	}
+}
+
+
+/*
+ *	dtInsertEntry()
+ *
+ * function: allocate free slot(s) and
+ *	     write a leaf/internal entry
+ *
+ * return: entry slot index
+ */
+static void dtInsertEntry(dtpage_t * p, int index, component_t * key,
+			  ddata_t * data, dtlock_t ** dtlock)
+{
+	dtslot_t *h, *t;
+	ldtentry_t *lh = 0;
+	idtentry_t *ih = 0;
+	int hsi, fsi, klen, len, nextindex;
+	wchar_t *kname, *name;
+	s8 *stbl;
+	pxd_t *xd;
+	dtlock_t *dtlck = *dtlock;
+	lv_t *lv;
+	int xsi, n;
+	s64 bn = 0;
+	metapage_t *mp = 0;
+
+	klen = key->namlen;
+	kname = key->name;
+
+	/* allocate a free slot */
+	hsi = fsi = p->header.freelist;
+	h = &p->slot[fsi];
+	p->header.freelist = h->next;
+	--p->header.freecnt;
+
+	/* open new linelock */
+	if (dtlck->index >= dtlck->maxcnt)
+		dtlck = (dtlock_t *) txLinelock(dtlck);
+
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+	lv->offset = hsi;
+
+	/* write head/only segment */
+	if (p->header.flag & BT_LEAF) {
+		lh = (ldtentry_t *) h;
+		lh->next = h->next;
+		lh->inumber = data->leaf.ino;	/* little-endian */
+		lh->namlen = klen;
+		name = lh->name;
+		if (data->leaf.ip) {
+			len = min(klen, DTLHDRDATALEN);
+			if (!(p->header.flag & BT_ROOT))
+				bn = addressPXD(&p->header.self);
+			lh->index = cpu_to_le32(add_index(data->leaf.tid,
+							  data->leaf.ip,
+							  bn, index));
+		} else
+			len = min(klen, DTLHDRDATALEN_LEGACY);
+	} else {
+		ih = (idtentry_t *) h;
+		ih->next = h->next;
+		xd = (pxd_t *) ih;
+		*xd = data->xd;
+		ih->namlen = klen;
+		name = ih->name;
+		len = min(klen, DTIHDRDATALEN);
+	}
+
+	UniStrncpy_le(name, kname, len);
+
+	n = 1;
+	xsi = hsi;
+
+	/* write additional segment(s) */
+	t = h;
+	klen -= len;
+	while (klen) {
+		/* get free slot */
+		fsi = p->header.freelist;
+		t = &p->slot[fsi];
+		p->header.freelist = t->next;
+		--p->header.freecnt;
+
+		/* is next slot contiguous ? */
+		if (fsi != xsi + 1) {
+			/* close current linelock */
+			lv->length = n;
+			dtlck->index++;
+
+			/* open new linelock */
+			if (dtlck->index < dtlck->maxcnt)
+				lv++;
+			else {
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+				lv = (lv_t *) & dtlck->lv[0];
+			}
+
+			lv->offset = fsi;
+			n = 0;
+		}
+
+		kname += len;
+		len = min(klen, DTSLOTDATALEN);
+		UniStrncpy_le(t->name, kname, len);
+
+		n++;
+		xsi = fsi;
+		klen -= len;
+	}
+
+	/* close current linelock */
+	lv->length = n;
+	dtlck->index++;
+
+	*dtlock = dtlck;
+
+	/* terminate last/only segment */
+	if (h == t) {
+		/* single segment entry */
+		if (p->header.flag & BT_LEAF)
+			lh->next = -1;
+		else
+			ih->next = -1;
+	} else
+		/* multi-segment entry */
+		t->next = -1;
+
+	/* if insert into middle, shift right succeeding entries in stbl */
+	stbl = DT_GETSTBL(p);
+	nextindex = p->header.nextindex;
+	if (index < nextindex) {
+		memmove(stbl + index + 1, stbl + index, nextindex - index);
+
+		if ((p->header.flag & BT_LEAF) && data->leaf.ip) {
+			/*
+			 * Need to update slot number for entries that moved
+			 * in the stbl
+			 */
+			mp = 0;
+			for (n = index + 1; n <= nextindex; n++) {
+				lh = (ldtentry_t *) & (p->slot[stbl[n]]);
+				modify_index(data->leaf.tid, data->leaf.ip,
+					     le32_to_cpu(lh->index), bn, n,
+					     &mp);
+			}
+			if (mp)
+				release_metapage(mp);
+		}
+	}
+
+	stbl[index] = hsi;
+
+	/* advance next available entry index of stbl */
+	++p->header.nextindex;
+}
+
+
+/*
+ *	dtMoveEntry()
+ *
+ * function: move entries from split/left page to new/right page
+ *
+ *	nextindex of dst page and freelist/freecnt of both pages
+ *	are updated.
+ */
+static void dtMoveEntry(dtpage_t * sp, int si, dtpage_t * dp,
+			dtlock_t ** sdtlock, dtlock_t ** ddtlock,
+			int do_index)
+{
+	int ssi, next;		/* src slot index */
+	int di;			/* dst entry index */
+	int dsi;		/* dst slot index */
+	s8 *sstbl, *dstbl;	/* sorted entry table */
+	int snamlen, len;
+	ldtentry_t *slh, *dlh = 0;
+	idtentry_t *sih, *dih = 0;
+	dtslot_t *h, *s, *d;
+	dtlock_t *sdtlck = *sdtlock, *ddtlck = *ddtlock;
+	lv_t *slv, *dlv;
+	int xssi, ns, nd;
+	int sfsi;
+
+	sstbl = (s8 *) & sp->slot[sp->header.stblindex];
+	dstbl = (s8 *) & dp->slot[dp->header.stblindex];
+
+	dsi = dp->header.freelist;	/* first (whole page) free slot */
+	sfsi = sp->header.freelist;
+
+	/* linelock destination entry slot */
+	dlv = (lv_t *) & ddtlck->lv[ddtlck->index];
+	dlv->offset = dsi;
+
+	/* linelock source entry slot */
+	slv = (lv_t *) & sdtlck->lv[sdtlck->index];
+	slv->offset = sstbl[si];
+	xssi = slv->offset - 1;
+
+	/*
+	 * move entries
+	 */
+	ns = nd = 0;
+	for (di = 0; si < sp->header.nextindex; si++, di++) {
+		ssi = sstbl[si];
+		dstbl[di] = dsi;
+
+		/* is next slot contiguous ? */
+		if (ssi != xssi + 1) {
+			/* close current linelock */
+			slv->length = ns;
+			sdtlck->index++;
+
+			/* open new linelock */
+			if (sdtlck->index < sdtlck->maxcnt)
+				slv++;
+			else {
+				sdtlck = (dtlock_t *) txLinelock(sdtlck);
+				slv = (lv_t *) & sdtlck->lv[0];
+			}
+
+			slv->offset = ssi;
+			ns = 0;
+		}
+
+		/*
+		 * move head/only segment of an entry
+		 */
+		/* get dst slot */
+		h = d = &dp->slot[dsi];
+
+		/* get src slot and move */
+		s = &sp->slot[ssi];
+		if (sp->header.flag & BT_LEAF) {
+			/* get source entry */
+			slh = (ldtentry_t *) s;
+			dlh = (ldtentry_t *) h;
+			snamlen = slh->namlen;
+
+			if (do_index) {
+				len = min(snamlen, DTLHDRDATALEN);
+				dlh->index = slh->index; /* little-endian */
+			} else
+				len = min(snamlen, DTLHDRDATALEN_LEGACY);
+
+			memcpy(dlh, slh, 6 + len * 2);
+
+			next = slh->next;
+
+			/* update dst head/only segment next field */
+			dsi++;
+			dlh->next = dsi;
+		} else {
+			sih = (idtentry_t *) s;
+			snamlen = sih->namlen;
+
+			len = min(snamlen, DTIHDRDATALEN);
+			dih = (idtentry_t *) h;
+			memcpy(dih, sih, 10 + len * 2);
+			next = sih->next;
+
+			dsi++;
+			dih->next = dsi;
+		}
+
+		/* free src head/only segment */
+		s->next = sfsi;
+		s->cnt = 1;
+		sfsi = ssi;
+
+		ns++;
+		nd++;
+		xssi = ssi;
+
+		/*
+		 * move additional segment(s) of the entry
+		 */
+		snamlen -= len;
+		while ((ssi = next) >= 0) {
+			/* is next slot contiguous ? */
+			if (ssi != xssi + 1) {
+				/* close current linelock */
+				slv->length = ns;
+				sdtlck->index++;
+
+				/* open new linelock */
+				if (sdtlck->index < sdtlck->maxcnt)
+					slv++;
+				else {
+					sdtlck =
+					    (dtlock_t *)
+					    txLinelock(sdtlck);
+					slv = (lv_t *) & sdtlck->lv[0];
+				}
+
+				slv->offset = ssi;
+				ns = 0;
+			}
+
+			/* get next source segment */
+			s = &sp->slot[ssi];
+
+			/* get next destination free slot */
+			d++;
+
+			len = min(snamlen, DTSLOTDATALEN);
+			UniStrncpy(d->name, s->name, len);
+
+			ns++;
+			nd++;
+			xssi = ssi;
+
+			dsi++;
+			d->next = dsi;
+
+			/* free source segment */
+			next = s->next;
+			s->next = sfsi;
+			s->cnt = 1;
+			sfsi = ssi;
+
+			snamlen -= len;
+		}		/* end while */
+
+		/* terminate dst last/only segment */
+		if (h == d) {
+			/* single segment entry */
+			if (dp->header.flag & BT_LEAF)
+				dlh->next = -1;
+			else
+				dih->next = -1;
+		} else
+			/* multi-segment entry */
+			d->next = -1;
+	}			/* end for */
+
+	/* close current linelock */
+	slv->length = ns;
+	sdtlck->index++;
+	*sdtlock = sdtlck;
+
+	dlv->length = nd;
+	ddtlck->index++;
+	*ddtlock = ddtlck;
+
+	/* update source header */
+	sp->header.freelist = sfsi;
+	sp->header.freecnt += nd;
+
+	/* update destination header */
+	dp->header.nextindex = di;
+
+	dp->header.freelist = dsi;
+	dp->header.freecnt -= nd;
+}
+
+
+/*
+ *	dtDeleteEntry()
+ *
+ * function: free a (leaf/internal) entry
+ *
+ * log freelist header, stbl, and each segment slot of entry
+ * (even though last/only segment next field is modified,
+ * physical image logging requires all segment slots of
+ * the entry logged to avoid applying previous updates
+ * to the same slots)
+ */
+static void dtDeleteEntry(dtpage_t * p, int fi, dtlock_t ** dtlock)
+{
+	int fsi;		/* free entry slot index */
+	s8 *stbl;
+	dtslot_t *t;
+	int si, freecnt;
+	dtlock_t *dtlck = *dtlock;
+	lv_t *lv;
+	int xsi, n;
+
+	/* get free entry slot index */
+	stbl = DT_GETSTBL(p);
+	fsi = stbl[fi];
+
+	/* open new linelock */
+	if (dtlck->index >= dtlck->maxcnt)
+		dtlck = (dtlock_t *) txLinelock(dtlck);
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+
+	lv->offset = fsi;
+
+	/* get the head/only segment */
+	t = &p->slot[fsi];
+	if (p->header.flag & BT_LEAF)
+		si = ((ldtentry_t *) t)->next;
+	else
+		si = ((idtentry_t *) t)->next;
+	t->next = si;
+	t->cnt = 1;
+
+	n = freecnt = 1;
+	xsi = fsi;
+
+	/* find the last/only segment */
+	while (si >= 0) {
+		/* is next slot contiguous ? */
+		if (si != xsi + 1) {
+			/* close current linelock */
+			lv->length = n;
+			dtlck->index++;
+
+			/* open new linelock */
+			if (dtlck->index < dtlck->maxcnt)
+				lv++;
+			else {
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+				lv = (lv_t *) & dtlck->lv[0];
+			}
+
+			lv->offset = si;
+			n = 0;
+		}
+
+		n++;
+		xsi = si;
+		freecnt++;
+
+		t = &p->slot[si];
+		t->cnt = 1;
+		si = t->next;
+	}
+
+	/* close current linelock */
+	lv->length = n;
+	dtlck->index++;
+
+	*dtlock = dtlck;
+
+	/* update freelist */
+	t->next = p->header.freelist;
+	p->header.freelist = fsi;
+	p->header.freecnt += freecnt;
+
+	/* if delete from middle,
+	 * shift left the succedding entries in the stbl
+	 */
+	si = p->header.nextindex;
+	if (fi < si - 1)
+		memmove(&stbl[fi], &stbl[fi + 1], si - fi - 1);
+
+	p->header.nextindex--;
+}
+
+
+/*
+ *	dtTruncateEntry()
+ *
+ * function: truncate a (leaf/internal) entry
+ *
+ * log freelist header, stbl, and each segment slot of entry
+ * (even though last/only segment next field is modified,
+ * physical image logging requires all segment slots of
+ * the entry logged to avoid applying previous updates
+ * to the same slots)
+ */
+static void dtTruncateEntry(dtpage_t * p, int ti, dtlock_t ** dtlock)
+{
+	int tsi;		/* truncate entry slot index */
+	s8 *stbl;
+	dtslot_t *t;
+	int si, freecnt;
+	dtlock_t *dtlck = *dtlock;
+	lv_t *lv;
+	int fsi, xsi, n;
+
+	/* get free entry slot index */
+	stbl = DT_GETSTBL(p);
+	tsi = stbl[ti];
+
+	/* open new linelock */
+	if (dtlck->index >= dtlck->maxcnt)
+		dtlck = (dtlock_t *) txLinelock(dtlck);
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+
+	lv->offset = tsi;
+
+	/* get the head/only segment */
+	t = &p->slot[tsi];
+	ASSERT(p->header.flag & BT_INTERNAL);
+	((idtentry_t *) t)->namlen = 0;
+	si = ((idtentry_t *) t)->next;
+	((idtentry_t *) t)->next = -1;
+
+	n = 1;
+	freecnt = 0;
+	fsi = si;
+	xsi = tsi;
+
+	/* find the last/only segment */
+	while (si >= 0) {
+		/* is next slot contiguous ? */
+		if (si != xsi + 1) {
+			/* close current linelock */
+			lv->length = n;
+			dtlck->index++;
+
+			/* open new linelock */
+			if (dtlck->index < dtlck->maxcnt)
+				lv++;
+			else {
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+				lv = (lv_t *) & dtlck->lv[0];
+			}
+
+			lv->offset = si;
+			n = 0;
+		}
+
+		n++;
+		xsi = si;
+		freecnt++;
+
+		t = &p->slot[si];
+		t->cnt = 1;
+		si = t->next;
+	}
+
+	/* close current linelock */
+	lv->length = n;
+	dtlck->index++;
+
+	*dtlock = dtlck;
+
+	/* update freelist */
+	if (freecnt == 0)
+		return;
+	t->next = p->header.freelist;
+	p->header.freelist = fsi;
+	p->header.freecnt += freecnt;
+}
+
+
+/*
+ *	dtLinelockFreelist()
+ */
+static void dtLinelockFreelist(dtpage_t * p,	/* directory page */
+			       int m,	/* max slot index */
+			       dtlock_t ** dtlock)
+{
+	int fsi;		/* free entry slot index */
+	dtslot_t *t;
+	int si;
+	dtlock_t *dtlck = *dtlock;
+	lv_t *lv;
+	int xsi, n;
+
+	/* get free entry slot index */
+	fsi = p->header.freelist;
+
+	/* open new linelock */
+	if (dtlck->index >= dtlck->maxcnt)
+		dtlck = (dtlock_t *) txLinelock(dtlck);
+	lv = (lv_t *) & dtlck->lv[dtlck->index];
+
+	lv->offset = fsi;
+
+	n = 1;
+	xsi = fsi;
+
+	t = &p->slot[fsi];
+	si = t->next;
+
+	/* find the last/only segment */
+	while (si < m && si >= 0) {
+		/* is next slot contiguous ? */
+		if (si != xsi + 1) {
+			/* close current linelock */
+			lv->length = n;
+			dtlck->index++;
+
+			/* open new linelock */
+			if (dtlck->index < dtlck->maxcnt)
+				lv++;
+			else {
+				dtlck = (dtlock_t *) txLinelock(dtlck);
+				lv = (lv_t *) & dtlck->lv[0];
+			}
+
+			lv->offset = si;
+			n = 0;
+		}
+
+		n++;
+		xsi = si;
+
+		t = &p->slot[si];
+		si = t->next;
+	}
+
+	/* close current linelock */
+	lv->length = n;
+	dtlck->index++;
+
+	*dtlock = dtlck;
+}
+
+
+/*
+ * NAME: dtModify
+ *
+ * FUNCTION: Modify the inode number part of a directory entry
+ *
+ * PARAMETERS:
+ *	tid	- Transaction id
+ *	ip	- Inode of parent directory
+ *	key	- Name of entry to be modified
+ *	orig_ino	- Original inode number expected in entry
+ *	new_ino	- New inode number to put into entry
+ *	flag	- JFS_RENAME
+ *
+ * RETURNS:
+ *	ESTALE	- If entry found does not match orig_ino passed in
+ *	ENOENT	- If no entry can be found to match key
+ *	0	- If successfully modified entry
+ */
+int dtModify(tid_t tid, struct inode *ip,
+	 component_t * key, ino_t * orig_ino, ino_t new_ino, int flag)
+{
+	int rc;
+	s64 bn;
+	metapage_t *mp;
+	dtpage_t *p;
+	int index;
+	btstack_t btstack;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	lv_t *lv;
+	s8 *stbl;
+	int entry_si;		/* entry slot index */
+	ldtentry_t *entry;
+
+	/*
+	 *      search for the entry to modify:
+	 *
+	 * dtSearch() returns (leaf page pinned, index at which to modify).
+	 */
+	if ((rc = dtSearch(ip, key, orig_ino, &btstack, flag)))
+		return rc;
+
+	/* retrieve search result */
+	DT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire a transaction lock on the leaf page of named entry
+	 */
+	tlck = txLock(tid, ip, mp, tlckDTREE | tlckENTRY);
+	dtlck = (dtlock_t *) & tlck->lock;
+
+	/* get slot index of the entry */
+	stbl = DT_GETSTBL(p);
+	entry_si = stbl[index];
+
+	/* linelock entry */
+	ASSERT(dtlck->index == 0);
+	lv = (lv_t *) & dtlck->lv[0];
+	lv->offset = entry_si;
+	lv->length = 1;
+	dtlck->index++;
+
+	/* get the head/only segment */
+	entry = (ldtentry_t *) & p->slot[entry_si];
+
+	/* substitute the inode number of the entry */
+	entry->inumber = cpu_to_le32(new_ino);
+
+	/* unpin the leaf page */
+	DT_PUTPAGE(mp);
+
+	return 0;
+}
+
+#ifdef _JFS_DEBUG_DTREE
+/*
+ *	dtDisplayTree()
+ *
+ * function: traverse forward
+ */
+int dtDisplayTree(struct inode *ip)
+{
+	int rc;
+	metapage_t *mp;
+	dtpage_t *p;
+	s64 bn, pbn;
+	int index, lastindex, v, h;
+	pxd_t *xd;
+	btstack_t btstack;
+	btframe_t *btsp;
+	btframe_t *parent;
+	u8 *stbl;
+	int psize = 256;
+
+	printk("display B+-tree.\n");
+
+	/* clear stack */
+	btsp = btstack.stack;
+
+	/*
+	 * start with root
+	 *
+	 * root resides in the inode
+	 */
+	bn = 0;
+	v = h = 0;
+
+	/*
+	 * first access of each page:
+	 */
+      newPage:
+	DT_GETPAGE(ip, bn, mp, psize, p, rc);
+	if (rc)
+		return rc;
+
+	/* process entries forward from first index */
+	index = 0;
+	lastindex = p->header.nextindex - 1;
+
+	if (p->header.flag & BT_INTERNAL) {
+		/*
+		 * first access of each internal page
+		 */
+		printf("internal page ");
+		dtDisplayPage(ip, bn, p);
+
+		goto getChild;
+	} else {		/* (p->header.flag & BT_LEAF) */
+
+		/*
+		 * first access of each leaf page
+		 */
+		printf("leaf page ");
+		dtDisplayPage(ip, bn, p);
+
+		/*
+		 * process leaf page entries
+		 *
+		 for ( ; index <= lastindex; index++)
+		 {
+		 }
+		 */
+
+		/* unpin the leaf page */
+		DT_PUTPAGE(mp);
+	}
+
+	/*
+	 * go back up to the parent page
+	 */
+      getParent:
+	/* pop/restore parent entry for the current child page */
+	if ((parent = (btsp == btstack.stack ? NULL : --btsp)) == NULL)
+		/* current page must have been root */
+		return;
+
+	/*
+	 * parent page scan completed
+	 */
+	if ((index = parent->index) == (lastindex = parent->lastindex)) {
+		/* go back up to the parent page */
+		goto getParent;
+	}
+
+	/*
+	 * parent page has entries remaining
+	 */
+	/* get back the parent page */
+	bn = parent->bn;
+	/* v = parent->level; */
+	DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/* get next parent entry */
+	index++;
+
+	/*
+	 * internal page: go down to child page of current entry
+	 */
+      getChild:
+	/* push/save current parent entry for the child page */
+	btsp->bn = pbn = bn;
+	btsp->index = index;
+	btsp->lastindex = lastindex;
+	/* btsp->level = v; */
+	/* btsp->node = h; */
+	++btsp;
+
+	/* get current entry for the child page */
+	stbl = DT_GETSTBL(p);
+	xd = (pxd_t *) & p->slot[stbl[index]];
+
+	/*
+	 * first access of each internal entry:
+	 */
+
+	/* get child page */
+	bn = addressPXD(xd);
+	psize = lengthPXD(xd) << ip->i_ipmnt->i_l2bsize;
+
+	printk("traverse down 0x%Lx[%d]->0x%Lx\n", pbn, index, bn);
+	v++;
+	h = index;
+
+	/* release parent page */
+	DT_PUTPAGE(mp);
+
+	/* process the child page */
+	goto newPage;
+}
+
+
+/*
+ *	dtDisplayPage()
+ *
+ * function: display page
+ */
+int dtDisplayPage(struct inode *ip, s64 bn, dtpage_t * p)
+{
+	int rc;
+	metapage_t *mp;
+	ldtentry_t *lh;
+	idtentry_t *ih;
+	pxd_t *xd;
+	int i, j;
+	u8 *stbl;
+	wchar_t name[JFS_NAME_MAX + 1];
+	component_t key = { 0, name };
+	int freepage = 0;
+
+	if (p == NULL) {
+		freepage = 1;
+		DT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+	}
+
+	/* display page control */
+	printk("bn:0x%Lx flag:0x%08x nextindex:%d\n",
+	       bn, p->header.flag, p->header.nextindex);
+
+	/* display entries */
+	stbl = DT_GETSTBL(p);
+	for (i = 0, j = 1; i < p->header.nextindex; i++, j++) {
+		dtGetKey(p, i, &key, JFS_SBI(ip->i_sb)->mntflag);
+		key.name[key.namlen] = '\0';
+		if (p->header.flag & BT_LEAF) {
+			lh = (ldtentry_t *) & p->slot[stbl[i]];
+			printf("\t[%d] %s:%d", i, key.name,
+			       le32_to_cpu(lh->inumber));
+		} else {
+			ih = (idtentry_t *) & p->slot[stbl[i]];
+			xd = (pxd_t *) ih;
+			bn = addressPXD(xd);
+			printf("\t[%d] %s:0x%Lx", i, key.name, bn);
+		}
+
+		if (j == 4) {
+			printf("\n");
+			j = 0;
+		}
+	}
+
+	printf("\n");
+
+	if (freepage)
+		DT_PUTPAGE(mp);
+
+	return 0;
+}
+#endif				/* _JFS_DEBUG_DTREE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_dtree.h linuxppc64_2_4/fs/jfs/jfs_dtree.h
--- linux-2.4.19/fs/jfs/jfs_dtree.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_dtree.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,288 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+/*
+ * Change History :
+ *
+ */
+
+#ifndef _H_JFS_DTREE
+#define	_H_JFS_DTREE
+
+/*
+ *	jfs_dtree.h: directory B+-tree manager
+ */
+
+#include "jfs_btree.h"
+
+typedef union {
+	struct {
+		tid_t tid;
+		struct inode *ip;
+		u32 ino;
+	} leaf;
+	pxd_t xd;
+} ddata_t;
+
+
+/*
+ *      entry segment/slot
+ *
+ * an entry consists of type dependent head/only segment/slot and
+ * additional segments/slots linked vi next field;
+ * N.B. last/only segment of entry is terminated by next = -1;
+ */
+/*
+ *	directory page slot
+ */
+typedef struct {
+	s8 next;		/* 1: */
+	s8 cnt;			/* 1: */
+	wchar_t name[15];	/* 30: */
+} dtslot_t;			/* (32) */
+
+
+#define DATASLOTSIZE	16
+#define L2DATASLOTSIZE	4
+#define	DTSLOTSIZE	32
+#define	L2DTSLOTSIZE	5
+#define DTSLOTHDRSIZE	2
+#define DTSLOTDATASIZE	30
+#define DTSLOTDATALEN	15
+
+/*
+ *	 internal node entry head/only segment
+ */
+typedef struct {
+	pxd_t xd;		/* 8: child extent descriptor */
+
+	s8 next;		/* 1: */
+	u8 namlen;		/* 1: */
+	wchar_t name[11];	/* 22: 2-byte aligned */
+} idtentry_t;			/* (32) */
+
+#define DTIHDRSIZE	10
+#define DTIHDRDATALEN	11
+
+/* compute number of slots for entry */
+#define	NDTINTERNAL(klen) ( ((4 + (klen)) + (15 - 1)) / 15 )
+
+
+/*
+ *	leaf node entry head/only segment
+ *
+ * 	For legacy filesystems, name contains 13 wchars -- no index field
+ */
+typedef struct {
+	u32 inumber;		/* 4: 4-byte aligned */
+	s8 next;		/* 1: */
+	u8 namlen;		/* 1: */
+	wchar_t name[11];	/* 22: 2-byte aligned */
+	u32 index;		/* 4: index into dir_table */
+} ldtentry_t;			/* (32) */
+
+#define DTLHDRSIZE	6
+#define DTLHDRDATALEN_LEGACY	13	/* Old (OS/2) format */
+#define DTLHDRDATALEN	11
+
+/*
+ * dir_table used for directory traversal during readdir
+ */
+
+/*
+ * Keep persistent index for directory entries
+ */
+#define DO_INDEX(INODE) (JFS_SBI((INODE)->i_sb)->mntflag & JFS_DIR_INDEX)
+
+/*
+ * Maximum entry in inline directory table
+ */
+#define MAX_INLINE_DIRTABLE_ENTRY 13
+
+typedef struct dir_table_slot {
+	u8 rsrvd;		/* 1: */
+	u8 flag;		/* 1: 0 if free */
+	u8 slot;		/* 1: slot within leaf page of entry */
+	u8 addr1;		/* 1: upper 8 bits of leaf page address */
+	u32 addr2;		/* 4: lower 32 bits of leaf page address -OR-
+				   index of next entry when this entry was deleted */
+} dir_table_slot_t;		/* (8) */
+
+/*
+ * flag values
+ */
+#define DIR_INDEX_VALID 1
+#define DIR_INDEX_FREE 0
+
+#define DTSaddress(dir_table_slot, address64)\
+{\
+	(dir_table_slot)->addr1 = ((u64)address64) >> 32;\
+	(dir_table_slot)->addr2 = __cpu_to_le32((address64) & 0xffffffff);\
+}
+
+#define addressDTS(dts)\
+	( ((s64)((dts)->addr1)) << 32 | __le32_to_cpu((dts)->addr2) )
+
+/* compute number of slots for entry */
+#define	NDTLEAF_LEGACY(klen)	( ((2 + (klen)) + (15 - 1)) / 15 )
+#define	NDTLEAF	NDTINTERNAL
+
+
+/*
+ *	directory root page (in-line in on-disk inode):
+ *
+ * cf. dtpage_t below.
+ */
+typedef union {
+	struct {
+		dasd_t DASD;	/* 16: DASD limit/usage info  F226941 */
+
+		u8 flag;	/* 1: */
+		u8 nextindex;	/* 1: next free entry in stbl */
+		s8 freecnt;	/* 1: free count */
+		s8 freelist;	/* 1: freelist header */
+
+		u32 idotdot;	/* 4: parent inode number */
+
+		s8 stbl[8];	/* 8: sorted entry index table */
+	} header;		/* (32) */
+
+	dtslot_t slot[9];
+} dtroot_t;
+
+#define PARENT(IP) \
+	(le32_to_cpu(JFS_IP(IP)->i_dtroot.header.idotdot))
+
+#define DTROOTMAXSLOT	9
+
+#define	dtEmpty(IP) (JFS_IP(IP)->i_dtroot.header.nextindex == 0)
+
+
+/*
+ *	directory regular page:
+ *
+ *	entry slot array of 32 byte slot
+ *
+ * sorted entry slot index table (stbl):
+ * contiguous slots at slot specified by stblindex,
+ * 1-byte per entry
+ *   512 byte block:  16 entry tbl (1 slot)
+ *  1024 byte block:  32 entry tbl (1 slot)
+ *  2048 byte block:  64 entry tbl (2 slot)
+ *  4096 byte block: 128 entry tbl (4 slot)
+ *
+ * data area:
+ *   512 byte block:  16 - 2 =  14 slot
+ *  1024 byte block:  32 - 2 =  30 slot
+ *  2048 byte block:  64 - 3 =  61 slot
+ *  4096 byte block: 128 - 5 = 123 slot
+ *
+ * N.B. index is 0-based; index fields refer to slot index
+ * except nextindex which refers to entry index in stbl;
+ * end of entry stot list or freelist is marked with -1.
+ */
+typedef union {
+	struct {
+		s64 next;	/* 8: next sibling */
+		s64 prev;	/* 8: previous sibling */
+
+		u8 flag;	/* 1: */
+		u8 nextindex;	/* 1: next entry index in stbl */
+		s8 freecnt;	/* 1: */
+		s8 freelist;	/* 1: slot index of head of freelist */
+
+		u8 maxslot;	/* 1: number of slots in page slot[] */
+		u8 stblindex;	/* 1: slot index of start of stbl */
+		u8 rsrvd[2];	/* 2: */
+
+		pxd_t self;	/* 8: self pxd */
+	} header;		/* (32) */
+
+	dtslot_t slot[128];
+} dtpage_t;
+
+#define DTPAGEMAXSLOT        128
+
+#define DT8THPGNODEBYTES     512
+#define DT8THPGNODETSLOTS      1
+#define DT8THPGNODESLOTS      16
+
+#define DTQTRPGNODEBYTES    1024
+#define DTQTRPGNODETSLOTS      1
+#define DTQTRPGNODESLOTS      32
+
+#define DTHALFPGNODEBYTES   2048
+#define DTHALFPGNODETSLOTS     2
+#define DTHALFPGNODESLOTS     64
+
+#define DTFULLPGNODEBYTES   4096
+#define DTFULLPGNODETSLOTS     4
+#define DTFULLPGNODESLOTS    128
+
+#define DTENTRYSTART	1
+
+/* get sorted entry table of the page */
+#define DT_GETSTBL(p) ( ((p)->header.flag & BT_ROOT) ?\
+	((dtroot_t *)(p))->header.stbl : \
+	(s8 *)&(p)->slot[(p)->header.stblindex] )
+
+/*
+ * Flags for dtSearch
+ */
+#define JFS_CREATE 1
+#define JFS_LOOKUP 2
+#define JFS_REMOVE 3
+#define JFS_RENAME 4
+
+#define DIRENTSIZ(namlen) \
+    ( (sizeof(struct dirent) - 2*(JFS_NAME_MAX+1) + 2*((namlen)+1) + 3) &~ 3 )
+
+/*
+ * Maximum file offset for directories.
+ */
+#define DIREND	INT_MAX
+
+/*
+ *	external declarations
+ */
+extern void dtInitRoot(tid_t tid, struct inode *ip, u32 idotdot);
+
+extern int dtSearch(struct inode *ip, component_t * key,
+		    ino_t * data, btstack_t * btstack, int flag);
+
+extern int dtInsert(tid_t tid, struct inode *ip,
+		    component_t * key, ino_t * ino, btstack_t * btstack);
+
+extern int dtDelete(tid_t tid,
+		    struct inode *ip, component_t * key, ino_t * data, int flag);
+
+extern int dtRelocate(tid_t tid,
+		      struct inode *ip, s64 lmxaddr, pxd_t * opxd, s64 nxaddr);
+
+extern int dtModify(tid_t tid, struct inode *ip,
+		    component_t * key, ino_t * orig_ino, ino_t new_ino, int flag);
+
+extern int jfs_readdir(struct file *filp, void *dirent, filldir_t filldir);
+
+#ifdef  _JFS_DEBUG_DTREE
+extern int dtDisplayTree(struct inode *ip);
+
+extern int dtDisplayPage(struct inode *ip, s64 bn, dtpage_t * p);
+#endif				/* _JFS_DEBUG_DTREE */
+
+#endif				/* !_H_JFS_DTREE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_extendfs.h linuxppc64_2_4/fs/jfs/jfs_extendfs.h
--- linux-2.4.19/fs/jfs/jfs_extendfs.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_extendfs.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,39 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef	_H_JFS_EXTENDFS
+#define _H_JFS_EXTENDFS
+
+/*
+ *	jfs_extendfs.h
+ */
+/*
+ *	extendfs parameter list
+ */
+typedef struct {
+	u32 flag;		/* 4: */
+	u8 dev;			/* 1: */
+	u8 pad[3];		/* 3: */
+	s64 LVSize;		/* 8: LV size in LV block */
+	s64 FSSize;		/* 8: FS size in LV block */
+	s32 LogSize;		/* 4: inlinelog size in LV block */
+} extendfs_t;			/* (28) */
+
+/* plist flag */
+#define EXTENDFS_QUERY		0x00000001
+
+#endif				/* _H_JFS_EXTENDFS */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_extent.c linuxppc64_2_4/fs/jfs/jfs_extent.c
--- linux-2.4.19/fs/jfs/jfs_extent.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_extent.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,637 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ *
+ *
+ * Module: jfs_extent.c:
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+#include "jfs_dmap.h"
+#include "jfs_extent.h"
+#include "jfs_debug.h"
+
+/*
+ * forward references
+ */
+static int extBalloc(struct inode *, s64, s64 *, s64 *);
+static int extBrealloc(struct inode *, s64, s64, s64 *, s64 *);
+int extRecord(struct inode *, xad_t *);
+static s64 extRoundDown(s64 nb);
+
+/*
+ * external references
+ */
+extern int dbExtend(struct inode *, s64, s64, s64);
+extern int jfs_commit_inode(struct inode *, int);
+
+
+#define DPD(a)          (printk("(a): %d\n",(a)))
+#define DPC(a)          (printk("(a): %c\n",(a)))
+#define DPL1(a)					\
+{						\
+	if ((a) >> 32)				\
+		printk("(a): %x%08x  ",(a));	\
+	else					\
+		printk("(a): %x  ",(a) << 32);	\
+}
+#define DPL(a)					\
+{						\
+	if ((a) >> 32)				\
+		printk("(a): %x%08x\n",(a));	\
+	else					\
+		printk("(a): %x\n",(a) << 32);	\
+}
+
+#define DPD1(a)         (printk("(a): %d  ",(a)))
+#define DPX(a)          (printk("(a): %08x\n",(a)))
+#define DPX1(a)         (printk("(a): %08x  ",(a)))
+#define DPS(a)          (printk("%s\n",(a)))
+#define DPE(a)          (printk("\nENTERING: %s\n",(a)))
+#define DPE1(a)          (printk("\nENTERING: %s",(a)))
+#define DPS1(a)         (printk("  %s  ",(a)))
+
+
+/*
+ * NAME:	extAlloc()
+ *
+ * FUNCTION:    allocate an extent for a specified page range within a
+ *		file.
+ *
+ * PARAMETERS:
+ *	ip	- the inode of the file.
+ *	xlen	- requested extent length.
+ *	pno	- the starting page number with the file.
+ *	xp	- pointer to an xad.  on entry, xad describes an
+ *		  extent that is used as an allocation hint if the
+ *		  xaddr of the xad is non-zero.  on successful exit,
+ *		  the xad describes the newly allocated extent.
+ *	abnr	- boolean_t indicating whether the newly allocated extent
+ *		  should be marked as allocated but not recorded.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+int
+extAlloc(struct inode *ip, s64 xlen, s64 pno, xad_t * xp, boolean_t abnr)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	s64 nxlen, nxaddr, xoff, hint, xaddr = 0;
+	int rc, nbperpage;
+	int xflag;
+
+	/* This blocks if we are low on resources */
+	txBeginAnon(ip->i_sb);
+
+	/* validate extent length */
+	if (xlen > MAXXLEN)
+		xlen = MAXXLEN;
+
+	/* get the number of blocks per page */
+	nbperpage = sbi->nbperpage;
+
+	/* get the page's starting extent offset */
+	xoff = pno << sbi->l2nbperpage;
+
+	/* check if an allocation hint was provided */
+	if ((hint = addressXAD(xp))) {
+		/* get the size of the extent described by the hint */
+		nxlen = lengthXAD(xp);
+
+		/* check if the hint is for the portion of the file
+		 * immediately previous to the current allocation
+		 * request and if hint extent has the same abnr
+		 * value as the current request.  if so, we can
+		 * extend the hint extent to include the current
+		 * extent if we can allocate the blocks immediately
+		 * following the hint extent.
+		 */
+		if (offsetXAD(xp) + nxlen == xoff &&
+		    abnr == ((xp->flag & XAD_NOTRECORDED) ? TRUE : FALSE))
+			xaddr = hint + nxlen;
+
+		/* adjust the hint to the last block of the extent */
+		hint += (nxlen - 1);
+	}
+
+	/* allocate the disk blocks for the extent.  initially, extBalloc()
+	 * will try to allocate disk blocks for the requested size (xlen). 
+	 * if this fails (xlen contigious free blocks not avaliable), it'll
+	 * try to allocate a smaller number of blocks (producing a smaller
+	 * extent), with this smaller number of blocks consisting of the
+	 * requested number of blocks rounded down to the next smaller
+	 * power of 2 number (i.e. 16 -> 8).  it'll continue to round down
+	 * and retry the allocation until the number of blocks to allocate
+	 * is smaller than the number of blocks per page.
+	 */
+	nxlen = xlen;
+	if ((rc =
+	     extBalloc(ip, hint ? hint : INOHINT(ip), &nxlen, &nxaddr))) {
+		return (rc);
+	}
+
+	/* determine the value of the extent flag */
+	xflag = (abnr == TRUE) ? XAD_NOTRECORDED : 0;
+
+	/* if we can extend the hint extent to cover the current request, 
+	 * extend it.  otherwise, insert a new extent to
+	 * cover the current request.
+	 */
+	if (xaddr && xaddr == nxaddr)
+		rc = xtExtend(0, ip, xoff, (int) nxlen, 0);
+	else
+		rc = xtInsert(0, ip, xflag, xoff, (int) nxlen, &nxaddr, 0);
+
+	/* if the extend or insert failed, 
+	 * free the newly allocated blocks and return the error.
+	 */
+	if (rc) {
+		dbFree(ip, nxaddr, nxlen);
+		return (rc);
+	}
+
+	/* update the number of blocks allocated to the file */
+	ip->i_blocks += LBLK2PBLK(ip->i_sb, nxlen);
+
+	/* set the results of the extent allocation */
+	XADaddress(xp, nxaddr);
+	XADlength(xp, nxlen);
+	XADoffset(xp, xoff);
+	xp->flag = xflag;
+
+	mark_inode_dirty(ip);
+
+	/*
+	 * COMMIT_SyncList flags an anonymous tlock on page that is on
+	 * sync list.
+	 * We need to commit the inode to get the page written disk.
+	 */
+	if (test_and_clear_cflag(COMMIT_Synclist,ip))
+		jfs_commit_inode(ip, 0);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        extRealloc()
+ *
+ * FUNCTION:    extend the allocation of a file extent containing a
+ *		partial back last page.
+ *
+ * PARAMETERS:
+ *	ip	- the inode of the file.
+ *	cp	- cbuf for the partial backed last page.
+ *	xlen	- request size of the resulting extent.
+ *	xp	- pointer to an xad. on successful exit, the xad
+ *		  describes the newly allocated extent.
+ *	abnr	- boolean_t indicating whether the newly allocated extent
+ *		  should be marked as allocated but not recorded.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+int extRealloc(struct inode *ip, s64 nxlen, xad_t * xp, boolean_t abnr)
+{
+	struct super_block *sb = ip->i_sb;
+	s64 xaddr, xlen, nxaddr, delta, xoff;
+	s64 ntail, nextend, ninsert;
+	int rc, nbperpage = JFS_SBI(sb)->nbperpage;
+	int xflag;
+
+	/* This blocks if we are low on resources */
+	txBeginAnon(ip->i_sb);
+
+	/* validate extent length */
+	if (nxlen > MAXXLEN)
+		nxlen = MAXXLEN;
+
+	/* get the extend (partial) page's disk block address and
+	 * number of blocks.
+	 */
+	xaddr = addressXAD(xp);
+	xlen = lengthXAD(xp);
+	xoff = offsetXAD(xp);
+
+	/* if the extend page is abnr and if the request is for
+	 * the extent to be allocated and recorded, 
+	 * make the page allocated and recorded.
+	 */
+	if ((xp->flag & XAD_NOTRECORDED) && !abnr) {
+		xp->flag = 0;
+		if ((rc = xtUpdate(0, ip, xp)))
+			return (rc);
+	}
+
+	/* try to allocated the request number of blocks for the
+	 * extent.  dbRealloc() first tries to satisfy the request
+	 * by extending the allocation in place. otherwise, it will
+	 * try to allocate a new set of blocks large enough for the
+	 * request.  in satisfying a request, dbReAlloc() may allocate
+	 * less than what was request but will always allocate enough
+	 * space as to satisfy the extend page.
+	 */
+	if ((rc = extBrealloc(ip, xaddr, xlen, &nxlen, &nxaddr)))
+		return (rc);
+
+	delta = nxlen - xlen;
+
+	/* check if the extend page is not abnr but the request is abnr
+	 * and the allocated disk space is for more than one page.  if this
+	 * is the case, there is a miss match of abnr between the extend page
+	 * and the one or more pages following the extend page.  as a result,
+	 * two extents will have to be manipulated. the first will be that
+	 * of the extent of the extend page and will be manipulated thru
+	 * an xtExtend() or an xtTailgate(), depending upon whether the
+	 * disk allocation occurred as an inplace extension.  the second
+	 * extent will be manipulated (created) through an xtInsert() and
+	 * will be for the pages following the extend page.
+	 */
+	if (abnr && (!(xp->flag & XAD_NOTRECORDED)) && (nxlen > nbperpage)) {
+		ntail = nbperpage;
+		nextend = ntail - xlen;
+		ninsert = nxlen - nbperpage;
+
+		xflag = XAD_NOTRECORDED;
+	} else {
+		ntail = nxlen;
+		nextend = delta;
+		ninsert = 0;
+
+		xflag = xp->flag;
+	}
+
+	/* if we were able to extend the disk allocation in place,
+	 * extend the extent.  otherwise, move the extent to a
+	 * new disk location.
+	 */
+	if (xaddr == nxaddr) {
+		/* extend the extent */
+		if ((rc = xtExtend(0, ip, xoff + xlen, (int) nextend, 0))) {
+			dbFree(ip, xaddr + xlen, delta);
+			return (rc);
+		}
+	} else {
+		/*
+		 * move the extent to a new location:
+		 *
+		 * xtTailgate() accounts for relocated tail extent;
+		 */
+		if ((rc = xtTailgate(0, ip, xoff, (int) ntail, nxaddr, 0))) {
+			dbFree(ip, nxaddr, nxlen);
+			return (rc);
+		}
+	}
+
+
+	/* check if we need to also insert a new extent */
+	if (ninsert) {
+		/* perform the insert.  if it fails, free the blocks
+		 * to be inserted and make it appear that we only did
+		 * the xtExtend() or xtTailgate() above.
+		 */
+		xaddr = nxaddr + ntail;
+		if (xtInsert (0, ip, xflag, xoff + ntail, (int) ninsert,
+			      &xaddr, 0)) {
+			dbFree(ip, xaddr, (s64) ninsert);
+			delta = nextend;
+			nxlen = ntail;
+			xflag = 0;
+		}
+	}
+
+	/* update the inode with the number of blocks allocated */
+	ip->i_blocks += LBLK2PBLK(sb, delta);
+
+	/* set the return results */
+	XADaddress(xp, nxaddr);
+	XADlength(xp, nxlen);
+	XADoffset(xp, xoff);
+	xp->flag = xflag;
+
+	mark_inode_dirty(ip);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        extHint()
+ *
+ * FUNCTION:    produce an extent allocation hint for a file offset.
+ *
+ * PARAMETERS:
+ *	ip	- the inode of the file.
+ *	offset  - file offset for which the hint is needed.
+ *	xp	- pointer to the xad that is to be filled in with
+ *		  the hint.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ */
+int extHint(struct inode *ip, s64 offset, xad_t * xp)
+{
+	struct super_block *sb = ip->i_sb;
+	xadlist_t xadl;
+	lxdlist_t lxdl;
+	lxd_t lxd;
+	s64 prev;
+	int rc, nbperpage = JFS_SBI(sb)->nbperpage;
+
+	/* init the hint as "no hint provided" */
+	XADaddress(xp, 0);
+
+	/* determine the starting extent offset of the page previous
+	 * to the page containing the offset.
+	 */
+	prev = ((offset & ~POFFSET) >> JFS_SBI(sb)->l2bsize) - nbperpage;
+
+	/* if the offsets in the first page of the file,
+	 * no hint provided.
+	 */
+	if (prev < 0)
+		return (0);
+
+	/* prepare to lookup the previous page's extent info */
+	lxdl.maxnlxd = 1;
+	lxdl.nlxd = 1;
+	lxdl.lxd = &lxd;
+	LXDoffset(&lxd, prev)
+	    LXDlength(&lxd, nbperpage);
+
+	xadl.maxnxad = 1;
+	xadl.nxad = 0;
+	xadl.xad = xp;
+
+	/* perform the lookup */
+	if ((rc = xtLookupList(ip, &lxdl, &xadl, 0)))
+		return (rc);
+
+	/* check if not extent exists for the previous page.  
+	 * this is possible for sparse files.
+	 */
+	if (xadl.nxad == 0) {
+//              assert(ISSPARSE(ip));
+		return (0);
+	}
+
+	/* only preserve the abnr flag within the xad flags
+	 * of the returned hint.
+	 */
+	xp->flag &= XAD_NOTRECORDED;
+
+	assert(xadl.nxad == 1);
+	assert(lengthXAD(xp) == nbperpage);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        extRecord()
+ *
+ * FUNCTION:    change a page with a file from not recorded to recorded.
+ *
+ * PARAMETERS:
+ *	ip	- inode of the file.
+ *	cp	- cbuf of the file page.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+int extRecord(struct inode *ip, xad_t * xp)
+{
+	int rc;
+
+	txBeginAnon(ip->i_sb);
+
+	/* update the extent */
+	if ((rc = xtUpdate(0, ip, xp)))
+		return (rc);
+
+#ifdef _STILL_TO_PORT
+	/* no longer abnr */
+	cp->cm_abnr = FALSE;
+
+	/* mark the cbuf as modified */
+	cp->cm_modified = TRUE;
+#endif				/*  _STILL_TO_PORT */
+
+	return (0);
+}
+
+
+/*
+ * NAME:        extFill()
+ *
+ * FUNCTION:    allocate disk space for a file page that represents
+ *		a file hole.
+ *
+ * PARAMETERS:
+ *	ip	- the inode of the file.
+ *	cp	- cbuf of the file page represent the hole.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+int extFill(struct inode *ip, xad_t * xp)
+{
+	int rc, nbperpage = JFS_SBI(ip->i_sb)->nbperpage;
+	s64 blkno = offsetXAD(xp) >> ip->i_blksize;
+
+//      assert(ISSPARSE(ip));
+
+	/* initialize the extent allocation hint */
+	XADaddress(xp, 0);
+
+	/* allocate an extent to fill the hole */
+	if ((rc = extAlloc(ip, nbperpage, blkno, xp, FALSE)))
+		return (rc);
+
+	assert(lengthPXD(xp) == nbperpage);
+
+	return (0);
+}
+
+
+/*
+ * NAME:	extBalloc()
+ *
+ * FUNCTION:    allocate disk blocks to form an extent.
+ *
+ *		initially, we will try to allocate disk blocks for the
+ *		requested size (nblocks).  if this fails (nblocks 
+ *		contigious free blocks not avaliable), we'll try to allocate
+ *		a smaller number of blocks (producing a smaller extent), with
+ *		this smaller number of blocks consisting of the requested
+ *		number of blocks rounded down to the next smaller power of 2
+ *		number (i.e. 16 -> 8).  we'll continue to round down and
+ *		retry the allocation until the number of blocks to allocate
+ *		is smaller than the number of blocks per page.
+ *		
+ * PARAMETERS:
+ *	ip	 - the inode of the file.
+ *	hint	 - disk block number to be used as an allocation hint.
+ *	*nblocks - pointer to an s64 value.  on entry, this value specifies
+ *		   the desired number of block to be allocated. on successful
+ *		   exit, this value is set to the number of blocks actually
+ *		   allocated.
+ *	blkno	 - pointer to a block address that is filled in on successful
+ *		   return with the starting block number of the newly 
+ *		   allocated block range.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+static int
+extBalloc(struct inode *ip, s64 hint, s64 * nblocks, s64 * blkno)
+{
+	s64 nb, nblks, daddr, max;
+	int rc, nbperpage = JFS_SBI(ip->i_sb)->nbperpage;
+	bmap_t *mp = JFS_SBI(ip->i_sb)->bmap;
+
+	/* get the number of blocks to initially attempt to allocate.
+	 * we'll first try the number of blocks requested unless this
+	 * number is greater than the maximum number of contigious free
+	 * blocks in the map. in that case, we'll start off with the 
+	 * maximum free.
+	 */
+	max = (s64) 1 << mp->db_maxfreebud;
+	if (*nblocks >= max && *nblocks > nbperpage)
+		nb = nblks = (max > nbperpage) ? max : nbperpage;
+	else
+		nb = nblks = *nblocks;
+
+	/* try to allocate blocks */
+	while ((rc = dbAlloc(ip, hint, nb, &daddr))) {
+		/* if something other than an out of space error,
+		 * stop and return this error.
+		 */
+		if (rc != ENOSPC)
+			return (rc);
+
+		/* decrease the allocation request size */
+		nb = min(nblks, extRoundDown(nb));
+
+		/* give up if we cannot cover a page */
+		if (nb < nbperpage)
+			return (rc);
+	}
+
+	*nblocks = nb;
+	*blkno = daddr;
+
+	return (0);
+}
+
+
+/*
+ * NAME:	extBrealloc()
+ *
+ * FUNCTION:    attempt to extend an extent's allocation.
+ *
+ *		initially, we will try to extend the extent's allocation
+ *		in place.  if this fails, we'll try to move the extent
+ *		to a new set of blocks. if moving the extent, we initially
+ *		will try to allocate disk blocks for the requested size
+ *		(nnew).  if this fails 	(nnew contigious free blocks not
+ *		avaliable), we'll try  to allocate a smaller number of
+ *		blocks (producing a smaller extent), with this smaller
+ *		number of blocks consisting of the requested number of
+ *		blocks rounded down to the next smaller power of 2
+ *		number (i.e. 16 -> 8).  we'll continue to round down and
+ *		retry the allocation until the number of blocks to allocate
+ *		is smaller than the number of blocks per page.
+ *		
+ * PARAMETERS:
+ *	ip	 - the inode of the file.
+ *	blkno    - starting block number of the extents current allocation.
+ *	nblks    - number of blocks within the extents current allocation.
+ *	newnblks - pointer to a s64 value.  on entry, this value is the
+ *		   the new desired extent size (number of blocks).  on
+ *		   successful exit, this value is set to the extent's actual
+ *		   new size (new number of blocks).
+ *	newblkno - the starting block number of the extents new allocation.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO	- i/o error.
+ *      ENOSPC	- insufficient disk resources.
+ */
+static int
+extBrealloc(struct inode *ip,
+	    s64 blkno, s64 nblks, s64 * newnblks, s64 * newblkno)
+{
+	int rc;
+
+	/* try to extend in place */
+	if ((rc = dbExtend(ip, blkno, nblks, *newnblks - nblks)) == 0) {
+		*newblkno = blkno;
+		return (0);
+	} else {
+		if (rc != ENOSPC)
+			return (rc);
+	}
+
+	/* in place extension not possible.  
+	 * try to move the extent to a new set of blocks.
+	 */
+	return (extBalloc(ip, blkno, newnblks, newblkno));
+}
+
+
+/*
+ * NAME:        extRoundDown()
+ *
+ * FUNCTION:    round down a specified number of blocks to the next
+ *		smallest power of 2 number.
+ *
+ * PARAMETERS:
+ *	nb	- the inode of the file.
+ *
+ * RETURN VALUES:
+ *      next smallest power of 2 number.
+ */
+static s64 extRoundDown(s64 nb)
+{
+	int i;
+	u64 m, k;
+
+	for (i = 0, m = (u64) 1 << 63; i < 64; i++, m >>= 1) {
+		if (m & nb)
+			break;
+	}
+
+	i = 63 - i;
+	k = (u64) 1 << i;
+	k = ((k - 1) & nb) ? k : k >> 1;
+
+	return (k);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_extent.h linuxppc64_2_4/fs/jfs/jfs_extent.h
--- linux-2.4.19/fs/jfs/jfs_extent.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_extent.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,31 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef	_H_JFS_EXTENT
+#define _H_JFS_EXTENT
+
+/*  get block allocation allocation hint as location of disk inode */
+#define	INOHINT(ip)	\
+	(addressPXD(&(JFS_IP(ip)->ixpxd)) + lengthPXD(&(JFS_IP(ip)->ixpxd)) - 1)
+
+extern int	extAlloc(struct inode *, s64, s64, xad_t *, boolean_t);
+extern int	extFill(struct inode *, xad_t *);
+extern int	extHint(struct inode *, s64, xad_t *);
+extern int	extRealloc(struct inode *, s64, xad_t *, boolean_t);
+extern int	extRecord(struct inode *, xad_t *);
+
+#endif	/* _H_JFS_EXTENT */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_filsys.h linuxppc64_2_4/fs/jfs/jfs_filsys.h
--- linux-2.4.19/fs/jfs/jfs_filsys.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_filsys.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,274 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/
+
+#ifndef _H_JFS_FILSYS
+#define _H_JFS_FILSYS
+
+/*
+ *	jfs_filsys.h
+ *
+ * file system (implementation-dependent) constants 
+ *
+ * refer to <limits.h> for system wide implementation-dependent constants 
+ */
+
+/*
+ *	 file system option (superblock flag)
+ */
+/* platform option (conditional compilation) */
+#define JFS_AIX		0x80000000	/* AIX support */
+/*	POSIX name/directory  support */
+
+#define JFS_OS2		0x40000000	/* OS/2 support */
+/*	case-insensitive name/directory support */
+
+#define JFS_DFS		0x20000000	/* DCE DFS LFS support */
+
+#define JFS_LINUX      	0x10000000	/* Linux support */
+/*	case-sensitive name/directory support */
+
+/* directory option */
+#define JFS_UNICODE	0x00000001	/* unicode name */
+
+/* commit option */
+#define	JFS_COMMIT	0x00000f00	/* commit option mask */
+#define	JFS_GROUPCOMMIT	0x00000100	/* group (of 1) commit */
+#define	JFS_LAZYCOMMIT	0x00000200	/* lazy commit */
+#define	JFS_TMPFS	0x00000400	/* temporary file system - 
+					 * do not log/commit:
+					 */
+
+/* log logical volume option */
+#define	JFS_INLINELOG	0x00000800	/* inline log within file system */
+#define JFS_INLINEMOVE	0x00001000	/* inline log being moved */
+
+/* Secondary aggregate inode table */
+#define JFS_BAD_SAIT	0x00010000	/* current secondary ait is bad */
+
+/* sparse regular file support */
+#define JFS_SPARSE	0x00020000	/* sparse regular file */
+
+/* DASD Limits		F226941 */
+#define JFS_DASD_ENABLED	0x00040000	/* DASD limits enabled */
+#define	JFS_DASD_PRIME		0x00080000	/* Prime DASD usage on boot */
+
+/* big endian flag */
+#define	JFS_SWAP_BYTES		0x00100000	/* running on big endian computer */
+
+/* Directory index */
+#define JFS_DIR_INDEX		0x00200000	/* Persistant index for */
+						/* directory entries    */
+
+
+/*
+ *	buffer cache configuration
+ */
+/* page size */
+#ifdef PSIZE
+#undef PSIZE
+#endif
+#define	PSIZE		4096	/* page size (in byte) */
+#define	L2PSIZE		12	/* log2(PSIZE) */
+#define	POFFSET		4095	/* offset within page */
+
+/* buffer page size */
+#define BPSIZE	PSIZE
+
+/*
+ *	fs fundamental size
+ *
+ * PSIZE >= file system block size >= PBSIZE >= DISIZE
+ */
+#define	PBSIZE		512	/* physical block size (in byte) */
+#define	L2PBSIZE	9	/* log2(PBSIZE) */
+
+#define DISIZE		512	/* on-disk inode size (in byte) */
+#define L2DISIZE	9	/* log2(DISIZE) */
+
+#define IDATASIZE	256	/* inode inline data size */
+#define	IXATTRSIZE	128	/* inode inline extended attribute size */
+
+#define XTPAGE_SIZE     4096
+#define log2_PAGESIZE     12
+
+#define IAG_SIZE        4096
+#define IAG_EXTENT_SIZE 4096
+#define	INOSPERIAG	4096	/* number of disk inodes per iag */
+#define	L2INOSPERIAG	12	/* l2 number of disk inodes per iag */
+#define INOSPEREXT	32	/* number of disk inode per extent */
+#define L2INOSPEREXT	5	/* l2 number of disk inode per extent */
+#define	IXSIZE		(DISIZE * INOSPEREXT)	/* inode extent size */
+#define	INOSPERPAGE	8	/* number of disk inodes per 4K page */
+#define	L2INOSPERPAGE	3	/* log2(INOSPERPAGE) */
+
+#define	IAGFREELIST_LWM	64
+
+#define INODE_EXTENT_SIZE	IXSIZE	/* inode extent size */
+#define NUM_INODE_PER_EXTENT	INOSPEREXT
+#define NUM_INODE_PER_IAG	INOSPERIAG
+
+#define MINBLOCKSIZE		512
+#define MAXBLOCKSIZE		4096
+#define	MAXFILESIZE		((s64)1 << 52)
+
+#define JFS_LINK_MAX		65535	/* nlink_t is unsigned short */
+
+/* Minimum number of bytes supported for a JFS partition */
+#define MINJFS			(0x1000000)
+#define MINJFSTEXT		"16"
+
+/*
+ * file system block size -> physical block size
+ */
+#define LBOFFSET(x)	((x) & (PBSIZE - 1))
+#define LBNUMBER(x)	((x) >> L2PBSIZE)
+#define	LBLK2PBLK(sb,b)	((b) << (sb->s_blocksize_bits - L2PBSIZE))
+#define	PBLK2LBLK(sb,b)	((b) >> (sb->s_blocksize_bits - L2PBSIZE))
+/* size in byte -> last page number */
+#define	SIZE2PN(size)	( ((s64)((size) - 1)) >> (L2PSIZE) )
+/* size in byte -> last file system block number */
+#define	SIZE2BN(size, l2bsize) ( ((s64)((size) - 1)) >> (l2bsize) )
+
+/*
+ * fixed physical block address (physical block size = 512 byte)
+ *
+ * NOTE: since we can't guarantee a physical block size of 512 bytes the use of
+ *	 these macros should be removed and the byte offset macros used instead.
+ */
+#define SUPER1_B	64	/* primary superblock */
+#define	AIMAP_B		(SUPER1_B + 8)	/* 1st extent of aggregate inode map */
+#define	AITBL_B		(AIMAP_B + 16)	/*
+					 * 1st extent of aggregate inode table
+					 */
+#define	SUPER2_B	(AITBL_B + 32)	/* 2ndary superblock pbn */
+#define	BMAP_B		(SUPER2_B + 8)	/* block allocation map */
+
+/*
+ * SIZE_OF_SUPER defines the total amount of space reserved on disk for the
+ * superblock.  This is not the same as the superblock structure, since all of
+ * this space is not currently being used.
+ */
+#define SIZE_OF_SUPER	PSIZE
+
+/*
+ * SIZE_OF_AG_TABLE defines the amount of space reserved to hold the AG table
+ */
+#define SIZE_OF_AG_TABLE	PSIZE
+
+/*
+ * SIZE_OF_MAP_PAGE defines the amount of disk space reserved for each page of
+ * the inode allocation map (to hold iag)
+ */
+#define SIZE_OF_MAP_PAGE	PSIZE
+
+/*
+ * fixed byte offset address
+ */
+#define SUPER1_OFF	0x8000	/* primary superblock */
+#define AIMAP_OFF	(SUPER1_OFF + SIZE_OF_SUPER)
+					/*
+					 * Control page of aggregate inode map
+					 * followed by 1st extent of map
+					 */
+#define AITBL_OFF	(AIMAP_OFF + (SIZE_OF_MAP_PAGE << 1))
+					/* 
+					 * 1st extent of aggregate inode table
+					 */
+#define SUPER2_OFF	(AITBL_OFF + INODE_EXTENT_SIZE)
+					/*
+					 * secondary superblock
+					 */
+#define BMAP_OFF	(SUPER2_OFF + SIZE_OF_SUPER)
+					/*
+					 * block allocation map
+					 */
+
+/*
+ * The following macro is used to indicate the number of reserved disk blocks at
+ * the front of an aggregate, in terms of physical blocks.  This value is
+ * currently defined to be 32K.  This turns out to be the same as the primary
+ * superblock's address, since it directly follows the reserved blocks.
+ */
+#define AGGR_RSVD_BLOCKS	SUPER1_B
+
+/*
+ * The following macro is used to indicate the number of reserved bytes at the
+ * front of an aggregate.  This value is currently defined to be 32K.  This
+ * turns out to be the same as the primary superblock's byte offset, since it
+ * directly follows the reserved blocks.
+ */
+#define AGGR_RSVD_BYTES	SUPER1_OFF
+
+/*
+ * The following macro defines the byte offset for the first inode extent in
+ * the aggregate inode table.  This allows us to find the self inode to find the
+ * rest of the table.  Currently this value is 44K.
+ */
+#define AGGR_INODE_TABLE_START	AITBL_OFF
+
+/*
+ *	fixed reserved inode number
+ */
+/* aggregate inode */
+#define AGGR_RESERVED_I	0	/* aggregate inode (reserved) */
+#define	AGGREGATE_I	1	/* aggregate inode map inode */
+#define	BMAP_I		2	/* aggregate block allocation map inode */
+#define	LOG_I		3	/* aggregate inline log inode */
+#define BADBLOCK_I	4	/* aggregate bad block inode */
+#define	FILESYSTEM_I	16	/* 1st/only fileset inode in ait:
+				 * fileset inode map inode
+				 */
+
+/* per fileset inode */
+#define FILESET_RSVD_I	0	/* fileset inode (reserved) */
+#define FILESET_EXT_I	1	/* fileset inode extension */
+#define	ROOT_I		2	/* fileset root inode */
+#define ACL_I		3	/* fileset ACL inode */
+
+#define FILESET_OBJECT_I 4	/* the first fileset inode available for a file
+				 * or directory or link...
+				 */
+#define FIRST_FILESET_INO 16	/* the first aggregate inode which describes
+				 * an inode.  (To fsck this is also the first
+				 * inode in part 2 of the agg inode table.)
+				 */
+
+/*
+ *	directory configuration
+ */
+#define JFS_NAME_MAX	255
+#define JFS_PATH_MAX	BPSIZE
+
+
+/*
+ *	file system state (superblock state)
+ */
+#define FM_CLEAN 0x00000000	/* file system is unmounted and clean */
+#define FM_MOUNT 0x00000001	/* file system is mounted cleanly */
+#define FM_DIRTY 0x00000002	/* file system was not unmounted and clean 
+				 * when mounted or 
+				 * commit failure occurred while being mounted:
+				 * fsck() must be run to repair 
+				 */
+#define	FM_LOGREDO 0x00000004	/* log based recovery (logredo()) failed:
+				 * fsck() must be run to repair 
+				 */
+#define	FM_EXTENDFS 0x00000008	/* file system extendfs() in progress */
+
+#endif				/* _H_JFS_FILSYS */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_imap.c linuxppc64_2_4/fs/jfs/jfs_imap.c
--- linux-2.4.19/fs/jfs/jfs_imap.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_imap.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,3236 @@
+/*
+
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+
+/*
+ * Change History :
+ *
+ */
+
+/*
+ *	jfs_imap.c: inode allocation map manager
+ *
+ * Serialization:
+ *   Each AG has a simple lock which is used to control the serialization of
+ *	the AG level lists.  This lock should be taken first whenever an AG
+ *	level list will be modified or accessed.
+ *
+ *   Each IAG is locked by obtaining the buffer for the IAG page.
+ *
+ *   There is also a inode lock for the inode map inode.  A read lock needs to
+ *	be taken whenever an IAG is read from the map or the global level
+ *	information is read.  A write lock needs to be taken whenever the global
+ *	level information is modified or an atomic operation needs to be used.
+ *
+ *	If more than one IAG is read at one time, the read lock may not
+ *	be given up until all of the IAG's are read.  Otherwise, a deadlock
+ *	may occur when trying to obtain the read lock while another thread
+ *	holding the read lock is waiting on the IAG already being held.
+ *
+ *   The control page of the inode map is read into memory by diMount().
+ *	Thereafter it should only be modified in memory and then it will be
+ *	written out when the filesystem is unmounted by diUnmount().
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_dinode.h"
+#include "jfs_dmap.h"
+#include "jfs_imap.h"
+#include "jfs_metapage.h"
+#include "jfs_superblock.h"
+#include "jfs_debug.h"
+
+/*
+ * imap locks
+ */
+/* iag free list lock */
+#define IAGFREE_LOCK_INIT(imap)		init_MUTEX(&imap->im_freelock)
+#define IAGFREE_LOCK(imap)		down(&imap->im_freelock)
+#define IAGFREE_UNLOCK(imap)		up(&imap->im_freelock)
+
+/* per ag iag list locks */
+#define AG_LOCK_INIT(imap,index)	init_MUTEX(&(imap->im_aglock[index]))
+#define AG_LOCK(imap,agno)		down(&imap->im_aglock[agno])
+#define AG_UNLOCK(imap,agno)		up(&imap->im_aglock[agno])
+
+/*
+ * external references
+ */
+extern struct address_space_operations jfs_aops;
+
+/*
+ * forward references
+ */
+static int diAllocAG(imap_t *, int, boolean_t, struct inode *);
+static int diAllocAny(imap_t *, int, boolean_t, struct inode *);
+static int diAllocBit(imap_t *, iag_t *, int);
+static int diAllocExt(imap_t *, int, struct inode *);
+static int diAllocIno(imap_t *, int, struct inode *);
+static int diFindFree(u32, int);
+static int diNewExt(imap_t *, iag_t *, int);
+static int diNewIAG(imap_t *, int *, int, metapage_t **);
+static void duplicateIXtree(struct super_block *, s64, int, s64 *);
+
+static int diIAGRead(imap_t * imap, int, metapage_t **);
+static int copy_from_dinode(dinode_t *, struct inode *);
+static void copy_to_dinode(dinode_t *, struct inode *);
+
+/*
+ *	debug code for double-checking inode map
+ */
+/* #define	_JFS_DEBUG_IMAP	1 */
+
+#ifdef	_JFS_DEBUG_IMAP
+#define DBG_DIINIT(imap)	DBGdiInit(imap)
+#define DBG_DIALLOC(imap, ino)	DBGdiAlloc(imap, ino)
+#define DBG_DIFREE(imap, ino)	DBGdiFree(imap, ino)
+
+static void *DBGdiInit(imap_t * imap);
+static void DBGdiAlloc(imap_t * imap, ino_t ino);
+static void DBGdiFree(imap_t * imap, ino_t ino);
+#else
+#define DBG_DIINIT(imap)
+#define DBG_DIALLOC(imap, ino)
+#define DBG_DIFREE(imap, ino)
+#endif				/* _JFS_DEBUG_IMAP */
+
+/*
+ * NAME:        diMount()
+ *
+ * FUNCTION:    initialize the incore inode map control structures for
+ *		a fileset or aggregate init time.
+ *
+ *              the inode map's control structure (dinomap_t) is 
+ *              brought in from disk and placed in virtual memory.
+ *
+ * PARAMETERS:
+ *      ipimap  - pointer to inode map inode for the aggregate or fileset.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      ENOMEM  - insufficient free virtual memory.
+ *      EIO  	- i/o error.
+ */
+int diMount(struct inode *ipimap)
+{
+	imap_t *imap;
+	metapage_t *mp;
+	int index;
+	dinomap_t *dinom_le;
+
+	/*
+	 * allocate/initialize the in-memory inode map control structure
+	 */
+	/* allocate the in-memory inode map control structure. */
+	imap = (imap_t *) kmalloc(sizeof(imap_t), GFP_KERNEL);
+	if (imap == NULL) {
+		jERROR(1, ("diMount: kmalloc returned NULL!\n"));
+		return (ENOMEM);
+	}
+
+	/* read the on-disk inode map control structure. */
+
+	mp = read_metapage(ipimap,
+			   IMAPBLKNO << JFS_SBI(ipimap->i_sb)->l2nbperpage,
+			   PSIZE, 0);
+	if (mp == NULL) {
+		kfree(imap);
+		return (EIO);
+	}
+
+	/* copy the on-disk version to the in-memory version. */
+	dinom_le = (dinomap_t *) mp->data;
+	imap->im_freeiag = le32_to_cpu(dinom_le->in_freeiag);
+	imap->im_nextiag = le32_to_cpu(dinom_le->in_nextiag);
+	atomic_set(&imap->im_numinos, le32_to_cpu(dinom_le->in_numinos));
+	atomic_set(&imap->im_numfree, le32_to_cpu(dinom_le->in_numfree));
+	imap->im_nbperiext = le32_to_cpu(dinom_le->in_nbperiext);
+	imap->im_l2nbperiext = le32_to_cpu(dinom_le->in_l2nbperiext);
+	for (index = 0; index < MAXAG; index++) {
+		imap->im_agctl[index].inofree =
+		    le32_to_cpu(dinom_le->in_agctl[index].inofree);
+		imap->im_agctl[index].extfree =
+		    le32_to_cpu(dinom_le->in_agctl[index].extfree);
+		imap->im_agctl[index].numinos =
+		    le32_to_cpu(dinom_le->in_agctl[index].numinos);
+		imap->im_agctl[index].numfree =
+		    le32_to_cpu(dinom_le->in_agctl[index].numfree);
+	}
+
+	/* release the buffer. */
+	release_metapage(mp);
+
+	/*
+	 * allocate/initialize inode allocation map locks
+	 */
+	/* allocate and init iag free list lock */
+	IAGFREE_LOCK_INIT(imap);
+
+	/* allocate and init ag list locks */
+	for (index = 0; index < MAXAG; index++) {
+		AG_LOCK_INIT(imap, index);
+	}
+
+	/* bind the inode map inode and inode map control structure
+	 * to each other.
+	 */
+	imap->im_ipimap = ipimap;
+	JFS_IP(ipimap)->i_imap = imap;
+
+//      DBG_DIINIT(imap);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        diUnmount()
+ *
+ * FUNCTION:    write to disk the incore inode map control structures for
+ *		a fileset or aggregate at unmount time.
+ *
+ * PARAMETERS:
+ *      ipimap  - pointer to inode map inode for the aggregate or fileset.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      ENOMEM  - insufficient free virtual memory.
+ *      EIO  	- i/o error.
+ */
+int diUnmount(struct inode *ipimap, int mounterror)
+{
+	imap_t *imap = JFS_IP(ipimap)->i_imap;
+
+	/*
+	 * update the on-disk inode map control structure
+	 */
+
+	if (!(mounterror || isReadOnly(ipimap)))
+		diSync(ipimap);
+
+	/*
+	 * Invalidate the page cache buffers
+	 */
+	truncate_inode_pages(ipimap->i_mapping, 0);
+
+	/*
+	 * free in-memory control structure
+	 */
+	kfree(imap);
+
+	return (0);
+}
+
+
+/*
+ *	diSync()
+ */
+int diSync(struct inode *ipimap)
+{
+	dinomap_t *dinom_le;
+	imap_t *imp = JFS_IP(ipimap)->i_imap;
+	metapage_t *mp;
+	int index;
+
+	/*
+	 * write imap global conrol page
+	 */
+	/* read the on-disk inode map control structure */
+	mp = get_metapage(ipimap,
+			  IMAPBLKNO << JFS_SBI(ipimap->i_sb)->l2nbperpage,
+			  PSIZE, 0);
+	if (mp == NULL) {
+		jERROR(1,("diSync: get_metapage failed!\n"));
+		return EIO;
+	}
+
+	/* copy the in-memory version to the on-disk version */
+	//memcpy(mp->data, &imp->im_imap,sizeof(dinomap_t));
+	dinom_le = (dinomap_t *) mp->data;
+	dinom_le->in_freeiag = cpu_to_le32(imp->im_freeiag);
+	dinom_le->in_nextiag = cpu_to_le32(imp->im_nextiag);
+	dinom_le->in_numinos = cpu_to_le32(atomic_read(&imp->im_numinos));
+	dinom_le->in_numfree = cpu_to_le32(atomic_read(&imp->im_numfree));
+	dinom_le->in_nbperiext = cpu_to_le32(imp->im_nbperiext);
+	dinom_le->in_l2nbperiext = cpu_to_le32(imp->im_l2nbperiext);
+	for (index = 0; index < MAXAG; index++) {
+		dinom_le->in_agctl[index].inofree =
+		    cpu_to_le32(imp->im_agctl[index].inofree);
+		dinom_le->in_agctl[index].extfree =
+		    cpu_to_le32(imp->im_agctl[index].extfree);
+		dinom_le->in_agctl[index].numinos =
+		    cpu_to_le32(imp->im_agctl[index].numinos);
+		dinom_le->in_agctl[index].numfree =
+		    cpu_to_le32(imp->im_agctl[index].numfree);
+	}
+
+	/* write out the control structure */
+	write_metapage(mp);
+
+	/*
+	 * write out dirty pages of imap
+	 */
+	fsync_inode_data_buffers(ipimap);
+
+	diWriteSpecial(ipimap);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        diRead()
+ *
+ * FUNCTION:    initialize an incore inode from disk.
+ *
+ *		on entry, the specifed incore inode should itself
+ *		specify the disk inode number corresponding to the
+ *		incore inode (i.e. i_number should be initialized).
+ *		
+ *		this routine handles incore inode initialization for
+ *		both "special" and "regular" inodes.  special inodes
+ *		are those required early in the mount process and
+ *	        require special handling since much of the file system
+ *		is not yet initialized.  these "special" inodes are
+ *		identified by a NULL inode map inode pointer and are
+ *		actually initialized by a call to diReadSpecial().
+ *		
+ *		for regular inodes, the iag describing the disk inode
+ *		is read from disk to determine the inode extent address
+ *		for the disk inode.  with the inode extent address in
+ *		hand, the page of the extent that contains the disk
+ *		inode is read and the disk inode is copied to the
+ *		incore inode.
+ *
+ * PARAMETERS:
+ *      ip  -  pointer to incore inode to be initialized from disk.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO  	- i/o error.
+ *      ENOMEM	- insufficient memory
+ *      
+ */
+int diRead(struct inode *ip)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	int iagno, ino, extno, rc;
+	struct inode *ipimap;
+	dinode_t *dp;
+	iag_t *iagp;
+	metapage_t *mp;
+	s64 blkno, agstart;
+	imap_t *imap;
+	int block_offset;
+	int inodes_left;
+	uint pageno;
+	int rel_inode;
+
+	jFYI(1, ("diRead: ino = %ld\n", ip->i_ino));
+
+	ipimap = sbi->ipimap;
+	JFS_IP(ip)->ipimap = ipimap;
+
+	/* determine the iag number for this inode (number) */
+	iagno = INOTOIAG(ip->i_ino);
+
+	/* read the iag */
+	imap = JFS_IP(ipimap)->i_imap;
+	IREAD_LOCK(ipimap);
+	rc = diIAGRead(imap, iagno, &mp);
+	IREAD_UNLOCK(ipimap);
+	if (rc) {
+		jERROR(1, ("diRead: diIAGRead returned %d\n", rc));
+		return (rc);
+	}
+
+	iagp = (iag_t *) mp->data;
+
+	/* determine inode extent that holds the disk inode */
+	ino = ip->i_ino & (INOSPERIAG - 1);
+	extno = ino >> L2INOSPEREXT;
+
+	if ((lengthPXD(&iagp->inoext[extno]) != imap->im_nbperiext) ||
+	    (addressPXD(&iagp->inoext[extno]) == 0)) {
+		jERROR(1, ("diRead: Bad inoext: 0x%lx, 0x%lx\n",
+			   (ulong) addressPXD(&iagp->inoext[extno]),
+			   (ulong) lengthPXD(&iagp->inoext[extno])));
+		release_metapage(mp);
+		updateSuper(ip->i_sb, FM_DIRTY);
+		return ESTALE;
+	}
+
+	/* get disk block number of the page within the inode extent
+	 * that holds the disk inode.
+	 */
+	blkno = INOPBLK(&iagp->inoext[extno], ino, sbi->l2nbperpage);
+
+	/* get the ag for the iag */
+	agstart = le64_to_cpu(iagp->agstart);
+
+	release_metapage(mp);
+
+	rel_inode = (ino & (INOSPERPAGE - 1));
+	pageno = blkno >> sbi->l2nbperpage;
+
+	if ((block_offset = ((u32) blkno & (sbi->nbperpage - 1)))) {
+		/*
+		 * OS/2 didn't always align inode extents on page boundaries
+		 */
+		inodes_left =
+		     (sbi->nbperpage - block_offset) << sbi->l2niperblk;
+
+		if (rel_inode < inodes_left)
+			rel_inode += block_offset << sbi->l2niperblk;
+		else {
+			pageno += 1;
+			rel_inode -= inodes_left;
+		}
+	}
+
+	/* read the page of disk inode */
+	mp = read_metapage(ipimap, pageno << sbi->l2nbperpage, PSIZE, 1);
+	if (mp == 0) {
+		jERROR(1, ("diRead: read_metapage failed\n"));
+		return EIO;
+	}
+
+	/* locate the the disk inode requested */
+	dp = (dinode_t *) mp->data;
+	dp += rel_inode;
+
+	if (ip->i_ino != le32_to_cpu(dp->di_number)) {
+		jERROR(1, ("diRead: i_ino != di_number\n"));
+		updateSuper(ip->i_sb, FM_DIRTY);
+		rc = EIO;
+	} else if (le32_to_cpu(dp->di_nlink) == 0) {
+		jERROR(1,
+		       ("diRead: di_nlink is zero. ino=%ld\n", ip->i_ino));
+		updateSuper(ip->i_sb, FM_DIRTY);
+		rc = ESTALE;
+	} else
+		/* copy the disk inode to the in-memory inode */
+		rc = copy_from_dinode(dp, ip);
+
+	release_metapage(mp);
+
+	/* set the ag for the inode */
+	JFS_IP(ip)->agno = BLKTOAG(agstart, sbi);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:        diReadSpecial()
+ *
+ * FUNCTION:    initialize a 'special' inode from disk.
+ *
+ *		this routines handles aggregate level inodes.  The
+ *		inode cache cannot differentiate between the
+ *		aggregate inodes and the filesystem inodes, so we
+ *		handle these here.  We don't actually use the aggregate
+ *	        inode map, since these inodes are at a fixed location
+ *		and in some cases the aggregate inode map isn't initialized
+ *		yet.
+ *
+ * PARAMETERS:
+ *      sb - filesystem superblock
+ *	inum - aggregate inode number
+ *
+ * RETURN VALUES:
+ *      new inode	- success
+ *      NULL		- i/o error.
+ */
+struct inode *diReadSpecial(struct super_block *sb, ino_t inum)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	uint address;
+	dinode_t *dp;
+	struct inode *ip;
+	metapage_t *mp;
+	int rc;
+
+	ip = new_inode(sb);
+	if (ip == NULL) {
+		jERROR(1,
+		       ("diReadSpecial: new_inode returned NULL!\n"));
+		return ip;
+	}
+
+	rc = alloc_jfs_inode(ip);
+	if (rc) {
+		make_bad_inode(ip);
+		iput(ip);
+		return NULL;
+	}
+
+	/*
+	 * If ip->i_number >= 32 (INOSPEREXT), then read from secondary
+	 * aggregate inode table.
+	 */
+
+	if (inum >= INOSPEREXT) {
+		address =
+		    addressPXD(&sbi->ait2) >> sbi->l2nbperpage;
+		inum -= INOSPEREXT;
+		ASSERT(inum < INOSPEREXT);
+		JFS_IP(ip)->ipimap = sbi->ipaimap2;
+	} else {
+		address = AITBL_OFF >> L2PSIZE;
+		JFS_IP(ip)->ipimap = sbi->ipaimap;
+	}
+	ip->i_ino = inum;
+
+	address += inum >> 3;	/* 8 inodes per 4K page */
+
+	/* read the page of fixed disk inode (AIT) in raw mode */
+	jEVENT(0,
+	       ("Reading aggregate inode %d from block %d\n", (uint) inum,
+		address));
+	mp = read_metapage(ip, address << sbi->l2nbperpage, PSIZE, 1);
+	if (mp == NULL) {
+		ip->i_sb = NULL;
+		ip->i_nlink = 1;	/* Don't want iput() deleting it */
+		iput(ip);
+		return (NULL);
+	}
+
+	/* get the pointer to the disk inode of interest */
+	dp = (dinode_t *) (mp->data);
+	dp += inum % 8;		/* 8 inodes per 4K page */
+
+	/* copy on-disk inode to in-memory inode */
+	if ((copy_from_dinode(dp, ip)) != 0) {
+		/* handle bad return by returning NULL for ip */
+		ip->i_sb = NULL;
+		ip->i_nlink = 1;	/* Don't want iput() deleting it */
+		iput(ip);
+		/* release the page */
+		release_metapage(mp);
+		return (NULL);
+
+	}
+
+	ip->i_mapping->a_ops = &jfs_aops;
+	ip->i_mapping->gfp_mask = GFP_NOFS;
+
+	if ((inum == FILESYSTEM_I) && (JFS_IP(ip)->ipimap == sbi->ipaimap)) {
+		sbi->gengen = le32_to_cpu(dp->di_gengen);
+		sbi->inostamp = le32_to_cpu(dp->di_inostamp);
+	}
+
+	/* release the page */
+	release_metapage(mp);
+
+	return (ip);
+}
+
+/*
+ * NAME:        diWriteSpecial()
+ *
+ * FUNCTION:    Write the special inode to disk
+ *
+ * PARAMETERS:
+ *      ip - special inode
+ *
+ * RETURN VALUES: none
+ */
+
+void diWriteSpecial(struct inode *ip)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	uint address;
+	dinode_t *dp;
+	ino_t inum = ip->i_ino;
+	metapage_t *mp;
+
+	/*
+	 * If ip->i_number >= 32 (INOSPEREXT), then write to secondary
+	 * aggregate inode table.
+	 */
+
+	if (!(ip->i_state & I_DIRTY))
+		return;
+
+	ip->i_state &= ~I_DIRTY;
+
+	if (inum >= INOSPEREXT) {
+		address =
+		    addressPXD(&sbi->ait2) >> sbi->l2nbperpage;
+		inum -= INOSPEREXT;
+		ASSERT(inum < INOSPEREXT);
+	} else {
+		address = AITBL_OFF >> L2PSIZE;
+	}
+
+	address += inum >> 3;	/* 8 inodes per 4K page */
+
+	/* read the page of fixed disk inode (AIT) in raw mode */
+	jEVENT(0,
+	       ("Reading aggregate inode %d from block %d\n", (uint) inum,
+		address));
+	mp = read_metapage(ip, address << sbi->l2nbperpage, PSIZE, 1);
+	if (mp == NULL) {
+		jERROR(1,
+		       ("diWriteSpecial: failed to read aggregate inode extent!\n"));
+		return;
+	}
+
+	/* get the pointer to the disk inode of interest */
+	dp = (dinode_t *) (mp->data);
+	dp += inum % 8;		/* 8 inodes per 4K page */
+
+	/* copy on-disk inode to in-memory inode */
+	copy_to_dinode(dp, ip);
+	memcpy(&dp->di_xtroot, &JFS_IP(ip)->i_xtroot, 288);
+
+	if (inum == FILESYSTEM_I)
+		dp->di_gengen = cpu_to_le32(sbi->gengen);
+
+	/* write the page */
+	write_metapage(mp);
+}
+
+/*
+ * NAME:        diFreeSpecial()
+ *
+ * FUNCTION:    Free allocated space for special inode
+ */
+void diFreeSpecial(struct inode *ip)
+{
+	if (ip == NULL) {
+		jERROR(1, ("diFreeSpecial called with NULL ip!\n"));
+		return;
+	}
+	fsync_inode_data_buffers(ip);
+	truncate_inode_pages(ip->i_mapping, 0);
+	iput(ip);
+}
+
+
+
+/*
+ * NAME:        diWrite()
+ *
+ * FUNCTION:    write the on-disk inode portion of the in-memory inode
+ *		to its corresponding on-disk inode.
+ *
+ *		on entry, the specifed incore inode should itself
+ *		specify the disk inode number corresponding to the
+ *		incore inode (i.e. i_number should be initialized).
+ *
+ *		the inode contains the inode extent address for the disk
+ *		inode.  with the inode extent address in hand, the
+ *		page of the extent that contains the disk inode is
+ *		read and the disk inode portion of the incore inode
+ *		is copied to the disk inode.
+ *		
+ * PARAMETERS:
+ *	tid -  transacation id
+ *      ip  -  pointer to incore inode to be written to the inode extent.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO  	- i/o error.
+ */
+int diWrite(tid_t tid, struct inode *ip)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	int rc = 0;
+	s32 ino;
+	dinode_t *dp;
+	s64 blkno;
+	int block_offset;
+	int inodes_left;
+	metapage_t *mp;
+	uint pageno;
+	int rel_inode;
+	int dioffset;
+	struct inode *ipimap;
+	uint type;
+	lid_t lid;
+	tlock_t *ditlck, *tlck;
+	linelock_t *dilinelock, *ilinelock;
+	lv_t *lv;
+	int n;
+
+	ipimap = jfs_ip->ipimap;
+
+	ino = ip->i_ino & (INOSPERIAG - 1);
+
+	assert(lengthPXD(&(jfs_ip->ixpxd)) ==
+	       JFS_IP(ipimap)->i_imap->im_nbperiext);
+	assert(addressPXD(&(jfs_ip->ixpxd)));
+
+	/*
+	 * read the page of disk inode containing the specified inode:
+	 */
+	/* compute the block address of the page */
+	blkno = INOPBLK(&(jfs_ip->ixpxd), ino, sbi->l2nbperpage);
+
+	rel_inode = (ino & (INOSPERPAGE - 1));
+	pageno = blkno >> sbi->l2nbperpage;
+
+	if ((block_offset = ((u32) blkno & (sbi->nbperpage - 1)))) {
+		/*
+		 * OS/2 didn't always align inode extents on page boundaries
+		 */
+		inodes_left =
+		    (sbi->nbperpage - block_offset) << sbi->l2niperblk;
+
+		if (rel_inode < inodes_left)
+			rel_inode += block_offset << sbi->l2niperblk;
+		else {
+			pageno += 1;
+			rel_inode -= inodes_left;
+		}
+	}
+	/* read the page of disk inode */
+      retry:
+	mp = read_metapage(ipimap, pageno << sbi->l2nbperpage, PSIZE, 1);
+	if (mp == 0)
+		return (EIO);
+
+	/* get the pointer to the disk inode */
+	dp = (dinode_t *) mp->data;
+	dp += rel_inode;
+
+	dioffset = (ino & (INOSPERPAGE - 1)) << L2DISIZE;
+
+	/*
+	 * acquire transaction lock on the on-disk inode;
+	 * N.B. tlock is acquired on ipimap not ip;
+	 */
+	if ((ditlck =
+	     txLock(tid, ipimap, mp, tlckINODE | tlckENTRY)) == NULL)
+		goto retry;
+	dilinelock = (linelock_t *) & ditlck->lock;
+
+	/*
+	 * copy btree root from in-memory inode to on-disk inode
+	 *
+	 * (tlock is taken from inline B+-tree root in in-memory
+	 * inode when the B+-tree root is updated, which is pointed 
+	 * by jfs_ip->blid as well as being on tx tlock list)
+	 *
+	 * further processing of btree root is based on the copy 
+	 * in in-memory inode, where txLog() will log from, and, 
+	 * for xtree root, txUpdateMap() will update map and reset
+	 * XAD_NEW bit;
+	 */
+
+	if (S_ISDIR(ip->i_mode) && (lid = jfs_ip->xtlid)) {
+		/*
+		 * This is the special xtree inside the directory for storing
+		 * the directory table
+		 */
+		xtpage_t *p, *xp;
+		xad_t *xad;
+
+		jfs_ip->xtlid = 0;
+		tlck = lid_to_tlock(lid);
+		assert(tlck->type & tlckXTREE);
+		tlck->type |= tlckBTROOT;
+		tlck->mp = mp;
+		ilinelock = (linelock_t *) & tlck->lock;
+
+		/*
+		 * copy xtree root from inode to dinode:
+		 */
+		p = &jfs_ip->i_xtroot;
+		xp = (xtpage_t *) &dp->di_dirtable;
+		lv = (lv_t *) & ilinelock->lv;
+		for (n = 0; n < ilinelock->index; n++, lv++) {
+			memcpy(&xp->xad[lv->offset], &p->xad[lv->offset],
+			       lv->length << L2XTSLOTSIZE);
+		}
+
+		/* reset on-disk (metadata page) xtree XAD_NEW bit */
+		xad = &xp->xad[XTENTRYSTART];
+		for (n = XTENTRYSTART;
+		     n < le16_to_cpu(xp->header.nextindex); n++, xad++)
+			if (xad->flag & (XAD_NEW | XAD_EXTENDED))
+				xad->flag &= ~(XAD_NEW | XAD_EXTENDED);
+	}
+
+	if ((lid = jfs_ip->blid) == 0)
+		goto inlineData;
+	jfs_ip->blid = 0;
+
+	tlck = lid_to_tlock(lid);
+	type = tlck->type;
+	tlck->type |= tlckBTROOT;
+	tlck->mp = mp;
+	ilinelock = (linelock_t *) & tlck->lock;
+
+	/*
+	 *      regular file: 16 byte (XAD slot) granularity
+	 */
+	if (type & tlckXTREE) {
+		xtpage_t *p, *xp;
+		xad_t *xad;
+
+		/*
+		 * copy xtree root from inode to dinode:
+		 */
+		p = &jfs_ip->i_xtroot;
+		xp = &dp->di_xtroot;
+		lv = (lv_t *) & ilinelock->lv;
+		for (n = 0; n < ilinelock->index; n++, lv++) {
+			memcpy(&xp->xad[lv->offset], &p->xad[lv->offset],
+			       lv->length << L2XTSLOTSIZE);
+		}
+
+		/* reset on-disk (metadata page) xtree XAD_NEW bit */
+		xad = &xp->xad[XTENTRYSTART];
+		for (n = XTENTRYSTART;
+		     n < le16_to_cpu(xp->header.nextindex); n++, xad++)
+			if (xad->flag & (XAD_NEW | XAD_EXTENDED))
+				xad->flag &= ~(XAD_NEW | XAD_EXTENDED);
+	}
+	/*
+	 *      directory: 32 byte (directory entry slot) granularity
+	 */
+	else if (type & tlckDTREE) {
+		dtpage_t *p, *xp;
+
+		/*
+		 * copy dtree root from inode to dinode:
+		 */
+		p = (dtpage_t *) &jfs_ip->i_dtroot;
+		xp = (dtpage_t *) & dp->di_dtroot;
+		lv = (lv_t *) & ilinelock->lv;
+		for (n = 0; n < ilinelock->index; n++, lv++) {
+			memcpy(&xp->slot[lv->offset], &p->slot[lv->offset],
+			       lv->length << L2DTSLOTSIZE);
+		}
+	} else {
+		jERROR(1, ("diWrite: UFO tlock\n"));
+	}
+
+      inlineData:
+	/*
+	 * copy inline symlink from in-memory inode to on-disk inode
+	 */
+	if (S_ISLNK(ip->i_mode) && ip->i_size < IDATASIZE) {
+		lv = (lv_t *) & dilinelock->lv[dilinelock->index];
+		lv->offset = (dioffset + 2 * 128) >> L2INODESLOTSIZE;
+		lv->length = 2;
+		memcpy(&dp->di_fastsymlink, jfs_ip->i_inline, IDATASIZE);
+		dilinelock->index++;
+	}
+#ifdef _STILL_TO_PORT
+	/*
+	 * copy inline data from in-memory inode to on-disk inode:
+	 * 128 byte slot granularity
+	 */
+	if (test_cflag(COMMIT_Inlineea, ip))
+		lv = (lv_t *) & dilinelock->lv[dilinelock->index];
+		lv->offset = (dioffset + 3 * 128) >> L2INODESLOTSIZE;
+		lv->length = 1;
+		memcpy(&dp->di_inlineea, &ip->i_inlineea, INODESLOTSIZE);
+		dilinelock->index++;
+
+		clear_cflag(COMMIT_Inlineea, ip);
+	}
+#endif				/* _STILL_TO_PORT */
+
+	/*
+	 *      lock/copy inode base: 128 byte slot granularity
+	 */
+// baseDinode:
+	lv = (lv_t *) & dilinelock->lv[dilinelock->index];
+	lv->offset = dioffset >> L2INODESLOTSIZE;
+	copy_to_dinode(dp, ip);
+	if (test_and_clear_cflag(COMMIT_Dirtable, ip)) {
+		lv->length = 2;
+		memcpy(&dp->di_dirtable, &jfs_ip->i_dirtable, 96);
+	} else
+		lv->length = 1;
+	dilinelock->index++;
+
+#ifdef _JFS_FASTDASD
+	/*
+	 * We aren't logging changes to the DASD used in directory inodes,
+	 * but we need to write them to disk.  If we don't unmount cleanly,
+	 * mount will recalculate the DASD used.
+	 */
+	if (S_ISDIR(ip->i_mode)
+	    && (ip->i_ipmnt->i_mntflag & JFS_DASD_ENABLED))
+		bcopy(&ip->i_DASD, &dp->di_DASD, sizeof(dasd_t));
+#endif				/*  _JFS_FASTDASD */
+
+	/* release the buffer holding the updated on-disk inode. 
+	 * the buffer will be later written by commit processing.
+	 */
+	write_metapage(mp);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:        diFree(ip)
+ *
+ * FUNCTION:    free a specified inode from the inode working map
+ *		for a fileset or aggregate.
+ *
+ *		if the inode to be freed represents the first (only)
+ *		free inode within the iag, the iag will be placed on
+ *		the ag free inode list.
+ *	
+ *		freeing the inode will cause the inode extent to be
+ *		freed if the inode is the only allocated inode within
+ *		the extent.  in this case all the disk resource backing
+ *		up the inode extent will be freed. in addition, the iag
+ *		will be placed on the ag extent free list if the extent
+ *		is the first free extent in the iag.  if freeing the
+ *		extent also means that no free inodes will exist for
+ *		the iag, the iag will also be removed from the ag free
+ *		inode list.
+ *
+ *		the iag describing the inode will be freed if the extent
+ *		is to be freed and it is the only backed extent within
+ *		the iag.  in this case, the iag will be removed from the
+ *		ag free extent list and ag free inode list and placed on
+ *		the inode map's free iag list.
+ *
+ *		a careful update approach is used to provide consistency
+ *		in the face of updates to multiple buffers.  under this
+ *		approach, all required buffers are obtained before making
+ *		any updates and are held until all updates are complete.
+ *
+ * PARAMETERS:
+ *      ip  	- inode to be freed.
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      EIO  	- i/o error.
+ */
+int diFree(struct inode *ip)
+{
+	int rc;
+	ino_t inum = ip->i_ino;
+	iag_t *iagp, *aiagp, *biagp, *ciagp, *diagp;
+	metapage_t *mp, *amp, *bmp, *cmp, *dmp;
+	int iagno, ino, extno, bitno, sword, agno;
+	int back, fwd;
+	u32 bitmap, mask;
+	struct inode *ipimap = JFS_SBI(ip->i_sb)->ipimap;
+	imap_t *imap = JFS_IP(ipimap)->i_imap;
+	s64 xaddr;
+	s64 xlen;
+	pxd_t freepxd;
+	tid_t tid;
+	struct inode *iplist[3];
+	tlock_t *tlck;
+	pxdlock_t *pxdlock;
+
+	/*
+	 * This is just to suppress compiler warnings.  The same logic that
+	 * references these variables is used to initialize them.
+	 */
+	aiagp = biagp = ciagp = diagp = NULL;
+
+	/* get the iag number containing the inode.
+	 */
+	iagno = INOTOIAG(inum);
+
+	/* make sure that the iag is contained within 
+	 * the map.
+	 */
+	//assert(iagno < imap->im_nextiag);
+	if (iagno >= imap->im_nextiag) {
+		jERROR(1, ("diFree: inum = %d, iagno = %d, nextiag = %d\n",
+			   (uint) inum, iagno, imap->im_nextiag));
+		dump_mem("imap", imap, 32);
+		updateSuper(ip->i_sb, FM_DIRTY);
+		return EIO;
+	}
+
+	/* get the allocation group for this ino.
+	 */
+	agno = JFS_IP(ip)->agno;
+
+	/* Lock the AG specific inode map information
+	 */
+	AG_LOCK(imap, agno);
+
+	/* Obtain read lock in imap inode.  Don't release it until we have
+	 * read all of the IAG's that we are going to.
+	 */
+	IREAD_LOCK(ipimap);
+
+	/* read the iag.
+	 */
+	if ((rc = diIAGRead(imap, iagno, &mp))) {
+		IREAD_UNLOCK(ipimap);
+		AG_UNLOCK(imap, agno);
+		return (rc);
+	}
+	iagp = (iag_t *) mp->data;
+
+	/* get the inode number and extent number of the inode within
+	 * the iag and the inode number within the extent.
+	 */
+	ino = inum & (INOSPERIAG - 1);
+	extno = ino >> L2INOSPEREXT;
+	bitno = ino & (INOSPEREXT - 1);
+	mask = HIGHORDER >> bitno;
+
+	assert(le32_to_cpu(iagp->wmap[extno]) & mask);
+#ifdef _STILL_TO_PORT
+	assert((le32_to_cpu(iagp->pmap[extno]) & mask) == 0);
+#endif				/*  _STILL_TO_PORT */
+	assert(addressPXD(&iagp->inoext[extno]));
+
+	/* compute the bitmap for the extent reflecting the freed inode.
+	 */
+	bitmap = le32_to_cpu(iagp->wmap[extno]) & ~mask;
+
+	if (imap->im_agctl[agno].numfree > imap->im_agctl[agno].numinos) {
+		jERROR(1,("diFree: numfree > numinos\n"));
+		release_metapage(mp);
+		IREAD_UNLOCK(ipimap);
+		AG_UNLOCK(imap, agno);
+		updateSuper(ip->i_sb, FM_DIRTY);
+		return EIO;
+	}
+	/*
+	 *      inode extent still has some inodes or below low water mark:
+	 *      keep the inode extent;
+	 */
+	if (bitmap ||
+	    imap->im_agctl[agno].numfree < 96 ||
+	    (imap->im_agctl[agno].numfree < 288 &&
+	     (((imap->im_agctl[agno].numfree * 100) /
+	       imap->im_agctl[agno].numinos) <= 25))) {
+		/* if the iag currently has no free inodes (i.e.,
+		 * the inode being freed is the first free inode of iag),
+		 * insert the iag at head of the inode free list for the ag.
+		 */
+		if (iagp->nfreeinos == 0) {
+			/* check if there are any iags on the ag inode
+			 * free list.  if so, read the first one so that
+			 * we can link the current iag onto the list at
+			 * the head.
+			 */
+			if ((fwd = imap->im_agctl[agno].inofree) >= 0) {
+				/* read the iag that currently is the head
+				 * of the list.
+				 */
+				if ((rc = diIAGRead(imap, fwd, &amp))) {
+					IREAD_UNLOCK(ipimap);
+					AG_UNLOCK(imap, agno);
+					release_metapage(mp);
+					return (rc);
+				}
+				aiagp = (iag_t *) amp->data;
+
+				/* make current head point back to the iag.
+				 */
+				aiagp->inofreeback = cpu_to_le32(iagno);
+
+				write_metapage(amp);
+			}
+
+			/* iag points forward to current head and iag
+			 * becomes the new head of the list.
+			 */
+			iagp->inofreefwd =
+			    cpu_to_le32(imap->im_agctl[agno].inofree);
+			iagp->inofreeback = -1;
+			imap->im_agctl[agno].inofree = iagno;
+		}
+		IREAD_UNLOCK(ipimap);
+
+		/* update the free inode summary map for the extent if
+		 * freeing the inode means the extent will now have free
+		 * inodes (i.e., the inode being freed is the first free 
+		 * inode of extent),
+		 */
+		if (iagp->wmap[extno] == ONES) {
+			sword = extno >> L2EXTSPERSUM;
+			bitno = extno & (EXTSPERSUM - 1);
+			iagp->inosmap[sword] &=
+			    cpu_to_le32(~(HIGHORDER >> bitno));
+		}
+
+		/* update the bitmap.
+		 */
+		iagp->wmap[extno] = cpu_to_le32(bitmap);
+		DBG_DIFREE(imap, inum);
+
+		/* update the free inode counts at the iag, ag and
+		 * map level.
+		 */
+		iagp->nfreeinos =
+		    cpu_to_le32(le32_to_cpu(iagp->nfreeinos) + 1);
+		imap->im_agctl[agno].numfree += 1;
+		atomic_inc(&imap->im_numfree);
+
+		/* release the AG inode map lock
+		 */
+		AG_UNLOCK(imap, agno);
+
+		/* write the iag */
+		write_metapage(mp);
+
+		return (0);
+	}
+
+
+	/*
+	 *      inode extent has become free and above low water mark:
+	 *      free the inode extent;
+	 */
+
+	/*
+	 *      prepare to update iag list(s) (careful update step 1)
+	 */
+	amp = bmp = cmp = dmp = NULL;
+	fwd = back = -1;
+
+	/* check if the iag currently has no free extents.  if so,
+	 * it will be placed on the head of the ag extent free list.
+	 */
+	if (iagp->nfreeexts == 0) {
+		/* check if the ag extent free list has any iags.
+		 * if so, read the iag at the head of the list now.
+		 * this (head) iag will be updated later to reflect
+		 * the addition of the current iag at the head of
+		 * the list.
+		 */
+		if ((fwd = imap->im_agctl[agno].extfree) >= 0) {
+			if ((rc = diIAGRead(imap, fwd, &amp)))
+				goto error_out;
+			aiagp = (iag_t *) amp->data;
+		}
+	} else {
+		/* iag has free extents. check if the addition of a free
+		 * extent will cause all extents to be free within this
+		 * iag.  if so, the iag will be removed from the ag extent
+		 * free list and placed on the inode map's free iag list.
+		 */
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG - 1)) {
+			/* in preparation for removing the iag from the
+			 * ag extent free list, read the iags preceeding
+			 * and following the iag on the ag extent free
+			 * list.
+			 */
+			if ((fwd = le32_to_cpu(iagp->extfreefwd)) >= 0) {
+				if ((rc = diIAGRead(imap, fwd, &amp)))
+					goto error_out;
+				aiagp = (iag_t *) amp->data;
+			}
+
+			if ((back = le32_to_cpu(iagp->extfreeback)) >= 0) {
+				if ((rc = diIAGRead(imap, back, &bmp)))
+					goto error_out;
+				biagp = (iag_t *) bmp->data;
+			}
+		}
+	}
+
+	/* remove the iag from the ag inode free list if freeing
+	 * this extent cause the iag to have no free inodes.
+	 */
+	if (iagp->nfreeinos == cpu_to_le32(INOSPEREXT - 1)) {
+		int inofreeback = le32_to_cpu(iagp->inofreeback);
+		int inofreefwd = le32_to_cpu(iagp->inofreefwd);
+
+		/* in preparation for removing the iag from the
+		 * ag inode free list, read the iags preceeding
+		 * and following the iag on the ag inode free
+		 * list.  before reading these iags, we must make
+		 * sure that we already don't have them in hand
+		 * from up above, since re-reading an iag (buffer)
+		 * we are currently holding would cause a deadlock.
+		 */
+		if (inofreefwd >= 0) {
+
+			if (inofreefwd == fwd)
+				ciagp = (iag_t *) amp->data;
+			else if (inofreefwd == back)
+				ciagp = (iag_t *) bmp->data;
+			else {
+				if ((rc =
+				     diIAGRead(imap, inofreefwd, &cmp)))
+					goto error_out;
+				assert(cmp != NULL);
+				ciagp = (iag_t *) cmp->data;
+			}
+			assert(ciagp != NULL);
+		}
+
+		if (inofreeback >= 0) {
+			if (inofreeback == fwd)
+				diagp = (iag_t *) amp->data;
+			else if (inofreeback == back)
+				diagp = (iag_t *) bmp->data;
+			else {
+				if ((rc =
+				     diIAGRead(imap, inofreeback, &dmp)))
+					goto error_out;
+				assert(dmp != NULL);
+				diagp = (iag_t *) dmp->data;
+			}
+			assert(diagp != NULL);
+		}
+	}
+
+	IREAD_UNLOCK(ipimap);
+
+	/*
+	 * invalidate any page of the inode extent freed from buffer cache;
+	 */
+	freepxd = iagp->inoext[extno];
+	xaddr = addressPXD(&iagp->inoext[extno]);
+	xlen = lengthPXD(&iagp->inoext[extno]);
+	invalidate_metapages(JFS_SBI(ip->i_sb)->direct_inode, xaddr, xlen);
+
+	/*
+	 *      update iag list(s) (careful update step 2)
+	 */
+	/* add the iag to the ag extent free list if this is the
+	 * first free extent for the iag.
+	 */
+	if (iagp->nfreeexts == 0) {
+		if (fwd >= 0)
+			aiagp->extfreeback = cpu_to_le32(iagno);
+
+		iagp->extfreefwd =
+		    cpu_to_le32(imap->im_agctl[agno].extfree);
+		iagp->extfreeback = -1;
+		imap->im_agctl[agno].extfree = iagno;
+	} else {
+		/* remove the iag from the ag extent list if all extents
+		 * are now free and place it on the inode map iag free list.
+		 */
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG - 1)) {
+			if (fwd >= 0)
+				aiagp->extfreeback = iagp->extfreeback;
+
+			if (back >= 0)
+				biagp->extfreefwd = iagp->extfreefwd;
+			else
+				imap->im_agctl[agno].extfree =
+				    le32_to_cpu(iagp->extfreefwd);
+
+			iagp->extfreefwd = iagp->extfreeback = -1;
+
+			IAGFREE_LOCK(imap);
+			iagp->iagfree = cpu_to_le32(imap->im_freeiag);
+			imap->im_freeiag = iagno;
+			IAGFREE_UNLOCK(imap);
+		}
+	}
+
+	/* remove the iag from the ag inode free list if freeing
+	 * this extent causes the iag to have no free inodes.
+	 */
+	if (iagp->nfreeinos == cpu_to_le32(INOSPEREXT - 1)) {
+		if ((int) le32_to_cpu(iagp->inofreefwd) >= 0)
+			ciagp->inofreeback = iagp->inofreeback;
+
+		if ((int) le32_to_cpu(iagp->inofreeback) >= 0)
+			diagp->inofreefwd = iagp->inofreefwd;
+		else
+			imap->im_agctl[agno].inofree =
+			    le32_to_cpu(iagp->inofreefwd);
+
+		iagp->inofreefwd = iagp->inofreeback = -1;
+	}
+
+	/* update the inode extent address and working map 
+	 * to reflect the free extent.
+	 * the permanent map should have been updated already 
+	 * for the inode being freed.
+	 */
+	assert(iagp->pmap[extno] == 0);
+	iagp->wmap[extno] = 0;
+	DBG_DIFREE(imap, inum);
+	PXDlength(&iagp->inoext[extno], 0);
+	PXDaddress(&iagp->inoext[extno], 0);
+
+	/* update the free extent and free inode summary maps
+	 * to reflect the freed extent.
+	 * the inode summary map is marked to indicate no inodes 
+	 * available for the freed extent.
+	 */
+	sword = extno >> L2EXTSPERSUM;
+	bitno = extno & (EXTSPERSUM - 1);
+	mask = HIGHORDER >> bitno;
+	iagp->inosmap[sword] |= cpu_to_le32(mask);
+	iagp->extsmap[sword] &= cpu_to_le32(~mask);
+
+	/* update the number of free inodes and number of free extents
+	 * for the iag.
+	 */
+	iagp->nfreeinos = cpu_to_le32(le32_to_cpu(iagp->nfreeinos) -
+				      (INOSPEREXT - 1));
+	iagp->nfreeexts = cpu_to_le32(le32_to_cpu(iagp->nfreeexts) + 1);
+
+	/* update the number of free inodes and backed inodes
+	 * at the ag and inode map level.
+	 */
+	imap->im_agctl[agno].numfree -= (INOSPEREXT - 1);
+	imap->im_agctl[agno].numinos -= INOSPEREXT;
+	atomic_sub(INOSPEREXT - 1, &imap->im_numfree);
+	atomic_sub(INOSPEREXT, &imap->im_numinos);
+
+	if (amp)
+		write_metapage(amp);
+	if (bmp)
+		write_metapage(bmp);
+	if (cmp)
+		write_metapage(cmp);
+	if (dmp)
+		write_metapage(dmp);
+
+	/*
+	 * start transaction to update block allocation map
+	 * for the inode extent freed;
+	 *
+	 * N.B. AG_LOCK is released and iag will be released below, and 
+	 * other thread may allocate inode from/reusing the ixad freed
+	 * BUT with new/different backing inode extent from the extent 
+	 * to be freed by the transaction;  
+	 */
+	tid = txBegin(ipimap->i_sb, COMMIT_FORCE);
+
+	/* acquire tlock of the iag page of the freed ixad 
+	 * to force the page NOHOMEOK (even though no data is
+	 * logged from the iag page) until NOREDOPAGE|FREEXTENT log 
+	 * for the free of the extent is committed;
+	 * write FREEXTENT|NOREDOPAGE log record
+	 * N.B. linelock is overlaid as freed extent descriptor;
+	 */
+	tlck = txLock(tid, ipimap, mp, tlckINODE | tlckFREE);
+	pxdlock = (pxdlock_t *) & tlck->lock;
+	pxdlock->flag = mlckFREEPXD;
+	pxdlock->pxd = freepxd;
+	pxdlock->index = 1;
+
+	write_metapage(mp);
+
+	iplist[0] = ipimap;
+
+	/*
+	 * logredo needs the IAG number and IAG extent index in order
+	 * to ensure that the IMap is consistent.  The least disruptive
+	 * way to pass these values through  to the transaction manager
+	 * is in the iplist array.  
+	 * 
+	 * It's not pretty, but it works.
+	 */
+	iplist[1] = (struct inode *) (size_t)iagno;
+	iplist[2] = (struct inode *) (size_t)extno;
+
+	rc = txCommit(tid, 1, &iplist[0], COMMIT_FORCE);	// D233382
+
+	txEnd(tid);
+
+	/* unlock the AG inode map information */
+	AG_UNLOCK(imap, agno);
+
+	return (0);
+
+      error_out:
+	IREAD_UNLOCK(ipimap);
+
+	if (amp)
+		release_metapage(amp);
+	if (bmp)
+		release_metapage(bmp);
+	if (cmp)
+		release_metapage(cmp);
+	if (dmp)
+		release_metapage(dmp);
+
+	AG_UNLOCK(imap, agno);
+
+	release_metapage(mp);
+
+	return (rc);
+}
+
+/*
+ * There are several places in the diAlloc* routines where we initialize
+ * the inode.
+ */
+static inline void
+diInitInode(struct inode *ip, int iagno, int ino, int extno, iag_t * iagp)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(ip->i_sb);
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+
+	ip->i_ino = (iagno << L2INOSPERIAG) + ino;
+	DBG_DIALLOC(JFS_IP(ipimap)->i_imap, ip->i_ino);
+	jfs_ip->ixpxd = iagp->inoext[extno];
+	jfs_ip->agno = BLKTOAG(le64_to_cpu(iagp->agstart), sbi);
+}
+
+
+/*
+ * NAME:        diAlloc(pip,dir,ip)
+ *
+ * FUNCTION:    allocate a disk inode from the inode working map 
+ *		for a fileset or aggregate.
+ *
+ * PARAMETERS:
+ *      pip  	- pointer to incore inode for the parent inode.
+ *      dir  	- TRUE if the new disk inode is for a directory.
+ *      ip  	- pointer to a new inode
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+int diAlloc(struct inode *pip, boolean_t dir, struct inode *ip)
+{
+	int rc, ino, iagno, addext, extno, bitno, sword;
+	int nwords, rem, i, agno;
+	u32 mask, inosmap, extsmap;
+	struct inode *ipimap;
+	metapage_t *mp;
+	ino_t inum;
+	iag_t *iagp;
+	imap_t *imap;
+
+	/* get the pointers to the inode map inode and the
+	 * corresponding imap control structure.
+	 */
+	ipimap = JFS_SBI(pip->i_sb)->ipimap;
+	imap = JFS_IP(ipimap)->i_imap;
+	JFS_IP(ip)->ipimap = ipimap;
+	JFS_IP(ip)->fileset = FILESYSTEM_I;
+
+	/* for a directory, the allocation policy is to start 
+	 * at the ag level using the preferred ag.
+	 */
+	if (dir == TRUE) {
+		agno = dbNextAG(JFS_SBI(pip->i_sb)->ipbmap);
+		AG_LOCK(imap, agno);
+		goto tryag;
+	}
+
+	/* for files, the policy starts off by trying to allocate from
+	 * the same iag containing the parent disk inode:
+	 * try to allocate the new disk inode close to the parent disk
+	 * inode, using parent disk inode number + 1 as the allocation
+	 * hint.  (we use a left-to-right policy to attempt to avoid
+	 * moving backward on the disk.)  compute the hint within the
+	 * file system and the iag.
+	 */
+	inum = pip->i_ino + 1;
+	ino = inum & (INOSPERIAG - 1);
+
+	/* back off the the hint if it is outside of the iag */
+	if (ino == 0)
+		inum = pip->i_ino;
+
+	/* get the ag number of this iag */
+	agno = JFS_IP(pip)->agno;
+
+	/* lock the AG inode map information */
+	AG_LOCK(imap, agno);
+
+	/* Get read lock on imap inode */
+	IREAD_LOCK(ipimap);
+
+	/* get the iag number and read the iag */
+	iagno = INOTOIAG(inum);
+	if ((rc = diIAGRead(imap, iagno, &mp))) {
+		IREAD_UNLOCK(ipimap);
+		return (rc);
+	}
+	iagp = (iag_t *) mp->data;
+
+	/* determine if new inode extent is allowed to be added to the iag.
+	 * new inode extent can be added to the iag if the ag
+	 * has less than 32 free disk inodes and the iag has free extents.
+	 */
+	addext = (imap->im_agctl[agno].numfree < 32 && iagp->nfreeexts);
+
+	/*
+	 *      try to allocate from the IAG
+	 */
+	/* check if the inode may be allocated from the iag 
+	 * (i.e. the inode has free inodes or new extent can be added).
+	 */
+	if (iagp->nfreeinos || addext) {
+		/* determine the extent number of the hint.
+		 */
+		extno = ino >> L2INOSPEREXT;
+
+		/* check if the extent containing the hint has backed
+		 * inodes.  if so, try to allocate within this extent.
+		 */
+		if (addressPXD(&iagp->inoext[extno])) {
+			bitno = ino & (INOSPEREXT - 1);
+			if ((bitno =
+			     diFindFree(le32_to_cpu(iagp->wmap[extno]),
+					bitno))
+			    < INOSPEREXT) {
+				ino = (extno << L2INOSPEREXT) + bitno;
+
+				/* a free inode (bit) was found within this
+				 * extent, so allocate it.
+				 */
+				rc = diAllocBit(imap, iagp, ino);
+				IREAD_UNLOCK(ipimap);
+				if (rc) {
+					assert(rc == EIO);
+				} else {
+					/* set the results of the allocation
+					 * and write the iag.
+					 */
+					diInitInode(ip, iagno, ino, extno,
+						    iagp);
+					mark_metapage_dirty(mp);
+				}
+				release_metapage(mp);
+
+				/* free the AG lock and return.
+				 */
+				AG_UNLOCK(imap, agno);
+				return (rc);
+			}
+
+			if (!addext)
+				extno =
+				    (extno ==
+				     EXTSPERIAG - 1) ? 0 : extno + 1;
+		}
+
+		/*
+		 * no free inodes within the extent containing the hint.
+		 *
+		 * try to allocate from the backed extents following
+		 * hint or, if appropriate (i.e. addext is true), allocate
+		 * an extent of free inodes at or following the extent
+		 * containing the hint.
+		 * 
+		 * the free inode and free extent summary maps are used
+		 * here, so determine the starting summary map position
+		 * and the number of words we'll have to examine.  again,
+		 * the approach is to allocate following the hint, so we
+		 * might have to initially ignore prior bits of the summary
+		 * map that represent extents prior to the extent containing
+		 * the hint and later revisit these bits.
+		 */
+		bitno = extno & (EXTSPERSUM - 1);
+		nwords = (bitno == 0) ? SMAPSZ : SMAPSZ + 1;
+		sword = extno >> L2EXTSPERSUM;
+
+		/* mask any prior bits for the starting words of the
+		 * summary map.
+		 */
+		mask = ONES << (EXTSPERSUM - bitno);
+		inosmap = le32_to_cpu(iagp->inosmap[sword]) | mask;
+		extsmap = le32_to_cpu(iagp->extsmap[sword]) | mask;
+
+		/* scan the free inode and free extent summary maps for
+		 * free resources.
+		 */
+		for (i = 0; i < nwords; i++) {
+			/* check if this word of the free inode summary
+			 * map describes an extent with free inodes.
+			 */
+			if (~inosmap) {
+				/* an extent with free inodes has been
+				 * found. determine the extent number
+				 * and the inode number within the extent.
+				 */
+				rem = diFindFree(inosmap, 0);
+				extno = (sword << L2EXTSPERSUM) + rem;
+				rem =
+				    diFindFree(le32_to_cpu
+					       (iagp->wmap[extno]), 0);
+				assert(rem < INOSPEREXT);
+
+				/* determine the inode number within the
+				 * iag and allocate the inode from the
+				 * map.
+				 */
+				ino = (extno << L2INOSPEREXT) + rem;
+				rc = diAllocBit(imap, iagp, ino);
+				IREAD_UNLOCK(ipimap);
+				if (rc) {
+					assert(rc == EIO);
+				} else {
+					/* set the results of the allocation
+					 * and write the iag.
+					 */
+					diInitInode(ip, iagno, ino, extno,
+						    iagp);
+					mark_metapage_dirty(mp);
+				}
+				release_metapage(mp);
+
+				/* free the AG lock and return.
+				 */
+				AG_UNLOCK(imap, agno);
+				return (rc);
+
+			}
+
+			/* check if we may allocate an extent of free
+			 * inodes and whether this word of the free
+			 * extents summary map describes a free extent.
+			 */
+			if (addext && ~extsmap) {
+				/* a free extent has been found.  determine
+				 * the extent number.
+				 */
+				rem = diFindFree(extsmap, 0);
+				extno = (sword << L2EXTSPERSUM) + rem;
+
+				/* allocate an extent of free inodes.
+				 */
+				if ((rc = diNewExt(imap, iagp, extno))) {
+					/* if there is no disk space for a
+					 * new extent, try to allocate the
+					 * disk inode from somewhere else.
+					 */
+					if (rc == ENOSPC)
+						break;
+
+					assert(rc == EIO);
+				} else {
+					/* set the results of the allocation
+					 * and write the iag.
+					 */
+					diInitInode(ip, iagno,
+						    extno << L2INOSPEREXT,
+						    extno, iagp);
+					mark_metapage_dirty(mp);
+				}
+				release_metapage(mp);
+				/* free the imap inode & the AG lock & return.
+				 */
+				IREAD_UNLOCK(ipimap);
+				AG_UNLOCK(imap, agno);
+				return (rc);
+			}
+
+			/* move on to the next set of summary map words.
+			 */
+			sword = (sword == SMAPSZ - 1) ? 0 : sword + 1;
+			inosmap = le32_to_cpu(iagp->inosmap[sword]);
+			extsmap = le32_to_cpu(iagp->extsmap[sword]);
+		}
+	}
+	/* unlock imap inode */
+	IREAD_UNLOCK(ipimap);
+
+	/* nothing doing in this iag, so release it. */
+	release_metapage(mp);
+
+      tryag:
+	/*
+	 * try to allocate anywhere within the same AG as the parent inode.
+	 */
+	rc = diAllocAG(imap, agno, dir, ip);
+
+	AG_UNLOCK(imap, agno);
+
+	if (rc != ENOSPC)
+		return (rc);
+
+	/*
+	 * try to allocate in any AG.
+	 */
+	return (diAllocAny(imap, agno, dir, ip));
+}
+
+
+/*
+ * NAME:        diAllocAG(imap,agno,dir,ip)
+ *
+ * FUNCTION:    allocate a disk inode from the allocation group.
+ *
+ *		this routine first determines if a new extent of free
+ *		inodes should be added for the allocation group, with
+ *		the current request satisfied from this extent. if this
+ *		is the case, an attempt will be made to do just that.  if
+ *		this attempt fails or it has been determined that a new 
+ *		extent should not be added, an attempt is made to satisfy
+ *		the request by allocating an existing (backed) free inode
+ *		from the allocation group.
+ *
+ * PRE CONDITION: Already have the AG lock for this AG.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      agno  	- allocation group to allocate from.
+ *      dir  	- TRUE if the new disk inode is for a directory.
+ *      ip  	- pointer to the new inode to be filled in on successful return
+ *		  with the disk inode number allocated, its extent address
+ *		  and the start of the ag.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int
+diAllocAG(imap_t * imap, int agno, boolean_t dir, struct inode *ip)
+{
+	int rc, addext, numfree, numinos;
+
+	/* get the number of free and the number of backed disk 
+	 * inodes currently within the ag.
+	 */
+	numfree = imap->im_agctl[agno].numfree;
+	numinos = imap->im_agctl[agno].numinos;
+
+	if (numfree > numinos) {
+		jERROR(1,("diAllocAG: numfree > numinos\n"));
+		updateSuper(ip->i_sb, FM_DIRTY);
+		return EIO;
+	}
+
+	/* determine if we should allocate a new extent of free inodes
+	 * within the ag: for directory inodes, add a new extent
+	 * if there are a small number of free inodes or number of free
+	 * inodes is a small percentage of the number of backed inodes.
+	 */
+	if (dir == TRUE)
+		addext = (numfree < 64 ||
+			  (numfree < 256
+			   && ((numfree * 100) / numinos) <= 20));
+	else
+		addext = (numfree == 0);
+
+	/*
+	 * try to allocate a new extent of free inodes.
+	 */
+	if (addext) {
+		/* if free space is not avaliable for this new extent, try
+		 * below to allocate a free and existing (already backed)
+		 * inode from the ag.
+		 */
+		if ((rc = diAllocExt(imap, agno, ip)) != ENOSPC)
+			return (rc);
+	}
+
+	/*
+	 * try to allocate an existing free inode from the ag.
+	 */
+	return (diAllocIno(imap, agno, ip));
+}
+
+
+/*
+ * NAME:        diAllocAny(imap,agno,dir,iap)
+ *
+ * FUNCTION:    allocate a disk inode from any other allocation group.
+ *
+ *		this routine is called when an allocation attempt within
+ *		the primary allocation group has failed. if attempts to
+ *		allocate an inode from any allocation group other than the
+ *		specified primary group.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      agno  	- primary allocation group (to avoid).
+ *      dir  	- TRUE if the new disk inode is for a directory.
+ *      ip  	- pointer to a new inode to be filled in on successful return
+ *		  with the disk inode number allocated, its extent address
+ *		  and the start of the ag.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int
+diAllocAny(imap_t * imap, int agno, boolean_t dir, struct inode *ip)
+{
+	int ag, rc;
+	int maxag = JFS_SBI(imap->im_ipimap->i_sb)->bmap->db_maxag;
+
+
+	/* try to allocate from the ags following agno up to 
+	 * the maximum ag number.
+	 */
+	for (ag = agno + 1; ag <= maxag; ag++) {
+		AG_LOCK(imap, ag);
+
+		rc = diAllocAG(imap, ag, dir, ip);
+
+		AG_UNLOCK(imap, ag);
+
+		if (rc != ENOSPC)
+			return (rc);
+	}
+
+	/* try to allocate from the ags in front of agno.
+	 */
+	for (ag = 0; ag < agno; ag++) {
+		AG_LOCK(imap, ag);
+
+		rc = diAllocAG(imap, ag, dir, ip);
+
+		AG_UNLOCK(imap, ag);
+
+		if (rc != ENOSPC)
+			return (rc);
+	}
+
+	/* no free disk inodes.
+	 */
+	return (ENOSPC);
+}
+
+
+/*
+ * NAME:        diAllocIno(imap,agno,ip)
+ *
+ * FUNCTION:    allocate a disk inode from the allocation group's free
+ *		inode list, returning an error if this free list is
+ *		empty (i.e. no iags on the list).
+ *
+ *		allocation occurs from the first iag on the list using
+ *		the iag's free inode summary map to find the leftmost
+ *		free inode in the iag. 
+ *		
+ * PRE CONDITION: Already have AG lock for this AG.
+ *		
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      agno  	- allocation group.
+ *      ip  	- pointer to new inode to be filled in on successful return
+ *		  with the disk inode number allocated, its extent address
+ *		  and the start of the ag.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int diAllocIno(imap_t * imap, int agno, struct inode *ip)
+{
+	int iagno, ino, rc, rem, extno, sword;
+	metapage_t *mp;
+	iag_t *iagp;
+
+	/* check if there are iags on the ag's free inode list.
+	 */
+	if ((iagno = imap->im_agctl[agno].inofree) < 0)
+		return (ENOSPC);
+
+	/* obtain read lock on imap inode */
+	IREAD_LOCK(imap->im_ipimap);
+
+	/* read the iag at the head of the list.
+	 */
+	if ((rc = diIAGRead(imap, iagno, &mp))) {
+		IREAD_UNLOCK(imap->im_ipimap);
+		return (rc);
+	}
+	iagp = (iag_t *) mp->data;
+
+	/* better be free inodes in this iag if it is on the
+	 * list.
+	 */
+	//assert(iagp->nfreeinos);
+	if (!iagp->nfreeinos) {
+		jERROR(1,
+		       ("diAllocIno: nfreeinos = 0, but iag on freelist\n"));
+		jERROR(1, ("  agno = %d, iagno = %d\n", agno, iagno));
+		dump_mem("iag", iagp, 64);
+		updateSuper(ip->i_sb, FM_DIRTY);
+		return EIO;
+	}
+
+	/* scan the free inode summary map to find an extent
+	 * with free inodes.
+	 */
+	for (sword = 0;; sword++) {
+		assert(sword < SMAPSZ);
+
+		if (~iagp->inosmap[sword])
+			break;
+	}
+
+	/* found a extent with free inodes. determine
+	 * the extent number.
+	 */
+	rem = diFindFree(le32_to_cpu(iagp->inosmap[sword]), 0);
+	assert(rem < EXTSPERSUM);
+	extno = (sword << L2EXTSPERSUM) + rem;
+
+	/* find the first free inode in the extent.
+	 */
+	rem = diFindFree(le32_to_cpu(iagp->wmap[extno]), 0);
+	assert(rem < INOSPEREXT);
+
+	/* compute the inode number within the iag. 
+	 */
+	ino = (extno << L2INOSPEREXT) + rem;
+
+	/* allocate the inode.
+	 */
+	rc = diAllocBit(imap, iagp, ino);
+	IREAD_UNLOCK(imap->im_ipimap);
+	if (rc) {
+		release_metapage(mp);
+		return (rc);
+	}
+
+	/* set the results of the allocation and write the iag.
+	 */
+	diInitInode(ip, iagno, ino, extno, iagp);
+	write_metapage(mp);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        diAllocExt(imap,agno,ip)
+ *
+ * FUNCTION:   	add a new extent of free inodes to an iag, allocating
+ *	       	an inode from this extent to satisfy the current allocation
+ *	       	request.
+ *		
+ *		this routine first tries to find an existing iag with free
+ *		extents through the ag free extent list.  if list is not
+ *		empty, the head of the list will be selected as the home
+ *		of the new extent of free inodes.  otherwise (the list is
+ *		empty), a new iag will be allocated for the ag to contain
+ *		the extent.
+ *		
+ *		once an iag has been selected, the free extent summary map
+ *		is used to locate a free extent within the iag and diNewExt()
+ *		is called to initialize the extent, with initialization
+ *		including the allocation of the first inode of the extent
+ *		for the purpose of satisfying this request.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      agno  	- allocation group number.
+ *      ip  	- pointer to new inode to be filled in on successful return
+ *		  with the disk inode number allocated, its extent address
+ *		  and the start of the ag.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int diAllocExt(imap_t * imap, int agno, struct inode *ip)
+{
+	int rem, iagno, sword, extno, rc;
+	metapage_t *mp;
+	iag_t *iagp;
+
+	/* check if the ag has any iags with free extents.  if not,
+	 * allocate a new iag for the ag.
+	 */
+	if ((iagno = imap->im_agctl[agno].extfree) < 0) {
+		/* If successful, diNewIAG will obtain the read lock on the
+		 * imap inode.
+		 */
+		if ((rc = diNewIAG(imap, &iagno, agno, &mp))) {
+			return (rc);
+		}
+		iagp = (iag_t *) mp->data;
+
+		/* set the ag number if this a brand new iag
+		 */
+		iagp->agstart =
+		    cpu_to_le64(AGTOBLK(agno, imap->im_ipimap));
+	} else {
+		/* read the iag.
+		 */
+		IREAD_LOCK(imap->im_ipimap);
+		if ((rc = diIAGRead(imap, iagno, &mp))) {
+			assert(0);
+		}
+		iagp = (iag_t *) mp->data;
+	}
+
+	/* using the free extent summary map, find a free extent.
+	 */
+	for (sword = 0;; sword++) {
+		assert(sword < SMAPSZ);
+		if (~iagp->extsmap[sword])
+			break;
+	}
+
+	/* determine the extent number of the free extent.
+	 */
+	rem = diFindFree(le32_to_cpu(iagp->extsmap[sword]), 0);
+	assert(rem < EXTSPERSUM);
+	extno = (sword << L2EXTSPERSUM) + rem;
+
+	/* initialize the new extent.
+	 */
+	rc = diNewExt(imap, iagp, extno);
+	IREAD_UNLOCK(imap->im_ipimap);
+	if (rc) {
+		/* something bad happened.  if a new iag was allocated,
+		 * place it back on the inode map's iag free list, and
+		 * clear the ag number information.
+		 */
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG)) {
+			IAGFREE_LOCK(imap);
+			iagp->iagfree = cpu_to_le32(imap->im_freeiag);
+			imap->im_freeiag = iagno;
+			IAGFREE_UNLOCK(imap);
+		}
+		write_metapage(mp);
+		return (rc);
+	}
+
+	/* set the results of the allocation and write the iag.
+	 */
+	diInitInode(ip, iagno, extno << L2INOSPEREXT, extno, iagp);
+
+	write_metapage(mp);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        diAllocBit(imap,iagp,ino)
+ *
+ * FUNCTION:   	allocate a backed inode from an iag.
+ *
+ *		this routine performs the mechanics of allocating a
+ *		specified inode from a backed extent.
+ *
+ *		if the inode to be allocated represents the last free
+ *		inode within the iag, the iag will be removed from the
+ *		ag free inode list.
+ *
+ *		a careful update approach is used to provide consistency
+ *		in the face of updates to multiple buffers.  under this
+ *		approach, all required buffers are obtained before making
+ *		any updates and are held all are updates are complete.
+ *		
+ * PRE CONDITION: Already have buffer lock on iagp.  Already have AG lock on
+ *	this AG.  Must have read lock on imap inode.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      iagp  	- pointer to iag. 
+ *      ino   	- inode number to be allocated within the iag.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int diAllocBit(imap_t * imap, iag_t * iagp, int ino)
+{
+	int extno, bitno, agno, sword, rc;
+	metapage_t *amp, *bmp;
+	iag_t *aiagp = 0, *biagp = 0;
+	u32 mask;
+
+	/* check if this is the last free inode within the iag.
+	 * if so, it will have to be removed from the ag free
+	 * inode list, so get the iags preceeding and following
+	 * it on the list.
+	 */
+	if (iagp->nfreeinos == cpu_to_le32(1)) {
+		amp = bmp = NULL;
+
+		if ((int) le32_to_cpu(iagp->inofreefwd) >= 0) {
+			if ((rc =
+			     diIAGRead(imap, le32_to_cpu(iagp->inofreefwd),
+				       &amp)))
+				return (rc);
+			aiagp = (iag_t *) amp->data;
+		}
+
+		if ((int) le32_to_cpu(iagp->inofreeback) >= 0) {
+			if ((rc =
+			     diIAGRead(imap,
+				       le32_to_cpu(iagp->inofreeback),
+				       &bmp))) {
+				if (amp)
+					release_metapage(amp);
+				return (rc);
+			}
+			biagp = (iag_t *) bmp->data;
+		}
+	}
+
+	/* get the ag number, extent number, inode number within
+	 * the extent.
+	 */
+	agno = BLKTOAG(le64_to_cpu(iagp->agstart), JFS_SBI(imap->im_ipimap->i_sb));
+	extno = ino >> L2INOSPEREXT;
+	bitno = ino & (INOSPEREXT - 1);
+
+	/* compute the mask for setting the map.
+	 */
+	mask = HIGHORDER >> bitno;
+
+	/* the inode should be free and backed.
+	 */
+	assert((le32_to_cpu(iagp->pmap[extno]) & mask) == 0);
+	assert((le32_to_cpu(iagp->wmap[extno]) & mask) == 0);
+	assert(addressPXD(&iagp->inoext[extno]) != 0);
+
+	/* mark the inode as allocated in the working map.
+	 */
+	iagp->wmap[extno] |= cpu_to_le32(mask);
+
+	/* check if all inodes within the extent are now
+	 * allocated.  if so, update the free inode summary
+	 * map to reflect this.
+	 */
+	if (iagp->wmap[extno] == ONES) {
+		sword = extno >> L2EXTSPERSUM;
+		bitno = extno & (EXTSPERSUM - 1);
+		iagp->inosmap[sword] |= cpu_to_le32(HIGHORDER >> bitno);
+	}
+
+	/* if this was the last free inode in the iag, remove the
+	 * iag from the ag free inode list.
+	 */
+	if (iagp->nfreeinos == cpu_to_le32(1)) {
+		if (amp) {
+			aiagp->inofreeback = iagp->inofreeback;
+			write_metapage(amp);
+		}
+
+		if (bmp) {
+			biagp->inofreefwd = iagp->inofreefwd;
+			write_metapage(bmp);
+		} else {
+			imap->im_agctl[agno].inofree =
+			    le32_to_cpu(iagp->inofreefwd);
+		}
+		iagp->inofreefwd = iagp->inofreeback = -1;
+	}
+
+	/* update the free inode count at the iag, ag, inode
+	 * map levels.
+	 */
+	iagp->nfreeinos = cpu_to_le32(le32_to_cpu(iagp->nfreeinos) - 1);
+	imap->im_agctl[agno].numfree -= 1;
+	atomic_dec(&imap->im_numfree);
+
+	return (0);
+}
+
+
+/*
+ * NAME:        diNewExt(imap,iagp,extno)
+ *
+ * FUNCTION:    initialize a new extent of inodes for an iag, allocating
+ *	        the first inode of the extent for use for the current
+ *	        allocation request.
+ *
+ *		disk resources are allocated for the new extent of inodes
+ *		and the inodes themselves are initialized to reflect their
+ *		existence within the extent (i.e. their inode numbers and
+ *		inode extent addresses are set) and their initial state
+ *		(mode and link count are set to zero).
+ *
+ *		if the iag is new, it is not yet on an ag extent free list
+ *		but will now be placed on this list.
+ *
+ *		if the allocation of the new extent causes the iag to
+ *		have no free extent, the iag will be removed from the
+ *		ag extent free list.
+ *
+ *		if the iag has no free backed inodes, it will be placed
+ *		on the ag free inode list, since the addition of the new
+ *		extent will now cause it to have free inodes.
+ *
+ *		a careful update approach is used to provide consistency
+ *		(i.e. list consistency) in the face of updates to multiple
+ *		buffers.  under this approach, all required buffers are
+ *		obtained before making any updates and are held until all
+ *		updates are complete.
+ *		
+ * PRE CONDITION: Already have buffer lock on iagp.  Already have AG lock on
+ *	this AG.  Must have read lock on imap inode.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      iagp  	- pointer to iag. 
+ *      extno  	- extent number.
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ */
+static int diNewExt(imap_t * imap, iag_t * iagp, int extno)
+{
+	int agno, iagno, fwd, back, freei = 0, sword, rc;
+	iag_t *aiagp = 0, *biagp = 0, *ciagp = 0;
+	metapage_t *amp, *bmp, *cmp, *dmp;
+	struct inode *ipimap;
+	s64 blkno, hint;
+	int i, j;
+	u32 mask;
+	ino_t ino;
+	dinode_t *dp;
+	struct jfs_sb_info *sbi;
+
+	/* better have free extents.
+	 */
+	assert(iagp->nfreeexts);
+
+	/* get the inode map inode.
+	 */
+	ipimap = imap->im_ipimap;
+	sbi = JFS_SBI(ipimap->i_sb);
+
+	amp = bmp = cmp = NULL;
+
+	/* get the ag and iag numbers for this iag.
+	 */
+	agno = BLKTOAG(le64_to_cpu(iagp->agstart), sbi);
+	iagno = le32_to_cpu(iagp->iagnum);
+
+	/* check if this is the last free extent within the
+	 * iag.  if so, the iag must be removed from the ag
+	 * free extent list, so get the iags preceeding and
+	 * following the iag on this list.
+	 */
+	if (iagp->nfreeexts == cpu_to_le32(1)) {
+		if ((fwd = le32_to_cpu(iagp->extfreefwd)) >= 0) {
+			if ((rc = diIAGRead(imap, fwd, &amp)))
+				return (rc);
+			aiagp = (iag_t *) amp->data;
+		}
+
+		if ((back = le32_to_cpu(iagp->extfreeback)) >= 0) {
+			if ((rc = diIAGRead(imap, back, &bmp)))
+				goto error_out;
+			biagp = (iag_t *) bmp->data;
+		}
+	} else {
+		/* the iag has free extents.  if all extents are free
+		 * (as is the case for a newly allocated iag), the iag
+		 * must be added to the ag free extent list, so get
+		 * the iag at the head of the list in preparation for
+		 * adding this iag to this list.
+		 */
+		fwd = back = -1;
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG)) {
+			if ((fwd = imap->im_agctl[agno].extfree) >= 0) {
+				if ((rc = diIAGRead(imap, fwd, &amp)))
+					goto error_out;
+				aiagp = (iag_t *) amp->data;
+			}
+		}
+	}
+
+	/* check if the iag has no free inodes.  if so, the iag
+	 * will have to be added to the ag free inode list, so get
+	 * the iag at the head of the list in preparation for
+	 * adding this iag to this list.  in doing this, we must
+	 * check if we already have the iag at the head of
+	 * the list in hand.
+	 */
+	if (iagp->nfreeinos == 0) {
+		freei = imap->im_agctl[agno].inofree;
+
+		if (freei >= 0) {
+			if (freei == fwd) {
+				ciagp = aiagp;
+			} else if (freei == back) {
+				ciagp = biagp;
+			} else {
+				if ((rc = diIAGRead(imap, freei, &cmp)))
+					goto error_out;
+				ciagp = (iag_t *) cmp->data;
+			}
+			assert(ciagp != NULL);
+		}
+	}
+
+	/* allocate disk space for the inode extent.
+	 */
+	if ((extno == 0) || (addressPXD(&iagp->inoext[extno - 1]) == 0))
+		hint = ((s64) agno << sbi->bmap->db_agl2size) - 1;
+	else
+		hint = addressPXD(&iagp->inoext[extno - 1]) +
+		    lengthPXD(&iagp->inoext[extno - 1]) - 1;
+
+	if ((rc = dbAlloc(ipimap, hint, (s64) imap->im_nbperiext, &blkno)))
+		goto error_out;
+
+	/* compute the inode number of the first inode within the
+	 * extent.
+	 */
+	ino = (iagno << L2INOSPERIAG) + (extno << L2INOSPEREXT);
+
+	/* initialize the inodes within the newly allocated extent a
+	 * page at a time.
+	 */
+	for (i = 0; i < imap->im_nbperiext; i += sbi->nbperpage) {
+		/* get a buffer for this page of disk inodes.
+		 */
+		dmp = get_metapage(ipimap, blkno + i, PSIZE, 1);
+		if (dmp == NULL) {
+			rc = EIO;
+			goto error_out;
+		}
+		dp = (dinode_t *) dmp->data;
+
+		/* initialize the inode number, mode, link count and
+		 * inode extent address.
+		 */
+		for (j = 0; j < INOSPERPAGE; j++, dp++, ino++) {
+			dp->di_inostamp = cpu_to_le32(sbi->inostamp);
+			dp->di_number = cpu_to_le32(ino);
+			dp->di_fileset = cpu_to_le32(FILESYSTEM_I);
+			dp->di_mode = 0;
+			dp->di_nlink = 0;
+			PXDaddress(&(dp->di_ixpxd), blkno);
+			PXDlength(&(dp->di_ixpxd), imap->im_nbperiext);
+		}
+		write_metapage(dmp);
+	}
+
+	/* if this is the last free extent within the iag, remove the
+	 * iag from the ag free extent list.
+	 */
+	if (iagp->nfreeexts == cpu_to_le32(1)) {
+		if (fwd >= 0)
+			aiagp->extfreeback = iagp->extfreeback;
+
+		if (back >= 0)
+			biagp->extfreefwd = iagp->extfreefwd;
+		else
+			imap->im_agctl[agno].extfree =
+			    le32_to_cpu(iagp->extfreefwd);
+
+		iagp->extfreefwd = iagp->extfreeback = -1;
+	} else {
+		/* if the iag has all free extents (newly allocated iag),
+		 * add the iag to the ag free extent list.
+		 */
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG)) {
+			if (fwd >= 0)
+				aiagp->extfreeback = cpu_to_le32(iagno);
+
+			iagp->extfreefwd = cpu_to_le32(fwd);
+			iagp->extfreeback = -1;
+			imap->im_agctl[agno].extfree = iagno;
+		}
+	}
+
+	/* if the iag has no free inodes, add the iag to the
+	 * ag free inode list.
+	 */
+	if (iagp->nfreeinos == 0) {
+		if (freei >= 0)
+			ciagp->inofreeback = cpu_to_le32(iagno);
+
+		iagp->inofreefwd =
+		    cpu_to_le32(imap->im_agctl[agno].inofree);
+		iagp->inofreeback = -1;
+		imap->im_agctl[agno].inofree = iagno;
+	}
+
+	/* initialize the extent descriptor of the extent. */
+	PXDlength(&iagp->inoext[extno], imap->im_nbperiext);
+	PXDaddress(&iagp->inoext[extno], blkno);
+
+	/* initialize the working and persistent map of the extent.
+	 * the working map will be initialized such that
+	 * it indicates the first inode of the extent is allocated.
+	 */
+	iagp->wmap[extno] = cpu_to_le32(HIGHORDER);
+	iagp->pmap[extno] = 0;
+
+	/* update the free inode and free extent summary maps
+	 * for the extent to indicate the extent has free inodes
+	 * and no longer represents a free extent.
+	 */
+	sword = extno >> L2EXTSPERSUM;
+	mask = HIGHORDER >> (extno & (EXTSPERSUM - 1));
+	iagp->extsmap[sword] |= cpu_to_le32(mask);
+	iagp->inosmap[sword] &= cpu_to_le32(~mask);
+
+	/* update the free inode and free extent counts for the
+	 * iag.
+	 */
+	iagp->nfreeinos = cpu_to_le32(le32_to_cpu(iagp->nfreeinos) +
+				      (INOSPEREXT - 1));
+	iagp->nfreeexts = cpu_to_le32(le32_to_cpu(iagp->nfreeexts) - 1);
+
+	/* update the free and backed inode counts for the ag.
+	 */
+	imap->im_agctl[agno].numfree += (INOSPEREXT - 1);
+	imap->im_agctl[agno].numinos += INOSPEREXT;
+
+	/* update the free and backed inode counts for the inode map.
+	 */
+	atomic_add(INOSPEREXT - 1, &imap->im_numfree);
+	atomic_add(INOSPEREXT, &imap->im_numinos);
+
+	/* write the iags.
+	 */
+	if (amp)
+		write_metapage(amp);
+	if (bmp)
+		write_metapage(bmp);
+	if (cmp)
+		write_metapage(cmp);
+
+	return (0);
+
+      error_out:
+
+	/* release the iags.
+	 */
+	if (amp)
+		release_metapage(amp);
+	if (bmp)
+		release_metapage(bmp);
+	if (cmp)
+		release_metapage(cmp);
+
+	return (rc);
+}
+
+
+/*
+ * NAME:        diNewIAG(imap,iagnop,agno)
+ *
+ * FUNCTION:   	allocate a new iag for an allocation group.
+ *		
+ *		first tries to allocate the iag from the inode map 
+ *		iagfree list:  
+ *		if the list has free iags, the head of the list is removed 
+ *		and returned to satisfy the request.
+ *		if the inode map's iag free list is empty, the inode map
+ *		is extended to hold a new iag. this new iag is initialized
+ *		and returned to satisfy the request.
+ *
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      iagnop 	- pointer to an iag number set with the number of the
+ *		  newly allocated iag upon successful return.
+ *      agno  	- allocation group number.
+ *	bpp	- Buffer pointer to be filled in with new IAG's buffer
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      ENOSPC 	- insufficient disk resources.
+ *      EIO  	- i/o error.
+ *
+ * serialization: 
+ *	AG lock held on entry/exit;
+ *	write lock on the map is held inside;
+ *	read lock on the map is held on successful completion;
+ *
+ * note: new iag transaction: 
+ * . synchronously write iag;
+ * . write log of xtree and inode  of imap;
+ * . commit;
+ * . synchronous write of xtree (right to left, bottom to top);
+ * . at start of logredo(): init in-memory imap with one additional iag page;
+ * . at end of logredo(): re-read imap inode to determine
+ *   new imap size;
+ */
+static int
+diNewIAG(imap_t * imap, int *iagnop, int agno, metapage_t ** mpp)
+{
+	int rc;
+	int iagno, i, xlen;
+	struct inode *ipimap;
+	struct super_block *sb;
+	struct jfs_sb_info *sbi;
+	metapage_t *mp;
+	iag_t *iagp;
+	s64 xaddr = 0;
+	s64 blkno;
+	tid_t tid;
+#ifdef _STILL_TO_PORT
+	xad_t xad;
+#endif				/*  _STILL_TO_PORT */
+	struct inode *iplist[1];
+
+	/* pick up pointers to the inode map and mount inodes */
+	ipimap = imap->im_ipimap;
+	sb = ipimap->i_sb;
+	sbi = JFS_SBI(sb);
+
+	/* acquire the free iag lock */
+	IAGFREE_LOCK(imap);
+
+	/* if there are any iags on the inode map free iag list, 
+	 * allocate the iag from the head of the list.
+	 */
+	if (imap->im_freeiag >= 0) {
+		/* pick up the iag number at the head of the list */
+		iagno = imap->im_freeiag;
+
+		/* determine the logical block number of the iag */
+		blkno = IAGTOLBLK(iagno, sbi->l2nbperpage);
+	} else {
+		/* no free iags. the inode map will have to be extented
+		 * to include a new iag.
+		 */
+
+		/* acquire inode map lock */
+		IWRITE_LOCK(ipimap);
+
+		assert(ipimap->i_size >> L2PSIZE == imap->im_nextiag + 1);
+
+		/* get the next avaliable iag number */
+		iagno = imap->im_nextiag;
+
+		/* make sure that we have not exceeded the maximum inode
+		 * number limit.
+		 */
+		if (iagno > (MAXIAGS - 1)) {
+			/* release the inode map lock */
+			IWRITE_UNLOCK(ipimap);
+
+			rc = ENOSPC;
+			goto out;
+		}
+
+		/*
+		 * synchronously append new iag page.
+		 */
+		/* determine the logical address of iag page to append */
+		blkno = IAGTOLBLK(iagno, sbi->l2nbperpage);
+
+		/* Allocate extent for new iag page */
+		xlen = sbi->nbperpage;
+		if ((rc = dbAlloc(ipimap, 0, (s64) xlen, &xaddr))) {
+			/* release the inode map lock */
+			IWRITE_UNLOCK(ipimap);
+
+			goto out;
+		}
+
+		/* assign a buffer for the page */
+		mp = get_metapage(ipimap, xaddr, PSIZE, 1);
+		//bp = bmAssign(ipimap, blkno, xaddr, PSIZE, bmREAD_PAGE);
+		if (!mp) {
+			/* Free the blocks allocated for the iag since it was
+			 * not successfully added to the inode map
+			 */
+			dbFree(ipimap, xaddr, (s64) xlen);
+
+			/* release the inode map lock */
+			IWRITE_UNLOCK(ipimap);
+
+			rc = EIO;
+			goto out;
+		}
+		iagp = (iag_t *) mp->data;
+
+		/* init the iag */
+		memset(iagp, 0, sizeof(iag_t));
+		iagp->iagnum = cpu_to_le32(iagno);
+		iagp->inofreefwd = iagp->inofreeback = -1;
+		iagp->extfreefwd = iagp->extfreeback = -1;
+		iagp->iagfree = -1;
+		iagp->nfreeinos = 0;
+		iagp->nfreeexts = cpu_to_le32(EXTSPERIAG);
+
+		/* initialize the free inode summary map (free extent
+		 * summary map initialization handled by bzero).
+		 */
+		for (i = 0; i < SMAPSZ; i++)
+			iagp->inosmap[i] = ONES;
+
+		flush_metapage(mp);
+#ifdef _STILL_TO_PORT
+		/* synchronously write the iag page */
+		if (bmWrite(bp)) {
+			/* Free the blocks allocated for the iag since it was
+			 * not successfully added to the inode map
+			 */
+			dbFree(ipimap, xaddr, (s64) xlen);
+
+			/* release the inode map lock */
+			IWRITE_UNLOCK(ipimap);
+
+			rc = EIO;
+			goto out;
+		}
+
+		/* Now the iag is on disk */
+
+		/*
+		 * start tyransaction of update of the inode map
+		 * addressing structure pointing to the new iag page;
+		 */
+#endif				/*  _STILL_TO_PORT */
+		tid = txBegin(sb, COMMIT_FORCE);
+
+		/* update the inode map addressing structure to point to it */
+		if ((rc =
+		     xtInsert(tid, ipimap, 0, blkno, xlen, &xaddr, 0))) {
+			/* Free the blocks allocated for the iag since it was
+			 * not successfully added to the inode map
+			 */
+			dbFree(ipimap, xaddr, (s64) xlen);
+
+			/* release the inode map lock */
+			IWRITE_UNLOCK(ipimap);
+
+			goto out;
+		}
+
+		/* update the inode map's inode to reflect the extension */
+		ipimap->i_size += PSIZE;
+		ipimap->i_blocks += LBLK2PBLK(sb, xlen);
+
+		/*
+		 * txCommit(COMMIT_FORCE) will synchronously write address 
+		 * index pages and inode after commit in careful update order 
+		 * of address index pages (right to left, bottom up);
+		 */
+		iplist[0] = ipimap;
+		rc = txCommit(tid, 1, &iplist[0], COMMIT_FORCE);
+
+		txEnd(tid);
+
+		duplicateIXtree(sb, blkno, xlen, &xaddr);
+
+		/* update the next avaliable iag number */
+		imap->im_nextiag += 1;
+
+		/* Add the iag to the iag free list so we don't lose the iag
+		 * if a failure happens now.
+		 */
+		imap->im_freeiag = iagno;
+
+		/* Until we have logredo working, we want the imap inode &
+		 * control page to be up to date.
+		 */
+		diSync(ipimap);
+
+		/* release the inode map lock */
+		IWRITE_UNLOCK(ipimap);
+	}
+
+	/* obtain read lock on map */
+	IREAD_LOCK(ipimap);
+
+	/* read the iag */
+	if ((rc = diIAGRead(imap, iagno, &mp))) {
+		IREAD_UNLOCK(ipimap);
+		rc = EIO;
+		goto out;
+	}
+	iagp = (iag_t *) mp->data;
+
+	/* remove the iag from the iag free list */
+	imap->im_freeiag = le32_to_cpu(iagp->iagfree);
+	iagp->iagfree = -1;
+
+	/* set the return iag number and buffer pointer */
+	*iagnop = iagno;
+	*mpp = mp;
+
+      out:
+	/* release the iag free lock */
+	IAGFREE_UNLOCK(imap);
+
+	return (rc);
+}
+
+/*
+ * NAME:        diIAGRead()
+ *
+ * FUNCTION:    get the buffer for the specified iag within a fileset
+ *		or aggregate inode map.
+ *		
+ * PARAMETERS:
+ *      imap  	- pointer to inode map control structure.
+ *      iagno  	- iag number.
+ *      bpp  	- point to buffer pointer to be filled in on successful
+ *		  exit.
+ *
+ * SERIALIZATION:
+ *	must have read lock on imap inode
+ *	(When called by diExtendFS, the filesystem is quiesced, therefore
+ *	 the read lock is unnecessary.)
+ *
+ * RETURN VALUES:
+ *      0       - success.
+ *      EIO  	- i/o error.
+ */
+static int diIAGRead(imap_t * imap, int iagno, metapage_t ** mpp)
+{
+	struct inode *ipimap = imap->im_ipimap;
+	s64 blkno;
+
+	/* compute the logical block number of the iag. */
+	blkno = IAGTOLBLK(iagno, JFS_SBI(ipimap->i_sb)->l2nbperpage);
+
+	/* read the iag. */
+	*mpp = read_metapage(ipimap, blkno, PSIZE, 0);
+	if (*mpp == NULL) {
+		return (EIO);
+	}
+
+	return (0);
+}
+
+/*
+ * NAME:        diFindFree()
+ *
+ * FUNCTION:    find the first free bit in a word starting at
+ *		the specified bit position.
+ *
+ * PARAMETERS:
+ *      word  	- word to be examined.
+ *      start  	- starting bit position.
+ *
+ * RETURN VALUES:
+ *      bit position of first free bit in the word or 32 if
+ *	no free bits were found.
+ */
+static int diFindFree(u32 word, int start)
+{
+	int bitno;
+	assert(start < 32);
+	/* scan the word for the first free bit. */
+	for (word <<= start, bitno = start; bitno < 32;
+	     bitno++, word <<= 1) {
+		if ((word & HIGHORDER) == 0)
+			break;
+	}
+	return (bitno);
+}
+
+/*
+ * NAME:	diUpdatePMap()
+ *                                                                    
+ * FUNCTION: Update the persistent map in an IAG for the allocation or 
+ *	freeing of the specified inode.
+ *                                                                    
+ * PRE CONDITIONS: Working map has already been updated for allocate.
+ *
+ * PARAMETERS:
+ *	ipimap	- Incore inode map inode
+ *	inum	- Number of inode to mark in permanent map
+ *	is_free	- If TRUE indicates inode should be marked freed, otherwise
+ *		  indicates inode should be marked allocated.
+ *
+ * RETURNS: 0 for success
+ */
+int
+diUpdatePMap(struct inode *ipimap,
+	     unsigned long inum, boolean_t is_free, tblock_t * tblk)
+{
+	int rc;
+	iag_t *iagp;
+	metapage_t *mp;
+	int iagno, ino, extno, bitno;
+	imap_t *imap;
+	u32 mask;
+	log_t *log;
+	int lsn, difft, diffp;
+
+	imap = JFS_IP(ipimap)->i_imap;
+	/* get the iag number containing the inode */
+	iagno = INOTOIAG(inum);
+	/* make sure that the iag is contained within the map */
+	assert(iagno < imap->im_nextiag);
+	/* read the iag */
+	IREAD_LOCK(ipimap);
+	rc = diIAGRead(imap, iagno, &mp);
+	IREAD_UNLOCK(ipimap);
+	if (rc)
+		return (rc);
+	iagp = (iag_t *) mp->data;
+	/* get the inode number and extent number of the inode within
+	 * the iag and the inode number within the extent.
+	 */
+	ino = inum & (INOSPERIAG - 1);
+	extno = ino >> L2INOSPEREXT;
+	bitno = ino & (INOSPEREXT - 1);
+	mask = HIGHORDER >> bitno;
+	/* 
+	 * mark the inode free in persistent map:
+	 */
+	if (is_free == TRUE) {
+		/* The inode should have been allocated both in working
+		 * map and in persistent map;
+		 * the inode will be freed from working map at the release
+		 * of last reference release;
+		 */
+//              assert(le32_to_cpu(iagp->wmap[extno]) & mask);
+		if (!(le32_to_cpu(iagp->wmap[extno]) & mask)) {
+			jERROR(1,
+			       ("diUpdatePMap: inode %ld not marked as allocated in wmap!\n",
+				inum));
+			updateSuper(ipimap->i_sb, FM_DIRTY);
+		}
+//              assert(le32_to_cpu(iagp->pmap[extno]) & mask);
+		if (!(le32_to_cpu(iagp->pmap[extno]) & mask)) {
+			jERROR(1,
+			       ("diUpdatePMap: inode %ld not marked as allocated in pmap!\n",
+				inum));
+			updateSuper(ipimap->i_sb, FM_DIRTY);
+		}
+		/* update the bitmap for the extent of the freed inode */
+		iagp->pmap[extno] &= cpu_to_le32(~mask);
+	}
+	/*
+	 * mark the inode allocated in persistent map:
+	 */
+	else {
+		/* The inode should be already allocated in the working map
+		 * and should be free in persistent map;
+		 */
+		assert(le32_to_cpu(iagp->wmap[extno]) & mask);
+		assert((le32_to_cpu(iagp->pmap[extno]) & mask) == 0);
+		/* update the bitmap for the extent of the allocated inode */
+		iagp->pmap[extno] |= cpu_to_le32(mask);
+	}
+	/*
+	 * update iag lsn
+	 */
+	lsn = tblk->lsn;
+	log = JFS_SBI(tblk->sb)->log;
+	if (mp->lsn != 0) {
+		/* inherit older/smaller lsn */
+		logdiff(difft, lsn, log);
+		logdiff(diffp, mp->lsn, log);
+		if (difft < diffp) {
+			mp->lsn = lsn;
+			/* move mp after tblock in logsync list */
+			LOGSYNC_LOCK(log);
+			list_del(&mp->synclist);
+			list_add(&mp->synclist, &tblk->synclist);
+			LOGSYNC_UNLOCK(log);
+		}
+		/* inherit younger/larger clsn */
+		LOGSYNC_LOCK(log);
+		assert(mp->clsn);
+		logdiff(difft, tblk->clsn, log);
+		logdiff(diffp, mp->clsn, log);
+		if (difft > diffp)
+			mp->clsn = tblk->clsn;
+		LOGSYNC_UNLOCK(log);
+	} else {
+		mp->log = log;
+		mp->lsn = lsn;
+		/* insert mp after tblock in logsync list */
+		LOGSYNC_LOCK(log);
+		log->count++;
+		list_add(&mp->synclist, &tblk->synclist);
+		mp->clsn = tblk->clsn;
+		LOGSYNC_UNLOCK(log);
+	}
+//      bmLazyWrite(mp, log->flag & JFS_COMMIT);
+	write_metapage(mp);
+	return (0);
+}
+
+/*
+ *	diExtendFS()
+ *
+ * function: update imap for extendfs();
+ * 
+ * note: AG size has been increased s.t. each k old contiguous AGs are 
+ * coalesced into a new AG;
+ */
+int diExtendFS(struct inode *ipimap, struct inode *ipbmap)
+{
+	int rc, rcx = 0;
+	imap_t *imap = JFS_IP(ipimap)->i_imap;
+	iag_t *iagp = 0, *hiagp = 0;
+	bmap_t *mp = JFS_SBI(ipbmap->i_sb)->bmap;
+	metapage_t *bp, *hbp;
+	int i, n, head;
+	int numinos, xnuminos = 0, xnumfree = 0;
+	s64 agstart;
+
+	jEVENT(0, ("diExtendFS: nextiag:%d numinos:%d numfree:%d\n",
+		   imap->im_nextiag, atomic_read(&imap->im_numinos),
+		   atomic_read(&imap->im_numfree)));
+
+	/*
+	 *      reconstruct imap 
+	 *
+	 * coalesce contiguous k (newAGSize/oldAGSize) AGs;
+	 * i.e., (AGi, ..., AGj) where i = k*n and j = k*(n+1) - 1 to AGn;
+	 * note: new AG size = old AG size * (2**x).
+	 */
+
+	/* init per AG control information im_agctl[] */
+	for (i = 0; i < MAXAG; i++) {
+		imap->im_agctl[i].inofree = -1;	/* free inode list */
+		imap->im_agctl[i].extfree = -1;	/* free extent list */
+		imap->im_agctl[i].numinos = 0;	/* number of backed inodes */
+		imap->im_agctl[i].numfree = 0;	/* number of free backed inodes */
+	}
+
+	/*
+	 *      process each iag_t page of the map.
+	 *
+	 * rebuild AG Free Inode List, AG Free Inode Extent List;
+	 */
+	for (i = 0; i < imap->im_nextiag; i++) {
+		if ((rc = diIAGRead(imap, i, &bp))) {
+			rcx = rc;
+			continue;
+		}
+		iagp = (iag_t *) bp->data;
+		assert(le32_to_cpu(iagp->iagnum) == i);
+
+		/* leave free iag in the free iag list */
+		if (iagp->nfreeexts == cpu_to_le32(EXTSPERIAG)) {  
+		        release_metapage(bp);
+			continue;
+		}
+
+		/* agstart that computes to the same ag is treated as same; */
+		agstart = le64_to_cpu(iagp->agstart);
+		/* iagp->agstart = agstart & ~(mp->db_agsize - 1); */
+		n = agstart >> mp->db_agl2size;
+/*
+printf("diExtendFS: iag:%d agstart:%Ld agno:%d\n", i, agstart, n);
+*/
+
+		/* compute backed inodes */
+		numinos = (EXTSPERIAG - le32_to_cpu(iagp->nfreeexts))
+		    << L2INOSPEREXT;
+		if (numinos > 0) {
+			/* merge AG backed inodes */
+			imap->im_agctl[n].numinos += numinos;
+			xnuminos += numinos;
+		}
+
+		/* if any backed free inodes, insert at AG free inode list */
+		if ((int) le32_to_cpu(iagp->nfreeinos) > 0) {
+			if ((head = imap->im_agctl[n].inofree) == -1)
+				iagp->inofreefwd = iagp->inofreeback = -1;
+			else {
+				if ((rc = diIAGRead(imap, head, &hbp))) {
+					rcx = rc;
+					goto nextiag;
+				}
+				hiagp = (iag_t *) hbp->data;
+				hiagp->inofreeback =
+				    le32_to_cpu(iagp->iagnum);
+				iagp->inofreefwd = cpu_to_le32(head);
+				iagp->inofreeback = -1;
+				write_metapage(hbp);
+			}
+
+			imap->im_agctl[n].inofree =
+			    le32_to_cpu(iagp->iagnum);
+
+			/* merge AG backed free inodes */
+			imap->im_agctl[n].numfree +=
+			    le32_to_cpu(iagp->nfreeinos);
+			xnumfree += le32_to_cpu(iagp->nfreeinos);
+		}
+
+		/* if any free extents, insert at AG free extent list */
+		if (le32_to_cpu(iagp->nfreeexts) > 0) {
+			if ((head = imap->im_agctl[n].extfree) == -1)
+				iagp->extfreefwd = iagp->extfreeback = -1;
+			else {
+				if ((rc = diIAGRead(imap, head, &hbp))) {
+					rcx = rc;
+					goto nextiag;
+				}
+				hiagp = (iag_t *) hbp->data;
+				hiagp->extfreeback = iagp->iagnum;
+				iagp->extfreefwd = cpu_to_le32(head);
+				iagp->extfreeback = -1;
+				write_metapage(hbp);
+			}
+
+			imap->im_agctl[n].extfree =
+			    le32_to_cpu(iagp->iagnum);
+		}
+
+	      nextiag:
+		write_metapage(bp);
+	}
+
+	ASSERT(xnuminos == atomic_read(&imap->im_numinos) &&
+	       xnumfree == atomic_read(&imap->im_numfree));
+
+	return rcx;
+}
+
+
+/*
+ *	duplicateIXtree()
+ *
+ * serialization: IWRITE_LOCK held on entry/exit
+ *
+ * note: shadow page with regular inode (rel.2);
+ */
+static void
+duplicateIXtree(struct super_block *sb, s64 blkno, int xlen, s64 * xaddr)
+{
+	int rc;
+	tid_t tid;
+	struct inode *ip;
+	metapage_t *mpsuper;
+	struct jfs_superblock *j_sb;
+
+	/* if AIT2 ipmap2 is bad, do not try to update it */
+	if (JFS_SBI(sb)->mntflag & JFS_BAD_SAIT)	/* s_flag */
+		return;
+	ip = diReadSpecial(sb, FILESYSTEM_I + INOSPEREXT);
+	if (ip == 0) {
+		JFS_SBI(sb)->mntflag |= JFS_BAD_SAIT;
+		if ((rc = readSuper(sb, &mpsuper)))
+			return;
+		j_sb = (struct jfs_superblock *) (mpsuper->data);
+		j_sb->s_flag |= JFS_BAD_SAIT;
+		write_metapage(mpsuper);
+		return;
+	}
+
+	/* start transaction */
+	tid = txBegin(sb, COMMIT_FORCE);
+	/* update the inode map addressing structure to point to it */
+	if ((rc = xtInsert(tid, ip, 0, blkno, xlen, xaddr, 0))) {
+		JFS_SBI(sb)->mntflag |= JFS_BAD_SAIT;
+		txAbort(tid, 1);
+		goto cleanup;
+
+	}
+	/* update the inode map's inode to reflect the extension */
+	ip->i_size += PSIZE;
+	ip->i_blocks += LBLK2PBLK(sb, xlen);
+	rc = txCommit(tid, 1, &ip, COMMIT_FORCE);
+      cleanup:
+	txEnd(tid);
+	diFreeSpecial(ip);
+}
+
+/*
+ * NAME:        copy_from_dinode()
+ *
+ * FUNCTION:    Copies inode info from disk inode to in-memory inode
+ *
+ * RETURN VALUES:
+ *      0       - success
+ *      ENOMEM	- insufficient memory
+ */
+static int copy_from_dinode(dinode_t * dip, struct inode *ip)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+
+	jfs_ip->fileset = le32_to_cpu(dip->di_fileset);
+	jfs_ip->mode2 = le32_to_cpu(dip->di_mode);
+
+	ip->i_mode = le32_to_cpu(dip->di_mode) & 0xffff;
+	ip->i_nlink = le32_to_cpu(dip->di_nlink);
+	ip->i_uid = le32_to_cpu(dip->di_uid);
+	ip->i_gid = le32_to_cpu(dip->di_gid);
+	ip->i_size = le64_to_cpu(dip->di_size);
+	ip->i_atime = le32_to_cpu(dip->di_atime.tv_sec);
+	ip->i_mtime = le32_to_cpu(dip->di_mtime.tv_sec);
+	ip->i_ctime = le32_to_cpu(dip->di_ctime.tv_sec);
+	ip->i_blksize = ip->i_sb->s_blocksize;
+	ip->i_blocks = LBLK2PBLK(ip->i_sb, le64_to_cpu(dip->di_nblocks));
+	ip->i_version = ++event;
+	ip->i_generation = le32_to_cpu(dip->di_gen);
+
+	jfs_ip->ixpxd = dip->di_ixpxd;	/* in-memory pxd's are little-endian */
+	jfs_ip->acl = dip->di_acl;	/* as are dxd's */
+	jfs_ip->ea = dip->di_ea;
+	jfs_ip->next_index = le32_to_cpu(dip->di_next_index);
+	jfs_ip->otime = le32_to_cpu(dip->di_otime.tv_sec);
+	jfs_ip->acltype = le32_to_cpu(dip->di_acltype);
+	/*
+	 * We may only need to do this for "special" inodes (dmap, imap)
+	 */
+	if (S_ISCHR(ip->i_mode) || S_ISBLK(ip->i_mode))
+		ip->i_rdev = to_kdev_t(le32_to_cpu(dip->di_rdev));
+	else if (S_ISDIR(ip->i_mode)) {
+		memcpy(&jfs_ip->i_dirtable, &dip->di_dirtable, 384);
+	} else if (!S_ISFIFO(ip->i_mode)) {
+		memcpy(&jfs_ip->i_xtroot, &dip->di_xtroot, 288);
+	}
+	/* Zero the in-memory-only stuff */
+	jfs_ip->cflag = 0;
+	jfs_ip->btindex = 0;
+	jfs_ip->btorder = 0;
+	jfs_ip->bxflag = 0;
+	jfs_ip->blid = 0;
+	jfs_ip->atlhead = 0;
+	jfs_ip->atltail = 0;
+	jfs_ip->xtlid = 0;
+	return (0);
+}
+
+/*
+ * NAME:        copy_to_dinode()
+ *
+ * FUNCTION:    Copies inode info from in-memory inode to disk inode
+ */
+static void copy_to_dinode(dinode_t * dip, struct inode *ip)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+
+	dip->di_fileset = cpu_to_le32(jfs_ip->fileset);
+	dip->di_inostamp = cpu_to_le32(JFS_SBI(ip->i_sb)->inostamp);
+	dip->di_number = cpu_to_le32(ip->i_ino);
+	dip->di_gen = cpu_to_le32(ip->i_generation);
+	dip->di_size = cpu_to_le64(ip->i_size);
+	dip->di_nblocks = cpu_to_le64(PBLK2LBLK(ip->i_sb, ip->i_blocks));
+	dip->di_nlink = cpu_to_le32(ip->i_nlink);
+	dip->di_uid = cpu_to_le32(ip->i_uid);
+	dip->di_gid = cpu_to_le32(ip->i_gid);
+	/*
+	 * mode2 is only needed for storing the higher order bits.
+	 * Trust i_mode for the lower order ones
+	 */
+	dip->di_mode = cpu_to_le32((jfs_ip->mode2 & 0xffff0000) | ip->i_mode);
+	dip->di_atime.tv_sec = cpu_to_le32(ip->i_atime);
+	dip->di_atime.tv_nsec = 0;
+	dip->di_ctime.tv_sec = cpu_to_le32(ip->i_ctime);
+	dip->di_ctime.tv_nsec = 0;
+	dip->di_mtime.tv_sec = cpu_to_le32(ip->i_mtime);
+	dip->di_mtime.tv_nsec = 0;
+	dip->di_ixpxd = jfs_ip->ixpxd;	/* in-memory pxd's are little-endian */
+	dip->di_acl = jfs_ip->acl;	/* as are dxd's */
+	dip->di_ea = jfs_ip->ea;
+	dip->di_next_index = cpu_to_le32(jfs_ip->next_index);
+	dip->di_otime.tv_sec = cpu_to_le32(jfs_ip->otime);
+	dip->di_otime.tv_nsec = 0;
+	dip->di_acltype = cpu_to_le32(jfs_ip->acltype);
+
+	if (S_ISCHR(ip->i_mode) || S_ISBLK(ip->i_mode))
+		dip->di_rdev = cpu_to_le32(kdev_t_to_nr(ip->i_rdev));
+}
+
+void diClearExtension(struct inode *ip)
+{
+	jFYI(1, ("diClearExtension called ip = 0x%p\n", ip));
+
+	assert(list_empty(&JFS_IP(ip)->mp_list));
+	assert(list_empty(&JFS_IP(ip)->anon_inode_list));
+
+	if (JFS_IP(ip)->atlhead) {
+		jERROR(1,
+		       ("diClearExtension: inode 0x%p has anonymous tlocks\n",
+			ip));
+	}
+
+	free_jfs_inode(ip);
+	ip->u.generic_ip = 0;
+}
+
+#ifdef	_JFS_DEBUG_IMAP
+/*
+ *	DBGdiInit()
+ */
+static void *DBGdiInit(imap_t * imap)
+{
+	u32 *dimap;
+	int size;
+	size = 64 * 1024;
+	if ((dimap = (u32 *) xmalloc(size, L2PSIZE, kernel_heap)) == NULL)
+		assert(0);
+	bzero((void *) dimap, size);
+	imap->im_DBGdimap = dimap;
+}
+
+/*
+ *	DBGdiAlloc()
+ */
+static void DBGdiAlloc(imap_t * imap, ino_t ino)
+{
+	u32 *dimap = imap->im_DBGdimap;
+	int w, b;
+	u32 m;
+	w = ino >> 5;
+	b = ino & 31;
+	m = 0x80000000 >> b;
+	assert(w < 64 * 256);
+	if (dimap[w] & m) {
+		printk("DEBUG diAlloc: duplicate alloc ino:0x%x\n", ino);
+	}
+	dimap[w] |= m;
+}
+
+/*
+ *	DBGdiFree()
+ */
+static void DBGdiFree(imap_t * imap, ino_t ino)
+{
+	u32 *dimap = imap->im_DBGdimap;
+	int w, b;
+	u32 m;
+	w = ino >> 5;
+	b = ino & 31;
+	m = 0x80000000 >> b;
+	assert(w < 64 * 256);
+	if ((dimap[w] & m) == 0) {
+		printk("DEBUG diFree: duplicate free ino:0x%x\n", ino);
+	}
+	dimap[w] &= ~m;
+}
+
+static void dump_cp(imap_t * ipimap, char *function, int line)
+{
+	printk("\n* ********* *\nControl Page %s %d\n", function, line);
+	printk("FreeIAG %d\tNextIAG %d\n", ipimap->im_freeiag,
+	       ipimap->im_nextiag);
+	printk("NumInos %d\tNumFree %d\n",
+	       atomic_read(&ipimap->im_numinos),
+	       atomic_read(&ipimap->im_numfree));
+	printk("AG InoFree %d\tAG ExtFree %d\n",
+	       ipimap->im_agctl[0].inofree, ipimap->im_agctl[0].extfree);
+	printk("AG NumInos %d\tAG NumFree %d\n",
+	       ipimap->im_agctl[0].numinos, ipimap->im_agctl[0].numfree);
+}
+
+static void dump_iag(iag_t * iag, char *function, int line)
+{
+	printk("\n* ********* *\nIAG %s %d\n", function, line);
+	printk("IagNum %d\tIAG Free %d\n", le32_to_cpu(iag->iagnum),
+	       le32_to_cpu(iag->iagfree));
+	printk("InoFreeFwd %d\tInoFreeBack %d\n",
+	       le32_to_cpu(iag->inofreefwd),
+	       le32_to_cpu(iag->inofreeback));
+	printk("ExtFreeFwd %d\tExtFreeBack %d\n",
+	       le32_to_cpu(iag->extfreefwd),
+	       le32_to_cpu(iag->extfreeback));
+	printk("NFreeInos %d\tNFreeExts %d\n", le32_to_cpu(iag->nfreeinos),
+	       le32_to_cpu(iag->nfreeexts));
+}
+#endif				/* _JFS_DEBUG_IMAP */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_imap.h linuxppc64_2_4/fs/jfs/jfs_imap.h
--- linux-2.4.19/fs/jfs/jfs_imap.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_imap.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,161 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef	_H_JFS_IMAP
+#define _H_JFS_IMAP
+
+#include "jfs_txnmgr.h"
+
+/*
+ *	jfs_imap.h: disk inode manager
+ */
+
+#define	EXTSPERIAG	128	/* number of disk inode extent per iag  */
+#define IMAPBLKNO	0	/* lblkno of dinomap within inode map   */
+#define SMAPSZ		4	/* number of words per summary map      */
+#define	EXTSPERSUM	32	/* number of extents per summary map entry */
+#define	L2EXTSPERSUM	5	/* l2 number of extents per summary map */
+#define	PGSPERIEXT	4	/* number of 4K pages per dinode extent */
+#define	MAXIAGS		((1<<20)-1)	/* maximum number of iags       */
+#define	MAXAG		128	/* maximum number of allocation groups  */
+
+#define AMAPSIZE      512	/* bytes in the IAG allocation maps */
+#define SMAPSIZE      16	/* bytes in the IAG summary maps */
+
+/* convert inode number to iag number */
+#define	INOTOIAG(ino)	((ino) >> L2INOSPERIAG)
+
+/* convert iag number to logical block number of the iag page */
+#define IAGTOLBLK(iagno,l2nbperpg)	(((iagno) + 1) << (l2nbperpg))
+
+/* get the starting block number of the 4K page of an inode extent
+ * that contains ino.
+ */
+#define INOPBLK(pxd,ino,l2nbperpg)    	(addressPXD((pxd)) +		\
+	((((ino) & (INOSPEREXT-1)) >> L2INOSPERPAGE) << (l2nbperpg)))
+
+/*
+ *	inode allocation map:
+ * 
+ * inode allocation map consists of 
+ * . the inode map control page and
+ * . inode allocation group pages (per 4096 inodes)
+ * which are addressed by standard JFS xtree.
+ */
+/*
+ *	inode allocation group page (per 4096 inodes of an AG)
+ */
+typedef struct {
+	s64 agstart;		/* 8: starting block of ag              */
+	s32 iagnum;		/* 4: inode allocation group number     */
+	s32 inofreefwd;		/* 4: ag inode free list forward        */
+	s32 inofreeback;	/* 4: ag inode free list back           */
+	s32 extfreefwd;		/* 4: ag inode extent free list forward */
+	s32 extfreeback;	/* 4: ag inode extent free list back    */
+	s32 iagfree;		/* 4: iag free list                     */
+
+	/* summary map: 1 bit per inode extent */
+	s32 inosmap[SMAPSZ];	/* 16: sum map of mapwords w/ free inodes;
+				 *      note: this indicates free and backed
+				 *      inodes, if the extent is not backed the
+				 *      value will be 1.  if the extent is
+				 *      backed but all inodes are being used the
+				 *      value will be 1.  if the extent is
+				 *      backed but at least one of the inodes is
+				 *      free the value will be 0.
+				 */
+	s32 extsmap[SMAPSZ];	/* 16: sum map of mapwords w/ free extents */
+	s32 nfreeinos;		/* 4: number of free inodes             */
+	s32 nfreeexts;		/* 4: number of free extents            */
+	/* (72) */
+	u8 pad[1976];		/* 1976: pad to 2048 bytes */
+	/* allocation bit map: 1 bit per inode (0 - free, 1 - allocated) */
+	u32 wmap[EXTSPERIAG];	/* 512: working allocation map  */
+	u32 pmap[EXTSPERIAG];	/* 512: persistent allocation map */
+	pxd_t inoext[EXTSPERIAG];	/* 1024: inode extent addresses */
+} iag_t;			/* (4096) */
+
+/*
+ *	per AG control information (in inode map control page)
+ */
+typedef struct {
+	s32 inofree;		/* 4: free inode list anchor            */
+	s32 extfree;		/* 4: free extent list anchor           */
+	s32 numinos;		/* 4: number of backed inodes           */
+	s32 numfree;		/* 4: number of free inodes             */
+} iagctl_t;			/* (16) */
+
+/*
+ *	per fileset/aggregate inode map control page
+ */
+typedef struct {
+	s32 in_freeiag;		/* 4: free iag list anchor     */
+	s32 in_nextiag;		/* 4: next free iag number     */
+	s32 in_numinos;		/* 4: num of backed inodes */
+	s32 in_numfree;		/* 4: num of free backed inodes */
+	s32 in_nbperiext;	/* 4: num of blocks per inode extent */
+	s32 in_l2nbperiext;	/* 4: l2 of in_nbperiext */
+	s32 in_diskblock;	/* 4: for standalone test driver  */
+	s32 in_maxag;		/* 4: for standalone test driver  */
+	u8 pad[2016];		/* 2016: pad to 2048 */
+	iagctl_t in_agctl[MAXAG];	/* 2048: AG control information */
+} dinomap_t;			/* (4096) */
+
+
+/*
+ *	In-core inode map control page
+ */
+typedef struct inomap {
+	dinomap_t im_imap;	/* 4096: inode allocation control */
+	struct inode *im_ipimap;	/* 4: ptr to inode for imap   */
+	struct semaphore im_freelock;	/* 4: iag free list lock      */
+	struct semaphore im_aglock[MAXAG];	/* 512: per AG locks          */
+	u32 *im_DBGdimap;
+	atomic_t im_numinos;	/* num of backed inodes */
+	atomic_t im_numfree;	/* num of free backed inodes */
+} imap_t;
+
+#define	im_freeiag	im_imap.in_freeiag
+#define	im_nextiag	im_imap.in_nextiag
+#define	im_agctl	im_imap.in_agctl
+#define	im_nbperiext	im_imap.in_nbperiext
+#define	im_l2nbperiext	im_imap.in_l2nbperiext
+
+/* for standalone testdriver
+ */
+#define	im_diskblock	im_imap.in_diskblock
+#define	im_maxag	im_imap.in_maxag
+
+extern int diFree(struct inode *);
+extern int diAlloc(struct inode *, boolean_t, struct inode *);
+extern int diSync(struct inode *);
+/* external references */
+extern int diUpdatePMap(struct inode *ipimap, unsigned long inum,
+			boolean_t is_free, tblock_t * tblk);
+#ifdef _STILL_TO_PORT
+extern int diExtendFS(inode_t * ipimap, inode_t * ipbmap);
+#endif				/* _STILL_TO_PORT */
+
+extern int diMount(struct inode *);
+extern int diUnmount(struct inode *, int);
+extern int diRead(struct inode *);
+extern void diClearExtension(struct inode *);
+extern struct inode *diReadSpecial(struct super_block *, ino_t);
+extern void diWriteSpecial(struct inode *);
+extern void diFreeSpecial(struct inode *);
+extern int diWrite(tid_t tid, struct inode *);
+#endif				/* _H_JFS_IMAP */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_incore.h linuxppc64_2_4/fs/jfs/jfs_incore.h
--- linux-2.4.19/fs/jfs/jfs_incore.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_incore.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,155 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/ 
+#ifndef _H_JFS_INCORE
+#define _H_JFS_INCORE
+
+#include <asm/bitops.h>
+#include <linux/slab.h>
+#include "jfs_types.h"
+#include "jfs_xtree.h"
+#include "jfs_dtree.h"
+
+/*
+ * JFS magic number
+ */
+#define JFS_SUPER_MAGIC 0x3153464a /* "JFS1" */
+
+/*
+ * Due to header ordering problems this can't be in jfs_lock.h
+ */
+typedef struct	jfs_rwlock {
+	struct rw_semaphore rw_sem;
+	atomic_t in_use;	/* for hacked implementation of trylock */
+} jfs_rwlock_t;
+
+/*
+ * JFS-private inode information
+ */
+struct jfs_inode_info {
+	struct inode *inode;	/* pointer back to fs-independent inode */
+	int	fileset;	/* fileset number (always 16)*/
+	uint	mode2;		/* jfs-specific mode		*/
+	pxd_t   ixpxd;		/* inode extent descriptor	*/
+	dxd_t	acl;		/* dxd describing acl	*/
+	dxd_t	ea;		/* dxd describing ea	*/
+	time_t	otime;		/* time created	*/
+	uint	next_index;	/* next available directory entry index */
+	int	acltype;	/* Type of ACL	*/
+	short	btorder;	/* access order	*/
+	short	btindex;	/* btpage entry index*/
+	struct inode *ipimap;	/* inode map			*/
+	long	cflag;		/* commit flags		*/
+	u16	bxflag;		/* xflag of pseudo buffer?	*/
+	unchar	agno;		/* ag number			*/
+	unchar	pad;		/* pad			*/
+	lid_t	blid;		/* lid of pseudo buffer?	*/
+	lid_t	atlhead;	/* anonymous tlock list head	*/
+	lid_t	atltail;	/* anonymous tlock list tail	*/
+	struct list_head anon_inode_list; /* inodes having anonymous txns */
+	struct list_head mp_list; /* metapages in inode's address space */
+	jfs_rwlock_t rdwrlock;	/* read/write lock	*/
+	lid_t	xtlid;		/* lid of xtree lock on directory */
+	union {
+		struct {
+			xtpage_t _xtroot;	/* 288: xtree root */
+			struct inomap *_imap;	/* 4: inode map header	*/
+		} file;
+		struct {
+			dir_table_slot_t _table[12]; /* 96: directory index */
+			dtroot_t _dtroot;	/* 288: dtree root */
+		} dir;
+		struct {
+			unchar _unused[16];	/* 16: */
+			dxd_t _dxd;		/* 16: */
+			unchar _inline[128];	/* 128: inline symlink */
+		} link;
+	} u;
+};
+#define i_xtroot u.file._xtroot
+#define i_imap u.file._imap
+#define i_dirtable u.dir._table
+#define i_dtroot u.dir._dtroot
+#define i_inline u.link._inline
+
+/*
+ * cflag
+ */
+enum cflags {
+	COMMIT_New,		/* never committed inode   */
+	COMMIT_Nolink,		/* inode committed with zero link count */
+	COMMIT_Inlineea,	/* commit inode inline EA */
+	COMMIT_Freewmap,	/* free WMAP at iClose() */
+	COMMIT_Dirty,		/* Inode is really dirty */
+	COMMIT_Holdlock,	/* Hold the IWRITE_LOCK until commit is done */
+	COMMIT_Dirtable,	/* commit changes to di_dirtable */
+	COMMIT_Stale,		/* data extent is no longer valid */
+	COMMIT_Synclist,	/* metadata pages on group commit synclist */
+};
+
+#define set_cflag(flag, ip)	set_bit(flag, &(JFS_IP(ip)->cflag))
+#define clear_cflag(flag, ip)	clear_bit(flag, &(JFS_IP(ip)->cflag))
+#define test_cflag(flag, ip)	test_bit(flag, &(JFS_IP(ip)->cflag))
+#define test_and_clear_cflag(flag, ip) \
+	test_and_clear_bit(flag, &(JFS_IP(ip)->cflag))
+/*
+ * JFS-private superblock information.
+ */
+struct jfs_sb_info {
+	unsigned long	mntflag;	/* 4: aggregate attributes	*/
+	struct inode	*ipbmap;	/* 4: block map inode		*/
+	struct inode	*ipaimap;	/* 4: aggregate inode map inode	*/
+	struct inode	*ipaimap2;	/* 4: secondary aimap inode	*/
+	struct inode	*ipimap;	/* 4: aggregate inode map inode	*/
+	struct jfs_log	*log;		/* 4: log			*/
+	short		bsize;		/* 2: logical block size	*/
+	short		l2bsize;	/* 2: log2 logical block size	*/
+	short		nbperpage;	/* 2: blocks per page		*/
+	short		l2nbperpage;	/* 2: log2 blocks per page	*/
+	short		l2niperblk;	/* 2: log2 inodes per page	*/
+	kdev_t		logdev;		/* 2: external log device	*/
+	pxd_t		logpxd;		/* 8: pxd describing log	*/
+	pxd_t		ait2;		/* 8: pxd describing AIT copy	*/
+	/* Formerly in ipimap */
+	uint		gengen;		/* 4: inode generation generator*/
+	uint		inostamp;	/* 4: shows inode belongs to fileset*/
+
+        /* Formerly in ipbmap */
+	struct bmap	*bmap;		/* 4: incore bmap descriptor	*/
+	struct nls_table *nls_tab;	/* 4: current codepage		*/
+	struct inode	*direct_inode;	/* 4: inode for physical I/O	*/
+	struct address_space *direct_mapping; /* 4: mapping for physical I/O */
+	uint		state;		/* 4: mount/recovery state	*/
+};
+
+#define JFS_IP(ip)	((struct jfs_inode_info *)(ip)->u.generic_ip)
+#define JFS_SBI(sb)	((struct jfs_sb_info *)(sb)->u.generic_sbp)
+
+#define isReadOnly(ip)	((JFS_SBI((ip)->i_sb)->log) ? 0 : 1)
+
+/*
+ * Allocating and freeing the structure
+ */
+extern kmem_cache_t *jfs_inode_cachep;
+extern int alloc_jfs_inode(struct inode *);
+
+#define free_jfs_inode(inode) \
+	kmem_cache_free(jfs_inode_cachep, (inode)->u.generic_ip)
+
+#endif /* _H_JFS_INCORE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_inode.c linuxppc64_2_4/fs/jfs/jfs_inode.c
--- linux-2.4.19/fs/jfs/jfs_inode.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_inode.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,160 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_imap.h"
+#include "jfs_dinode.h"
+#include "jfs_debug.h"
+
+kmem_cache_t *jfs_inode_cachep;
+
+/*
+ * NAME:	ialloc()
+ *
+ * FUNCTION:	Allocate a new inode
+ *
+ */
+struct inode *ialloc(struct inode *parent, umode_t mode)
+{
+	struct super_block *sb = parent->i_sb;
+	struct inode *inode;
+	struct jfs_inode_info *jfs_inode;
+	int rc;
+
+	inode = new_inode(sb);
+	if (!inode) {
+		jERROR(1, ("ialloc: new_inode returned NULL!\n"));
+		return inode;
+	}
+
+	rc = alloc_jfs_inode(inode);
+	if (rc) {
+		make_bad_inode(inode);
+		iput(inode);
+		return NULL;
+	}
+	jfs_inode = JFS_IP(inode);
+
+	rc = diAlloc(parent, S_ISDIR(mode), inode);
+	if (rc) {
+		jERROR(1, ("ialloc: diAlloc returned %d!\n", rc));
+		free_jfs_inode(inode);
+		make_bad_inode(inode);
+		iput(inode);
+		return NULL;
+	}
+
+	inode->i_uid = current->fsuid;
+	if (parent->i_mode & S_ISGID) {
+		inode->i_gid = parent->i_gid;
+		if (S_ISDIR(mode))
+			mode |= S_ISGID;
+	} else
+		inode->i_gid = current->fsgid;
+
+	inode->i_mode = mode;
+	if (S_ISDIR(mode))
+		jfs_inode->mode2 = IDIRECTORY | mode;
+	else
+		jfs_inode->mode2 = INLINEEA | ISPARSE | mode;
+	inode->i_blksize = sb->s_blocksize;
+	inode->i_blocks = 0;
+	inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
+	jfs_inode->otime = inode->i_ctime;
+	inode->i_version = ++event;
+	inode->i_generation = JFS_SBI(sb)->gengen++;
+
+	jfs_inode->cflag = 0;
+	set_cflag(COMMIT_New, inode);
+
+	/* Zero remaining fields */
+	memset(&jfs_inode->acl, 0, sizeof(dxd_t));
+	memset(&jfs_inode->ea, 0, sizeof(dxd_t));
+	jfs_inode->next_index = 0;
+	jfs_inode->acltype = 0;
+	jfs_inode->btorder = 0;
+	jfs_inode->btindex = 0;
+	jfs_inode->bxflag = 0;
+	jfs_inode->blid = 0;
+	jfs_inode->atlhead = 0;
+	jfs_inode->atltail = 0;
+	jfs_inode->xtlid = 0;
+
+	jFYI(1, ("ialloc returns inode = 0x%p\n", inode));
+
+	return inode;
+}
+
+/*
+ * NAME:	iwritelocklist()
+ *
+ * FUNCTION:	Lock multiple inodes in sorted order to avoid deadlock
+ *
+ */
+void iwritelocklist(int n, ...)
+{
+	va_list ilist;
+	struct inode *sort[4];
+	struct inode *ip;
+	int k, m;
+
+	va_start(ilist, n);
+	for (k = 0; k < n; k++)
+		sort[k] = va_arg(ilist, struct inode *);
+	va_end(ilist);
+
+	/* Bubble sort in descending order */
+	do {
+		m = 0;
+		for (k = 0; k < n; k++)
+			if ((k + 1) < n
+			    && sort[k + 1]->i_ino > sort[k]->i_ino) {
+				ip = sort[k];
+				sort[k] = sort[k + 1];
+				sort[k + 1] = ip;
+				m++;
+			}
+	} while (m);
+
+	/* Lock them */
+	for (k = 0; k < n; k++) {
+		IWRITE_LOCK(sort[k]);
+	}
+}
+
+/*
+ * NAME:	alloc_jfs_inode()
+ *
+ * FUNCTION:	Allocate jfs portion of in-memory inode
+ *
+ */
+int alloc_jfs_inode(struct inode *inode)
+{
+	struct jfs_inode_info *jfs_inode;
+
+	jfs_inode = kmem_cache_alloc(jfs_inode_cachep, GFP_NOFS);
+	JFS_IP(inode) = jfs_inode;
+	if (!jfs_inode)
+		return -ENOSPC;
+	jfs_inode->inode = inode;
+
+	return 0;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_inode.h linuxppc64_2_4/fs/jfs/jfs_inode.h
--- linux-2.4.19/fs/jfs/jfs_inode.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_inode.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,23 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef	_H_JFS_INODE
+#define _H_JFS_INODE
+
+extern struct inode *ialloc(struct inode *, umode_t);
+
+#endif				/* _H_JFS_INODE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_lock.h linuxppc64_2_4/fs/jfs/jfs_lock.h
--- linux-2.4.19/fs/jfs/jfs_lock.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_lock.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,106 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef _H_JFS_LOCK
+#define _H_JFS_LOCK
+
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+
+/*
+ *	jfs_lock.h
+ *
+ * JFS lock definition for globally referenced locks
+ */
+
+/* readers/writer lock: thread-thread */
+
+/*
+ * RW semaphores do not currently have a trylock function.  Since the
+ * implementation varies by platform, I have implemented a platform-independent
+ * wrapper around the rw_semaphore routines.  If this turns out to be the best
+ * way of avoiding our locking problems, I will push to get a trylock
+ * implemented in the kernel, but I'd rather find a way to avoid having to
+ * use it.
+ */
+#define RDWRLOCK_T jfs_rwlock_t
+static inline void RDWRLOCK_INIT(jfs_rwlock_t * Lock)
+{
+	init_rwsem(&Lock->rw_sem);
+	atomic_set(&Lock->in_use, 0);
+}
+static inline void READ_LOCK(jfs_rwlock_t * Lock)
+{
+	atomic_inc(&Lock->in_use);
+	down_read(&Lock->rw_sem);
+}
+static inline void READ_UNLOCK(jfs_rwlock_t * Lock)
+{
+	up_read(&Lock->rw_sem);
+	atomic_dec(&Lock->in_use);
+}
+static inline void WRITE_LOCK(jfs_rwlock_t * Lock)
+{
+	atomic_inc(&Lock->in_use);
+	down_write(&Lock->rw_sem);
+}
+
+static inline int WRITE_TRYLOCK(jfs_rwlock_t * Lock)
+{
+	if (atomic_read(&Lock->in_use))
+		return 0;
+	WRITE_LOCK(Lock);
+	return 1;
+}
+static inline void WRITE_UNLOCK(jfs_rwlock_t * Lock)
+{
+	up_write(&Lock->rw_sem);
+	atomic_dec(&Lock->in_use);
+}
+
+#define IREAD_LOCK(ip)		READ_LOCK(&JFS_IP(ip)->rdwrlock)
+#define IREAD_UNLOCK(ip)	READ_UNLOCK(&JFS_IP(ip)->rdwrlock)
+#define IWRITE_LOCK(ip)		WRITE_LOCK(&JFS_IP(ip)->rdwrlock)
+#define IWRITE_TRYLOCK(ip)	WRITE_TRYLOCK(&JFS_IP(ip)->rdwrlock)
+#define IWRITE_UNLOCK(ip)	WRITE_UNLOCK(&JFS_IP(ip)->rdwrlock)
+#define IWRITE_LOCK_LIST	iwritelocklist
+
+extern void iwritelocklist(int, ...);
+
+/*
+ * Conditional sleep where condition is protected by spinlock
+ *
+ * lock_cmd and unlock_cmd take and release the spinlock
+ */
+#define __SLEEP_COND(wq, cond, lock_cmd, unlock_cmd)	\
+do {							\
+	DECLARE_WAITQUEUE(__wait, current);		\
+							\
+	add_wait_queue(&wq, &__wait);			\
+	for (;;) {					\
+		set_current_state(TASK_UNINTERRUPTIBLE);\
+		if (cond)				\
+			break;				\
+		unlock_cmd;				\
+		schedule();				\
+		lock_cmd;				\
+	}						\
+	current->state = TASK_RUNNING;			\
+	remove_wait_queue(&wq, &__wait);		\
+} while (0)
+
+#endif				/* _H_JFS_LOCK */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_logmgr.c linuxppc64_2_4/fs/jfs/jfs_logmgr.c
--- linux-2.4.19/fs/jfs/jfs_logmgr.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_logmgr.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,2327 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/
+
+/*
+ *	jfs_logmgr.c: log manager
+ *
+ * for related information, see transaction manager (jfs_txnmgr.c), and
+ * recovery manager (jfs_logredo.c).
+ *
+ * note: for detail, RTFS.
+ *
+ *	log buffer manager:
+ * special purpose buffer manager supporting log i/o requirements.
+ * per log serial pageout of logpage
+ * queuing i/o requests and redrive i/o at iodone
+ * maintain current logpage buffer
+ * no caching since append only
+ * appropriate jfs buffer cache buffers as needed
+ *
+ *	group commit:
+ * transactions which wrote COMMIT records in the same in-memory
+ * log page during the pageout of previous/current log page(s) are
+ * committed together by the pageout of the page.
+ *
+ *	TBD lazy commit:
+ * transactions are committed asynchronously when the log page
+ * containing it COMMIT is paged out when it becomes full;
+ *
+ *	serialization:
+ * . a per log lock serialize log write.
+ * . a per log lock serialize group commit.
+ * . a per log lock serialize log open/close;
+ *
+ *	TBD log integrity:
+ * careful-write (ping-pong) of last logpage to recover from crash
+ * in overwrite.
+ * detection of split (out-of-order) write of physical sectors
+ * of last logpage via timestamp at end of each sector
+ * with its mirror data array at trailer).
+ *
+ *	alternatives:
+ * lsn - 64-bit monotonically increasing integer vs
+ * 32-bit lspn and page eor.
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include <linux/blkdev.h>
+#include <linux/interrupt.h>
+#include <linux/smp_lock.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_txnmgr.h"
+#include "jfs_debug.h"
+
+
+/*
+ * lbuf's ready to be redriven.  Protected by log_redrive_lock (jfsIOtask)
+ */
+static lbuf_t *log_redrive_list;
+static spinlock_t log_redrive_lock = SPIN_LOCK_UNLOCKED;
+
+
+/*
+ *	log read/write serialization (per log)
+ */
+#define LOG_LOCK_INIT(log)	init_MUTEX(&(log)->loglock)
+#define LOG_LOCK(log)		down(&((log)->loglock))
+#define LOG_UNLOCK(log)		up(&((log)->loglock))
+
+
+/*
+ *	log group commit serialization (per log)
+ */
+
+#define LOGGC_LOCK_INIT(log)	spin_lock_init(&(log)->gclock)
+#define LOGGC_LOCK(log)		spin_lock_irq(&(log)->gclock)
+#define LOGGC_UNLOCK(log)	spin_unlock_irq(&(log)->gclock)
+#define LOGGC_WAKEUP(tblk)	wake_up(&(tblk)->gcwait)
+
+/*
+ *	log sync serialization (per log)
+ */
+#define	LOGSYNC_DELTA(logsize)		min((logsize)/8, 128*LOGPSIZE)
+#define	LOGSYNC_BARRIER(logsize)	((logsize)/4)
+/*
+#define	LOGSYNC_DELTA(logsize)		min((logsize)/4, 256*LOGPSIZE)
+#define	LOGSYNC_BARRIER(logsize)	((logsize)/2)
+*/
+
+
+/*
+ *	log buffer cache synchronization
+ */
+static spinlock_t jfsLCacheLock = SPIN_LOCK_UNLOCKED;
+
+#define	LCACHE_LOCK(flags)	spin_lock_irqsave(&jfsLCacheLock, flags)
+#define	LCACHE_UNLOCK(flags)	spin_unlock_irqrestore(&jfsLCacheLock, flags)
+
+/*
+ * See __SLEEP_COND in jfs_locks.h
+ */
+#define LCACHE_SLEEP_COND(wq, cond, flags)	\
+do {						\
+	if (cond)				\
+		break;				\
+	__SLEEP_COND(wq, cond, LCACHE_LOCK(flags), LCACHE_UNLOCK(flags)); \
+} while (0)
+
+#define	LCACHE_WAKEUP(event)	wake_up(event)
+
+
+/*
+ *	lbuf buffer cache (lCache) control
+ */
+/* log buffer manager pageout control (cumulative, inclusive) */
+#define	lbmREAD		0x0001
+#define	lbmWRITE	0x0002	/* enqueue at tail of write queue;
+				 * init pageout if at head of queue;
+				 */
+#define	lbmRELEASE	0x0004	/* remove from write queue
+				 * at completion of pageout;
+				 * do not free/recycle it yet:
+				 * caller will free it;
+				 */
+#define	lbmSYNC		0x0008	/* do not return to freelist
+				 * when removed from write queue;
+				 */
+#define lbmFREE		0x0010	/* return to freelist
+				 * at completion of pageout;
+				 * the buffer may be recycled;
+				 */
+#define	lbmDONE		0x0020
+#define	lbmERROR	0x0040
+#define lbmGC		0x0080	/* lbmIODone to perform post-GC processing
+				 * of log page
+				 */
+#define lbmDIRECT	0x0100
+
+/*
+ * external references
+ */
+extern void txLazyUnlock(tblock_t * tblk);
+extern int jfs_thread_stopped(void);
+extern struct task_struct *jfsIOtask;
+extern struct completion jfsIOwait;
+
+/*
+ * forward references
+ */
+static int lmWriteRecord(log_t * log, tblock_t * tblk, lrd_t * lrd,
+			 tlock_t * tlck);
+
+static int lmNextPage(log_t * log);
+static int lmLogFileSystem(log_t * log, kdev_t fsdev, int activate);
+static int lmLogInit(log_t * log);
+static int lmLogShutdown(log_t * log);
+
+static int lbmLogInit(log_t * log);
+static void lbmLogShutdown(log_t * log);
+static lbuf_t *lbmAllocate(log_t * log, int);
+static void lbmFree(lbuf_t * bp);
+static void lbmfree(lbuf_t * bp);
+static int lbmRead(log_t * log, int pn, lbuf_t ** bpp);
+static void lbmWrite(log_t * log, lbuf_t * bp, int flag, int cant_block);
+static void lbmDirectWrite(log_t * log, lbuf_t * bp, int flag);
+static int lbmIOWait(lbuf_t * bp, int flag);
+static void lbmIODone(struct buffer_head *bh, int);
+
+void lbmStartIO(lbuf_t * bp);
+void lmGCwrite(log_t * log, int cant_block);
+
+
+/*
+ *	statistics
+ */
+#ifdef CONFIG_JFS_STATISTICS
+struct lmStat {
+	uint commit;		/* # of commit */
+	uint pagedone;		/* # of page written */
+	uint submitted;		/* # of pages submitted */
+} lmStat;
+#endif
+
+
+/*
+ * NAME:	lmLog()
+ *
+ * FUNCTION:	write a log record;
+ *
+ * PARAMETER:
+ *
+ * RETURN:	lsn - offset to the next log record to write (end-of-log);
+ *		-1  - error;
+ *
+ * note: todo: log error handler
+ */
+int lmLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	int lsn;
+	int diffp, difft;
+	metapage_t *mp = NULL;
+
+	jFYI(1, ("lmLog: log:0x%p tblk:0x%p, lrd:0x%p tlck:0x%p\n",
+		 log, tblk, lrd, tlck));
+
+	LOG_LOCK(log);
+
+	/* log by (out-of-transaction) JFS ? */
+	if (tblk == NULL)
+		goto writeRecord;
+
+	/* log from page ? */
+	if (tlck == NULL ||
+	    tlck->type & tlckBTROOT || (mp = tlck->mp) == NULL)
+		goto writeRecord;
+
+	/*
+	 *      initialize/update page/transaction recovery lsn
+	 */
+	lsn = log->lsn;
+
+	LOGSYNC_LOCK(log);
+
+	/*
+	 * initialize page lsn if first log write of the page
+	 */
+	if (mp->lsn == 0) {
+		mp->log = log;
+		mp->lsn = lsn;
+		log->count++;
+
+		/* insert page at tail of logsynclist */
+		list_add_tail(&mp->synclist, &log->synclist);
+	}
+
+	/*
+	 *      initialize/update lsn of tblock of the page
+	 *
+	 * transaction inherits oldest lsn of pages associated
+	 * with allocation/deallocation of resources (their
+	 * log records are used to reconstruct allocation map
+	 * at recovery time: inode for inode allocation map,
+	 * B+-tree index of extent descriptors for block
+	 * allocation map);
+	 * allocation map pages inherit transaction lsn at
+	 * commit time to allow forwarding log syncpt past log
+	 * records associated with allocation/deallocation of
+	 * resources only after persistent map of these map pages
+	 * have been updated and propagated to home.
+	 */
+	/*
+	 * initialize transaction lsn:
+	 */
+	if (tblk->lsn == 0) {
+		/* inherit lsn of its first page logged */
+		tblk->lsn = mp->lsn;
+		log->count++;
+
+		/* insert tblock after the page on logsynclist */
+		list_add(&tblk->synclist, &mp->synclist);
+	}
+	/*
+	 * update transaction lsn:
+	 */
+	else {
+		/* inherit oldest/smallest lsn of page */
+		logdiff(diffp, mp->lsn, log);
+		logdiff(difft, tblk->lsn, log);
+		if (diffp < difft) {
+			/* update tblock lsn with page lsn */
+			tblk->lsn = mp->lsn;
+
+			/* move tblock after page on logsynclist */
+			list_del(&tblk->synclist);
+			list_add(&tblk->synclist, &mp->synclist);
+		}
+	}
+
+	LOGSYNC_UNLOCK(log);
+
+	/*
+	 *      write the log record
+	 */
+      writeRecord:
+	lsn = lmWriteRecord(log, tblk, lrd, tlck);
+
+	/*
+	 * forward log syncpt if log reached next syncpt trigger
+	 */
+	logdiff(diffp, lsn, log);
+	if (diffp >= log->nextsync)
+		lsn = lmLogSync(log, 0);
+
+	/* update end-of-log lsn */
+	log->lsn = lsn;
+
+	LOG_UNLOCK(log);
+
+	/* return end-of-log address */
+	return lsn;
+}
+
+
+/*
+ * NAME:	lmWriteRecord()
+ *
+ * FUNCTION:	move the log record to current log page
+ *
+ * PARAMETER:	cd	- commit descriptor
+ *
+ * RETURN:	end-of-log address
+ *			
+ * serialization: LOG_LOCK() held on entry/exit
+ */
+static int
+lmWriteRecord(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	int lsn = 0;		/* end-of-log address */
+	lbuf_t *bp;		/* dst log page buffer */
+	logpage_t *lp;		/* dst log page */
+	caddr_t dst;		/* destination address in log page */
+	int dstoffset;		/* end-of-log offset in log page */
+	int freespace;		/* free space in log page */
+	caddr_t p;		/* src meta-data page */
+	caddr_t src;
+	int srclen;
+	int nbytes;		/* number of bytes to move */
+	int i;
+	int len;
+	linelock_t *linelock;
+	lv_t *lv;
+	lvd_t *lvd;
+	int l2linesize;
+
+	len = 0;
+
+	/* retrieve destination log page to write */
+	bp = (lbuf_t *) log->bp;
+	lp = (logpage_t *) bp->l_ldata;
+	dstoffset = log->eor;
+
+	/* any log data to write ? */
+	if (tlck == NULL)
+		goto moveLrd;
+
+	/*
+	 *      move log record data
+	 */
+	/* retrieve source meta-data page to log */
+	if (tlck->flag & tlckPAGELOCK) {
+		p = (caddr_t) (tlck->mp->data);
+		linelock = (linelock_t *) & tlck->lock;
+	}
+	/* retrieve source in-memory inode to log */
+	else if (tlck->flag & tlckINODELOCK) {
+		if (tlck->type & tlckDTREE)
+			p = (caddr_t) &JFS_IP(tlck->ip)->i_dtroot;
+		else
+			p = (caddr_t) &JFS_IP(tlck->ip)->i_xtroot;
+		linelock = (linelock_t *) & tlck->lock;
+	}
+#ifdef	_JFS_WIP
+	else if (tlck->flag & tlckINLINELOCK) {
+
+		inlinelock = (inlinelock_t *) & tlck;
+		p = (caddr_t) & inlinelock->pxd;
+		linelock = (linelock_t *) & tlck;
+	}
+#endif				/* _JFS_WIP */
+	else {
+		jERROR(2, ("lmWriteRecord: UFO tlck:0x%p\n", tlck));
+		return 0;	/* Probably should trap */
+	}
+	l2linesize = linelock->l2linesize;
+
+      moveData:
+	ASSERT(linelock->index <= linelock->maxcnt);
+
+	lv = (lv_t *) & linelock->lv;
+	for (i = 0; i < linelock->index; i++, lv++) {
+		if (lv->length == 0)
+			continue;
+
+		/* is page full ? */
+		if (dstoffset >= LOGPSIZE - LOGPTLRSIZE) {
+			/* page become full: move on to next page */
+			lmNextPage(log);
+
+			bp = log->bp;
+			lp = (logpage_t *) bp->l_ldata;
+			dstoffset = LOGPHDRSIZE;
+		}
+
+		/*
+		 * move log vector data
+		 */
+		src = (u8 *) p + (lv->offset << l2linesize);
+		srclen = lv->length << l2linesize;
+		len += srclen;
+		while (srclen > 0) {
+			freespace = (LOGPSIZE - LOGPTLRSIZE) - dstoffset;
+			nbytes = min(freespace, srclen);
+			dst = (caddr_t) lp + dstoffset;
+			memcpy(dst, src, nbytes);
+			dstoffset += nbytes;
+
+			/* is page not full ? */
+			if (dstoffset < LOGPSIZE - LOGPTLRSIZE)
+				break;
+
+			/* page become full: move on to next page */
+			lmNextPage(log);
+
+			bp = (lbuf_t *) log->bp;
+			lp = (logpage_t *) bp->l_ldata;
+			dstoffset = LOGPHDRSIZE;
+
+			srclen -= nbytes;
+			src += nbytes;
+		}
+
+		/*
+		 * move log vector descriptor
+		 */
+		len += 4;
+		lvd = (lvd_t *) ((caddr_t) lp + dstoffset);
+		lvd->offset = cpu_to_le16(lv->offset);
+		lvd->length = cpu_to_le16(lv->length);
+		dstoffset += 4;
+		jFYI(1,
+		     ("lmWriteRecord: lv offset:%d length:%d\n",
+		      lv->offset, lv->length));
+	}
+
+	if ((i = linelock->next)) {
+		linelock = (linelock_t *) lid_to_tlock(i);
+		goto moveData;
+	}
+
+	/*
+	 *      move log record descriptor
+	 */
+      moveLrd:
+	lrd->length = cpu_to_le16(len);
+
+	src = (caddr_t) lrd;
+	srclen = LOGRDSIZE;
+
+	while (srclen > 0) {
+		freespace = (LOGPSIZE - LOGPTLRSIZE) - dstoffset;
+		nbytes = min(freespace, srclen);
+		dst = (caddr_t) lp + dstoffset;
+		memcpy(dst, src, nbytes);
+
+		dstoffset += nbytes;
+		srclen -= nbytes;
+
+		/* are there more to move than freespace of page ? */
+		if (srclen)
+			goto pageFull;
+
+		/*
+		 * end of log record descriptor
+		 */
+
+		/* update last log record eor */
+		log->eor = dstoffset;
+		bp->l_eor = dstoffset;
+		lsn = (log->page << L2LOGPSIZE) + dstoffset;
+
+		if (lrd->type & cpu_to_le16(LOG_COMMIT)) {
+			tblk->clsn = lsn;
+			jFYI(1,
+			     ("wr: tclsn:0x%x, beor:0x%x\n", tblk->clsn,
+			      bp->l_eor));
+
+			INCREMENT(lmStat.commit);	/* # of commit */
+
+			/*
+			 * enqueue tblock for group commit:
+			 *
+			 * enqueue tblock of non-trivial/synchronous COMMIT
+			 * at tail of group commit queue
+			 * (trivial/asynchronous COMMITs are ignored by
+			 * group commit.)
+			 */
+			LOGGC_LOCK(log);
+
+			/* init tblock gc state */
+			tblk->flag = tblkGC_QUEUE;
+			tblk->bp = log->bp;
+			tblk->pn = log->page;
+			tblk->eor = log->eor;
+			init_waitqueue_head(&tblk->gcwait);
+
+			/* enqueue transaction to commit queue */
+			tblk->cqnext = NULL;
+			if (log->cqueue.head) {
+				log->cqueue.tail->cqnext = tblk;
+				log->cqueue.tail = tblk;
+			} else
+				log->cqueue.head = log->cqueue.tail = tblk;
+
+			LOGGC_UNLOCK(log);
+		}
+
+		jFYI(1,
+		     ("lmWriteRecord: lrd:0x%04x bp:0x%p pn:%d eor:0x%x\n",
+		      le16_to_cpu(lrd->type), log->bp, log->page,
+		      dstoffset));
+
+		/* page not full ? */
+		if (dstoffset < LOGPSIZE - LOGPTLRSIZE)
+			return lsn;
+
+	      pageFull:
+		/* page become full: move on to next page */
+		lmNextPage(log);
+
+		bp = (lbuf_t *) log->bp;
+		lp = (logpage_t *) bp->l_ldata;
+		dstoffset = LOGPHDRSIZE;
+		src += nbytes;
+	}
+
+	return lsn;
+}
+
+
+/*
+ * NAME:	lmNextPage()
+ *
+ * FUNCTION:	write current page and allocate next page.
+ *
+ * PARAMETER:	log
+ *
+ * RETURN:	0
+ *			
+ * serialization: LOG_LOCK() held on entry/exit
+ */
+static int lmNextPage(log_t * log)
+{
+	logpage_t *lp;
+	int lspn;		/* log sequence page number */
+	int pn;			/* current page number */
+	lbuf_t *bp;
+	lbuf_t *nextbp;
+	tblock_t *tblk;
+
+	jFYI(1, ("lmNextPage\n"));
+
+	/* get current log page number and log sequence page number */
+	pn = log->page;
+	bp = log->bp;
+	lp = (logpage_t *) bp->l_ldata;
+	lspn = le32_to_cpu(lp->h.page);
+
+	LOGGC_LOCK(log);
+
+	/*
+	 *      write or queue the full page at the tail of write queue
+	 */
+	/* get the tail tblk on commit queue */
+	tblk = log->cqueue.tail;
+
+	/* every tblk who has COMMIT record on the current page,
+	 * and has not been committed, must be on commit queue
+	 * since tblk is queued at commit queueu at the time
+	 * of writing its COMMIT record on the page before
+	 * page becomes full (even though the tblk thread
+	 * who wrote COMMIT record may have been suspended
+	 * currently);
+	 */
+
+	/* is page bound with outstanding tail tblk ? */
+	if (tblk && tblk->pn == pn) {
+		/* mark tblk for end-of-page */
+		tblk->flag |= tblkGC_EOP;
+
+		/* if page is not already on write queue,
+		 * just enqueue (no lbmWRITE to prevent redrive)
+		 * buffer to wqueue to ensure correct serial order
+		 * of the pages since log pages will be added
+		 * continuously (tblk bound with the page hasn't
+		 * got around to init write of the page, either
+		 * preempted or the page got filled by its COMMIT
+		 * record);
+		 * pages with COMMIT are paged out explicitly by
+		 * tblk in lmGroupCommit();
+		 */
+		if (bp->l_wqnext == NULL) {
+			/* bp->l_ceor = bp->l_eor; */
+			/* lp->h.eor = lp->t.eor = bp->l_ceor; */
+			lbmWrite(log, bp, 0, 0);
+		}
+	}
+	/* page is not bound with outstanding tblk:
+	 * init write or mark it to be redriven (lbmWRITE)
+	 */
+	else {
+		/* finalize the page */
+		bp->l_ceor = bp->l_eor;
+		lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_ceor);
+		lbmWrite(log, bp, lbmWRITE | lbmRELEASE | lbmFREE, 0);
+	}
+	LOGGC_UNLOCK(log);
+
+	/*
+	 *      allocate/initialize next page
+	 */
+	/* if log wraps, the first data page of log is 2
+	 * (0 never used, 1 is superblock).
+	 */
+	log->page = (pn == log->size - 1) ? 2 : pn + 1;
+	log->eor = LOGPHDRSIZE;	/* ? valid page empty/full at logRedo() */
+
+	/* allocate/initialize next log page buffer */
+	nextbp = lbmAllocate(log, log->page);
+	nextbp->l_eor = log->eor;
+	log->bp = nextbp;
+
+	/* initialize next log page */
+	lp = (logpage_t *) nextbp->l_ldata;
+	lp->h.page = lp->t.page = cpu_to_le32(lspn + 1);
+	lp->h.eor = lp->t.eor = cpu_to_le16(LOGPHDRSIZE);
+
+	jFYI(1, ("lmNextPage done\n"));
+	return 0;
+}
+
+
+/*
+ * NAME:	lmGroupCommit()
+ *
+ * FUNCTION:	group commit
+ *	initiate pageout of the pages with COMMIT in the order of
+ *	page number - redrive pageout of the page at the head of
+ *	pageout queue until full page has been written.
+ *
+ * RETURN:	
+ *
+ * NOTE:
+ *	LOGGC_LOCK serializes log group commit queue, and
+ *	transaction blocks on the commit queue.
+ *	N.B. LOG_LOCK is NOT held during lmGroupCommit().
+ */
+int lmGroupCommit(log_t * log, tblock_t * tblk)
+{
+	int rc = 0;
+
+	LOGGC_LOCK(log);
+
+	/* group committed already ? */
+	if (tblk->flag & tblkGC_COMMITTED) {
+		if (tblk->flag & tblkGC_ERROR)
+			rc = EIO;
+
+		LOGGC_UNLOCK(log);
+		return rc;
+	}
+	jFYI(1,
+	     ("lmGroup Commit: tblk = 0x%p, gcrtc = %d\n", tblk,
+	      log->gcrtc));
+
+	/*
+	 * group commit pageout in progress
+	 */
+	if ((!(log->cflag & logGC_PAGEOUT)) && log->cqueue.head) {
+		/*
+		 * only transaction in the commit queue:
+		 *
+		 * start one-transaction group commit as
+		 * its group leader.
+		 */
+		log->cflag |= logGC_PAGEOUT;
+
+		lmGCwrite(log, 0);
+	}
+	/* lmGCwrite gives up LOGGC_LOCK, check again */
+
+	if (tblk->flag & tblkGC_COMMITTED) {
+		if (tblk->flag & tblkGC_ERROR)
+			rc = EIO;
+
+		LOGGC_UNLOCK(log);
+		return rc;
+	}
+
+	/* upcount transaction waiting for completion
+	 */
+	log->gcrtc++;
+
+	if (tblk->xflag & COMMIT_LAZY) {
+		tblk->flag |= tblkGC_LAZY;
+		LOGGC_UNLOCK(log);
+		return 0;
+	}
+	tblk->flag |= tblkGC_READY;
+
+	__SLEEP_COND(tblk->gcwait, (tblk->flag & tblkGC_COMMITTED),
+		     LOGGC_LOCK(log), LOGGC_UNLOCK(log));
+
+	/* removed from commit queue */
+	if (tblk->flag & tblkGC_ERROR)
+		rc = EIO;
+
+	LOGGC_UNLOCK(log);
+	return rc;
+}
+
+/*
+ * NAME:	lmGCwrite()
+ *
+ * FUNCTION:	group commit write
+ *	initiate write of log page, building a group of all transactions
+ *	with commit records on that page.
+ *
+ * RETURN:	None
+ *
+ * NOTE:
+ *	LOGGC_LOCK must be held by caller.
+ *	N.B. LOG_LOCK is NOT held during lmGroupCommit().
+ */
+void lmGCwrite(log_t * log, int cant_write)
+{
+	lbuf_t *bp;
+	logpage_t *lp;
+	int gcpn;		/* group commit page number */
+	tblock_t *tblk;
+	tblock_t *xtblk;
+
+	/*
+	 * build the commit group of a log page
+	 *
+	 * scan commit queue and make a commit group of all
+	 * transactions with COMMIT records on the same log page.
+	 */
+	/* get the head tblk on the commit queue */
+	tblk = xtblk = log->cqueue.head;
+	gcpn = tblk->pn;
+
+	while (tblk && tblk->pn == gcpn) {
+		xtblk = tblk;
+
+		/* state transition: (QUEUE, READY) -> COMMIT */
+		tblk->flag |= tblkGC_COMMIT;
+		tblk = tblk->cqnext;
+	}
+	tblk = xtblk;		/* last tblk of the page */
+
+	/*
+	 * pageout to commit transactions on the log page.
+	 */
+	bp = (lbuf_t *) tblk->bp;
+	lp = (logpage_t *) bp->l_ldata;
+	/* is page already full ? */
+	if (tblk->flag & tblkGC_EOP) {
+		/* mark page to free at end of group commit of the page */
+		tblk->flag &= ~tblkGC_EOP;
+		tblk->flag |= tblkGC_FREE;
+		bp->l_ceor = bp->l_eor;
+		lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_ceor);
+		jEVENT(0,
+		       ("gc: tclsn:0x%x, bceor:0x%x\n", tblk->clsn,
+			bp->l_ceor));
+		lbmWrite(log, bp, lbmWRITE | lbmRELEASE | lbmGC,
+			 cant_write);
+	}
+	/* page is not yet full */
+	else {
+		bp->l_ceor = tblk->eor;	/* ? bp->l_ceor = bp->l_eor; */
+		lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_ceor);
+		jEVENT(0,
+		       ("gc: tclsn:0x%x, bceor:0x%x\n", tblk->clsn,
+			bp->l_ceor));
+		lbmWrite(log, bp, lbmWRITE | lbmGC, cant_write);
+	}
+}
+
+/*
+ * NAME:	lmPostGC()
+ *
+ * FUNCTION:	group commit post-processing
+ *	Processes transactions after their commit records have been written
+ *	to disk, redriving log I/O if necessary.
+ *
+ * RETURN:	None
+ *
+ * NOTE:
+ *	This routine is called a interrupt time by lbmIODone
+ */
+void lmPostGC(lbuf_t * bp)
+{
+	unsigned long flags;
+	log_t *log = bp->l_log;
+	logpage_t *lp;
+	tblock_t *tblk;
+
+	//LOGGC_LOCK(log);
+	spin_lock_irqsave(&log->gclock, flags);
+	/*
+	 * current pageout of group commit completed.
+	 *
+	 * remove/wakeup transactions from commit queue who were
+	 * group committed with the current log page
+	 */
+	while ((tblk = log->cqueue.head) && (tblk->flag & tblkGC_COMMIT)) {
+		/* if transaction was marked GC_COMMIT then
+		 * it has been shipped in the current pageout
+		 * and made it to disk - it is committed.
+		 */
+
+		if (bp->l_flag & lbmERROR)
+			tblk->flag |= tblkGC_ERROR;
+
+		/* remove it from the commit queue */
+		log->cqueue.head = tblk->cqnext;
+		if (log->cqueue.head == NULL)
+			log->cqueue.tail = NULL;
+		tblk->flag &= ~tblkGC_QUEUE;
+		tblk->cqnext = 0;
+
+		jEVENT(0,
+		       ("lmPostGC: tblk = 0x%p, flag = 0x%x\n", tblk,
+			tblk->flag));
+
+		if (!(tblk->xflag & COMMIT_FORCE))
+			/*
+			 * Hand tblk over to lazy commit thread
+			 */
+			txLazyUnlock(tblk);
+		else {
+			/* state transition: COMMIT -> COMMITTED */
+			tblk->flag |= tblkGC_COMMITTED;
+
+			if (tblk->flag & tblkGC_READY) {
+				log->gcrtc--;
+				LOGGC_WAKEUP(tblk);
+			}
+		}
+
+		/* was page full before pageout ?
+		 * (and this is the last tblk bound with the page)
+		 */
+		if (tblk->flag & tblkGC_FREE)
+			lbmFree(bp);
+		/* did page become full after pageout ?
+		 * (and this is the last tblk bound with the page)
+		 */
+		else if (tblk->flag & tblkGC_EOP) {
+			/* finalize the page */
+			lp = (logpage_t *) bp->l_ldata;
+			bp->l_ceor = bp->l_eor;
+			lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_eor);
+			jEVENT(0, ("lmPostGC: calling lbmWrite\n"));
+			lbmWrite(log, bp, lbmWRITE | lbmRELEASE | lbmFREE,
+				 1);
+		}
+
+	}
+
+	/* are there any transactions who have entered lnGroupCommit()
+	 * (whose COMMITs are after that of the last log page written.
+	 * They are waiting for new group commit (above at (SLEEP 1)):
+	 * select the latest ready transaction as new group leader and
+	 * wake her up to lead her group.
+	 */
+	if ((log->gcrtc > 0) && log->cqueue.head)
+		/*
+		 * Call lmGCwrite with new group leader
+		 */
+		lmGCwrite(log, 1);
+
+	/* no transaction are ready yet (transactions are only just
+	 * queued (GC_QUEUE) and not entered for group commit yet).
+	 * let the first transaction entering group commit
+	 * will elect hetself as new group leader.
+	 */
+	else
+		log->cflag &= ~logGC_PAGEOUT;
+
+	//LOGGC_UNLOCK(log);
+	spin_unlock_irqrestore(&log->gclock, flags);
+	return;
+}
+
+/*
+ * NAME:	lmLogSync()
+ *
+ * FUNCTION:	write log SYNCPT record for specified log
+ *	if new sync address is available
+ *	(normally the case if sync() is executed by back-ground
+ *	process).
+ *	if not, explicitly run jfs_blogsync() to initiate
+ *	getting of new sync address.
+ *	calculate new value of i_nextsync which determines when
+ *	this code is called again.
+ *
+ *	this is called only from lmLog().
+ *
+ * PARAMETER:	ip	- pointer to logs inode.
+ *
+ * RETURN:	0
+ *			
+ * serialization: LOG_LOCK() held on entry/exit
+ */
+int lmLogSync(log_t * log, int nosyncwait)
+{
+	int logsize;
+	int written;		/* written since last syncpt */
+	int free;		/* free space left available */
+	int delta;		/* additional delta to write normally */
+	int more;		/* additional write granted */
+	lrd_t lrd;
+	int lsn;
+	struct logsyncblk *lp;
+
+	/*
+	 *      forward syncpt
+	 */
+	/* if last sync is same as last syncpt,
+	 * invoke sync point forward processing to update sync.
+	 */
+
+	if (log->sync == log->syncpt) {
+		LOGSYNC_LOCK(log);
+		/* ToDo: push dirty metapages out to disk */
+//              bmLogSync(log);
+
+		if (list_empty(&log->synclist))
+			log->sync = log->lsn;
+		else {
+			lp = list_entry(log->synclist.next,
+					struct logsyncblk, synclist);
+			log->sync = lp->lsn;
+		}
+		LOGSYNC_UNLOCK(log);
+
+	}
+
+	/* if sync is different from last syncpt,
+	 * write a SYNCPT record with syncpt = sync.
+	 * reset syncpt = sync
+	 */
+	if (log->sync != log->syncpt) {
+		struct jfs_sb_info	*sbi = JFS_SBI(log->sb);
+		/*
+		 * We need to make sure all of the "written" metapages
+		 * actually make it to disk
+		 */
+		fsync_inode_data_buffers(sbi->ipbmap);
+		fsync_inode_data_buffers(sbi->ipimap);
+		fsync_inode_data_buffers(sbi->direct_inode);
+
+		lrd.logtid = 0;
+		lrd.backchain = 0;
+		lrd.type = cpu_to_le16(LOG_SYNCPT);
+		lrd.length = 0;
+		lrd.log.syncpt.sync = cpu_to_le32(log->sync);
+		lsn = lmWriteRecord(log, NULL, &lrd, NULL);
+
+		log->syncpt = log->sync;
+	} else
+		lsn = log->lsn;
+
+	/*
+	 *      setup next syncpt trigger (SWAG)
+	 */
+	logsize = log->logsize;
+
+	logdiff(written, lsn, log);
+	free = logsize - written;
+	delta = LOGSYNC_DELTA(logsize);
+	more = min(free / 2, delta);
+	if (more < 2 * LOGPSIZE) {
+		jEVENT(1,
+		       ("\n ... Log Wrap ... Log Wrap ... Log Wrap ...\n\n"));
+		/*
+		 *      log wrapping
+		 *
+		 * option 1 - panic ? No.!
+		 * option 2 - shutdown file systems
+		 *            associated with log ?
+		 * option 3 - extend log ?
+		 */
+		/*
+		 * option 4 - second chance
+		 *
+		 * mark log wrapped, and continue.
+		 * when all active transactions are completed,
+		 * mark log vaild for recovery.
+		 * if crashed during invalid state, log state
+		 * implies invald log, forcing fsck().
+		 */
+		/* mark log state log wrap in log superblock */
+		/* log->state = LOGWRAP; */
+
+		/* reset sync point computation */
+		log->syncpt = log->sync = lsn;
+		log->nextsync = delta;
+	} else
+		/* next syncpt trigger = written + more */
+		log->nextsync = written + more;
+
+	/* return if lmLogSync() from outside of transaction, e.g., sync() */
+	if (nosyncwait)
+		return lsn;
+
+	/* if number of bytes written from last sync point is more
+	 * than 1/4 of the log size, stop new transactions from
+	 * starting until all current transactions are completed
+	 * by setting syncbarrier flag.
+	 */
+	if (written > LOGSYNC_BARRIER(logsize) && logsize > 32 * LOGPSIZE) {
+		log->syncbarrier = 1;
+		jFYI(1, ("log barrier on: lsn=0x%x syncpt=0x%x\n", lsn,
+			 log->syncpt));
+	}
+
+	return lsn;
+}
+
+
+/*
+ * NAME:	lmLogOpen()
+ *
+ * FUNCTION:    open the log on first open;
+ *	insert filesystem in the active list of the log.
+ *
+ * PARAMETER:	ipmnt	- file system mount inode
+ *		iplog 	- log inode (out)
+ *
+ * RETURN:
+ *
+ * serialization:
+ */
+int lmLogOpen(struct super_block *sb, log_t ** logptr)
+{
+	int rc;
+	struct block_device *bdev;
+	log_t *log;
+
+	if (!(log = kmalloc(sizeof(log_t), GFP_KERNEL)))
+		return ENOMEM;
+	memset(log, 0, sizeof(log_t));
+
+	if (!(JFS_SBI(sb)->mntflag & JFS_INLINELOG))
+		goto externalLog;
+
+	/*
+	 *      in-line log in host file system
+	 *
+	 * file system to log have 1-to-1 relationship;
+	 */
+
+	log->sb = sb;		/* This should be a list */
+	log->flag = JFS_INLINELOG;
+	log->dev = sb->s_dev;
+	log->base = addressPXD(&JFS_SBI(sb)->logpxd);
+	log->size = lengthPXD(&JFS_SBI(sb)->logpxd) >>
+	    (L2LOGPSIZE - sb->s_blocksize_bits);
+	log->l2bsize = sb->s_blocksize_bits;
+	ASSERT(L2LOGPSIZE >= sb->s_blocksize_bits);
+
+	/*
+	 * initialize log.
+	 */
+	if ((rc = lmLogInit(log)))
+		goto errout10;
+	goto out;
+
+	/*
+	 *      external log as separate logical volume
+	 *
+	 * file systems to log may have n-to-1 relationship;
+	 */
+      externalLog:
+	if (!(bdev = bdget(kdev_t_to_nr(JFS_SBI(sb)->logdev)))) {
+		rc = ENODEV;
+		goto errout10;
+	}
+
+	if ((rc = blkdev_get(bdev, FMODE_READ|FMODE_WRITE, 0, BDEV_FS))) {
+		rc = -rc;
+		goto errout10;
+	}
+
+	log->sb = sb;		/* This should be a list */
+	log->dev = JFS_SBI(sb)->logdev;
+	log->bdev = bdev;
+	
+	/*
+	 * initialize log:
+	 */
+	if ((rc = lmLogInit(log)))
+		goto errout20;
+
+	/*
+	 * add file system to log active file system list
+	 */
+	if ((rc = lmLogFileSystem(log, sb->s_dev, 1)))
+		goto errout30;
+
+      out:
+	jFYI(1, ("lmLogOpen: exit(0)\n"));
+	*logptr = log;
+	return 0;
+
+	/*
+	 *      unwind on error
+	 */
+      errout30:		/* unwind lbmLogInit() */
+	lbmLogShutdown(log);
+
+      errout20:		/* close external log device */
+	blkdev_put(bdev, BDEV_FS);
+
+      errout10:		/* free log descriptor */
+	kfree(log);
+
+	jFYI(1, ("lmLogOpen: exit(%d)\n", rc));
+	return rc;
+}
+
+
+/*
+ * NAME:	lmLogInit()
+ *
+ * FUNCTION:	log initialization at first log open.
+ *
+ *	logredo() (or logformat()) should have been run previously.
+ *	initialize the log inode from log superblock.
+ *	set the log state in the superblock to LOGMOUNT and
+ *	write SYNCPT log record.
+ *		
+ * PARAMETER:	log	- log structure
+ *
+ * RETURN:	0	- if ok
+ *		EINVAL	- bad log magic number or superblock dirty
+ *		error returned from logwait()
+ *			
+ * serialization: single first open thread
+ */
+static int lmLogInit(log_t * log)
+{
+	int rc = 0;
+	lrd_t lrd;
+	logsuper_t *logsuper;
+	lbuf_t *bpsuper;
+	lbuf_t *bp;
+	logpage_t *lp;
+	int lsn;
+
+	jFYI(1, ("lmLogInit: log:0x%p\n", log));
+
+	/*
+	 * log inode is overlaid on generic inode where
+	 * dinode have been zeroed out by iRead();
+	 */
+
+	/*
+	 * initialize log i/o
+	 */
+	if ((rc = lbmLogInit(log)))
+		return rc;
+
+	/*
+	 * validate log superblock
+	 */
+
+
+	if (!(log->flag & JFS_INLINELOG))
+		log->l2bsize = 12;	/* XXX kludge alert XXX */
+	if ((rc = lbmRead(log, 1, &bpsuper)))
+		goto errout10;
+
+	logsuper = (logsuper_t *) bpsuper->l_ldata;
+
+	if (logsuper->magic != cpu_to_le32(LOGMAGIC)) {
+		jERROR(1, ("*** Log Format Error ! ***\n"));
+		rc = EINVAL;
+		goto errout20;
+	}
+
+	/* logredo() should have been run successfully. */
+	if (logsuper->state != cpu_to_le32(LOGREDONE)) {
+		jERROR(1, ("*** Log Is Dirty ! ***\n"));
+		rc = EINVAL;
+		goto errout20;
+	}
+
+	/* initialize log inode from log superblock */
+	if (log->flag & JFS_INLINELOG) {
+		if (log->size != le32_to_cpu(logsuper->size)) {
+			rc = EINVAL;
+			goto errout20;
+		}
+		jFYI(0,
+		     ("lmLogInit: inline log:0x%p base:0x%Lx size:0x%x\n",
+		      log, (unsigned long long) log->base, log->size));
+	} else {
+		log->size = le32_to_cpu(logsuper->size);
+		log->l2bsize = le32_to_cpu(logsuper->l2bsize);
+		jFYI(0,
+		     ("lmLogInit: external log:0x%p base:0x%Lx size:0x%x\n",
+		      log, (unsigned long long) log->base, log->size));
+	}
+
+	log->flag |= JFS_GROUPCOMMIT;
+/*
+	log->flag |= JFS_LAZYCOMMIT;
+*/
+	log->page = le32_to_cpu(logsuper->end) / LOGPSIZE;
+	log->eor = le32_to_cpu(logsuper->end) - (LOGPSIZE * log->page);
+
+	/*
+	 * initialize for log append write mode
+	 */
+	/* establish current/end-of-log page/buffer */
+	if ((rc = lbmRead(log, log->page, &bp)))
+		goto errout20;
+
+	lp = (logpage_t *) bp->l_ldata;
+
+	jFYI(1, ("lmLogInit: lsn:0x%x page:%d eor:%d:%d\n",
+		 le32_to_cpu(logsuper->end), log->page, log->eor,
+		 le16_to_cpu(lp->h.eor)));
+
+//      ASSERT(log->eor == lp->h.eor);
+
+	log->bp = bp;
+	bp->l_pn = log->page;
+	bp->l_eor = log->eor;
+
+	/* initialize the group commit serialization lock */
+	LOGGC_LOCK_INIT(log);
+
+	/* if current page is full, move on to next page */
+	if (log->eor >= LOGPSIZE - LOGPTLRSIZE)
+		lmNextPage(log);
+
+	/* allocate/initialize the log write serialization lock */
+	LOG_LOCK_INIT(log);
+
+	/*
+	 * initialize log syncpoint
+	 */
+	/*
+	 * write the first SYNCPT record with syncpoint = 0
+	 * (i.e., log redo up to HERE !);
+	 * remove current page from lbm write queue at end of pageout
+	 * (to write log superblock update), but do not release to freelist;
+	 */
+	lrd.logtid = 0;
+	lrd.backchain = 0;
+	lrd.type = cpu_to_le16(LOG_SYNCPT);
+	lrd.length = 0;
+	lrd.log.syncpt.sync = 0;
+	lsn = lmWriteRecord(log, NULL, &lrd, NULL);
+	bp = log->bp;
+	bp->l_ceor = bp->l_eor;
+	lp = (logpage_t *) bp->l_ldata;
+	lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_eor);
+	lbmWrite(log, bp, lbmWRITE | lbmSYNC, 0);
+	if ((rc = lbmIOWait(bp, 0)))
+		goto errout30;
+
+	/* initialize logsync parameters */
+	log->logsize = (log->size - 2) << L2LOGPSIZE;
+	log->lsn = lsn;
+	log->syncpt = lsn;
+	log->sync = log->syncpt;
+	log->nextsync = LOGSYNC_DELTA(log->logsize);
+	init_waitqueue_head(&log->syncwait);
+
+	jFYI(1, ("lmLogInit: lsn:0x%x syncpt:0x%x sync:0x%x\n",
+		 log->lsn, log->syncpt, log->sync));
+
+	LOGSYNC_LOCK_INIT(log);
+
+	INIT_LIST_HEAD(&log->synclist);
+
+	log->cqueue.head = log->cqueue.tail = 0;
+
+	log->count = 0;
+
+	/*
+	 * initialize for lazy/group commit
+	 */
+	log->clsn = lsn;
+
+	/*
+	 * update/write superblock
+	 */
+	logsuper->state = cpu_to_le32(LOGMOUNT);
+	log->serial = le32_to_cpu(logsuper->serial) + 1;
+	logsuper->serial = cpu_to_le32(log->serial);
+	logsuper->device = cpu_to_le32(kdev_t_to_nr(log->dev));
+	lbmDirectWrite(log, bpsuper, lbmWRITE | lbmRELEASE | lbmSYNC);
+	if ((rc = lbmIOWait(bpsuper, lbmFREE)))
+		goto errout30;
+
+	jFYI(1, ("lmLogInit: exit(%d)\n", rc));
+	return 0;
+
+	/*
+	 *      unwind on error
+	 */
+      errout30:		/* release log page */
+	lbmFree(bp);
+
+      errout20:		/* release log superblock */
+	lbmFree(bpsuper);
+
+      errout10:		/* unwind lbmLogInit() */
+	lbmLogShutdown(log);
+
+	jFYI(1, ("lmLogInit: exit(%d)\n", rc));
+	return rc;
+}
+
+
+/*
+ * NAME:	lmLogClose()
+ *
+ * FUNCTION:	remove file system <ipmnt> from active list of log <iplog>
+ *		and close it on last close.
+ *
+ * PARAMETER:	sb	- superblock
+ *		log	- log inode
+ *
+ * RETURN:	errors from subroutines
+ *
+ * serialization:
+ */
+int lmLogClose(struct super_block *sb, log_t * log)
+{
+	int rc;
+
+	jFYI(1, ("lmLogClose: log:0x%p\n", log));
+
+	if (!(log->flag & JFS_INLINELOG))
+		goto externalLog;
+	
+	/*
+	 *      in-line log in host file system
+	 */
+	rc = lmLogShutdown(log);
+	goto out;
+
+	/*
+	 *      external log as separate logical volume
+	 */
+      externalLog:
+	lmLogFileSystem(log, sb->s_dev, 0);
+	rc = lmLogShutdown(log);
+	blkdev_put(log->bdev, BDEV_FS);
+
+      out:
+	jFYI(0, ("lmLogClose: exit(%d)\n", rc));
+	return rc;
+}
+
+
+/*
+ * NAME:	lmLogShutdown()
+ *
+ * FUNCTION:	log shutdown at last LogClose().
+ *
+ *		write log syncpt record.
+ *		update super block to set redone flag to 0.
+ *
+ * PARAMETER:	log	- log inode
+ *
+ * RETURN:	0	- success
+ *			
+ * serialization: single last close thread
+ */
+static int lmLogShutdown(log_t * log)
+{
+	int rc;
+	lrd_t lrd;
+	int lsn;
+	logsuper_t *logsuper;
+	lbuf_t *bpsuper;
+	lbuf_t *bp;
+	logpage_t *lp;
+
+	jFYI(1, ("lmLogShutdown: log:0x%p\n", log));
+
+	if (log->cqueue.head || !list_empty(&log->synclist)) {
+		/*
+		 * If there was very recent activity, we may need to wait
+		 * for the lazycommit thread to catch up
+		 */
+		int i;
+
+		for (i = 0; i < 800; i++) {	/* Too much? */
+			current->state = TASK_INTERRUPTIBLE;
+			schedule_timeout(HZ / 4);
+			if ((log->cqueue.head == NULL) &&
+			    list_empty(&log->synclist))
+				break;
+		}
+	}
+	assert(log->cqueue.head == NULL);
+	assert(list_empty(&log->synclist));
+
+	/*
+	 * We need to make sure all of the "written" metapages
+	 * actually make it to disk
+	 */
+#if ( (LINUX_VERSION_CODE < KERNEL_VERSION(2,4,8)) || \
+      ( (LINUX_VERSION_CODE == KERNEL_VERSION(2,4,8)) && defined(MODULE) ) )
+	/*
+	 * fsync_no_super not added until 2.4.8, not exported until 2.4.9
+	 */
+	{
+		struct jfs_sb_info *sbi = JFS_SBI(log->sb);
+
+		fsync_inode_data_buffers(sbi->ipbmap);
+		fsync_inode_data_buffers(sbi->ipimap);
+		fsync_inode_data_buffers(sbi->direct_inode);
+	}
+#else
+	fsync_no_super(log->sb->s_dev);
+#endif
+
+	/*
+	 * write the last SYNCPT record with syncpoint = 0
+	 * (i.e., log redo up to HERE !)
+	 */
+	lrd.logtid = 0;
+	lrd.backchain = 0;
+	lrd.type = cpu_to_le16(LOG_SYNCPT);
+	lrd.length = 0;
+	lrd.log.syncpt.sync = 0;
+	lsn = lmWriteRecord(log, NULL, &lrd, NULL);
+	bp = log->bp;
+	lp = (logpage_t *) bp->l_ldata;
+	lp->h.eor = lp->t.eor = cpu_to_le16(bp->l_eor);
+	lbmWrite(log, log->bp, lbmWRITE | lbmRELEASE | lbmSYNC, 0);
+	lbmIOWait(log->bp, lbmFREE);
+
+	/*
+	 * synchronous update log superblock
+	 * mark log state as shutdown cleanly
+	 * (i.e., Log does not need to be replayed).
+	 */
+	if ((rc = lbmRead(log, 1, &bpsuper)))
+		goto out;
+
+	logsuper = (logsuper_t *) bpsuper->l_ldata;
+	logsuper->state = cpu_to_le32(LOGREDONE);
+	logsuper->end = cpu_to_le32(lsn);
+	lbmDirectWrite(log, bpsuper, lbmWRITE | lbmRELEASE | lbmSYNC);
+	rc = lbmIOWait(bpsuper, lbmFREE);
+
+	jFYI(1, ("lmLogShutdown: lsn:0x%x page:%d eor:%d\n",
+		 lsn, log->page, log->eor));
+
+      out:    
+	/*
+	 * shutdown per log i/o
+	 */
+	lbmLogShutdown(log);
+
+	if (rc) {
+		jFYI(1, ("lmLogShutdown: exit(%d)\n", rc));
+	}
+	return rc;
+}
+
+
+/*
+ * NAME:	lmLogFileSystem()
+ *
+ * FUNCTION:	insert (<activate> = true)/remove (<activate> = false)
+ *	file system into/from log active file system list.
+ *
+ * PARAMETE:	log	- pointer to logs inode.
+ *		fsdev	- kdev_t of filesystem.
+ *		serial  - pointer to returned log serial number
+ *		activate - insert/remove device from active list.
+ *
+ * RETURN:	0	- success
+ *		errors returned by vms_iowait().
+ *			
+ * serialization: IWRITE_LOCK(log inode) held on entry/exit
+ */
+static int lmLogFileSystem(log_t * log, kdev_t fsdev, int activate)
+{
+	int rc = 0;
+	int i;
+	u32 dev_le = cpu_to_le32(kdev_t_to_nr(fsdev));
+	logsuper_t *logsuper;
+	lbuf_t *bpsuper;
+
+	/*
+	 * insert/remove file system device to log active file system list.
+	 */
+	if ((rc = lbmRead(log, 1, &bpsuper)))
+		return rc;
+
+	logsuper = (logsuper_t *) bpsuper->l_ldata;
+	if (activate) {
+		for (i = 0; i < MAX_ACTIVE; i++)
+			if (logsuper->active[i] == 0) {
+				logsuper->active[i] = dev_le;
+				break;
+			}
+		if (i == MAX_ACTIVE) {
+			jERROR(1,("Too many file systems sharing journal!\n"));
+			lbmFree(bpsuper);
+			return EMFILE;	/* Is there a better rc? */
+		}
+	} else {
+		for (i = 0; i < MAX_ACTIVE; i++)
+			if (logsuper->active[i] == dev_le) {
+				logsuper->active[i] = 0;
+				break;
+			}
+		assert(i < MAX_ACTIVE);
+	}
+
+	/*
+	 * synchronous write log superblock:
+	 *
+	 * write sidestream bypassing write queue:
+	 * at file system mount, log super block is updated for
+	 * activation of the file system before any log record
+	 * (MOUNT record) of the file system, and at file system
+	 * unmount, all meta data for the file system has been
+	 * flushed before log super block is updated for deactivation
+	 * of the file system.
+	 */
+	lbmDirectWrite(log, bpsuper, lbmWRITE | lbmRELEASE | lbmSYNC);
+	rc = lbmIOWait(bpsuper, lbmFREE);
+
+	return rc;
+}
+
+
+/*
+ *	lmLogQuiesce()
+ */
+int lmLogQuiesce(log_t * log)
+{
+	int rc;
+
+	rc = lmLogShutdown(log);
+
+	return rc;
+}
+
+
+/*
+ *	lmLogResume()
+ */
+int lmLogResume(log_t * log, struct super_block *sb)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	int rc;
+
+	log->base = addressPXD(&sbi->logpxd);
+	log->size =
+	    (lengthPXD(&sbi->logpxd) << sb->s_blocksize_bits) >> L2LOGPSIZE;
+	rc = lmLogInit(log);
+
+	return rc;
+}
+
+
+/*
+ *		log buffer manager (lbm)
+ *		------------------------
+ *
+ * special purpose buffer manager supporting log i/o requirements.
+ *
+ * per log write queue:
+ * log pageout occurs in serial order by fifo write queue and
+ * restricting to a single i/o in pregress at any one time.
+ * a circular singly-linked list
+ * (log->wrqueue points to the tail, and buffers are linked via
+ * bp->wrqueue field), and
+ * maintains log page in pageout ot waiting for pageout in serial pageout.
+ */
+
+/*
+ *	lbmLogInit()
+ *
+ * initialize per log I/O setup at lmLogInit()
+ */
+static int lbmLogInit(log_t * log)
+{				/* log inode */
+	int i;
+	lbuf_t *lbuf;
+
+	jFYI(1, ("lbmLogInit: log:0x%p\n", log));
+
+	/* initialize current buffer cursor */
+	log->bp = NULL;
+
+	/* initialize log device write queue */
+	log->wqueue = NULL;
+
+	/*
+	 * Each log has its own buffer pages allocated to it.  These are
+	 * not managed by the page cache.  This ensures that a transaction
+	 * writing to the log does not block trying to allocate a page from
+	 * the page cache (for the log).  This would be bad, since page
+	 * allocation waits on the kswapd thread that may be committing inodes
+	 * which would cause log activity.  Was that clear?  I'm trying to
+	 * avoid deadlock here.
+	 */
+	init_waitqueue_head(&log->free_wait);
+
+	log->lbuf_free = NULL;
+
+	for (i = 0; i < LOGPAGES; i++) {
+		lbuf = kmalloc(sizeof(lbuf_t), GFP_KERNEL);
+		if (lbuf == 0)
+			goto error;
+		lbuf->l_bh.b_data = lbuf->l_ldata =
+		    (char *) __get_free_page(GFP_KERNEL);
+		if (lbuf->l_ldata == 0) {
+			kfree(lbuf);
+			goto error;
+		}
+		lbuf->l_log = log;
+		init_waitqueue_head(&lbuf->l_ioevent);
+
+		lbuf->l_bh.b_size = LOGPSIZE;
+		lbuf->l_bh.b_dev = log->dev;
+		lbuf->l_bh.b_end_io = lbmIODone;
+		lbuf->l_bh.b_private = lbuf;
+		lbuf->l_bh.b_page = virt_to_page(lbuf->l_ldata);
+		lbuf->l_bh.b_state = 0;
+		init_waitqueue_head(&lbuf->l_bh.b_wait);
+
+		lbuf->l_freelist = log->lbuf_free;
+		log->lbuf_free = lbuf;
+	}
+
+	return (0);
+
+      error:
+	lbmLogShutdown(log);
+	return (ENOMEM);
+}
+
+
+/*
+ *	lbmLogShutdown()
+ *
+ * finalize per log I/O setup at lmLogShutdown()
+ */
+static void lbmLogShutdown(log_t * log)
+{
+	lbuf_t *lbuf;
+
+	jFYI(1, ("lbmLogShutdown: log:0x%p\n", log));
+
+	lbuf = log->lbuf_free;
+	while (lbuf) {
+		lbuf_t *next = lbuf->l_freelist;
+		free_page((unsigned long) lbuf->l_ldata);
+		kfree(lbuf);
+		lbuf = next;
+	}
+
+	log->bp = NULL;
+}
+
+
+/*
+ *	lbmAllocate()
+ *
+ * allocate an empty log buffer
+ */
+static lbuf_t *lbmAllocate(log_t * log, int pn)
+{
+	lbuf_t *bp;
+	unsigned long flags;
+
+	/*
+	 * recycle from log buffer freelist if any
+	 */
+	LCACHE_LOCK(flags);
+	LCACHE_SLEEP_COND(log->free_wait, (bp = log->lbuf_free), flags);
+	log->lbuf_free = bp->l_freelist;
+	LCACHE_UNLOCK(flags);
+
+	bp->l_flag = 0;
+
+	bp->l_wqnext = NULL;
+	bp->l_freelist = NULL;
+
+	bp->l_pn = pn;
+	bp->l_blkno = log->base + (pn << (L2LOGPSIZE - log->l2bsize));
+	bp->l_bh.b_blocknr = bp->l_blkno;
+	bp->l_ceor = 0;
+
+	return bp;
+}
+
+
+/*
+ *	lbmFree()
+ *
+ * release a log buffer to freelist
+ */
+static void lbmFree(lbuf_t * bp)
+{
+	unsigned long flags;
+
+	LCACHE_LOCK(flags);
+
+	lbmfree(bp);
+
+	LCACHE_UNLOCK(flags);
+}
+
+static void lbmfree(lbuf_t * bp)
+{
+	log_t *log = bp->l_log;
+
+	assert(bp->l_wqnext == NULL);
+
+	/*
+	 * return the buffer to head of freelist
+	 */
+	bp->l_freelist = log->lbuf_free;
+	log->lbuf_free = bp;
+
+	wake_up(&log->free_wait);
+	return;
+}
+
+
+/*
+ * NAME:	lbmRedrive
+ *
+ * FUNCTION:	add a log buffer to the the log redrive list
+ *
+ * PARAMETER:
+ *     bp	- log buffer
+ *
+ * NOTES:
+ *	Takes log_redrive_lock.
+ */
+static inline void lbmRedrive(lbuf_t *bp)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&log_redrive_lock, flags);
+	bp->l_redrive_next = log_redrive_list;
+	log_redrive_list = bp;
+	spin_unlock_irqrestore(&log_redrive_lock, flags);
+
+	wake_up_process(jfsIOtask);
+}
+
+
+/*
+ *	lbmRead()
+ */
+static int lbmRead(log_t * log, int pn, lbuf_t ** bpp)
+{
+	lbuf_t *bp;
+
+	/*
+	 * allocate a log buffer
+	 */
+	*bpp = bp = lbmAllocate(log, pn);
+	jFYI(1, ("lbmRead: bp:0x%p pn:0x%x\n", bp, pn));
+
+	bp->l_flag |= lbmREAD;
+	bp->l_bh.b_reqnext = NULL;
+	clear_bit(BH_Uptodate, &bp->l_bh.b_state);
+	lock_buffer(&bp->l_bh);
+	set_bit(BH_Mapped, &bp->l_bh.b_state);
+	set_bit(BH_Req, &bp->l_bh.b_state);
+	bp->l_bh.b_rdev = bp->l_bh.b_dev;
+	bp->l_bh.b_rsector = bp->l_blkno << (log->l2bsize - 9);
+	generic_make_request(READ, &bp->l_bh);
+	run_task_queue(&tq_disk);
+
+	wait_event(bp->l_ioevent, (bp->l_flag != lbmREAD));
+
+	return 0;
+}
+
+
+/*
+ *	lbmWrite()
+ *
+ * buffer at head of pageout queue stays after completion of
+ * partial-page pageout and redriven by explicit initiation of
+ * pageout by caller until full-page pageout is completed and
+ * released.
+ *
+ * device driver i/o done redrives pageout of new buffer at
+ * head of pageout queue when current buffer at head of pageout
+ * queue is released at the completion of its full-page pageout.
+ *
+ * LOGGC_LOCK() serializes lbmWrite() by lmNextPage() and lmGroupCommit().
+ * LCACHE_LOCK() serializes xflag between lbmWrite() and lbmIODone()
+ */
+static void lbmWrite(log_t * log, lbuf_t * bp, int flag, int cant_block)
+{
+	lbuf_t *tail;
+	unsigned long flags;
+
+	jFYI(1, ("lbmWrite: bp:0x%p flag:0x%x pn:0x%x\n",
+		 bp, flag, bp->l_pn));
+
+	/* map the logical block address to physical block address */
+	bp->l_blkno =
+	    log->base + (bp->l_pn << (L2LOGPSIZE - log->l2bsize));
+
+	LCACHE_LOCK(flags);		/* disable+lock */
+
+	/*
+	 * initialize buffer for device driver
+	 */
+	bp->l_flag = flag;
+
+	/*
+	 *      insert bp at tail of write queue associated with log
+	 *
+	 * (request is either for bp already/currently at head of queue
+	 * or new bp to be inserted at tail)
+	 */
+	tail = log->wqueue;
+
+	/* is buffer not already on write queue ? */
+	if (bp->l_wqnext == NULL) {
+		/* insert at tail of wqueue */
+		if (tail == NULL) {
+			log->wqueue = bp;
+			bp->l_wqnext = bp;
+		} else {
+			log->wqueue = bp;
+			bp->l_wqnext = tail->l_wqnext;
+			tail->l_wqnext = bp;
+		}
+
+		tail = bp;
+	}
+
+	/* is buffer at head of wqueue and for write ? */
+	if ((bp != tail->l_wqnext) || !(flag & lbmWRITE)) {
+		LCACHE_UNLOCK(flags);	/* unlock+enable */
+		return;
+	}
+
+	LCACHE_UNLOCK(flags);	/* unlock+enable */
+
+	if (cant_block)
+		lbmRedrive(bp);
+	else if (flag & lbmSYNC)
+		lbmStartIO(bp);
+	else {
+		LOGGC_UNLOCK(log);
+		lbmStartIO(bp);
+		LOGGC_LOCK(log);
+	}
+}
+
+
+/*
+ *	lbmDirectWrite()
+ *
+ * initiate pageout bypassing write queue for sidestream
+ * (e.g., log superblock) write;
+ */
+static void lbmDirectWrite(log_t * log, lbuf_t * bp, int flag)
+{
+	jEVENT(0, ("lbmDirectWrite: bp:0x%p flag:0x%x pn:0x%x\n",
+		   bp, flag, bp->l_pn));
+
+	/*
+	 * initialize buffer for device driver
+	 */
+	bp->l_flag = flag | lbmDIRECT;
+
+	/* map the logical block address to physical block address */
+	bp->l_blkno =
+	    log->base + (bp->l_pn << (L2LOGPSIZE - log->l2bsize));
+
+	/*
+	 *      initiate pageout of the page
+	 */
+	lbmStartIO(bp);
+}
+
+
+/*
+ * NAME:	lbmStartIO()
+ *
+ * FUNCTION:	Interface to DD strategy routine
+ *
+ * RETURN:      none
+ *
+ * serialization: LCACHE_LOCK() is NOT held during log i/o;
+ */
+void lbmStartIO(lbuf_t * bp)
+{
+	jFYI(1, ("lbmStartIO\n"));
+
+	bp->l_bh.b_reqnext = NULL;
+	set_bit(BH_Dirty, &bp->l_bh.b_state);
+//      lock_buffer(&bp->l_bh);
+	assert(!test_bit(BH_Lock, &bp->l_bh.b_state));
+	set_bit(BH_Lock, &bp->l_bh.b_state);
+
+	set_bit(BH_Mapped, &bp->l_bh.b_state);
+	set_bit(BH_Req, &bp->l_bh.b_state);
+	bp->l_bh.b_rdev = bp->l_bh.b_dev;
+	bp->l_bh.b_rsector = bp->l_blkno << (bp->l_log->l2bsize - 9);
+	generic_make_request(WRITE, &bp->l_bh);
+
+	INCREMENT(lmStat.submitted);
+	run_task_queue(&tq_disk);
+
+	jFYI(1, ("lbmStartIO done\n"));
+}
+
+
+/*
+ *	lbmIOWait()
+ */
+static int lbmIOWait(lbuf_t * bp, int flag)
+{
+	unsigned long flags;
+	int rc = 0;
+
+	jFYI(1,
+	     ("lbmIOWait1: bp:0x%p flag:0x%x:0x%x\n", bp, bp->l_flag,
+	      flag));
+
+	LCACHE_LOCK(flags);		/* disable+lock */
+
+	LCACHE_SLEEP_COND(bp->l_ioevent, (bp->l_flag & lbmDONE), flags);
+
+	rc = (bp->l_flag & lbmERROR) ? EIO : 0;
+
+	if (flag & lbmFREE)
+		lbmfree(bp);
+
+	LCACHE_UNLOCK(flags);	/* unlock+enable */
+
+	jFYI(1,
+	     ("lbmIOWait2: bp:0x%p flag:0x%x:0x%x\n", bp, bp->l_flag,
+	      flag));
+	return rc;
+}
+
+/*
+ *	lbmIODone()
+ *
+ * executed at INTIODONE level
+ */
+static void lbmIODone(struct buffer_head *bh, int uptodate)
+{
+	lbuf_t *bp = bh->b_private;
+	lbuf_t *nextbp, *tail;
+	log_t *log;
+	unsigned long flags;
+
+	/*
+	 * get back jfs buffer bound to the i/o buffer
+	 */
+	jEVENT(0, ("lbmIODone: bp:0x%p flag:0x%x\n", bp, bp->l_flag));
+
+	LCACHE_LOCK(flags);		/* disable+lock */
+
+	unlock_buffer(&bp->l_bh);
+	bp->l_flag |= lbmDONE;
+
+	if (!uptodate) {
+		bp->l_flag |= lbmERROR;
+
+		jERROR(1, ("lbmIODone: I/O error in JFS log\n"));
+	}
+
+	/*
+	 *      pagein completion
+	 */
+	if (bp->l_flag & lbmREAD) {
+		bp->l_flag &= ~lbmREAD;
+
+		LCACHE_UNLOCK(flags);	/* unlock+enable */
+
+		/* wakeup I/O initiator */
+		LCACHE_WAKEUP(&bp->l_ioevent);
+
+		return;
+	}
+
+	/*
+	 *      pageout completion
+	 *
+	 * the bp at the head of write queue has completed pageout.
+	 *
+	 * if single-commit/full-page pageout, remove the current buffer
+	 * from head of pageout queue, and redrive pageout with
+	 * the new buffer at head of pageout queue;
+	 * otherwise, the partial-page pageout buffer stays at
+	 * the head of pageout queue to be redriven for pageout
+	 * by lmGroupCommit() until full-page pageout is completed.
+	 */
+	bp->l_flag &= ~lbmWRITE;
+	INCREMENT(lmStat.pagedone);
+
+	/* update committed lsn */
+	log = bp->l_log;
+	log->clsn = (bp->l_pn << L2LOGPSIZE) + bp->l_ceor;
+
+	if (bp->l_flag & lbmDIRECT) {
+		LCACHE_WAKEUP(&bp->l_ioevent);
+		LCACHE_UNLOCK(flags);
+		return;
+	}
+
+	tail = log->wqueue;
+
+	/* single element queue */
+	if (bp == tail) {
+		/* remove head buffer of full-page pageout
+		 * from log device write queue
+		 */
+		if (bp->l_flag & lbmRELEASE) {
+			log->wqueue = NULL;
+			bp->l_wqnext = NULL;
+		}
+	}
+	/* multi element queue */
+	else {
+		/* remove head buffer of full-page pageout
+		 * from log device write queue
+		 */
+		if (bp->l_flag & lbmRELEASE) {
+			nextbp = tail->l_wqnext = bp->l_wqnext;
+			bp->l_wqnext = NULL;
+
+			/*
+			 * redrive pageout of next page at head of write queue:
+			 * redrive next page without any bound tblk
+			 * (i.e., page w/o any COMMIT records), or
+			 * first page of new group commit which has been
+			 * queued after current page (subsequent pageout
+			 * is performed synchronously, except page without
+			 * any COMMITs) by lmGroupCommit() as indicated
+			 * by lbmWRITE flag;
+			 */
+			if (nextbp->l_flag & lbmWRITE) {
+				/*
+				 * We can't do the I/O at interrupt time.
+				 * The jfsIO thread can do it
+				 */
+				lbmRedrive(nextbp);
+			}
+		}
+	}
+
+	/*
+	 *      synchronous pageout:
+	 *
+	 * buffer has not necessarily been removed from write queue
+	 * (e.g., synchronous write of partial-page with COMMIT):
+	 * leave buffer for i/o initiator to dispose
+	 */
+	if (bp->l_flag & lbmSYNC) {
+		LCACHE_UNLOCK(flags);	/* unlock+enable */
+
+		/* wakeup I/O initiator */
+		LCACHE_WAKEUP(&bp->l_ioevent);
+	}
+
+	/*
+	 *      Group Commit pageout:
+	 */
+	else if (bp->l_flag & lbmGC) {
+		LCACHE_UNLOCK(flags);
+		lmPostGC(bp);
+	}
+
+	/*
+	 *      asynchronous pageout:
+	 *
+	 * buffer must have been removed from write queue:
+	 * insert buffer at head of freelist where it can be recycled
+	 */
+	else {
+		assert(bp->l_flag & lbmRELEASE);
+		assert(bp->l_flag & lbmFREE);
+		lbmfree(bp);
+
+		LCACHE_UNLOCK(flags);	/* unlock+enable */
+	}
+}
+
+int jfsIOWait(void *arg)
+{
+	lbuf_t *bp;
+
+	jFYI(1, ("jfsIOWait is here!\n"));
+
+	lock_kernel();
+
+	daemonize();
+	current->tty = NULL;
+	strcpy(current->comm, "jfsIO");
+
+	unlock_kernel();
+
+	jfsIOtask = current;
+
+	spin_lock_irq(&current->sigmask_lock);
+	siginitsetinv(&current->blocked,
+		      sigmask(SIGHUP) | sigmask(SIGKILL) | sigmask(SIGSTOP)
+		      | sigmask(SIGCONT));
+	spin_unlock_irq(&current->sigmask_lock);
+
+	complete(&jfsIOwait);
+
+	do {
+		spin_lock_irq(&log_redrive_lock);
+		while ((bp = log_redrive_list)) {
+			log_redrive_list = bp->l_redrive_next;
+			bp->l_redrive_next = NULL;
+			spin_unlock_irq(&log_redrive_lock);
+			lbmStartIO(bp);
+			spin_lock_irq(&log_redrive_lock);
+		}
+		spin_unlock_irq(&log_redrive_lock);
+
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	} while (!jfs_thread_stopped());
+
+	jFYI(1,("jfsIOWait being killed!\n"));
+	complete(&jfsIOwait);
+	return 0;
+}
+
+
+#ifdef _STILL_TO_PORT
+/*
+ * NAME:	lmLogFormat()/jfs_logform()
+ *
+ * FUNCTION:	format file system log (ref. jfs_logform()).
+ *
+ * PARAMETERS:
+ *	log	- log inode (with common mount inode base);
+ *	logAddress - start address of log space in FS block;
+ *	logSize	- length of log space in FS block;
+ *
+ * RETURN:	0 -	success
+ *		-1 -	i/o error
+ */
+int lmLogFormat(inode_t * ipmnt, s64 logAddress, int logSize)
+{
+	int rc = 0;
+	cbuf_t *bp;
+	logsuper_t *logsuper;
+	logpage_t *lp;
+	int lspn;		/* log sequence page number */
+	struct lrd *lrd_ptr;
+	int npbperpage, npages;
+
+	jFYI(0, ("lmLogFormat: logAddress:%Ld logSize:%d\n",
+		 logAddress, logSize));
+
+	/* allocate a JFS buffer */
+	bp = rawAllocate();
+
+	/* map the logical block address to physical block address */
+	bp->cm_blkno = logAddress << ipmnt->i_l2bfactor;
+
+	npbperpage = LOGPSIZE >> ipmnt->i_l2pbsize;
+	npages = logSize / (LOGPSIZE >> ipmnt->i_l2bsize);
+
+	/*
+	 *      log space:
+	 *
+	 * page 0 - reserved;
+	 * page 1 - log superblock;
+	 * page 2 - log data page: A SYNC log record is written
+	 *          into this page at logform time;
+	 * pages 3-N - log data page: set to empty log data pages;
+	 */
+	/*
+	 *      init log superblock: log page 1
+	 */
+	logsuper = (logsuper_t *) bp->cm_cdata;
+
+	logsuper->magic = cpu_to_le32(LOGMAGIC);
+	logsuper->version = cpu_to_le32(LOGVERSION);
+	logsuper->state = cpu_to_le32(LOGREDONE);
+	logsuper->flag = cpu_to_le32(ipmnt->i_mntflag);	/* ? */
+	logsuper->size = cpu_to_le32(npages);
+	logsuper->bsize = cpu_to_le32(ipmnt->i_bsize);
+	logsuper->l2bsize = cpu_to_le32(ipmnt->i_l2bsize);
+	logsuper->end =
+	    cpu_to_le32(2 * LOGPSIZE + LOGPHDRSIZE + LOGRDSIZE);
+
+	bp->cm_blkno += npbperpage;
+	rawWrite(ipmnt, bp, 0);
+
+	/*
+	 *      init pages 2 to npages-1 as log data pages:
+	 *
+	 * log page sequence number (lpsn) initialization:
+	 *
+	 * pn:   0     1     2     3                 n-1
+	 *       +-----+-----+=====+=====+===.....===+=====+
+	 * lspn:             N-1   0     1           N-2
+	 *                   <--- N page circular file ---->
+	 *
+	 * the N (= npages-2) data pages of the log is maintained as
+	 * a circular file for the log records;
+	 * lpsn grows by 1 monotonically as each log page is written
+	 * to the circular file of the log;
+	 * Since the AIX DUMMY log record is dropped for this XJFS,
+	 * and setLogpage() will not reset the page number even if
+	 * the eor is equal to LOGPHDRSIZE. In order for binary search
+	 * still work in find log end process, we have to simulate the
+	 * log wrap situation at the log format time.
+	 * The 1st log page written will have the highest lpsn. Then
+	 * the succeeding log pages will have ascending order of
+	 * the lspn starting from 0, ... (N-2)
+	 */
+	lp = (logpage_t *) bp->cm_cdata;
+
+	/*
+	 * initialize 1st log page to be written: lpsn = N - 1,
+	 * write a SYNCPT log record is written to this page
+	 */
+	lp->h.page = lp->t.page = cpu_to_le32(npages - 3);
+	lp->h.eor = lp->t.eor = cpu_to_le16(LOGPHDRSIZE + LOGRDSIZE);
+
+	lrd_ptr = (struct lrd *) &lp->data;
+	lrd_ptr->logtid = 0;
+	lrd_ptr->backchain = 0;
+	lrd_ptr->type = cpu_to_le16(LOG_SYNCPT);
+	lrd_ptr->length = 0;
+	lrd_ptr->log.syncpt.sync = 0;
+
+	bp->cm_blkno += npbperpage;
+	rawWrite(ipmnt, bp, 0);
+
+	/*
+	 *      initialize succeeding log pages: lpsn = 0, 1, ..., (N-2)
+	 */
+	for (lspn = 0; lspn < npages - 3; lspn++) {
+		lp->h.page = lp->t.page = cpu_to_le32(lspn);
+		lp->h.eor = lp->t.eor = cpu_to_le16(LOGPHDRSIZE);
+
+		bp->cm_blkno += npbperpage;
+		rawWrite(ipmnt, bp, 0);
+	}
+
+	/*
+	 *      finalize log
+	 */
+	/* release the buffer */
+	rawRelease(bp);
+
+	return rc;
+}
+#endif				/* _STILL_TO_PORT */
+
+
+#ifdef CONFIG_JFS_STATISTICS
+int jfs_lmstats_read(char *buffer, char **start, off_t offset, int length,
+		      int *eof, void *data)
+{
+	int len = 0;
+	off_t begin;
+
+	len += sprintf(buffer,
+		       "JFS Logmgr stats\n"
+		       "================\n"
+		       "commits = %d\n"
+		       "writes submitted = %d\n"
+		       "writes completed = %d\n",
+		       lmStat.commit,
+		       lmStat.submitted,
+		       lmStat.pagedone);
+
+	begin = offset;
+	*start = buffer + begin;
+	len -= begin;
+
+	if (len > length)
+		len = length;
+	else
+		*eof = 1;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+#endif /* CONFIG_JFS_STATISTICS */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_logmgr.h linuxppc64_2_4/fs/jfs/jfs_logmgr.h
--- linux-2.4.19/fs/jfs/jfs_logmgr.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_logmgr.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,502 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#ifndef	_H_JFS_LOGMGR
+#define _H_JFS_LOGMGR
+
+
+#include "jfs_filsys.h"
+#include "jfs_lock.h"
+
+/*
+ *	log manager configuration parameters
+ */
+
+/* log page size */
+#define	LOGPSIZE	4096
+#define	L2LOGPSIZE	12
+
+#define LOGPAGES	16	/* Log pages per mounted file system */
+
+/*
+ *	log logical volume
+ *
+ * a log is used to make the commit operation on journalled 
+ * files within the same logical volume group atomic.
+ * a log is implemented with a logical volume.
+ * there is one log per logical volume group. 
+ *
+ * block 0 of the log logical volume is not used (ipl etc).
+ * block 1 contains a log "superblock" and is used by logFormat(),
+ * lmLogInit(), lmLogShutdown(), and logRedo() to record status 
+ * of the log but is not otherwise used during normal processing. 
+ * blocks 2 - (N-1) are used to contain log records.
+ *
+ * when a volume group is varied-on-line, logRedo() must have 
+ * been executed before the file systems (logical volumes) in 
+ * the volume group can be mounted.
+ */
+/*
+ *	log superblock (block 1 of logical volume)
+ */
+#define	LOGSUPER_B	1
+#define	LOGSTART_B	2
+
+#define	LOGMAGIC	0x87654321
+#define	LOGVERSION	1
+
+#define MAX_ACTIVE	512	/* Max active file systems sharing log */
+
+typedef struct {
+	u32 magic;		/* 4: log lv identifier */
+	s32 version;		/* 4: version number */
+	s32 serial;		/* 4: log open/mount counter */
+	s32 size;		/* 4: size in number of LOGPSIZE blocks */
+	s32 bsize;		/* 4: logical block size in byte */
+	s32 l2bsize;		/* 4: log2 of bsize */
+
+	u32 flag;		/* 4: option */
+	u32 state;		/* 4: state - see below */
+
+	s32 end;		/* 4: addr of last log record set by logredo */
+	u32 device;		/* 4: save device in case location changes */
+	u32 active[MAX_ACTIVE];	/* 2048: active file systems list */
+} logsuper_t;
+
+/* log flag: commit option (see jfs_filsys.h) */
+
+/* log state */
+#define	LOGMOUNT	0	/* log mounted by lmLogInit() */
+#define LOGREDONE	1	/* log shutdown by lmLogShutdown().
+				 * log redo completed by logredo().
+				 */
+#define LOGWRAP		2	/* log wrapped */
+#define LOGREADERR	3	/* log read error detected in logredo() */
+
+
+/*
+ *	log logical page
+ *
+ * (this comment should be rewritten !)
+ * the header and trailer structures (h,t) will normally have 
+ * the same page and eor value.
+ * An exception to this occurs when a complete page write is not 
+ * accomplished on a power failure. Since the hardware may "split write"
+ * sectors in the page, any out of order sequence may occur during powerfail 
+ * and needs to be recognized during log replay.  The xor value is
+ * an "exclusive or" of all log words in the page up to eor.  This
+ * 32 bit eor is stored with the top 16 bits in the header and the
+ * bottom 16 bits in the trailer.  logredo can easily recognize pages
+ * that were not completed by reconstructing this eor and checking 
+ * the log page.
+ *
+ * Previous versions of the operating system did not allow split 
+ * writes and detected partially written records in logredo by 
+ * ordering the updates to the header, trailer, and the move of data 
+ * into the logdata area.  The order: (1) data is moved (2) header 
+ * is updated (3) trailer is updated.  In logredo, when the header 
+ * differed from the trailer, the header and trailer were reconciled 
+ * as follows: if h.page != t.page they were set to the smaller of 
+ * the two and h.eor and t.eor set to 8 (i.e. empty page). if (only) 
+ * h.eor != t.eor they were set to the smaller of their two values.
+ */
+typedef struct {
+	struct {		/* header */
+		s32 page;	/* 4: log sequence page number */
+		s16 rsrvd;	/* 2: */
+		s16 eor;	/* 2: end-of-log offset of lasrt record write */
+	} h;
+
+	s32 data[LOGPSIZE / 4 - 4];	/* log record area */
+
+	struct {		/* trailer */
+		s32 page;	/* 4: normally the same as h.page */
+		s16 rsrvd;	/* 2: */
+		s16 eor;	/* 2: normally the same as h.eor */
+	} t;
+} logpage_t;
+
+#define LOGPHDRSIZE	8	/* log page header size */
+#define LOGPTLRSIZE	8	/* log page trailer size */
+
+
+/*
+ *	log record
+ *
+ * (this comment should be rewritten !)
+ * jfs uses only "after" log records (only a single writer is allowed
+ * in a  page, pages are written to temporary paging space if
+ * if they must be written to disk before commit, and i/o is
+ * scheduled for modified pages to their home location after
+ * the log records containing the after values and the commit 
+ * record is written to the log on disk, undo discards the copy
+ * in main-memory.)
+ *
+ * a log record consists of a data area of variable length followed by 
+ * a descriptor of fixed size LOGRDSIZE bytes.
+ * the  data area is rounded up to an integral number of 4-bytes and 
+ * must be no longer than LOGPSIZE.
+ * the descriptor is of size of multiple of 4-bytes and aligned on a 
+ * 4-byte boundary. 
+ * records are packed one after the other in the data area of log pages.
+ * (sometimes a DUMMY record is inserted so that at least one record ends 
+ * on every page or the longest record is placed on at most two pages).
+ * the field eor in page header/trailer points to the byte following 
+ * the last record on a page.
+ */
+
+/* log record types */
+#define LOG_COMMIT		0x8000
+#define LOG_SYNCPT		0x4000
+#define LOG_MOUNT		0x2000
+#define LOG_REDOPAGE		0x0800
+#define LOG_NOREDOPAGE		0x0080
+#define LOG_NOREDOINOEXT	0x0040
+#define LOG_UPDATEMAP		0x0008
+#define LOG_NOREDOFILE		0x0001
+
+/* REDOPAGE/NOREDOPAGE log record data type */
+#define	LOG_INODE		0x0001
+#define	LOG_XTREE		0x0002
+#define	LOG_DTREE		0x0004
+#define	LOG_BTROOT		0x0010
+#define	LOG_EA			0x0020
+#define	LOG_ACL			0x0040
+#define	LOG_DATA		0x0080
+#define	LOG_NEW			0x0100
+#define	LOG_EXTEND		0x0200
+#define LOG_RELOCATE		0x0400
+#define LOG_DIR_XTREE		0x0800	/* Xtree is in directory inode */
+
+/* UPDATEMAP log record descriptor type */
+#define	LOG_ALLOCXADLIST	0x0080
+#define	LOG_ALLOCPXDLIST	0x0040
+#define	LOG_ALLOCXAD		0x0020
+#define	LOG_ALLOCPXD		0x0010
+#define	LOG_FREEXADLIST		0x0008
+#define	LOG_FREEPXDLIST		0x0004
+#define	LOG_FREEXAD		0x0002
+#define	LOG_FREEPXD		0x0001
+
+
+typedef struct lrd {
+	/*
+	 * type independent area
+	 */
+	s32 logtid;		/* 4: log transaction identifier */
+	s32 backchain;		/* 4: ptr to prev record of same transaction */
+	u16 type;		/* 2: record type */
+	s16 length;		/* 2: length of data in record (in byte) */
+	u32 aggregate;		/* 4: file system lv/aggregate */
+	/* (16) */
+
+	/*
+	 * type dependent area (20)
+	 */
+	union {
+
+		/*
+		 *      COMMIT: commit
+		 *
+		 * transaction commit: no type-dependent information;
+		 */
+
+		/*
+		 *      REDOPAGE: after-image
+		 *
+		 * apply after-image;
+		 *
+		 * N.B. REDOPAGE, NOREDOPAGE, and UPDATEMAP must be same format;
+		 */
+		struct {
+			u32 fileset;	/* 4: fileset number */
+			u32 inode;	/* 4: inode number */
+			u16 type;	/* 2: REDOPAGE record type */
+			s16 l2linesize;	/* 2: log2 of line size */
+			pxd_t pxd;	/* 8: on-disk page pxd */
+		} redopage;	/* (20) */
+
+		/*
+		 *      NOREDOPAGE: the page is freed
+		 *
+		 * do not apply after-image records which precede this record
+		 * in the log with the same page block number to this page.
+		 *
+		 * N.B. REDOPAGE, NOREDOPAGE, and UPDATEMAP must be same format;
+		 */
+		struct {
+			s32 fileset;	/* 4: fileset number */
+			u32 inode;	/* 4: inode number */
+			u16 type;	/* 2: NOREDOPAGE record type */
+			s16 rsrvd;	/* 2: reserved */
+			pxd_t pxd;	/* 8: on-disk page pxd */
+		} noredopage;	/* (20) */
+
+		/*
+		 *      UPDATEMAP: update block allocation map
+		 *
+		 * either in-line PXD,
+		 * or     out-of-line  XADLIST;
+		 *
+		 * N.B. REDOPAGE, NOREDOPAGE, and UPDATEMAP must be same format;
+		 */
+		struct {
+			u32 fileset;	/* 4: fileset number */
+			u32 inode;	/* 4: inode number */
+			u16 type;	/* 2: UPDATEMAP record type */
+			s16 nxd;	/* 2: number of extents */
+			pxd_t pxd;	/* 8: pxd */
+		} updatemap;	/* (20) */
+
+		/*
+		 *      NOREDOINOEXT: the inode extent is freed
+		 *
+		 * do not apply after-image records which precede this 
+		 * record in the log with the any of the 4 page block 
+		 * numbers in this inode extent. 
+		 * 
+		 * NOTE: The fileset and pxd fields MUST remain in 
+		 *       the same fields in the REDOPAGE record format.
+		 *
+		 */
+		struct {
+			s32 fileset;	/* 4: fileset number */
+			s32 iagnum;	/* 4: IAG number     */
+			s32 inoext_idx;	/* 4: inode extent index */
+			pxd_t pxd;	/* 8: on-disk page pxd */
+		} noredoinoext;	/* (20) */
+
+		/*
+		 *      SYNCPT: log sync point
+		 *
+		 * replay log upto syncpt address specified;
+		 */
+		struct {
+			s32 sync;	/* 4: syncpt address (0 = here) */
+		} syncpt;
+
+		/*
+		 *      MOUNT: file system mount
+		 *
+		 * file system mount: no type-dependent information;
+		 */
+
+		/*
+		 *      ? FREEXTENT: free specified extent(s)
+		 *
+		 * free specified extent(s) from block allocation map
+		 * N.B.: nextents should be length of data/sizeof(xad_t)
+		 */
+		struct {
+			s32 type;	/* 4: FREEXTENT record type */
+			s32 nextent;	/* 4: number of extents */
+
+			/* data: PXD or XAD list */
+		} freextent;
+
+		/*
+		 *      ? NOREDOFILE: this file is freed
+		 *
+		 * do not apply records which precede this record in the log
+		 * with the same inode number.
+		 *
+		 * NOREDILE must be the first to be written at commit
+		 * (last to be read in logredo()) - it prevents
+		 * replay of preceding updates of all preceding generations
+		 * of the inumber esp. the on-disk inode itself, 
+		 * but does NOT prevent
+		 * replay of the 
+		 */
+		struct {
+			s32 fileset;	/* 4: fileset number */
+			u32 inode;	/* 4: inode number */
+		} noredofile;
+
+		/*
+		 *      ? NEWPAGE: 
+		 *
+		 * metadata type dependent
+		 */
+		struct {
+			s32 fileset;	/* 4: fileset number */
+			u32 inode;	/* 4: inode number */
+			s32 type;	/* 4: NEWPAGE record type */
+			pxd_t pxd;	/* 8: on-disk page pxd */
+		} newpage;
+
+		/*
+		 *      ? DUMMY: filler
+		 *
+		 * no type-dependent information
+		 */
+	} log;
+} lrd_t;			/* (36) */
+
+#define	LOGRDSIZE	(sizeof(struct lrd))
+
+/*
+ *	line vector descriptor
+ */
+typedef struct {
+	s16 offset;
+	s16 length;
+} lvd_t;
+
+
+/*
+ *	log logical volume
+ */
+typedef struct jfs_log {
+
+	struct super_block *sb;	/* 4: This is used to sync metadata
+				 *    before writing syncpt.  Will
+				 *    need to be a list if we share
+				 *    the log between fs's
+				 */
+	kdev_t dev;		/* 4: log lv number */
+	struct block_device *bdev; /* 4: log lv pointer */
+	s32 serial;		/* 4: log mount serial number */
+
+	s64 base;		/* @8: log extent address (inline log ) */
+	int size;		/* 4: log size in log page (in page) */
+	int l2bsize;		/* 4: log2 of bsize */
+
+	uint flag;		/* 4: flag */
+	uint state;		/* 4: state */
+
+	struct lbuf *lbuf_free;	/* 4: free lbufs */
+	wait_queue_head_t free_wait;	/* 4: */
+
+	/* log write */
+	int logtid;		/* 4: log tid */
+	int page;		/* 4: page number of eol page */
+	int eor;		/* 4: eor of last record in eol page */
+	struct lbuf *bp;	/* 4: current log page buffer */
+
+	struct semaphore loglock;	/* 4: log write serialization lock */
+
+	/* syncpt */
+	int nextsync;		/* 4: bytes to write before next syncpt */
+	int active;		/* 4: */
+	int syncbarrier;	/* 4: */
+	wait_queue_head_t syncwait;	/* 4: */
+
+	/* commit */
+	uint cflag;		/* 4: */
+	struct {		/* 8: FIFO commit queue header */
+		struct tblock *head;
+		struct tblock *tail;
+	} cqueue;
+	int gcrtc;		/* 4: GC_READY transaction count */
+	struct tblock *gclrt;	/* 4: latest GC_READY transaction */
+	spinlock_t gclock;	/* 4: group commit lock */
+	int logsize;		/* 4: log data area size in byte */
+	int lsn;		/* 4: end-of-log */
+	int clsn;		/* 4: clsn */
+	int syncpt;		/* 4: addr of last syncpt record */
+	int sync;		/* 4: addr from last logsync() */
+	struct list_head synclist;	/* 8: logsynclist anchor */
+	spinlock_t synclock;	/* 4: synclist lock */
+	struct lbuf *wqueue;	/* 4: log pageout queue */
+	int count;		/* 4: count */
+} log_t;
+
+/*
+ * group commit flag
+ */
+/* log_t */
+#define logGC_PAGEOUT	0x00000001
+
+/* tblock_t/lbuf_t */
+#define tblkGC_QUEUE		0x0001
+#define tblkGC_READY		0x0002
+#define tblkGC_COMMIT		0x0004
+#define tblkGC_COMMITTED	0x0008
+#define tblkGC_EOP		0x0010
+#define tblkGC_FREE		0x0020
+#define tblkGC_LEADER		0x0040
+#define tblkGC_ERROR		0x0080
+#define tblkGC_LAZY		0x0100	// D230860
+#define tblkGC_UNLOCKED		0x0200	// D230860
+
+/*
+ *		log cache buffer header
+ */
+typedef struct lbuf {
+	struct buffer_head l_bh;	/* for doing I/O */
+	log_t *l_log;		/* 4: log associated with buffer */
+
+	/*
+	 * data buffer base area
+	 */
+	uint l_flag;		/* 4: pageout control flags */
+
+	struct lbuf *l_wqnext;	/* 4: write queue link */
+	struct lbuf *l_freelist;	/* 4: freelistlink */
+
+	int l_pn;		/* 4: log page number */
+	int l_eor;		/* 4: log record eor */
+	int l_ceor;		/* 4: committed log record eor */
+
+	s64 l_blkno;		/* 8: log page block number */
+	caddr_t l_ldata;	/* 4: data page */
+
+	wait_queue_head_t l_ioevent;	/* 4: i/o done event */
+	struct page *l_page;	/* The page itself */
+} lbuf_t;
+
+/* Reuse l_freelist for redrive list */
+#define l_redrive_next l_freelist
+
+/*
+ *	logsynclist block
+ *
+ * common logsyncblk prefix for jbuf_t and tblock_t
+ */
+typedef struct logsyncblk {
+	u16 xflag;		/* flags */
+	u16 flag;		/* only meaninful in tblock_t */
+	lid_t lid;		/* lock id */
+	s32 lsn;		/* log sequence number */
+	struct list_head synclist;	/* log sync list link */
+} logsyncblk_t;
+
+/*
+ *	logsynclist serialization (per log)
+ */
+
+#define LOGSYNC_LOCK_INIT(log) spin_lock_init(&(log)->synclock)
+#define LOGSYNC_LOCK(log) spin_lock(&(log)->synclock)
+#define LOGSYNC_UNLOCK(log) spin_unlock(&(log)->synclock)
+
+/* compute the difference in bytes of lsn from sync point */
+#define logdiff(diff, lsn, log)\
+{\
+	diff = (lsn) - (log)->syncpt;\
+	if (diff < 0)\
+		diff += (log)->logsize;\
+}
+
+extern int lmLogOpen(struct super_block *sb, log_t ** log);
+extern int lmLogClose(struct super_block *sb, log_t * log);
+extern int lmLogSync(log_t * log, int nosyncwait);
+extern int lmLogQuiesce(log_t * log);
+extern int lmLogResume(log_t * log, struct super_block *sb);
+extern int lmLogFormat(struct super_block *sb, s64 logAddress, int logSize);
+
+#endif				/* _H_JFS_LOGMGR */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_metapage.c linuxppc64_2_4/fs/jfs/jfs_metapage.c
--- linux-2.4.19/fs/jfs/jfs_metapage.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_metapage.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,688 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Module: jfs/jfs_metapage.c
+ *
+ */
+
+#include <linux/fs.h>
+#include <linux/init.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_txnmgr.h"
+#include "jfs_debug.h"
+
+extern struct task_struct *jfsCommitTask;
+static unsigned int metapages = 1024;	/* ??? Need a better number */
+static unsigned int free_metapages;
+static metapage_t *metapage_buf;
+static unsigned long meta_order;
+static metapage_t *meta_free_list = NULL;
+static spinlock_t meta_lock = SPIN_LOCK_UNLOCKED;
+static wait_queue_head_t meta_wait;
+
+#ifdef CONFIG_JFS_STATISTICS
+struct {
+	uint	pagealloc;	/* # of page allocations */
+	uint	pagefree;	/* # of page frees */
+	uint	lockwait;	/* # of sleeping lock_metapage() calls */
+	uint	allocwait;	/* # of sleeping alloc_metapage() calls */
+} mpStat;
+#endif
+
+
+#define HASH_BITS 10		/* This makes hash_table 1 4K page */
+#define HASH_SIZE (1 << HASH_BITS)
+static metapage_t **hash_table = NULL;
+static unsigned long hash_order;
+
+
+static inline int metapage_locked(struct metapage *mp)
+{
+	return test_bit(META_locked, &mp->flag);
+}
+
+static inline int trylock_metapage(struct metapage *mp)
+{
+	return test_and_set_bit(META_locked, &mp->flag);
+}
+
+static inline void unlock_metapage(struct metapage *mp)
+{
+	clear_bit(META_locked, &mp->flag);
+	wake_up(&mp->wait);
+}
+
+static void __lock_metapage(struct metapage *mp)
+{
+	DECLARE_WAITQUEUE(wait, current);
+
+	INCREMENT(mpStat.lockwait);
+
+	add_wait_queue_exclusive(&mp->wait, &wait);
+	do {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		if (metapage_locked(mp)) {
+			spin_unlock(&meta_lock);
+			schedule();
+			spin_lock(&meta_lock);
+		}
+	} while (trylock_metapage(mp));
+	__set_current_state(TASK_RUNNING);
+	remove_wait_queue(&mp->wait, &wait);
+}
+
+/* needs meta_lock */
+static inline void lock_metapage(struct metapage *mp)
+{
+	if (trylock_metapage(mp))
+		__lock_metapage(mp);
+}
+
+/* We're currently re-evaluating the method we use to write metadata
+ * pages.  Currently, we have to make sure there no dirty buffer_heads
+ * hanging around after we free the metadata page, since the same
+ * physical disk blocks may be used in a different address space and we
+ * can't write old data over the good data.
+ *
+ * The best way to do this now is with block_invalidate_page.  However,
+ * this is only available in the newer kernels and is not exported
+ * to modules.  block_flushpage is the next best, but it too is not exported
+ * to modules.
+ *
+ * In a module, about the best we have is generic_buffer_fdatasync.  This
+ * synchronously writes any dirty buffers.  This is not optimal, but it will
+ * keep old dirty buffers from overwriting newer data.
+ */
+static inline void invalidate_page(metapage_t *mp)
+{
+#ifdef MODULE
+	generic_buffer_fdatasync(mp->mapping->host, mp->index, mp->index + 1);
+#else
+	lock_page(mp->page);
+	block_flushpage(mp->page, 0);
+	UnlockPage(mp->page);
+#endif
+}
+
+int __init metapage_init(void)
+{
+	int i;
+	metapage_t *last = NULL;
+	metapage_t *mp;
+
+	/*
+	 * Initialize wait queue
+	 */
+	init_waitqueue_head(&meta_wait);
+
+	/*
+	 * Allocate the metapage structures
+	 */
+	for (meta_order = 0;
+	     ((PAGE_SIZE << meta_order) / sizeof(metapage_t)) < metapages;
+	     meta_order++);
+	metapages = (PAGE_SIZE << meta_order) / sizeof(metapage_t);
+
+	jFYI(1, ("metapage_init: metapage size = %Zd, metapages = %d\n",
+		 sizeof(metapage_t), metapages));
+
+	metapage_buf =
+	    (metapage_t *) __get_free_pages(GFP_KERNEL, meta_order);
+	assert(metapage_buf);
+	memset(metapage_buf, 0, PAGE_SIZE << meta_order);
+
+	mp = metapage_buf;
+	for (i = 0; i < metapages; i++, mp++) {
+		mp->flag = 0;
+		set_bit(META_free, &mp->flag);
+		init_waitqueue_head(&mp->wait);
+		mp->hash_next = last;
+		last = mp;
+	}
+	meta_free_list = last;
+	free_metapages = metapages;
+
+	/*
+	 * Now the hash list
+	 */
+	for (hash_order = 0;
+	     ((PAGE_SIZE << hash_order) / sizeof(void *)) < HASH_SIZE;
+	     hash_order++);
+	hash_table =
+	    (metapage_t **) __get_free_pages(GFP_KERNEL, hash_order);
+	assert(hash_table);
+	memset(hash_table, 0, PAGE_SIZE << hash_order);
+
+	return 0;
+}
+
+void metapage_exit(void)
+{
+	free_pages((unsigned long) metapage_buf, meta_order);
+	free_pages((unsigned long) hash_table, hash_order);
+	metapage_buf = 0;	/* This is a signal to the jfsIOwait thread */
+}
+
+/*
+ * Get metapage structure from freelist
+ * 
+ * Caller holds meta_lock
+ */
+static metapage_t *alloc_metapage(int *dropped_lock)
+{
+	metapage_t *new;
+
+	*dropped_lock = FALSE;
+
+	/*
+	 * Reserve two metapages for the lazy commit thread.  Otherwise
+	 * we may deadlock with holders of metapages waiting for tlocks
+	 * that lazy thread should be freeing.
+	 */
+	if ((free_metapages < 3) && (current != jfsCommitTask)) {
+		INCREMENT(mpStat.allocwait);
+		*dropped_lock = TRUE;
+		__SLEEP_COND(meta_wait, (free_metapages > 2),
+			     spin_lock(&meta_lock), spin_unlock(&meta_lock));
+	}
+
+	assert(meta_free_list);
+
+	new = meta_free_list;
+	meta_free_list = new->hash_next;
+	free_metapages--;
+
+	return new;
+}
+
+/*
+ * Put metapage on freelist (holding meta_lock)
+ */
+static inline void __free_metapage(metapage_t * mp)
+{
+	mp->flag = 0;
+	set_bit(META_free, &mp->flag);
+	mp->hash_next = meta_free_list;
+	meta_free_list = mp;
+	free_metapages++;
+	wake_up(&meta_wait);
+}
+
+/*
+ * Put metapage on freelist (not holding meta_lock)
+ */
+static inline void free_metapage(metapage_t * mp)
+{
+	spin_lock(&meta_lock);
+	__free_metapage(mp);
+	spin_unlock(&meta_lock);
+}
+
+/*
+ * Basically same hash as in pagemap.h, but using our hash table
+ */
+static metapage_t **meta_hash(struct address_space *mapping,
+			      unsigned long index)
+{
+#define i (((unsigned long)mapping)/ \
+	   (sizeof(struct inode) & ~(sizeof(struct inode) -1 )))
+#define s(x) ((x) + ((x) >> HASH_BITS))
+	return hash_table + (s(i + index) & (HASH_SIZE - 1));
+#undef i
+#undef s
+}
+
+static metapage_t *search_hash(metapage_t ** hash_ptr,
+			       struct address_space *mapping,
+			       unsigned long index)
+{
+	metapage_t *ptr;
+
+	for (ptr = *hash_ptr; ptr; ptr = ptr->hash_next) {
+		if ((ptr->mapping == mapping) && (ptr->index == index))
+			return ptr;
+	}
+
+	return NULL;
+}
+
+static void add_to_hash(metapage_t * mp, metapage_t ** hash_ptr)
+{
+	if (*hash_ptr)
+		(*hash_ptr)->hash_prev = mp;
+
+	mp->hash_prev = NULL;
+	mp->hash_next = *hash_ptr;
+	*hash_ptr = mp;
+	list_add(&mp->inode_list, &JFS_IP(mp->mapping->host)->mp_list);
+}
+
+static void remove_from_hash(metapage_t * mp, metapage_t ** hash_ptr)
+{
+	list_del(&mp->inode_list);
+
+	if (mp->hash_prev)
+		mp->hash_prev->hash_next = mp->hash_next;
+	else {
+		assert(*hash_ptr == mp);
+		*hash_ptr = mp->hash_next;
+	}
+
+	if (mp->hash_next)
+		mp->hash_next->hash_prev = mp->hash_prev;
+}
+
+/*
+ * Direct address space operations
+ */
+
+static int direct_get_block(struct inode *ip, long lblock,
+			    struct buffer_head *bh_result, int create)
+{
+	bh_result->b_dev = ip->i_dev;
+	bh_result->b_blocknr = lblock;
+	if (create)
+		bh_result->b_state |= (1UL << BH_Mapped) | (1UL << BH_New);
+	else
+		bh_result->b_state |= (1UL << BH_Mapped);
+
+	return 0;
+}
+
+static int direct_writepage(struct page *page)
+{
+	return block_write_full_page(page, direct_get_block);
+}
+
+static int direct_readpage(struct file *fp, struct page *page)
+{
+	return block_read_full_page(page, direct_get_block);
+}
+
+static int direct_prepare_write(struct file *file, struct page *page,
+				unsigned from, unsigned to)
+{
+	return block_prepare_write(page, from, to, direct_get_block);
+}
+
+static int direct_bmap(struct address_space *mapping, long block)
+{
+	return generic_block_bmap(mapping, block, direct_get_block);
+}
+
+struct address_space_operations direct_aops = {
+	readpage:	direct_readpage,
+	writepage:	direct_writepage,
+	sync_page:	block_sync_page,
+	prepare_write:	direct_prepare_write,
+	commit_write:	generic_commit_write,
+	bmap:		direct_bmap,
+};
+
+metapage_t *__get_metapage(struct inode *inode,
+			   unsigned long lblock, unsigned int size,
+			   int absolute, unsigned long new)
+{
+	int dropped_lock;
+	metapage_t **hash_ptr;
+	int l2BlocksPerPage;
+	int l2bsize;
+	struct address_space *mapping;
+	metapage_t *mp;
+	unsigned long page_index;
+	unsigned long page_offset;
+
+	jFYI(1, ("__get_metapage: inode = 0x%p, lblock = 0x%lx\n",
+		 inode, lblock));
+
+	if (absolute)
+		mapping = JFS_SBI(inode->i_sb)->direct_mapping;
+	else
+		mapping = inode->i_mapping;
+
+	spin_lock(&meta_lock);
+
+	hash_ptr = meta_hash(mapping, lblock);
+
+	mp = search_hash(hash_ptr, mapping, lblock);
+	if (mp) {
+	      page_found:
+		if (test_bit(META_discard, &mp->flag)) {
+			assert(new);	/* It's okay to reuse a discarded
+					 * if we expect it to be empty
+					 */
+			clear_bit(META_discard, &mp->flag);
+		}
+		mp->count++;
+		jFYI(1, ("__get_metapage: found 0x%p, in hash\n", mp));
+		assert(mp->logical_size == size);
+		lock_metapage(mp);
+		spin_unlock(&meta_lock);
+	} else {
+		l2bsize = inode->i_sb->s_blocksize_bits;
+		l2BlocksPerPage = PAGE_CACHE_SHIFT - l2bsize;
+		page_index = lblock >> l2BlocksPerPage;
+		page_offset = (lblock - (page_index << l2BlocksPerPage)) <<
+		    l2bsize;
+		if ((page_offset + size) > PAGE_SIZE) {
+			spin_unlock(&meta_lock);
+			jERROR(1, ("MetaData crosses page boundary!!\n"));
+			return NULL;
+		}
+
+		mp = alloc_metapage(&dropped_lock);
+		if (dropped_lock) {
+			/* alloc_metapage blocked, we need to search the hash
+			 * again.  (The goto is ugly, maybe we'll clean this
+			 * up in the future.)
+			 */
+			metapage_t *mp2;
+			mp2 = search_hash(hash_ptr, mapping, lblock);
+			if (mp2) {
+				__free_metapage(mp);
+				mp = mp2;
+				goto page_found;
+			}
+		}
+		mp->flag = 0;
+		lock_metapage(mp);
+		if (absolute)
+			set_bit(META_absolute, &mp->flag);
+		mp->xflag = COMMIT_PAGE;
+		mp->count = 1;
+		atomic_set(&mp->nohomeok,0);
+		mp->mapping = mapping;
+		mp->index = lblock;
+		mp->page = 0;
+		mp->logical_size = size;
+		add_to_hash(mp, hash_ptr);
+		spin_unlock(&meta_lock);
+
+		if (new) {
+			jFYI(1,
+			     ("__get_metapage: Calling grab_cache_page\n"));
+			mp->page = grab_cache_page(mapping, page_index);
+			if (!mp->page) {
+				jERROR(1, ("grab_cache_page failed!\n"));
+				spin_lock(&meta_lock);
+				remove_from_hash(mp, hash_ptr);
+				__free_metapage(mp);
+				spin_unlock(&meta_lock);
+				return NULL;
+			} else
+				INCREMENT(mpStat.pagealloc);
+		} else {
+			jFYI(1,
+			     ("__get_metapage: Calling read_cache_page\n"));
+			mp->page =
+			    read_cache_page(mapping, lblock,
+					    (filler_t *) mapping->a_ops->
+					    readpage, NULL);
+			if (IS_ERR(mp->page)) {
+				jERROR(1, ("read_cache_page failed!\n"));
+				spin_lock(&meta_lock);
+				remove_from_hash(mp, hash_ptr);
+				__free_metapage(mp);
+				spin_unlock(&meta_lock);
+				return NULL;
+			} else
+				INCREMENT(mpStat.pagealloc);
+			lock_page(mp->page);
+		}
+		mp->data = (void *) (kmap(mp->page) + page_offset);
+	}
+	jFYI(1, ("__get_metapage: returning = 0x%p\n", mp));
+	return mp;
+}
+
+void hold_metapage(metapage_t * mp, int force)
+{
+	spin_lock(&meta_lock);
+
+	mp->count++;
+
+	if (force) {
+		ASSERT (!(test_bit(META_forced, &mp->flag)));
+		if (trylock_metapage(mp))
+			set_bit(META_forced, &mp->flag);
+	} else
+		lock_metapage(mp);
+
+	spin_unlock(&meta_lock);
+}
+
+static void __write_metapage(metapage_t * mp)
+{
+	struct inode *ip = (struct inode *) mp->mapping->host;
+	unsigned long page_index;
+	unsigned long page_offset;
+	int rc;
+	int l2bsize = ip->i_sb->s_blocksize_bits;
+	int l2BlocksPerPage = PAGE_CACHE_SHIFT - l2bsize;
+
+	jFYI(1, ("__write_metapage: mp = 0x%p\n", mp));
+
+	if (test_bit(META_discard, &mp->flag)) {
+		/*
+		 * This metadata is no longer valid
+		 */
+		clear_bit(META_dirty, &mp->flag);
+		return;
+	}
+
+	page_index = mp->page->index;
+	page_offset =
+	    (mp->index - (page_index << l2BlocksPerPage)) << l2bsize;
+
+	rc = mp->mapping->a_ops->prepare_write(NULL, mp->page, page_offset,
+					       page_offset +
+					       mp->logical_size);
+	if (rc) {
+		jERROR(1, ("prepare_write return %d!\n", rc));
+		ClearPageUptodate(mp->page);
+		kunmap(mp->page);
+		clear_bit(META_dirty, &mp->flag);
+		return;
+	}
+	rc = mp->mapping->a_ops->commit_write(NULL, mp->page, page_offset,
+					      page_offset +
+					      mp->logical_size);
+	if (rc) {
+		jERROR(1, ("commit_write returned %d\n", rc));
+	}
+
+	clear_bit(META_dirty, &mp->flag);
+
+	jFYI(1, ("__write_metapage done\n"));
+}
+
+void release_metapage(metapage_t * mp)
+{
+	log_t *log;
+	struct inode *ip;
+
+	jFYI(1,
+	     ("release_metapage: mp = 0x%p, flag = 0x%lx\n", mp,
+	      mp->flag));
+
+	spin_lock(&meta_lock);
+	if (test_bit(META_forced, &mp->flag)) {
+		clear_bit(META_forced, &mp->flag);
+		mp->count--;
+		spin_unlock(&meta_lock);
+		return;
+	}
+
+	ip = (struct inode *) mp->mapping->host;
+
+	assert(mp->count);
+	if (--mp->count || atomic_read(&mp->nohomeok)) {
+		unlock_metapage(mp);
+		spin_unlock(&meta_lock);
+	} else {
+		remove_from_hash(mp, meta_hash(mp->mapping, mp->index));
+		spin_unlock(&meta_lock);
+
+		if (mp->page) {
+			kunmap(mp->page);
+			mp->data = 0;
+			if (test_bit(META_dirty, &mp->flag))
+				__write_metapage(mp);
+			UnlockPage(mp->page);
+			if (test_bit(META_sync, &mp->flag)) {
+				sync_metapage(mp);
+				clear_bit(META_sync, &mp->flag);
+			}
+
+			if (test_bit(META_discard, &mp->flag))
+				invalidate_page(mp);
+
+			page_cache_release(mp->page);
+			INCREMENT(mpStat.pagefree);
+		}
+
+		if (mp->lsn) {
+			/*
+			 * Remove metapage from logsynclist.
+			 */
+			log = mp->log;
+			LOGSYNC_LOCK(log);
+			mp->log = 0;
+			mp->lsn = 0;
+			mp->clsn = 0;
+			log->count--;
+			list_del(&mp->synclist);
+			LOGSYNC_UNLOCK(log);
+		}
+
+		free_metapage(mp);
+	}
+	jFYI(1, ("release_metapage: done\n"));
+}
+
+void invalidate_metapages(struct inode *ip, unsigned long addr,
+			 unsigned long len)
+{
+	metapage_t **hash_ptr;
+	unsigned long lblock;
+	int l2BlocksPerPage = PAGE_CACHE_SHIFT - ip->i_sb->s_blocksize_bits;
+	struct address_space *mapping = ip->i_mapping;
+	metapage_t *mp;
+#ifndef MODULE
+	struct page *page;
+#endif
+
+	/*
+	 * First, mark metapages to discard.  They will eventually be
+	 * released, but should not be written.
+	 */
+	for (lblock = addr; lblock < addr + len;
+	     lblock += 1 << l2BlocksPerPage) {
+		hash_ptr = meta_hash(mapping, lblock);
+		spin_lock(&meta_lock);
+		mp = search_hash(hash_ptr, mapping, lblock);
+		if (mp) {
+			set_bit(META_discard, &mp->flag);
+			spin_unlock(&meta_lock);
+			/*
+			 * If in the metapage cache, we've got the page locked
+			 */
+#ifdef MODULE
+			UnlockPage(mp->page);
+			generic_buffer_fdatasync(mp->mapping->host, mp->index,
+						 mp->index+1);
+			lock_page(mp->page);
+#else
+			block_flushpage(mp->page, 0);
+#endif
+		} else {
+			spin_unlock(&meta_lock);
+#ifdef MODULE
+			generic_buffer_fdatasync(ip, lblock << l2BlocksPerPage,
+					(lblock + 1) << l2BlocksPerPage);
+#else
+			page = find_lock_page(mapping,
+					      lblock >> l2BlocksPerPage);
+			if (page) {
+				block_flushpage(page, 0);
+				UnlockPage(page);
+			}
+#endif
+		}
+	}
+}
+
+void invalidate_inode_metapages(struct inode *inode)
+{
+	struct list_head *ptr;
+	metapage_t *mp;
+
+	spin_lock(&meta_lock);
+	list_for_each(ptr, &JFS_IP(inode)->mp_list) {
+		mp = list_entry(ptr, metapage_t, inode_list);
+		clear_bit(META_dirty, &mp->flag);
+		set_bit(META_discard, &mp->flag);
+		kunmap(mp->page);
+		UnlockPage(mp->page);
+		page_cache_release(mp->page);
+		INCREMENT(mpStat.pagefree);
+		mp->data = 0;
+		mp->page = 0;
+	}
+	spin_unlock(&meta_lock);
+	truncate_inode_pages(inode->i_mapping, 0);
+}
+
+#ifdef CONFIG_JFS_STATISTICS
+int jfs_mpstat_read(char *buffer, char **start, off_t offset, int length,
+		    int *eof, void *data)
+{
+	int len = 0;
+	off_t begin;
+
+	len += sprintf(buffer,
+		       "JFS Metapage statistics\n"
+		       "=======================\n"
+		       "metapages in use = %d\n"
+		       "page allocations = %d\n"
+		       "page frees = %d\n"
+		       "lock waits = %d\n"
+		       "allocation waits = %d\n",
+		       metapages - free_metapages,
+		       mpStat.pagealloc,
+		       mpStat.pagefree,
+		       mpStat.lockwait,
+		       mpStat.allocwait);
+
+	begin = offset;
+	*start = buffer + begin;
+	len -= begin;
+
+	if (len > length)
+		len = length;
+	else
+		*eof = 1;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_metapage.h linuxppc64_2_4/fs/jfs/jfs_metapage.h
--- linux-2.4.19/fs/jfs/jfs_metapage.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_metapage.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,123 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#ifndef	_H_JFS_METAPAGE
+#define _H_JFS_METAPAGE
+
+#include <linux/pagemap.h>
+
+typedef struct metapage {
+	/* Common logsyncblk prefix (see jfs_logmgr.h) */
+	u16 xflag;
+	u16 unused;
+	lid_t lid;
+	int lsn;
+	struct list_head synclist;
+	/* End of logsyncblk prefix */
+
+	unsigned long flag;	/* See Below */
+	unsigned long count;	/* Reference count */
+	void *data;		/* Data pointer */
+
+	/* list management stuff */
+	struct metapage *hash_prev;
+	struct metapage *hash_next;	/* Also used for free list */
+
+	struct list_head inode_list;	/* per-inode metapage list */
+	/*
+	 * mapping & index become redundant, but we need these here to
+	 * add the metapage to the hash before we have the real page
+	 */
+	struct address_space *mapping;
+	unsigned long index;
+	wait_queue_head_t wait;
+
+	/* implementation */
+	struct page *page;
+	unsigned long logical_size;
+
+	/* Journal management */
+	int clsn;
+	atomic_t nohomeok;
+	struct jfs_log *log;
+} metapage_t;
+
+/*
+ * Direct-access address space operations
+ */
+extern struct address_space_operations direct_aops;
+
+/* metapage flag */
+#define META_locked	0
+#define META_absolute	1
+#define META_free	2
+#define META_dirty	3
+#define META_sync	4
+#define META_discard	5
+#define META_forced	6
+
+#define mark_metapage_dirty(mp) set_bit(META_dirty, &(mp)->flag)
+
+/* function prototypes */
+extern metapage_t *__get_metapage(struct inode *inode,
+				  unsigned long lblock, unsigned int size,
+				  int absolute, unsigned long new);
+
+#define read_metapage(inode, lblock, size, absolute)\
+	 __get_metapage(inode, lblock, size, absolute, FALSE)
+
+#define get_metapage(inode, lblock, size, absolute)\
+	 __get_metapage(inode, lblock, size, absolute, TRUE)
+
+extern void release_metapage(metapage_t *);
+
+#define flush_metapage(mp) \
+{\
+	set_bit(META_dirty, &(mp)->flag);\
+	set_bit(META_sync, &(mp)->flag);\
+	release_metapage(mp);\
+}
+
+#define sync_metapage(mp) \
+	generic_buffer_fdatasync((struct inode *)mp->mapping->host,\
+				 mp->page->index, mp->page->index + 1)
+
+#define write_metapage(mp) \
+{\
+	set_bit(META_dirty, &(mp)->flag);\
+	release_metapage(mp);\
+}
+
+#define discard_metapage(mp) \
+{\
+	clear_bit(META_dirty, &(mp)->flag);\
+	set_bit(META_discard, &(mp)->flag);\
+	release_metapage(mp);\
+}
+
+extern void hold_metapage(metapage_t *, int);
+
+/*
+ * This routine uses hash to explicitly find small number of pages
+ */
+extern void invalidate_metapages(struct inode *, unsigned long, unsigned long);
+
+/*
+ * This one uses mp_list to invalidate all pages for an inode
+ */
+extern void invalidate_inode_metapages(struct inode *inode);
+#endif				/* _H_JFS_METAPAGE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_mount.c linuxppc64_2_4/fs/jfs/jfs_mount.c
--- linux-2.4.19/fs/jfs/jfs_mount.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_mount.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,525 @@
+/*
+ *   MODULE_NAME:		jfs_mount.c
+ *
+ *   COMPONENT_NAME:		sysjfs
+ *
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+/*
+ * Change History :
+ *
+ */
+
+/*
+ * Module: jfs_mount.c
+ *
+ * note: file system in transition to aggregate/fileset:
+ *
+ * file system mount is interpreted as the mount of aggregate, 
+ * if not already mounted, and mount of the single/only fileset in 
+ * the aggregate;
+ *
+ * a file system/aggregate is represented by an internal inode
+ * (aka mount inode) initialized with aggregate superblock;
+ * each vfs represents a fileset, and points to its "fileset inode 
+ * allocation map inode" (aka fileset inode):
+ * (an aggregate itself is structured recursively as a filset: 
+ * an internal vfs is constructed and points to its "fileset inode 
+ * allocation map inode" (aka aggregate inode) where each inode 
+ * represents a fileset inode) so that inode number is mapped to 
+ * on-disk inode in uniform way at both aggregate and fileset level;
+ *
+ * each vnode/inode of a fileset is linked to its vfs (to facilitate
+ * per fileset inode operations, e.g., unmount of a fileset, etc.);
+ * each inode points to the mount inode (to facilitate access to
+ * per aggregate information, e.g., block size, etc.) as well as
+ * its file set inode.
+ *
+ *   aggregate 
+ *   ipmnt
+ *   mntvfs -> fileset ipimap+ -> aggregate ipbmap -> aggregate ipaimap;
+ *             fileset vfs     -> vp(1) <-> ... <-> vp(n) <->vproot;
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_superblock.h"
+#include "jfs_dmap.h"
+#include "jfs_imap.h"
+#include "jfs_metapage.h"
+#include "jfs_debug.h"
+
+
+/*
+ * forward references
+ */
+static int chkSuper(struct super_block *);
+static int logMOUNT(struct super_block *sb);
+
+/*
+ * NAME:	jfs_mount(sb)
+ *
+ * FUNCTION:	vfs_mount()
+ *
+ * PARAMETER:	sb	- super block
+ *
+ * RETURN:	EBUSY	- device already mounted or open for write
+ *		EBUSY	- cvrdvp already mounted;
+ *		EBUSY	- mount table full
+ *		ENOTDIR	- cvrdvp not directory on a device mount
+ *		ENXIO	- device open failure
+ */
+int jfs_mount(struct super_block *sb)
+{
+	int rc = 0;		/* Return code          */
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	struct inode *ipaimap = NULL;
+	struct inode *ipaimap2 = NULL;
+	struct inode *ipimap = NULL;
+	struct inode *ipbmap = NULL;
+
+	jFYI(1, ("\nMount JFS\n"));
+
+	/*
+	 * read/validate superblock 
+	 * (initialize mount inode from the superblock)
+	 */
+	if ((rc = chkSuper(sb))) {
+		goto errout20;
+	}
+
+	ipaimap = diReadSpecial(sb, AGGREGATE_I);
+	if (ipaimap == NULL) {
+		jERROR(1, ("jfs_mount: Faild to read AGGREGATE_I\n"));
+		rc = EIO;
+		goto errout20;
+	}
+	sbi->ipaimap = ipaimap;
+
+	jFYI(1, ("jfs_mount: ipaimap:0x%p\n", ipaimap));
+
+	/*
+	 * initialize aggregate inode allocation map
+	 */
+	if ((rc = diMount(ipaimap))) {
+		jERROR(1,
+		       ("jfs_mount: diMount(ipaimap) failed w/rc = %d\n",
+			rc));
+		goto errout21;
+	}
+
+	/*
+	 * open aggregate block allocation map
+	 */
+	ipbmap = diReadSpecial(sb, BMAP_I);
+	if (ipbmap == NULL) {
+		rc = EIO;
+		goto errout22;
+	}
+
+	jFYI(1, ("jfs_mount: ipbmap:0x%p\n", ipbmap));
+
+	sbi->ipbmap = ipbmap;
+
+	/*
+	 * initialize aggregate block allocation map
+	 */
+	if ((rc = dbMount(ipbmap))) {
+		jERROR(1, ("jfs_mount: dbMount failed w/rc = %d\n", rc));
+		goto errout22;
+	}
+
+	/*
+	 * open the secondary aggregate inode allocation map
+	 *
+	 * This is a duplicate of the aggregate inode allocation map.
+	 *
+	 * hand craft a vfs in the same fashion as we did to read ipaimap.
+	 * By adding INOSPEREXT (32) to the inode number, we are telling
+	 * diReadSpecial that we are reading from the secondary aggregate
+	 * inode table.  This also creates a unique entry in the inode hash
+	 * table.
+	 */
+	if ((sbi->mntflag & JFS_BAD_SAIT) == 0) {
+		ipaimap2 = diReadSpecial(sb, AGGREGATE_I + INOSPEREXT);
+		if (ipaimap2 == 0) {
+			jERROR(1,
+			       ("jfs_mount: Faild to read AGGREGATE_I\n"));
+			rc = EIO;
+			goto errout35;
+		}
+		sbi->ipaimap2 = ipaimap2;
+
+		jFYI(1, ("jfs_mount: ipaimap2:0x%p\n", ipaimap2));
+
+		/*
+		 * initialize secondary aggregate inode allocation map
+		 */
+		if ((rc = diMount(ipaimap2))) {
+			jERROR(1,
+			       ("jfs_mount: diMount(ipaimap2) failed, rc = %d\n",
+				rc));
+			goto errout35;
+		}
+	} else
+		/* Secondary aggregate inode table is not valid */
+		sbi->ipaimap2 = 0;
+
+	/*
+	 *      mount (the only/single) fileset
+	 */
+	/*
+	 * open fileset inode allocation map (aka fileset inode)
+	 */
+	ipimap = diReadSpecial(sb, FILESYSTEM_I);
+	if (ipimap == NULL) {
+		jERROR(1, ("jfs_mount: Failed to read FILESYSTEM_I\n"));
+		/* open fileset secondary inode allocation map */
+		rc = EIO;
+		goto errout40;
+	}
+	jFYI(1, ("jfs_mount: ipimap:0x%p\n", ipimap));
+
+	/* map further access of per fileset inodes by the fileset inode */
+	sbi->ipimap = ipimap;
+
+	/* initialize fileset inode allocation map */
+	if ((rc = diMount(ipimap))) {
+		jERROR(1, ("jfs_mount: diMount failed w/rc = %d\n", rc));
+		goto errout41;
+	}
+
+	jFYI(1, ("Mount JFS Complete.\n"));
+	goto out;
+
+	/*
+	 *      unwind on error
+	 */
+//errout42: /* close fileset inode allocation map */
+	diUnmount(ipimap, 1);
+
+      errout41:		/* close fileset inode allocation map inode */
+	diFreeSpecial(ipimap);
+
+      errout40:		/* fileset closed */
+
+	/* close secondary aggregate inode allocation map */
+	if (ipaimap2) {
+		diUnmount(ipaimap2, 1);
+		diFreeSpecial(ipaimap2);
+	}
+
+      errout35:
+
+	/* close aggregate block allocation map */
+	dbUnmount(ipbmap, 1);
+	diFreeSpecial(ipbmap);
+
+      errout22:		/* close aggregate inode allocation map */
+
+	diUnmount(ipaimap, 1);
+
+      errout21:		/* close aggregate inodes */
+	diFreeSpecial(ipaimap);
+      errout20:		/* aggregate closed */
+
+      out:
+
+	if (rc) {
+		jERROR(1, ("Mount JFS Failure: %d\n", rc));
+	}
+	return rc;
+}
+
+/*
+ * NAME:	jfs_mount_rw(sb, remount)
+ *
+ * FUNCTION:	Completes read-write mount, or remounts read-only volume
+ *		as read-write
+ */
+int jfs_mount_rw(struct super_block *sb, int remount)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);  
+	log_t *log;
+	int rc;
+
+	/*
+	 * If we are re-mounting a previously read-only volume, we want to
+	 * re-read the inode and block maps, since fsck.jfs may have updated
+	 * them.
+	 */
+	if (remount) {
+		if (chkSuper(sb) || (sbi->state != FM_CLEAN))
+			return -EINVAL;
+
+		truncate_inode_pages(sbi->ipimap->i_mapping, 0);
+		truncate_inode_pages(sbi->ipbmap->i_mapping, 0);
+		diUnmount(sbi->ipimap, 1);
+		if ((rc = diMount(sbi->ipimap))) {
+			jERROR(1,("jfs_mount_rw: diMount failed!\n"));
+			return rc;
+		}
+
+		dbUnmount(sbi->ipbmap, 1);
+		if ((rc = dbMount(sbi->ipbmap))) {
+			jERROR(1,("jfs_mount_rw: dbMount failed!\n"));
+			return rc;
+		}
+	}
+
+	/*
+	 * open/initialize log
+	 */
+	if ((rc = lmLogOpen(sb, &log)))
+		return rc;
+
+	JFS_SBI(sb)->log = log;
+
+	/*
+	 * update file system superblock;
+	 */
+	if ((rc = updateSuper(sb, FM_MOUNT))) {
+		jERROR(1,
+		       ("jfs_mount: updateSuper failed w/rc = %d\n", rc));
+		lmLogClose(sb, log);
+		JFS_SBI(sb)->log = 0;
+		return rc;
+	}
+
+	/*
+	 * write MOUNT log record of the file system
+	 */
+	logMOUNT(sb);
+
+	return rc;
+}
+
+/*
+ *	chkSuper()
+ *
+ * validate the superblock of the file system to be mounted and 
+ * get the file system parameters.
+ *
+ * returns
+ *	0 with fragsize set if check successful
+ *	error code if not successful
+ */
+static int chkSuper(struct super_block *sb)
+{
+	int rc = 0;
+	metapage_t *mp;
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	struct jfs_superblock *j_sb;
+	int AIM_bytesize, AIT_bytesize;
+	int expected_AIM_bytesize, expected_AIT_bytesize;
+	s64 AIM_byte_addr, AIT_byte_addr, fsckwsp_addr;
+	s64 byte_addr_diff0, byte_addr_diff1;
+	s32 bsize;
+
+	if ((rc = readSuper(sb, &mp)))
+		return rc;
+	j_sb = (struct jfs_superblock *) (mp->data);
+
+	/*
+	 * validate superblock
+	 */
+	/* validate fs signature */
+	if (strncmp(j_sb->s_magic, JFS_MAGIC, 4) ||
+	    j_sb->s_version > cpu_to_le32(JFS_VERSION)) {
+		//rc = EFORMAT;
+		rc = EINVAL;
+		goto out;
+	}
+
+	bsize = le32_to_cpu(j_sb->s_bsize);
+#ifdef _JFS_4K
+	if (bsize != PSIZE) {
+		jERROR(1, ("Currently only 4K block size supported!\n"));
+		rc = EINVAL;
+		goto out;
+	}
+#endif				/* _JFS_4K */
+
+	jFYI(1, ("superblock: flag:0x%08x state:0x%08x size:0x%Lx\n",
+		 le32_to_cpu(j_sb->s_flag), le32_to_cpu(j_sb->s_state),
+		 (unsigned long long) le64_to_cpu(j_sb->s_size)));
+
+	/* validate the descriptors for Secondary AIM and AIT */
+	if ((j_sb->s_flag & cpu_to_le32(JFS_BAD_SAIT)) !=
+	    cpu_to_le32(JFS_BAD_SAIT)) {
+		expected_AIM_bytesize = 2 * PSIZE;
+		AIM_bytesize = lengthPXD(&(j_sb->s_aim2)) * bsize;
+		expected_AIT_bytesize = 4 * PSIZE;
+		AIT_bytesize = lengthPXD(&(j_sb->s_ait2)) * bsize;
+		AIM_byte_addr = addressPXD(&(j_sb->s_aim2)) * bsize;
+		AIT_byte_addr = addressPXD(&(j_sb->s_ait2)) * bsize;
+		byte_addr_diff0 = AIT_byte_addr - AIM_byte_addr;
+		fsckwsp_addr = addressPXD(&(j_sb->s_fsckpxd)) * bsize;
+		byte_addr_diff1 = fsckwsp_addr - AIT_byte_addr;
+		if ((AIM_bytesize != expected_AIM_bytesize) ||
+		    (AIT_bytesize != expected_AIT_bytesize) ||
+		    (byte_addr_diff0 != AIM_bytesize) ||
+		    (byte_addr_diff1 <= AIT_bytesize))
+			j_sb->s_flag |= cpu_to_le32(JFS_BAD_SAIT);
+	}
+
+	if ((j_sb->s_flag & cpu_to_le32(JFS_GROUPCOMMIT)) !=
+	    cpu_to_le32(JFS_GROUPCOMMIT))
+		j_sb->s_flag |= cpu_to_le32(JFS_GROUPCOMMIT);
+	jFYI(0, ("superblock: flag:0x%08x state:0x%08x size:0x%Lx\n",
+		 le32_to_cpu(j_sb->s_flag), le32_to_cpu(j_sb->s_state),
+		 (unsigned long long) le64_to_cpu(j_sb->s_size)));
+
+	/* validate fs state */
+	if (j_sb->s_state != cpu_to_le32(FM_CLEAN) &&
+	    !(sb->s_flags & MS_RDONLY)) {
+		jERROR(1,
+		       ("jfs_mount: Mount Failure: File System Dirty.\n"));
+		rc = EINVAL;
+		goto out;
+	}
+
+	sbi->state = le32_to_cpu(j_sb->s_state);
+	sbi->mntflag = le32_to_cpu(j_sb->s_flag);
+
+	/*
+	 * JFS always does I/O by 4K pages.  Don't tell the buffer cache
+	 * that we use anything else (leave s_blocksize alone).
+	 */
+	sbi->bsize = bsize;
+	sbi->l2bsize = le16_to_cpu(j_sb->s_l2bsize);
+
+	/*
+	 * For now, ignore s_pbsize, l2bfactor.  All I/O going through buffer
+	 * cache.
+	 */
+	sbi->nbperpage = PSIZE >> sbi->l2bsize;
+	sbi->l2nbperpage = L2PSIZE - sbi->l2bsize;
+	sbi->l2niperblk = sbi->l2bsize - L2DISIZE;
+	if (sbi->mntflag & JFS_INLINELOG)
+		sbi->logpxd = j_sb->s_logpxd;
+	else
+		sbi->logdev = to_kdev_t(le32_to_cpu(j_sb->s_logdev));
+	sbi->ait2 = j_sb->s_ait2;
+
+      out:
+	release_metapage(mp);
+
+	return rc;
+}
+
+
+/*
+ *	updateSuper()
+ *
+ * update synchronously superblock if it is mounted read-write.
+ */
+int updateSuper(struct super_block *sb, uint state)
+{
+	int rc;
+	metapage_t *mp;
+	struct jfs_superblock *j_sb;
+
+	/*
+	 * Only fsck can fix dirty state
+	 */
+	if (JFS_SBI(sb)->state == FM_DIRTY)
+		return 0;
+
+	if ((rc = readSuper(sb, &mp)))
+		return rc;
+
+	j_sb = (struct jfs_superblock *) (mp->data);
+
+	j_sb->s_state = cpu_to_le32(state);
+	JFS_SBI(sb)->state = state;
+
+	if (state == FM_MOUNT) {
+		/* record log's dev_t and mount serial number */
+		j_sb->s_logdev =
+			cpu_to_le32(kdev_t_to_nr(JFS_SBI(sb)->log->dev));
+		j_sb->s_logserial = cpu_to_le32(JFS_SBI(sb)->log->serial);
+		/* record our own device number in case the location
+		 * changes after a reboot
+		 */
+		j_sb->s_device = cpu_to_le32(kdev_t_to_nr(sb->s_dev));
+	} else if (state == FM_CLEAN) {
+		/*
+		 * If this volume is shared with OS/2, OS/2 will need to
+		 * recalculate DASD usage, since we don't deal with it.
+		 */
+		if (j_sb->s_flag & cpu_to_le32(JFS_DASD_ENABLED))
+			j_sb->s_flag |= cpu_to_le32(JFS_DASD_PRIME);
+	}
+
+	flush_metapage(mp);
+
+	return 0;
+}
+
+
+/*
+ *	readSuper()
+ *
+ * read superblock by raw sector address
+ */
+int readSuper(struct super_block *sb, metapage_t ** mpp)
+{
+	/* read in primary superblock */
+	*mpp = read_metapage(JFS_SBI(sb)->direct_inode,
+			     SUPER1_OFF >> sb->s_blocksize_bits, PSIZE, 1);
+	if (*mpp == NULL) {
+		/* read in secondary/replicated superblock */
+		*mpp = read_metapage(JFS_SBI(sb)->direct_inode,
+				     SUPER2_OFF >> sb->s_blocksize_bits,
+				     PSIZE, 1);
+	}
+	return *mpp ? 0 : 1;
+}
+
+
+/*
+ *	logMOUNT()
+ *
+ * function: write a MOUNT log record for file system.
+ *
+ * MOUNT record keeps logredo() from processing log records
+ * for this file system past this point in log.
+ * it is harmless if mount fails.
+ *
+ * note: MOUNT record is at aggregate level, not at fileset level, 
+ * since log records of previous mounts of a fileset
+ * (e.g., AFTER record of extent allocation) have to be processed 
+ * to update block allocation map at aggregate level.
+ */
+static int logMOUNT(struct super_block *sb)
+{
+	log_t *log = JFS_SBI(sb)->log;
+	lrd_t lrd;
+
+	lrd.logtid = 0;
+	lrd.backchain = 0;
+	lrd.type = cpu_to_le16(LOG_MOUNT);
+	lrd.length = 0;
+	lrd.aggregate = cpu_to_le32(kdev_t_to_nr(sb->s_dev));
+	lmLog(log, NULL, &lrd, NULL);
+
+	return 0;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_superblock.h linuxppc64_2_4/fs/jfs/jfs_superblock.h
--- linux-2.4.19/fs/jfs/jfs_superblock.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_superblock.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,115 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef	_H_JFS_SUPERBLOCK
+#define _H_JFS_SUPERBLOCK
+/*
+ *	jfs_superblock.h
+ */
+
+/*
+ * make the magic number something a human could read
+ */
+#define JFS_MAGIC 	"JFS1"	/* Magic word */
+
+#define JFS_VERSION	2	/* Version number: Version 2 */
+
+#define LV_NAME_SIZE	11	/* MUST BE 11 for OS/2 boot sector */
+
+/* 
+ *	aggregate superblock 
+ *
+ * The name superblock is too close to super_block, so the name has been
+ * changed to jfs_superblock.  The utilities are still using the old name.
+ */
+struct jfs_superblock {
+	char s_magic[4];	/* 4: magic number */
+	u32 s_version;		/* 4: version number */
+
+	s64 s_size;		/* 8: aggregate size in hardware/LVM blocks;
+				 * VFS: number of blocks
+				 */
+	s32 s_bsize;		/* 4: aggregate block size in bytes; 
+				 * VFS: fragment size
+				 */
+	s16 s_l2bsize;		/* 2: log2 of s_bsize */
+	s16 s_l2bfactor;	/* 2: log2(s_bsize/hardware block size) */
+	s32 s_pbsize;		/* 4: hardware/LVM block size in bytes */
+	s16 s_l2pbsize;		/* 2: log2 of s_pbsize */
+	s16 pad;		/* 2: padding necessary for alignment */
+
+	u32 s_agsize;		/* 4: allocation group size in aggr. blocks */
+
+	u32 s_flag;		/* 4: aggregate attributes:
+				 *    see jfs_filsys.h
+				 */
+	u32 s_state;		/* 4: mount/unmount/recovery state: 
+				 *    see jfs_filsys.h
+				 */
+	s32 s_compress;		/* 4: > 0 if data compression */
+
+	pxd_t s_ait2;		/* 8: first extent of secondary
+				 *    aggregate inode table
+				 */
+
+	pxd_t s_aim2;		/* 8: first extent of secondary
+				 *    aggregate inode map
+				 */
+	u32 s_logdev;		/* 4: device address of log */
+	s32 s_logserial;	/* 4: log serial number at aggregate mount */
+	pxd_t s_logpxd;		/* 8: inline log extent */
+
+	pxd_t s_fsckpxd;	/* 8: inline fsck work space extent */
+
+	struct timestruc_t s_time;	/* 8: time last updated */
+
+	s32 s_fsckloglen;	/* 4: Number of filesystem blocks reserved for
+				 *    the fsck service log.  
+				 *    N.B. These blocks are divided among the
+				 *         versions kept.  This is not a per
+				 *         version size.
+				 *    N.B. These blocks are included in the 
+				 *         length field of s_fsckpxd.
+				 */
+	s8 s_fscklog;		/* 1: which fsck service log is most recent
+				 *    0 => no service log data yet
+				 *    1 => the first one
+				 *    2 => the 2nd one
+				 */
+	char s_fpack[11];	/* 11: file system volume name 
+				 *     N.B. This must be 11 bytes to
+				 *          conform with the OS/2 BootSector
+				 *          requirements
+				 */
+
+	/* extendfs() parameter under s_state & FM_EXTENDFS */
+	s64 s_xsize;		/* 8: extendfs s_size */
+	pxd_t s_xfsckpxd;	/* 8: extendfs fsckpxd */
+	pxd_t s_xlogpxd;	/* 8: extendfs logpxd */
+	/* - 128 byte boundary - */
+
+	u32 s_device;		/* Store device in case location changes
+				 * between reboots
+				 */
+
+};
+
+extern int readSuper(struct super_block *, struct metapage **);
+extern int updateSuper(struct super_block *, uint);
+
+#endif /*_H_JFS_SUPERBLOCK */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_txnmgr.c linuxppc64_2_4/fs/jfs/jfs_txnmgr.c
--- linux-2.4.19/fs/jfs/jfs_txnmgr.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_txnmgr.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,3019 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+/*
+ *      jfs_txnmgr.c: transaction manager
+ *
+ * notes:
+ * transaction starts with txBegin() and ends with txCommit()
+ * or txAbort().
+ *
+ * tlock is acquired at the time of update;
+ * (obviate scan at commit time for xtree and dtree)
+ * tlock and mp points to each other;
+ * (no hashlist for mp -> tlock).
+ *
+ * special cases:
+ * tlock on in-memory inode:
+ * in-place tlock in the in-memory inode itself;
+ * converted to page lock by iWrite() at commit time.
+ *
+ * tlock during write()/mmap() under anonymous transaction (tid = 0):
+ * transferred (?) to transaction at commit time.
+ *
+ * use the page itself to update allocation maps
+ * (obviate intermediate replication of allocation/deallocation data)
+ * hold on to mp+lock thru update of maps
+ */
+
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include <linux/vmalloc.h>
+#include <linux/smp_lock.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_dinode.h"
+#include "jfs_imap.h"
+#include "jfs_dmap.h"
+#include "jfs_superblock.h"
+#include "jfs_debug.h"
+
+/*
+ *      transaction management structures
+ */
+static struct {
+	/* tblock */
+	int freetid;		/* 4: index of a free tid structure */
+	wait_queue_head_t freewait;	/* 4: eventlist of free tblock */
+
+	/* tlock */
+	int freelock;		/* 4: index first free lock word */
+	wait_queue_head_t freelockwait;	/* 4: eventlist of free tlock */
+	wait_queue_head_t lowlockwait;	/* 4: eventlist of ample tlocks */
+	int tlocksInUse;	/* 4: Number of tlocks in use */
+	spinlock_t LazyLock;	/* 4: synchronize sync_queue & unlock_queue */
+/*	tblock_t *sync_queue;	 * 4: Transactions waiting for data sync */
+	tblock_t *unlock_queue;	/* 4: Transactions waiting to be released */
+	tblock_t *unlock_tail;	/* 4: Tail of unlock_queue */
+	struct list_head anon_list;	/* inodes having anonymous txns */
+	struct list_head anon_list2;	/* inodes having anonymous txns
+					   that couldn't be sync'ed */
+} TxAnchor;
+
+static int nTxBlock = 512;	/* number of transaction blocks */
+struct tblock *TxBlock;	        /* transaction block table */
+
+static int nTxLock = 2048;	/* number of transaction locks */
+static int TxLockLWM = 2048*.4;	/* Low water mark for number of txLocks used */
+static int TxLockHWM = 2048*.8;	/* High water mark for number of txLocks used */
+struct tlock *TxLock;           /* transaction lock table */
+static int TlocksLow = 0;	/* Indicates low number of available tlocks */
+
+
+/*
+ *      transaction management lock
+ */
+static spinlock_t jfsTxnLock = SPIN_LOCK_UNLOCKED;
+
+#define TXN_LOCK()              spin_lock(&jfsTxnLock)
+#define TXN_UNLOCK()            spin_unlock(&jfsTxnLock)
+
+#define LAZY_LOCK_INIT()	spin_lock_init(&TxAnchor.LazyLock);
+#define LAZY_LOCK(flags)	spin_lock_irqsave(&TxAnchor.LazyLock, flags)
+#define LAZY_UNLOCK(flags) spin_unlock_irqrestore(&TxAnchor.LazyLock, flags)
+
+/*
+ * Retry logic exist outside these macros to protect from spurrious wakeups.
+ */
+static inline void TXN_SLEEP_DROP_LOCK(wait_queue_head_t * event)
+{
+	DECLARE_WAITQUEUE(wait, current);
+
+	add_wait_queue(event, &wait);
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	TXN_UNLOCK();
+	schedule();
+	current->state = TASK_RUNNING;
+	remove_wait_queue(event, &wait);
+}
+
+#define TXN_SLEEP(event)\
+{\
+	TXN_SLEEP_DROP_LOCK(event);\
+	TXN_LOCK();\
+}
+
+#define TXN_WAKEUP(event) wake_up_all(event)
+
+
+/*
+ *      statistics
+ */
+struct {
+	tid_t maxtid;		/* 4: biggest tid ever used */
+	lid_t maxlid;		/* 4: biggest lid ever used */
+	int ntid;		/* 4: # of transactions performed */
+	int nlid;		/* 4: # of tlocks acquired */
+	int waitlock;		/* 4: # of tlock wait */
+} stattx;
+
+
+/*
+ * external references
+ */
+extern int lmGroupCommit(log_t * log, tblock_t * tblk);
+extern void lmSync(log_t *);
+extern int readSuper(struct super_block *sb, metapage_t ** bpp);
+extern int jfs_commit_inode(struct inode *, int);
+extern int jfs_thread_stopped(void);
+
+extern struct task_struct *jfsCommitTask;
+extern struct completion jfsIOwait;
+extern struct task_struct *jfsSyncTask;
+
+/*
+ * forward references
+ */
+int diLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck,
+	  commit_t * cd);
+int dataLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+void dtLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+void inlineLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+void mapLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+void txAbortCommit(commit_t * cd, int exval);
+static void txAllocPMap(struct inode *ip, maplock_t * maplock,
+			tblock_t * tblk);
+void txForce(tblock_t * tblk);
+static int txLog(log_t * log, tblock_t * tblk, commit_t * cd);
+int txMoreLock(void);
+static void txUpdateMap(tblock_t * tblk);
+static void txRelease(tblock_t * tblk);
+void xtLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+static void LogSyncRelease(metapage_t * mp);
+
+/*
+ *              transaction block/lock management
+ *              ---------------------------------
+ */
+
+/*
+ * Get a transaction lock from the free list.  If the number in use is
+ * greater than the high water mark, wake up the sync daemon.  This should
+ * free some anonymous transaction locks.  (TXN_LOCK must be held.)
+ */
+static lid_t txLockAlloc(void)
+{
+	lid_t lid;
+
+	while (!(lid = TxAnchor.freelock))
+		TXN_SLEEP(&TxAnchor.freelockwait);
+	TxAnchor.freelock = TxLock[lid].next;
+	HIGHWATERMARK(stattx.maxlid, lid);
+	if ((++TxAnchor.tlocksInUse > TxLockHWM) && (TlocksLow == 0)) {
+		jEVENT(0,("txLockAlloc TlocksLow\n"));
+		TlocksLow = 1;
+	wake_up_process(jfsSyncTask);
+	}
+
+	return lid;
+}
+
+static void txLockFree(lid_t lid)
+{
+	TxLock[lid].next = TxAnchor.freelock;
+	TxAnchor.freelock = lid;
+	TxAnchor.tlocksInUse--;
+	if (TlocksLow && (TxAnchor.tlocksInUse < TxLockLWM)) {
+		jEVENT(0,("txLockFree TlocksLow no more\n"));
+		TlocksLow = 0;
+		TXN_WAKEUP(&TxAnchor.lowlockwait);
+	}
+	TXN_WAKEUP(&TxAnchor.freelockwait);
+}
+
+/*
+ * NAME:        txInit()
+ *
+ * FUNCTION:    initialize transaction management structures
+ *
+ * RETURN:
+ *
+ * serialization: single thread at jfs_init()
+ */
+int txInit(void)
+{
+	int k, size;
+
+	/*
+	 * initialize transaction block (tblock) table
+	 *
+	 * transaction id (tid) = tblock index
+	 * tid = 0 is reserved.
+	 */
+	size = sizeof(tblock_t) * nTxBlock;
+	TxBlock = (tblock_t *) vmalloc(size);
+	if (TxBlock == NULL)
+		return ENOMEM;
+
+	for (k = 1; k < nTxBlock - 1; k++) {
+		TxBlock[k].next = k + 1;
+		init_waitqueue_head(&TxBlock[k].gcwait);
+		init_waitqueue_head(&TxBlock[k].waitor);
+	}
+	TxBlock[k].next = 0;
+	init_waitqueue_head(&TxBlock[k].gcwait);
+	init_waitqueue_head(&TxBlock[k].waitor);
+
+	TxAnchor.freetid = 1;
+	init_waitqueue_head(&TxAnchor.freewait);
+
+	stattx.maxtid = 1;	/* statistics */
+
+	/*
+	 * initialize transaction lock (tlock) table
+	 *
+	 * transaction lock id = tlock index
+	 * tlock id = 0 is reserved.
+	 */
+	size = sizeof(tlock_t) * nTxLock;
+	TxLock = (tlock_t *) vmalloc(size);
+	if (TxLock == NULL) {
+		vfree(TxBlock);
+		return ENOMEM;
+	}
+
+	/* initialize tlock table */
+	for (k = 1; k < nTxLock - 1; k++)
+		TxLock[k].next = k + 1;
+	TxLock[k].next = 0;
+	init_waitqueue_head(&TxAnchor.freelockwait);
+	init_waitqueue_head(&TxAnchor.lowlockwait);
+
+	TxAnchor.freelock = 1;
+	TxAnchor.tlocksInUse = 0;
+	INIT_LIST_HEAD(&TxAnchor.anon_list);
+	INIT_LIST_HEAD(&TxAnchor.anon_list2);
+
+	stattx.maxlid = 1;	/* statistics */
+
+	return 0;
+}
+
+/*
+ * NAME:        txExit()
+ *
+ * FUNCTION:    clean up when module is unloaded
+ */
+void txExit(void)
+{
+	vfree(TxLock);
+	TxLock = 0;
+	vfree(TxBlock);
+	TxBlock = 0;
+}
+
+
+/*
+ * NAME:        txBegin()
+ *
+ * FUNCTION:    start a transaction.
+ *
+ * PARAMETER:   sb	- superblock
+ *              flag	- force for nested tx;
+ *
+ * RETURN:	tid	- transaction id
+ *
+ * note: flag force allows to start tx for nested tx
+ * to prevent deadlock on logsync barrier;
+ */
+tid_t txBegin(struct super_block *sb, int flag)
+{
+	tid_t t;
+	tblock_t *tblk;
+	log_t *log;
+
+	jFYI(1, ("txBegin: flag = 0x%x\n", flag));
+	log = (log_t *) JFS_SBI(sb)->log;
+
+	TXN_LOCK();
+
+      retry:
+	if (flag != COMMIT_FORCE) {
+		/*
+		 * synchronize with logsync barrier
+		 */
+		if (log->syncbarrier) {
+			TXN_SLEEP(&log->syncwait);
+			goto retry;
+		}
+	}
+	if (flag == 0) {
+		/*
+		 * Don't begin transaction if we're getting starved for tlocks
+		 * unless COMMIT_FORCE (imap changes) or COMMIT_INODE (which
+		 * may ultimately free tlocks)
+		 */
+		if (TlocksLow) {
+			TXN_SLEEP(&TxAnchor.lowlockwait);
+			goto retry;
+		}
+	}
+
+	/*
+	 * allocate transaction id/block
+	 */
+	if ((t = TxAnchor.freetid) == 0) {
+		jFYI(1, ("txBegin: waiting for free tid\n"));
+		TXN_SLEEP(&TxAnchor.freewait);
+		goto retry;
+	}
+
+	tblk = tid_to_tblock(t);
+
+	if ((tblk->next == 0) && (current != jfsCommitTask)) {
+		/* Save one tblk for jfsCommit thread */
+		jFYI(1, ("txBegin: waiting for free tid\n"));
+		TXN_SLEEP(&TxAnchor.freewait);
+		goto retry;
+	}
+
+	TxAnchor.freetid = tblk->next;
+
+	/*
+	 * initialize transaction
+	 */
+
+	/*
+	 * We can't zero the whole thing or we screw up another thread being
+	 * awakened after sleeping on tblk->waitor
+	 *
+	 * memset(tblk, 0, sizeof(tblock_t));
+	 */
+	tblk->next = tblk->last = tblk->xflag = tblk->flag = tblk->lsn = 0;
+
+	tblk->sb = sb;
+	++log->logtid;
+	tblk->logtid = log->logtid;
+
+	++log->active;
+
+	HIGHWATERMARK(stattx.maxtid, t);	/* statistics */
+	INCREMENT(stattx.ntid);	/* statistics */
+
+	TXN_UNLOCK();
+
+	jFYI(1, ("txBegin: returning tid = %d\n", t));
+
+	return t;
+}
+
+
+/*
+ * NAME:        txBeginAnon()
+ *
+ * FUNCTION:    start an anonymous transaction.
+ *		Blocks if logsync or available tlocks are low to prevent
+ *		anonymous tlocks from depleting supply.
+ *
+ * PARAMETER:   sb	- superblock
+ *
+ * RETURN:	none
+ */
+void txBeginAnon(struct super_block *sb)
+{
+	log_t *log;
+
+	log = (log_t *) JFS_SBI(sb)->log;
+
+	TXN_LOCK();
+
+      retry:
+	/*
+	 * synchronize with logsync barrier
+	 */
+	if (log->syncbarrier) {
+		TXN_SLEEP(&log->syncwait);
+		goto retry;
+	}
+
+	/*
+	 * Don't begin transaction if we're getting starved for tlocks
+	 */
+	if (TlocksLow) {
+		TXN_SLEEP(&TxAnchor.lowlockwait);
+		goto retry;
+	}
+	TXN_UNLOCK();
+}
+
+
+/*
+ *      txEnd()
+ *
+ * function: free specified transaction block.
+ *
+ *      logsync barrier processing:
+ *
+ * serialization:
+ */
+void txEnd(tid_t tid)
+{
+	tblock_t *tblk = tid_to_tblock(tid);
+	log_t *log;
+
+	jFYI(1, ("txEnd: tid = %d\n", tid));
+	TXN_LOCK();
+
+	/*
+	 * wakeup transactions waiting on the page locked
+	 * by the current transaction
+	 */
+	TXN_WAKEUP(&tblk->waitor);
+
+	log = (log_t *) JFS_SBI(tblk->sb)->log;
+
+	/*
+	 * Lazy commit thread can't free this guy until we mark it UNLOCKED,
+	 * otherwise, we would be left with a transaction that may have been
+	 * reused.
+	 *
+	 * Lazy commit thread will turn off tblkGC_LAZY before calling this
+	 * routine.
+	 */
+	if (tblk->flag & tblkGC_LAZY) {
+		jFYI(1,
+		     ("txEnd called w/lazy tid: %d, tblk = 0x%p\n",
+		      tid, tblk));
+		TXN_UNLOCK();
+
+		spin_lock_irq(&log->gclock);	// LOGGC_LOCK
+		tblk->flag |= tblkGC_UNLOCKED;
+		spin_unlock_irq(&log->gclock);	// LOGGC_UNLOCK
+		return;
+	}
+
+	jFYI(1, ("txEnd: tid: %d, tblk = 0x%p\n", tid, tblk));
+
+	assert(tblk->next == 0);
+
+	/*
+	 * insert tblock back on freelist
+	 */
+	tblk->next = TxAnchor.freetid;
+	TxAnchor.freetid = tid;
+
+	/*
+	 * mark the tblock not active
+	 */
+	--log->active;
+
+	/*
+	 * synchronize with logsync barrier
+	 */
+	if (log->syncbarrier && log->active == 0) {
+		/* forward log syncpt */
+		/* lmSync(log); */
+
+		jFYI(1, ("     log barrier off: 0x%x\n", log->lsn));
+
+		/* enable new transactions start */
+		log->syncbarrier = 0;
+
+		/* wakeup all waitors for logsync barrier */
+		TXN_WAKEUP(&log->syncwait);
+	}
+
+	/*
+	 * wakeup all waitors for a free tblock
+	 */
+	TXN_WAKEUP(&TxAnchor.freewait);
+
+	TXN_UNLOCK();
+	jFYI(1, ("txEnd: exitting\n"));
+}
+
+
+/*
+ *      txLock()
+ *
+ * function: acquire a transaction lock on the specified <mp>
+ *
+ * parameter:
+ *
+ * return:      transaction lock id
+ *
+ * serialization:
+ */
+tlock_t *txLock(tid_t tid, struct inode *ip, metapage_t * mp, int type)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	int dir_xtree = 0;
+	lid_t lid;
+	tid_t xtid;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+	linelock_t *linelock;
+	xtpage_t *p;
+	tblock_t *tblk;
+
+	TXN_LOCK();
+
+	if (S_ISDIR(ip->i_mode) && (type & tlckXTREE) &&
+	    !(mp->xflag & COMMIT_PAGE)) {
+		/*
+		 * Directory inode is special.  It can have both an xtree tlock
+		 * and a dtree tlock associated with it.
+		 */
+		dir_xtree = 1;
+		lid = jfs_ip->xtlid;
+	} else
+		lid = mp->lid;
+
+	/* is page not locked by a transaction ? */
+	if (lid == 0)
+		goto allocateLock;
+
+	jFYI(1, ("txLock: tid:%d ip:0x%p mp:0x%p lid:%d\n",
+		 tid, ip, mp, lid));
+
+	/* is page locked by the requester transaction ? */
+	tlck = lid_to_tlock(lid);
+	if ((xtid = tlck->tid) == tid)
+		goto grantLock;
+
+	/*
+	 * is page locked by anonymous transaction/lock ?
+	 *
+	 * (page update without transaction (i.e., file write) is
+	 * locked under anonymous transaction tid = 0:
+	 * anonymous tlocks maintained on anonymous tlock list of
+	 * the inode of the page and available to all anonymous
+	 * transactions until txCommit() time at which point
+	 * they are transferred to the transaction tlock list of
+	 * the commiting transaction of the inode)
+	 */
+	if (xtid == 0) {
+		tlck->tid = tid;
+		tblk = tid_to_tblock(tid);
+		/*
+		 * The order of the tlocks in the transaction is important
+		 * (during truncate, child xtree pages must be freed before
+		 * parent's tlocks change the working map).
+		 * Take tlock off anonymous list and add to tail of
+		 * transaction list
+		 *
+		 * Note:  We really need to get rid of the tid & lid and
+		 * use list_head's.  This code is getting UGLY!
+		 */
+		if (jfs_ip->atlhead == lid) {
+			if (jfs_ip->atltail == lid) {
+				/* only anonymous txn.
+				 * Remove from anon_list
+				 */
+				list_del_init(&jfs_ip->anon_inode_list);
+			}
+			jfs_ip->atlhead = tlck->next;
+		} else {
+			lid_t last;
+			for (last = jfs_ip->atlhead;
+			     lid_to_tlock(last)->next != lid;
+			     last = lid_to_tlock(last)->next) {
+				assert(last);
+			}
+			lid_to_tlock(last)->next = tlck->next;
+			if (jfs_ip->atltail == lid)
+				jfs_ip->atltail = last;
+		}
+
+		/* insert the tlock at tail of transaction tlock list */
+
+		if (tblk->next)
+			lid_to_tlock(tblk->last)->next = lid;
+		else
+			tblk->next = lid;
+		tlck->next = 0;
+		tblk->last = lid;
+
+		goto grantLock;
+	}
+
+	goto waitLock;
+
+	/*
+	 * allocate a tlock
+	 */
+      allocateLock:
+	lid = txLockAlloc();
+	tlck = lid_to_tlock(lid);
+
+	/*
+	 * initialize tlock
+	 */
+	tlck->tid = tid;
+
+	/* mark tlock for meta-data page */
+	if (mp->xflag & COMMIT_PAGE) {
+
+		tlck->flag = tlckPAGELOCK;
+
+		/* mark the page dirty and nohomeok */
+		mark_metapage_dirty(mp);
+		atomic_inc(&mp->nohomeok);
+
+		jFYI(1,
+		     ("locking mp = 0x%p, nohomeok = %d tid = %d tlck = 0x%p\n",
+		      mp, atomic_read(&mp->nohomeok), tid, tlck));
+
+		/* if anonymous transaction, and buffer is on the group
+		 * commit synclist, mark inode to show this.  This will
+		 * prevent the buffer from being marked nohomeok for too
+		 * long a time.
+		 */
+		if ((tid == 0) && mp->lsn)
+			set_cflag(COMMIT_Synclist, ip);
+	}
+	/* mark tlock for in-memory inode */
+	else
+		tlck->flag = tlckINODELOCK;
+
+	tlck->type = 0;
+
+	/* bind the tlock and the page */
+	tlck->ip = ip;
+	tlck->mp = mp;
+	if (dir_xtree)
+		jfs_ip->xtlid = lid;
+	else
+		mp->lid = lid;
+
+	/*
+	 * enqueue transaction lock to transaction/inode
+	 */
+	/* insert the tlock at tail of transaction tlock list */
+	if (tid) {
+		tblk = tid_to_tblock(tid);
+		if (tblk->next)
+			lid_to_tlock(tblk->last)->next = lid;
+		else
+			tblk->next = lid;
+		tlck->next = 0;
+		tblk->last = lid;
+	}
+	/* anonymous transaction:
+	 * insert the tlock at head of inode anonymous tlock list
+	 */
+	else {
+		tlck->next = jfs_ip->atlhead;
+		jfs_ip->atlhead = lid;
+		if (tlck->next == 0) {
+			/* This inode's first anonymous transaction */
+			jfs_ip->atltail = lid;
+			list_add_tail(&jfs_ip->anon_inode_list,
+				      &TxAnchor.anon_list);
+		}
+	}
+
+	/* initialize type dependent area for linelock */
+	linelock = (linelock_t *) & tlck->lock;
+	linelock->next = 0;
+	linelock->flag = tlckLINELOCK;
+	linelock->maxcnt = TLOCKSHORT;
+	linelock->index = 0;
+
+	switch (type & tlckTYPE) {
+	case tlckDTREE:
+		linelock->l2linesize = L2DTSLOTSIZE;
+		break;
+
+	case tlckXTREE:
+		linelock->l2linesize = L2XTSLOTSIZE;
+
+		xtlck = (xtlock_t *) linelock;
+		xtlck->header.offset = 0;
+		xtlck->header.length = 2;
+
+		if (type & tlckNEW) {
+			xtlck->lwm.offset = XTENTRYSTART;
+		} else {
+			if (mp->xflag & COMMIT_PAGE)
+				p = (xtpage_t *) mp->data;
+			else
+				p = &jfs_ip->i_xtroot;
+			xtlck->lwm.offset =
+			    le16_to_cpu(p->header.nextindex);
+		}
+		xtlck->lwm.length = 0;	/* ! */
+
+		xtlck->index = 2;
+		break;
+
+	case tlckINODE:
+		linelock->l2linesize = L2INODESLOTSIZE;
+		break;
+
+	case tlckDATA:
+		linelock->l2linesize = L2DATASLOTSIZE;
+		break;
+
+	default:
+		jERROR(1, ("UFO tlock:0x%p\n", tlck));
+	}
+
+	/*
+	 * update tlock vector
+	 */
+      grantLock:
+	tlck->type |= type;
+
+	TXN_UNLOCK();
+
+	return tlck;
+
+	/*
+	 * page is being locked by another transaction:
+	 */
+      waitLock:
+	/* Only locks on ipimap or ipaimap should reach here */
+	/* assert(jfs_ip->fileset == AGGREGATE_I); */
+	if (jfs_ip->fileset != AGGREGATE_I) {
+		jERROR(1, ("txLock: trying to lock locked page!\n"));
+		dump_mem("ip", ip, sizeof(struct inode));
+		dump_mem("mp", mp, sizeof(metapage_t));
+		dump_mem("Locker's tblk", tid_to_tblock(tid),
+			 sizeof(tblock_t));
+		dump_mem("Tlock", tlck, sizeof(tlock_t));
+		BUG();
+	}
+	INCREMENT(stattx.waitlock);	/* statistics */
+	release_metapage(mp);
+
+	jEVENT(0, ("txLock: in waitLock, tid = %d, xtid = %d, lid = %d\n",
+		   tid, xtid, lid));
+	TXN_SLEEP_DROP_LOCK(&tid_to_tblock(xtid)->waitor);
+	jEVENT(0, ("txLock: awakened     tid = %d, lid = %d\n", tid, lid));
+
+	return NULL;
+}
+
+
+/*
+ * NAME:        txRelease()
+ *
+ * FUNCTION:    Release buffers associated with transaction locks, but don't
+ *		mark homeok yet.  The allows other transactions to modify
+ *		buffers, but won't let them go to disk until commit record
+ *		actually gets written.
+ *
+ * PARAMETER:
+ *              tblk    -
+ *
+ * RETURN:      Errors from subroutines.
+ */
+static void txRelease(tblock_t * tblk)
+{
+	metapage_t *mp;
+	lid_t lid;
+	tlock_t *tlck;
+
+	TXN_LOCK();
+
+	for (lid = tblk->next; lid; lid = tlck->next) {
+		tlck = lid_to_tlock(lid);
+		if ((mp = tlck->mp) != NULL &&
+		    (tlck->type & tlckBTROOT) == 0) {
+			assert(mp->xflag & COMMIT_PAGE);
+			mp->lid = 0;
+		}
+	}
+
+	/*
+	 * wakeup transactions waiting on a page locked
+	 * by the current transaction
+	 */
+	TXN_WAKEUP(&tblk->waitor);
+
+	TXN_UNLOCK();
+}
+
+
+/*
+ * NAME:        txUnlock()
+ *
+ * FUNCTION:    Initiates pageout of pages modified by tid in journalled
+ *              objects and frees their lockwords.
+ *
+ * PARAMETER:
+ *              flag    -
+ *
+ * RETURN:      Errors from subroutines.
+ */
+static void txUnlock(tblock_t * tblk, int flag)
+{
+	tlock_t *tlck;
+	linelock_t *linelock;
+	lid_t lid, next, llid, k;
+	metapage_t *mp;
+	log_t *log;
+	int force;
+	int difft, diffp;
+
+	jFYI(1, ("txUnlock: tblk = 0x%p\n", tblk));
+	log = (log_t *) JFS_SBI(tblk->sb)->log;
+	force = flag & COMMIT_FLUSH;
+	if (log->syncbarrier)
+		force |= COMMIT_FORCE;
+
+	/*
+	 * mark page under tlock homeok (its log has been written):
+	 * if caller has specified FORCE (e.g., iRecycle()), or
+	 * if syncwait for the log is set (i.e., the log sync point
+	 * has fallen behind), or
+	 * if syncpt is set for the page, or
+	 * if the page is new, initiate pageout;
+	 * otherwise, leave the page in memory.
+	 */
+	for (lid = tblk->next; lid; lid = next) {
+		tlck = lid_to_tlock(lid);
+		next = tlck->next;
+
+		jFYI(1, ("unlocking lid = %d, tlck = 0x%p\n", lid, tlck));
+
+		/* unbind page from tlock */
+		if ((mp = tlck->mp) != NULL &&
+		    (tlck->type & tlckBTROOT) == 0) {
+			assert(mp->xflag & COMMIT_PAGE);
+
+			/* hold buffer
+			 *
+			 * It's possible that someone else has the metapage.
+			 * The only things were changing are nohomeok, which
+			 * is handled atomically, and clsn which is protected
+			 * by the LOGSYNC_LOCK.
+			 */
+			hold_metapage(mp, 1);
+
+			assert(atomic_read(&mp->nohomeok) > 0);
+			atomic_dec(&mp->nohomeok);
+
+			/* inherit younger/larger clsn */
+			LOGSYNC_LOCK(log);
+			if (mp->clsn) {
+				logdiff(difft, tblk->clsn, log);
+				logdiff(diffp, mp->clsn, log);
+				if (difft > diffp)
+					mp->clsn = tblk->clsn;
+			} else
+				mp->clsn = tblk->clsn;
+			LOGSYNC_UNLOCK(log);
+
+			assert(!(tlck->flag & tlckFREEPAGE));
+
+			if (tlck->flag & tlckWRITEPAGE) {
+				write_metapage(mp);
+			} else {
+				/* release page which has been forced */
+				release_metapage(mp);
+			}
+		}
+
+		/* insert tlock, and linelock(s) of the tlock if any,
+		 * at head of freelist
+		 */
+		TXN_LOCK();
+
+		llid = ((linelock_t *) & tlck->lock)->next;
+		while (llid) {
+			linelock = (linelock_t *) lid_to_tlock(llid);
+			k = linelock->next;
+			txLockFree(llid);
+			llid = k;
+		}
+		txLockFree(lid);
+
+		TXN_UNLOCK();
+	}
+	tblk->next = tblk->last = 0;
+
+	/*
+	 * remove tblock from logsynclist
+	 * (allocation map pages inherited lsn of tblk and
+	 * has been inserted in logsync list at txUpdateMap())
+	 */
+	if (tblk->lsn) {
+		LOGSYNC_LOCK(log);
+		log->count--;
+		list_del(&tblk->synclist);
+		LOGSYNC_UNLOCK(log);
+	}
+}
+
+
+/*
+ *      txMaplock()
+ *
+ * function: allocate a transaction lock for freed page/entry;
+ *      for freed page, maplock is used as xtlock/dtlock type;
+ */
+tlock_t *txMaplock(tid_t tid, struct inode *ip, int type)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	lid_t lid;
+	tblock_t *tblk;
+	tlock_t *tlck;
+	maplock_t *maplock;
+
+	TXN_LOCK();
+
+	/*
+	 * allocate a tlock
+	 */
+	lid = txLockAlloc();
+	tlck = lid_to_tlock(lid);
+
+	/*
+	 * initialize tlock
+	 */
+	tlck->tid = tid;
+
+	/* bind the tlock and the object */
+	tlck->flag = tlckINODELOCK;
+	tlck->ip = ip;
+	tlck->mp = NULL;
+
+	tlck->type = type;
+
+	/*
+	 * enqueue transaction lock to transaction/inode
+	 */
+	/* insert the tlock at tail of transaction tlock list */
+	if (tid) {
+		tblk = tid_to_tblock(tid);
+		if (tblk->next)
+			lid_to_tlock(tblk->last)->next = lid;
+		else
+			tblk->next = lid;
+		tlck->next = 0;
+		tblk->last = lid;
+	}
+	/* anonymous transaction:
+	 * insert the tlock at head of inode anonymous tlock list
+	 */
+	else {
+		tlck->next = jfs_ip->atlhead;
+		jfs_ip->atlhead = lid;
+		if (tlck->next == 0) {
+			/* This inode's first anonymous transaction */
+			jfs_ip->atltail = lid;
+			list_add_tail(&jfs_ip->anon_inode_list,
+				      &TxAnchor.anon_list);
+		}
+	}
+
+	TXN_UNLOCK();
+
+	/* initialize type dependent area for maplock */
+	maplock = (maplock_t *) & tlck->lock;
+	maplock->next = 0;
+	maplock->maxcnt = 0;
+	maplock->index = 0;
+
+	return tlck;
+}
+
+
+/*
+ *      txLinelock()
+ *
+ * function: allocate a transaction lock for log vector list
+ */
+linelock_t *txLinelock(linelock_t * tlock)
+{
+	lid_t lid;
+	tlock_t *tlck;
+	linelock_t *linelock;
+
+	TXN_LOCK();
+
+	/* allocate a TxLock structure */
+	lid = txLockAlloc();
+	tlck = lid_to_tlock(lid);
+
+	TXN_UNLOCK();
+
+	/* initialize linelock */
+	linelock = (linelock_t *) tlck;
+	linelock->next = 0;
+	linelock->flag = tlckLINELOCK;
+	linelock->maxcnt = TLOCKLONG;
+	linelock->index = 0;
+
+	/* append linelock after tlock */
+	linelock->next = tlock->next;
+	tlock->next = lid;
+
+	return linelock;
+}
+
+
+
+/*
+ *              transaction commit management
+ *              -----------------------------
+ */
+
+/*
+ * NAME:        txCommit()
+ *
+ * FUNCTION:    commit the changes to the objects specified in
+ *              clist.  For journalled segments only the
+ *              changes of the caller are committed, ie by tid.
+ *              for non-journalled segments the data are flushed to
+ *              disk and then the change to the disk inode and indirect
+ *              blocks committed (so blocks newly allocated to the
+ *              segment will be made a part of the segment atomically).
+ *
+ *              all of the segments specified in clist must be in
+ *              one file system. no more than 6 segments are needed
+ *              to handle all unix svcs.
+ *
+ *              if the i_nlink field (i.e. disk inode link count)
+ *              is zero, and the type of inode is a regular file or
+ *              directory, or symbolic link , the inode is truncated
+ *              to zero length. the truncation is committed but the
+ *              VM resources are unaffected until it is closed (see
+ *              iput and iclose).
+ *
+ * PARAMETER:
+ *
+ * RETURN:
+ *
+ * serialization:
+ *              on entry the inode lock on each segment is assumed
+ *              to be held.
+ *
+ * i/o error:
+ */
+int txCommit(tid_t tid,		/* transaction identifier */
+	     int nip,		/* number of inodes to commit */
+	     struct inode **iplist,	/* list of inode to commit */
+	     int flag)
+{
+	int rc = 0, rc1 = 0;
+	commit_t cd;
+	log_t *log;
+	tblock_t *tblk;
+	lrd_t *lrd;
+	int lsn;
+	struct inode *ip;
+	struct jfs_inode_info *jfs_ip;
+	int k, n;
+	ino_t top;
+	struct super_block *sb;
+
+	jFYI(1, ("txCommit, tid = %d, flag = %d\n", tid, flag));
+	/* is read-only file system ? */
+	if (isReadOnly(iplist[0])) {
+		rc = EROFS;
+		goto TheEnd;
+	}
+
+	sb = cd.sb = iplist[0]->i_sb;
+	cd.tid = tid;
+
+	if (tid == 0)
+		tid = txBegin(sb, 0);
+	tblk = tid_to_tblock(tid);
+
+	/*
+	 * initialize commit structure
+	 */
+	log = (log_t *) JFS_SBI(sb)->log;
+	cd.log = log;
+
+	/* initialize log record descriptor in commit */
+	lrd = &cd.lrd;
+	lrd->logtid = cpu_to_le32(tblk->logtid);
+	lrd->backchain = 0;
+
+	tblk->xflag |= flag;
+
+	if ((flag & (COMMIT_FORCE | COMMIT_SYNC)) == 0)
+		tblk->xflag |= COMMIT_LAZY;
+	/*
+	 *      prepare non-journaled objects for commit
+	 *
+	 * flush data pages of non-journaled file
+	 * to prevent the file getting non-initialized disk blocks
+	 * in case of crash.
+	 * (new blocks - )
+	 */
+	cd.iplist = iplist;
+	cd.nip = nip;
+
+	/*
+	 *      acquire transaction lock on (on-disk) inodes
+	 *
+	 * update on-disk inode from in-memory inode
+	 * acquiring transaction locks for AFTER records
+	 * on the on-disk inode of file object
+	 *
+	 * sort the inodes array by inode number in descending order
+	 * to prevent deadlock when acquiring transaction lock
+	 * of on-disk inodes on multiple on-disk inode pages by
+	 * multiple concurrent transactions
+	 */
+	for (k = 0; k < cd.nip; k++) {
+		top = (cd.iplist[k])->i_ino;
+		for (n = k + 1; n < cd.nip; n++) {
+			ip = cd.iplist[n];
+			if (ip->i_ino > top) {
+				top = ip->i_ino;
+				cd.iplist[n] = cd.iplist[k];
+				cd.iplist[k] = ip;
+			}
+		}
+
+		ip = cd.iplist[k];
+		jfs_ip = JFS_IP(ip);
+
+		/*
+		 * BUGBUG - Should we call filemap_fdatasync here instead
+		 * of fsync_inode_data?
+		 * If we do, we have a deadlock condition since we may end
+		 * up recursively calling jfs_get_block with the IWRITELOCK
+		 * held.  We may be able to do away with IWRITELOCK while
+		 * committing transactions and use i_sem instead.
+		 */
+		if ((!S_ISDIR(ip->i_mode))
+		    && (tblk->flag & COMMIT_DELETE) == 0)
+			fsync_inode_data_buffers(ip);
+
+		/*
+		 * Mark inode as not dirty.  It will still be on the dirty
+		 * inode list, but we'll know not to commit it again unless
+		 * it gets marked dirty again
+		 */
+		clear_cflag(COMMIT_Dirty, ip);
+
+		/* inherit anonymous tlock(s) of inode */
+		if (jfs_ip->atlhead) {
+			lid_to_tlock(jfs_ip->atltail)->next = tblk->next;
+			tblk->next = jfs_ip->atlhead;
+			if (!tblk->last)
+				tblk->last = jfs_ip->atltail;
+			jfs_ip->atlhead = jfs_ip->atltail = 0;
+			TXN_LOCK();
+			list_del_init(&jfs_ip->anon_inode_list);
+			TXN_UNLOCK();
+		}
+
+		/*
+		 * acquire transaction lock on on-disk inode page
+		 * (become first tlock of the tblk's tlock list)
+		 */
+		if (((rc = diWrite(tid, ip))))
+			goto out;
+	}
+
+	/*
+	 *      write log records from transaction locks
+	 *
+	 * txUpdateMap() resets XAD_NEW in XAD.
+	 */
+	if ((rc = txLog(log, tblk, &cd)))
+		goto TheEnd;
+
+	/*
+	 * Ensure that inode isn't reused before
+	 * lazy commit thread finishes processing
+	 */
+	if (tblk->xflag & (COMMIT_CREATE | COMMIT_DELETE))
+		atomic_inc(&tblk->ip->i_count);
+	if (tblk->xflag & COMMIT_DELETE) {
+		ip = tblk->ip;
+		assert((ip->i_nlink == 0) && !test_cflag(COMMIT_Nolink, ip));
+		set_cflag(COMMIT_Nolink, ip);
+	}
+
+	/*
+	 *      write COMMIT log record
+	 */
+	lrd->type = cpu_to_le16(LOG_COMMIT);
+	lrd->length = 0;
+	lsn = lmLog(log, tblk, lrd, NULL);
+
+	lmGroupCommit(log, tblk);
+
+	/*
+	 *      - transaction is now committed -
+	 */
+
+	/*
+	 * force pages in careful update
+	 * (imap addressing structure update)
+	 */
+	if (flag & COMMIT_FORCE)
+		txForce(tblk);
+
+	/*
+	 *      update allocation map.
+	 *
+	 * update inode allocation map and inode:
+	 * free pager lock on memory object of inode if any.
+	 * update  block allocation map.
+	 *
+	 * txUpdateMap() resets XAD_NEW in XAD.
+	 */
+	if (tblk->xflag & COMMIT_FORCE)
+		txUpdateMap(tblk);
+
+	/*
+	 *      free transaction locks and pageout/free pages
+	 */
+	txRelease(tblk);
+
+	if ((tblk->flag & tblkGC_LAZY) == 0)
+		txUnlock(tblk, flag);
+
+
+	/*
+	 *      reset in-memory object state
+	 */
+	for (k = 0; k < cd.nip; k++) {
+		ip = cd.iplist[k];
+		jfs_ip = JFS_IP(ip);
+
+		/*
+		 * reset in-memory inode state
+		 */
+		jfs_ip->bxflag = 0;
+		jfs_ip->blid = 0;
+	}
+
+      out:
+	if (rc != 0)
+		txAbortCommit(&cd, rc);
+	else
+		rc = rc1;
+
+      TheEnd:
+	jFYI(1, ("txCommit: tid = %d, returning %d\n", tid, rc));
+	return rc;
+}
+
+
+/*
+ * NAME:        txLog()
+ *
+ * FUNCTION:    Writes AFTER log records for all lines modified
+ *              by tid for segments specified by inodes in comdata.
+ *              Code assumes only WRITELOCKS are recorded in lockwords.
+ *
+ * PARAMETERS:
+ *
+ * RETURN :
+ */
+static int txLog(log_t * log, tblock_t * tblk, commit_t * cd)
+{
+	int rc = 0;
+	struct inode *ip;
+	lid_t lid;
+	tlock_t *tlck;
+	lrd_t *lrd = &cd->lrd;
+
+	/*
+	 * write log record(s) for each tlock of transaction,
+	 */
+	for (lid = tblk->next; lid; lid = tlck->next) {
+		tlck = lid_to_tlock(lid);
+
+		tlck->flag |= tlckLOG;
+
+		/* initialize lrd common */
+		ip = tlck->ip;
+		lrd->aggregate = cpu_to_le32(kdev_t_to_nr(ip->i_dev));
+		lrd->log.redopage.fileset = cpu_to_le32(JFS_IP(ip)->fileset);
+		lrd->log.redopage.inode = cpu_to_le32(ip->i_ino);
+
+		if (tlck->mp)
+			hold_metapage(tlck->mp, 0);
+
+		/* write log record of page from the tlock */
+		switch (tlck->type & tlckTYPE) {
+		case tlckXTREE:
+			xtLog(log, tblk, lrd, tlck);
+			break;
+
+		case tlckDTREE:
+			dtLog(log, tblk, lrd, tlck);
+			break;
+
+		case tlckINODE:
+			diLog(log, tblk, lrd, tlck, cd);
+			break;
+
+		case tlckMAP:
+			mapLog(log, tblk, lrd, tlck);
+			break;
+
+		case tlckDATA:
+			dataLog(log, tblk, lrd, tlck);
+			break;
+
+		default:
+			jERROR(1, ("UFO tlock:0x%p\n", tlck));
+		}
+		if (tlck->mp)
+			release_metapage(tlck->mp);
+	}
+
+	return rc;
+}
+
+
+/*
+ *      diLog()
+ *
+ * function:    log inode tlock and format maplock to update bmap;
+ */
+int diLog(log_t * log,
+	  tblock_t * tblk, lrd_t * lrd, tlock_t * tlck, commit_t * cd)
+{
+	int rc = 0;
+	metapage_t *mp;
+	pxd_t *pxd;
+	pxdlock_t *pxdlock;
+
+	mp = tlck->mp;
+
+	/* initialize as REDOPAGE record format */
+	lrd->log.redopage.type = cpu_to_le16(LOG_INODE);
+	lrd->log.redopage.l2linesize = cpu_to_le16(L2INODESLOTSIZE);
+
+	pxd = &lrd->log.redopage.pxd;
+
+	/*
+	 *      inode after image
+	 */
+	if (tlck->type & tlckENTRY) {
+		/* log after-image for logredo(): */
+		lrd->type = cpu_to_le16(LOG_REDOPAGE);
+//              *pxd = mp->cm_pxd;
+		PXDaddress(pxd, mp->index);
+		PXDlength(pxd,
+			  mp->logical_size >> tblk->sb->s_blocksize_bits);
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+	} else if (tlck->type & tlckFREE) {
+		/*
+		 *      free inode extent
+		 *
+		 * (pages of the freed inode extent have been invalidated and
+		 * a maplock for free of the extent has been formatted at
+		 * txLock() time);
+		 *
+		 * the tlock had been acquired on the inode allocation map page
+		 * (iag) that specifies the freed extent, even though the map
+		 * page is not itself logged, to prevent pageout of the map
+		 * page before the log;
+		 */
+		assert(tlck->type & tlckFREE);
+
+		/* log LOG_NOREDOINOEXT of the freed inode extent for
+		 * logredo() to start NoRedoPage filters, and to update
+		 * imap and bmap for free of the extent;
+		 */
+		lrd->type = cpu_to_le16(LOG_NOREDOINOEXT);
+		/*
+		 * For the LOG_NOREDOINOEXT record, we need
+		 * to pass the IAG number and inode extent
+		 * index (within that IAG) from which the
+		 * the extent being released.  These have been
+		 * passed to us in the iplist[1] and iplist[2].
+		 */
+		lrd->log.noredoinoext.iagnum =
+		    cpu_to_le32((u32) (size_t) cd->iplist[1]);
+		lrd->log.noredoinoext.inoext_idx =
+		    cpu_to_le32((u32) (size_t) cd->iplist[2]);
+
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		*pxd = pxdlock->pxd;
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+
+		/* update bmap */
+		tlck->flag |= tlckUPDATEMAP;
+
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+	} else {
+		jERROR(2, ("diLog: UFO type tlck:0x%p\n", tlck));
+	}
+#ifdef  _JFS_WIP
+	/*
+	 *      alloc/free external EA extent
+	 *
+	 * a maplock for txUpdateMap() to update bPWMAP for alloc/free
+	 * of the extent has been formatted at txLock() time;
+	 */
+	else {
+		assert(tlck->type & tlckEA);
+
+		/* log LOG_UPDATEMAP for logredo() to update bmap for
+		 * alloc of new (and free of old) external EA extent;
+		 */
+		lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		nlock = pxdlock->index;
+		for (i = 0; i < nlock; i++, pxdlock++) {
+			if (pxdlock->flag & mlckALLOCPXD)
+				lrd->log.updatemap.type =
+				    cpu_to_le16(LOG_ALLOCPXD);
+			else
+				lrd->log.updatemap.type =
+				    cpu_to_le16(LOG_FREEPXD);
+			lrd->log.updatemap.nxd = cpu_to_le16(1);
+			lrd->log.updatemap.pxd = pxdlock->pxd;
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+		}
+
+		/* update bmap */
+		tlck->flag |= tlckUPDATEMAP;
+	}
+#endif				/* _JFS_WIP */
+
+	return rc;
+}
+
+
+/*
+ *      dataLog()
+ *
+ * function:    log data tlock
+ */
+int dataLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	metapage_t *mp;
+	pxd_t *pxd;
+	int rc;
+	s64 xaddr;
+	int xflag;
+	s32 xlen;
+
+	mp = tlck->mp;
+
+	/* initialize as REDOPAGE record format */
+	lrd->log.redopage.type = cpu_to_le16(LOG_DATA);
+	lrd->log.redopage.l2linesize = cpu_to_le16(L2DATASLOTSIZE);
+
+	pxd = &lrd->log.redopage.pxd;
+
+	/* log after-image for logredo(): */
+	lrd->type = cpu_to_le16(LOG_REDOPAGE);
+
+	if (JFS_IP(tlck->ip)->next_index < MAX_INLINE_DIRTABLE_ENTRY) {
+		/*
+		 * The table has been truncated, we've must have deleted
+		 * the last entry, so don't bother logging this
+		 */
+		mp->lid = 0;
+		atomic_dec(&mp->nohomeok);
+		discard_metapage(mp);
+		tlck->mp = 0;
+		return 0;
+	}
+
+	rc = xtLookup(tlck->ip, mp->index, 1, &xflag, &xaddr, &xlen, 1);
+	if (rc || (xlen == 0)) {
+		jERROR(1, ("dataLog: can't find physical address\n"));
+		return 0;
+	}
+
+	PXDaddress(pxd, xaddr);
+	PXDlength(pxd, mp->logical_size >> tblk->sb->s_blocksize_bits);
+
+	lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+	/* mark page as homeward bound */
+	tlck->flag |= tlckWRITEPAGE;
+
+	return 0;
+}
+
+
+/*
+ *      dtLog()
+ *
+ * function:    log dtree tlock and format maplock to update bmap;
+ */
+void dtLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	struct inode *ip;
+	metapage_t *mp;
+	pxdlock_t *pxdlock;
+	pxd_t *pxd;
+
+	ip = tlck->ip;
+	mp = tlck->mp;
+
+	/* initialize as REDOPAGE/NOREDOPAGE record format */
+	lrd->log.redopage.type = cpu_to_le16(LOG_DTREE);
+	lrd->log.redopage.l2linesize = cpu_to_le16(L2DTSLOTSIZE);
+
+	pxd = &lrd->log.redopage.pxd;
+
+	if (tlck->type & tlckBTROOT)
+		lrd->log.redopage.type |= cpu_to_le16(LOG_BTROOT);
+
+	/*
+	 *      page extension via relocation: entry insertion;
+	 *      page extension in-place: entry insertion;
+	 *      new right page from page split, reinitialized in-line
+	 *      root from root page split: entry insertion;
+	 */
+	if (tlck->type & (tlckNEW | tlckEXTEND)) {
+		/* log after-image of the new page for logredo():
+		 * mark log (LOG_NEW) for logredo() to initialize
+		 * freelist and update bmap for alloc of the new page;
+		 */
+		lrd->type = cpu_to_le16(LOG_REDOPAGE);
+		if (tlck->type & tlckEXTEND)
+			lrd->log.redopage.type |= cpu_to_le16(LOG_EXTEND);
+		else
+			lrd->log.redopage.type |= cpu_to_le16(LOG_NEW);
+//              *pxd = mp->cm_pxd;
+		PXDaddress(pxd, mp->index);
+		PXDlength(pxd,
+			  mp->logical_size >> tblk->sb->s_blocksize_bits);
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+		/* format a maplock for txUpdateMap() to update bPMAP for
+		 * alloc of the new page;
+		 */
+		if (tlck->type & tlckBTROOT)
+			return;
+		tlck->flag |= tlckUPDATEMAP;
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		pxdlock->flag = mlckALLOCPXD;
+		pxdlock->pxd = *pxd;
+
+		pxdlock->index = 1;
+
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+		return;
+	}
+
+	/*
+	 *      entry insertion/deletion,
+	 *      sibling page link update (old right page before split);
+	 */
+	if (tlck->type & (tlckENTRY | tlckRELINK)) {
+		/* log after-image for logredo(): */
+		lrd->type = cpu_to_le16(LOG_REDOPAGE);
+		PXDaddress(pxd, mp->index);
+		PXDlength(pxd,
+			  mp->logical_size >> tblk->sb->s_blocksize_bits);
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+		return;
+	}
+
+	/*
+	 *      page deletion: page has been invalidated
+	 *      page relocation: source extent
+	 *
+	 *      a maplock for free of the page has been formatted
+	 *      at txLock() time);
+	 */
+	if (tlck->type & (tlckFREE | tlckRELOCATE)) {
+		/* log LOG_NOREDOPAGE of the deleted page for logredo()
+		 * to start NoRedoPage filter and to update bmap for free
+		 * of the deletd page
+		 */
+		lrd->type = cpu_to_le16(LOG_NOREDOPAGE);
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		*pxd = pxdlock->pxd;
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+
+		/* a maplock for txUpdateMap() for free of the page
+		 * has been formatted at txLock() time;
+		 */
+		tlck->flag |= tlckUPDATEMAP;
+	}
+	return;
+}
+
+
+/*
+ *      xtLog()
+ *
+ * function:    log xtree tlock and format maplock to update bmap;
+ */
+void xtLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	struct inode *ip;
+	metapage_t *mp;
+	xtpage_t *p;
+	xtlock_t *xtlck;
+	maplock_t *maplock;
+	xdlistlock_t *xadlock;
+	pxdlock_t *pxdlock;
+	pxd_t *pxd;
+	int next, lwm, hwm;
+
+	ip = tlck->ip;
+	mp = tlck->mp;
+
+	/* initialize as REDOPAGE/NOREDOPAGE record format */
+	lrd->log.redopage.type = cpu_to_le16(LOG_XTREE);
+	lrd->log.redopage.l2linesize = cpu_to_le16(L2XTSLOTSIZE);
+
+	pxd = &lrd->log.redopage.pxd;
+
+	if (tlck->type & tlckBTROOT) {
+		lrd->log.redopage.type |= cpu_to_le16(LOG_BTROOT);
+		p = &JFS_IP(ip)->i_xtroot;
+		if (S_ISDIR(ip->i_mode))
+			lrd->log.redopage.type |=
+			    cpu_to_le16(LOG_DIR_XTREE);
+	} else
+		p = (xtpage_t *) mp->data;
+	next = le16_to_cpu(p->header.nextindex);
+
+	xtlck = (xtlock_t *) & tlck->lock;
+
+	maplock = (maplock_t *) & tlck->lock;
+	xadlock = (xdlistlock_t *) maplock;
+
+	/*
+	 *      entry insertion/extension;
+	 *      sibling page link update (old right page before split);
+	 */
+	if (tlck->type & (tlckNEW | tlckGROW | tlckRELINK)) {
+		/* log after-image for logredo():
+		 * logredo() will update bmap for alloc of new/extended
+		 * extents (XAD_NEW|XAD_EXTEND) of XAD[lwm:next) from
+		 * after-image of XADlist;
+		 * logredo() resets (XAD_NEW|XAD_EXTEND) flag when
+		 * applying the after-image to the meta-data page.
+		 */
+		lrd->type = cpu_to_le16(LOG_REDOPAGE);
+//              *pxd = mp->cm_pxd;
+		PXDaddress(pxd, mp->index);
+		PXDlength(pxd,
+			  mp->logical_size >> tblk->sb->s_blocksize_bits);
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+		/* format a maplock for txUpdateMap() to update bPMAP
+		 * for alloc of new/extended extents of XAD[lwm:next)
+		 * from the page itself;
+		 * txUpdateMap() resets (XAD_NEW|XAD_EXTEND) flag.
+		 */
+		lwm = xtlck->lwm.offset;
+		if (lwm == 0)
+			lwm = XTPAGEMAXSLOT;
+
+		if (lwm == next)
+			goto out;
+		assert(lwm < next);
+		tlck->flag |= tlckUPDATEMAP;
+		xadlock->flag = mlckALLOCXADLIST;
+		xadlock->count = next - lwm;
+		if ((xadlock->count <= 2) && (tblk->xflag & COMMIT_LAZY)) {
+			int i;
+			/*
+			 * Lazy commit may allow xtree to be modified before
+			 * txUpdateMap runs.  Copy xad into linelock to
+			 * preserve correct data.
+			 */
+			xadlock->xdlist = &xtlck->pxdlock;
+			memcpy(xadlock->xdlist, &p->xad[lwm],
+			       sizeof(xad_t) * xadlock->count);
+
+			for (i = 0; i < xadlock->count; i++)
+				p->xad[lwm + i].flag &=
+				    ~(XAD_NEW | XAD_EXTENDED);
+		} else {
+			/*
+			 * xdlist will point to into inode's xtree, ensure
+			 * that transaction is not committed lazily.
+			 */
+			xadlock->xdlist = &p->xad[lwm];
+			tblk->xflag &= ~COMMIT_LAZY;
+		}
+		jFYI(1,
+		     ("xtLog: alloc ip:0x%p mp:0x%p tlck:0x%p lwm:%d count:%d\n",
+		      tlck->ip, mp, tlck, lwm, xadlock->count));
+
+		maplock->index = 1;
+
+	      out:
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+
+		return;
+	}
+
+	/*
+	 *      page deletion: file deletion/truncation (ref. xtTruncate())
+	 *
+	 * (page will be invalidated after log is written and bmap
+	 * is updated from the page);
+	 */
+	if (tlck->type & tlckFREE) {
+		/* LOG_NOREDOPAGE log for NoRedoPage filter:
+		 * if page free from file delete, NoRedoFile filter from
+		 * inode image of zero link count will subsume NoRedoPage
+		 * filters for each page;
+		 * if page free from file truncattion, write NoRedoPage
+		 * filter;
+		 *
+		 * upadte of block allocation map for the page itself:
+		 * if page free from deletion and truncation, LOG_UPDATEMAP
+		 * log for the page itself is generated from processing
+		 * its parent page xad entries;
+		 */
+		/* if page free from file truncation, log LOG_NOREDOPAGE
+		 * of the deleted page for logredo() to start NoRedoPage
+		 * filter for the page;
+		 */
+		if (tblk->xflag & COMMIT_TRUNCATE) {
+			/* write NOREDOPAGE for the page */
+			lrd->type = cpu_to_le16(LOG_NOREDOPAGE);
+			PXDaddress(pxd, mp->index);
+			PXDlength(pxd,
+				  mp->logical_size >> tblk->sb->
+				  s_blocksize_bits);
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+
+			if (tlck->type & tlckBTROOT) {
+				/* Empty xtree must be logged */
+				lrd->type = cpu_to_le16(LOG_REDOPAGE);
+				lrd->backchain =
+				    cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+			}
+		}
+
+		/* init LOG_UPDATEMAP of the freed extents
+		 * XAD[XTENTRYSTART:hwm) from the deleted page itself
+		 * for logredo() to update bmap;
+		 */
+		lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+		lrd->log.updatemap.type = cpu_to_le16(LOG_FREEXADLIST);
+		xtlck = (xtlock_t *) & tlck->lock;
+		hwm = xtlck->hwm.offset;
+		lrd->log.updatemap.nxd =
+		    cpu_to_le16(hwm - XTENTRYSTART + 1);
+		/* reformat linelock for lmLog() */
+		xtlck->header.offset = XTENTRYSTART;
+		xtlck->header.length = hwm - XTENTRYSTART + 1;
+		xtlck->index = 1;
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+
+		/* format a maplock for txUpdateMap() to update bmap
+		 * to free extents of XAD[XTENTRYSTART:hwm) from the
+		 * deleted page itself;
+		 */
+		tlck->flag |= tlckUPDATEMAP;
+		xadlock->flag = mlckFREEXADLIST;
+		xadlock->count = hwm - XTENTRYSTART + 1;
+		if ((xadlock->count <= 2) && (tblk->xflag & COMMIT_LAZY)) {
+			/*
+			 * Lazy commit may allow xtree to be modified before
+			 * txUpdateMap runs.  Copy xad into linelock to
+			 * preserve correct data.
+			 */
+			xadlock->xdlist = &xtlck->pxdlock;
+			memcpy(xadlock->xdlist, &p->xad[XTENTRYSTART],
+			       sizeof(xad_t) * xadlock->count);
+		} else {
+			/*
+			 * xdlist will point to into inode's xtree, ensure
+			 * that transaction is not committed lazily unless
+			 * we're deleting the inode (unlink).  In that case
+			 * we have special logic for the inode to be
+			 * unlocked by the lazy commit thread.
+			 */
+			xadlock->xdlist = &p->xad[XTENTRYSTART];
+			if ((tblk->xflag & COMMIT_LAZY) &&
+			    (tblk->xflag & COMMIT_DELETE) &&
+			    (tblk->ip == ip))
+				set_cflag(COMMIT_Holdlock, ip);
+			else
+				tblk->xflag &= ~COMMIT_LAZY;
+		}
+		jFYI(1,
+		     ("xtLog: free ip:0x%p mp:0x%p count:%d lwm:2\n",
+		      tlck->ip, mp, xadlock->count));
+
+		maplock->index = 1;
+
+		/* mark page as invalid */
+		if (((tblk->xflag & COMMIT_PWMAP) || S_ISDIR(ip->i_mode))
+		    && !(tlck->type & tlckBTROOT))
+			tlck->flag |= tlckFREEPAGE;
+		/*
+		   else (tblk->xflag & COMMIT_PMAP)
+		   ? release the page;
+		 */
+		return;
+	}
+
+	/*
+	 *      page/entry truncation: file truncation (ref. xtTruncate())
+	 *
+	 *     |----------+------+------+---------------|
+	 *                |      |      |
+	 *                |      |     hwm - hwm before truncation
+	 *                |     next - truncation point
+	 *               lwm - lwm before truncation
+	 * header ?
+	 */
+	if (tlck->type & tlckTRUNCATE) {
+		pxd_t tpxd;	/* truncated extent of xad */
+
+		/*
+		 * For truncation the entire linelock may be used, so it would
+		 * be difficult to store xad list in linelock itself.
+		 * Therefore, we'll just force transaction to be committed
+		 * synchronously, so that xtree pages won't be changed before
+		 * txUpdateMap runs.
+		 */
+		tblk->xflag &= ~COMMIT_LAZY;
+		lwm = xtlck->lwm.offset;
+		if (lwm == 0)
+			lwm = XTPAGEMAXSLOT;
+		hwm = xtlck->hwm.offset;
+
+		/*
+		 *      write log records
+		 */
+		/*
+		 * allocate entries XAD[lwm:next]:
+		 */
+		if (lwm < next) {
+			/* log after-image for logredo():
+			 * logredo() will update bmap for alloc of new/extended
+			 * extents (XAD_NEW|XAD_EXTEND) of XAD[lwm:next) from
+			 * after-image of XADlist;
+			 * logredo() resets (XAD_NEW|XAD_EXTEND) flag when
+			 * applying the after-image to the meta-data page.
+			 */
+			lrd->type = cpu_to_le16(LOG_REDOPAGE);
+			PXDaddress(pxd, mp->index);
+			PXDlength(pxd,
+				  mp->logical_size >> tblk->sb->
+				  s_blocksize_bits);
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+		}
+
+		/*
+		 * truncate entry XAD[hwm == next - 1]:
+		 */
+		if (hwm == next - 1) {
+			/* init LOG_UPDATEMAP for logredo() to update bmap for
+			 * free of truncated delta extent of the truncated
+			 * entry XAD[next - 1]:
+			 * (xtlck->pxdlock = truncated delta extent);
+			 */
+			pxdlock = (pxdlock_t *) & xtlck->pxdlock;
+			/* assert(pxdlock->type & tlckTRUNCATE); */
+			lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+			lrd->log.updatemap.type = cpu_to_le16(LOG_FREEPXD);
+			lrd->log.updatemap.nxd = cpu_to_le16(1);
+			lrd->log.updatemap.pxd = pxdlock->pxd;
+			tpxd = pxdlock->pxd;	/* save to format maplock */
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+		}
+
+		/*
+		 * free entries XAD[next:hwm]:
+		 */
+		if (hwm >= next) {
+			/* init LOG_UPDATEMAP of the freed extents
+			 * XAD[next:hwm] from the deleted page itself
+			 * for logredo() to update bmap;
+			 */
+			lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+			lrd->log.updatemap.type =
+			    cpu_to_le16(LOG_FREEXADLIST);
+			xtlck = (xtlock_t *) & tlck->lock;
+			hwm = xtlck->hwm.offset;
+			lrd->log.updatemap.nxd =
+			    cpu_to_le16(hwm - next + 1);
+			/* reformat linelock for lmLog() */
+			xtlck->header.offset = next;
+			xtlck->header.length = hwm - next + 1;
+			xtlck->index = 1;
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, tlck));
+		}
+
+		/*
+		 *      format maplock(s) for txUpdateMap() to update bmap
+		 */
+		maplock->index = 0;
+
+		/*
+		 * allocate entries XAD[lwm:next):
+		 */
+		if (lwm < next) {
+			/* format a maplock for txUpdateMap() to update bPMAP
+			 * for alloc of new/extended extents of XAD[lwm:next)
+			 * from the page itself;
+			 * txUpdateMap() resets (XAD_NEW|XAD_EXTEND) flag.
+			 */
+			tlck->flag |= tlckUPDATEMAP;
+			xadlock->flag = mlckALLOCXADLIST;
+			xadlock->count = next - lwm;
+			xadlock->xdlist = &p->xad[lwm];
+
+			jFYI(1,
+			     ("xtLog: alloc ip:0x%p mp:0x%p count:%d lwm:%d next:%d\n",
+			      tlck->ip, mp, xadlock->count, lwm, next));
+			maplock->index++;
+			xadlock++;
+		}
+
+		/*
+		 * truncate entry XAD[hwm == next - 1]:
+		 */
+		if (hwm == next - 1) {
+			pxdlock_t *pxdlock;
+
+			/* format a maplock for txUpdateMap() to update bmap
+			 * to free truncated delta extent of the truncated
+			 * entry XAD[next - 1];
+			 * (xtlck->pxdlock = truncated delta extent);
+			 */
+			tlck->flag |= tlckUPDATEMAP;
+			pxdlock = (pxdlock_t *) xadlock;
+			pxdlock->flag = mlckFREEPXD;
+			pxdlock->count = 1;
+			pxdlock->pxd = tpxd;
+
+			jFYI(1,
+			     ("xtLog: truncate ip:0x%p mp:0x%p count:%d hwm:%d\n",
+			      ip, mp, pxdlock->count, hwm));
+			maplock->index++;
+			xadlock++;
+		}
+
+		/*
+		 * free entries XAD[next:hwm]:
+		 */
+		if (hwm >= next) {
+			/* format a maplock for txUpdateMap() to update bmap
+			 * to free extents of XAD[next:hwm] from thedeleted
+			 * page itself;
+			 */
+			tlck->flag |= tlckUPDATEMAP;
+			xadlock->flag = mlckFREEXADLIST;
+			xadlock->count = hwm - next + 1;
+			xadlock->xdlist = &p->xad[next];
+
+			jFYI(1,
+			     ("xtLog: free ip:0x%p mp:0x%p count:%d next:%d hwm:%d\n",
+			      tlck->ip, mp, xadlock->count, next, hwm));
+			maplock->index++;
+		}
+
+		/* mark page as homeward bound */
+		tlck->flag |= tlckWRITEPAGE;
+	}
+	return;
+}
+
+
+/*
+ *      mapLog()
+ *
+ * function:    log from maplock of freed data extents;
+ */
+void mapLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck)
+{
+	pxdlock_t *pxdlock;
+	int i, nlock;
+	pxd_t *pxd;
+
+	/*
+	 *      page relocation: free the source page extent
+	 *
+	 * a maplock for txUpdateMap() for free of the page
+	 * has been formatted at txLock() time saving the src
+	 * relocated page address;
+	 */
+	if (tlck->type & tlckRELOCATE) {
+		/* log LOG_NOREDOPAGE of the old relocated page
+		 * for logredo() to start NoRedoPage filter;
+		 */
+		lrd->type = cpu_to_le16(LOG_NOREDOPAGE);
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		pxd = &lrd->log.redopage.pxd;
+		*pxd = pxdlock->pxd;
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+
+		/* (N.B. currently, logredo() does NOT update bmap
+		 * for free of the page itself for (LOG_XTREE|LOG_NOREDOPAGE);
+		 * if page free from relocation, LOG_UPDATEMAP log is
+		 * specifically generated now for logredo()
+		 * to update bmap for free of src relocated page;
+		 * (new flag LOG_RELOCATE may be introduced which will
+		 * inform logredo() to start NORedoPage filter and also
+		 * update block allocation map at the same time, thus
+		 * avoiding an extra log write);
+		 */
+		lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+		lrd->log.updatemap.type = cpu_to_le16(LOG_FREEPXD);
+		lrd->log.updatemap.nxd = cpu_to_le16(1);
+		lrd->log.updatemap.pxd = pxdlock->pxd;
+		lrd->backchain = cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+
+		/* a maplock for txUpdateMap() for free of the page
+		 * has been formatted at txLock() time;
+		 */
+		tlck->flag |= tlckUPDATEMAP;
+		return;
+	}
+	/*
+
+	 * Otherwise it's not a relocate request
+	 *
+	 */
+	else {
+		/* log LOG_UPDATEMAP for logredo() to update bmap for
+		 * free of truncated/relocated delta extent of the data;
+		 * e.g.: external EA extent, relocated/truncated extent
+		 * from xtTailgate();
+		 */
+		lrd->type = cpu_to_le16(LOG_UPDATEMAP);
+		pxdlock = (pxdlock_t *) & tlck->lock;
+		nlock = pxdlock->index;
+		for (i = 0; i < nlock; i++, pxdlock++) {
+			if (pxdlock->flag & mlckALLOCPXD)
+				lrd->log.updatemap.type =
+				    cpu_to_le16(LOG_ALLOCPXD);
+			else
+				lrd->log.updatemap.type =
+				    cpu_to_le16(LOG_FREEPXD);
+			lrd->log.updatemap.nxd = cpu_to_le16(1);
+			lrd->log.updatemap.pxd = pxdlock->pxd;
+			lrd->backchain =
+			    cpu_to_le32(lmLog(log, tblk, lrd, NULL));
+			jFYI(1, ("mapLog: xaddr:0x%lx xlen:0x%x\n",
+				 (ulong) addressPXD(&pxdlock->pxd),
+				 lengthPXD(&pxdlock->pxd)));
+		}
+
+		/* update bmap */
+		tlck->flag |= tlckUPDATEMAP;
+	}
+}
+
+
+/*
+ *      txEA()
+ *
+ * function:    acquire maplock for EA/ACL extents or
+ *              set COMMIT_INLINE flag;
+ */
+void txEA(tid_t tid, struct inode *ip, dxd_t * oldea, dxd_t * newea)
+{
+	tlock_t *tlck = NULL;
+	pxdlock_t *maplock = NULL, *pxdlock = NULL;
+
+	/*
+	 * format maplock for alloc of new EA extent
+	 */
+	if (newea) {
+		/* Since the newea could be a completely zeroed entry we need to
+		 * check for the two flags which indicate we should actually
+		 * commit new EA data
+		 */
+		if (newea->flag & DXD_EXTENT) {
+			tlck = txMaplock(tid, ip, tlckMAP);
+			maplock = (pxdlock_t *) & tlck->lock;
+			pxdlock = (pxdlock_t *) maplock;
+			pxdlock->flag = mlckALLOCPXD;
+			PXDaddress(&pxdlock->pxd, addressDXD(newea));
+			PXDlength(&pxdlock->pxd, lengthDXD(newea));
+			pxdlock++;
+			maplock->index = 1;
+		} else if (newea->flag & DXD_INLINE) {
+			tlck = NULL;
+
+			set_cflag(COMMIT_Inlineea, ip);
+		}
+	}
+
+	/*
+	 * format maplock for free of old EA extent
+	 */
+	if (!test_cflag(COMMIT_Nolink, ip) && oldea->flag & DXD_EXTENT) {
+		if (tlck == NULL) {
+			tlck = txMaplock(tid, ip, tlckMAP);
+			maplock = (pxdlock_t *) & tlck->lock;
+			pxdlock = (pxdlock_t *) maplock;
+			maplock->index = 0;
+		}
+		pxdlock->flag = mlckFREEPXD;
+		PXDaddress(&pxdlock->pxd, addressDXD(oldea));
+		PXDlength(&pxdlock->pxd, lengthDXD(oldea));
+		maplock->index++;
+	}
+}
+
+
+/*
+ *      txForce()
+ *
+ * function: synchronously write pages locked by transaction
+ *              after txLog() but before txUpdateMap();
+ */
+void txForce(tblock_t * tblk)
+{
+	tlock_t *tlck;
+	lid_t lid, next;
+	metapage_t *mp;
+
+	/*
+	 * reverse the order of transaction tlocks in
+	 * careful update order of address index pages
+	 * (right to left, bottom up)
+	 */
+	tlck = lid_to_tlock(tblk->next);
+	lid = tlck->next;
+	tlck->next = 0;
+	while (lid) {
+		tlck = lid_to_tlock(lid);
+		next = tlck->next;
+		tlck->next = tblk->next;
+		tblk->next = lid;
+		lid = next;
+	}
+
+	/*
+	 * synchronously write the page, and
+	 * hold the page for txUpdateMap();
+	 */
+	for (lid = tblk->next; lid; lid = next) {
+		tlck = lid_to_tlock(lid);
+		next = tlck->next;
+
+		if ((mp = tlck->mp) != NULL &&
+		    (tlck->type & tlckBTROOT) == 0) {
+			assert(mp->xflag & COMMIT_PAGE);
+
+			if (tlck->flag & tlckWRITEPAGE) {
+				tlck->flag &= ~tlckWRITEPAGE;
+
+				/* do not release page to freelist */
+				assert(atomic_read(&mp->nohomeok));
+				hold_metapage(mp, 0);
+				write_metapage(mp);
+			}
+		}
+	}
+}
+
+
+/*
+ *      txUpdateMap()
+ *
+ * function:    update persistent allocation map (and working map
+ *              if appropriate);
+ *
+ * parameter:
+ */
+static void txUpdateMap(tblock_t * tblk)
+{
+	struct inode *ip;
+	struct inode *ipimap;
+	lid_t lid;
+	tlock_t *tlck;
+	maplock_t *maplock;
+	pxdlock_t pxdlock;
+	int maptype;
+	int k, nlock;
+	metapage_t *mp = 0;
+
+	ipimap = JFS_SBI(tblk->sb)->ipimap;
+
+	maptype = (tblk->xflag & COMMIT_PMAP) ? COMMIT_PMAP : COMMIT_PWMAP;
+
+
+	/*
+	 *      update block allocation map
+	 *
+	 * update allocation state in pmap (and wmap) and
+	 * update lsn of the pmap page;
+	 */
+	/*
+	 * scan each tlock/page of transaction for block allocation/free:
+	 *
+	 * for each tlock/page of transaction, update map.
+	 *  ? are there tlock for pmap and pwmap at the same time ?
+	 */
+	for (lid = tblk->next; lid; lid = tlck->next) {
+		tlck = lid_to_tlock(lid);
+
+		if ((tlck->flag & tlckUPDATEMAP) == 0)
+			continue;
+
+		if (tlck->flag & tlckFREEPAGE) {
+			/*
+			 * Another thread may attempt to reuse freed space
+			 * immediately, so we want to get rid of the metapage
+			 * before anyone else has a chance to get it.
+			 * Lock metapage, update maps, then invalidate
+			 * the metapage.
+			 */
+			mp = tlck->mp;
+			ASSERT(mp->xflag & COMMIT_PAGE);
+			hold_metapage(mp, 0);
+		}
+
+		/*
+		 * extent list:
+		 * . in-line PXD list:
+		 * . out-of-line XAD list:
+		 */
+		maplock = (maplock_t *) & tlck->lock;
+		nlock = maplock->index;
+
+		for (k = 0; k < nlock; k++, maplock++) {
+			/*
+			 * allocate blocks in persistent map:
+			 *
+			 * blocks have been allocated from wmap at alloc time;
+			 */
+			if (maplock->flag & mlckALLOC) {
+				txAllocPMap(ipimap, maplock, tblk);
+			}
+			/*
+			 * free blocks in persistent and working map:
+			 * blocks will be freed in pmap and then in wmap;
+			 *
+			 * ? tblock specifies the PMAP/PWMAP based upon
+			 * transaction
+			 *
+			 * free blocks in persistent map:
+			 * blocks will be freed from wmap at last reference
+			 * release of the object for regular files;
+			 *
+			 * Alway free blocks from both persistent & working
+			 * maps for directories
+			 */
+			else {	/* (maplock->flag & mlckFREE) */
+
+				if (S_ISDIR(tlck->ip->i_mode))
+					txFreeMap(ipimap, maplock,
+						  tblk, COMMIT_PWMAP);
+				else
+					txFreeMap(ipimap, maplock,
+						  tblk, maptype);
+			}
+		}
+		if (tlck->flag & tlckFREEPAGE) {
+			if (!(tblk->flag & tblkGC_LAZY)) {
+				/* This is equivalent to txRelease */
+				ASSERT(mp->lid == lid);
+				tlck->mp->lid = 0;
+			}
+			assert(atomic_read(&mp->nohomeok) == 1);
+			atomic_dec(&mp->nohomeok);
+			discard_metapage(mp);
+			tlck->mp = 0;
+		}
+	}
+	/*
+	 *      update inode allocation map
+	 *
+	 * update allocation state in pmap and
+	 * update lsn of the pmap page;
+	 * update in-memory inode flag/state
+	 *
+	 * unlock mapper/write lock
+	 */
+	if (tblk->xflag & COMMIT_CREATE) {
+		ip = tblk->ip;
+
+		ASSERT(test_cflag(COMMIT_New, ip));
+		clear_cflag(COMMIT_New, ip);
+
+		diUpdatePMap(ipimap, ip->i_ino, FALSE, tblk);
+		ipimap->i_state |= I_DIRTY;
+		/* update persistent block allocation map
+		 * for the allocation of inode extent;
+		 */
+		pxdlock.flag = mlckALLOCPXD;
+		pxdlock.pxd = JFS_IP(ip)->ixpxd;
+		pxdlock.index = 1;
+		txAllocPMap(ip, (maplock_t *) & pxdlock, tblk);
+		iput(ip);
+	} else if (tblk->xflag & COMMIT_DELETE) {
+		ip = tblk->ip;
+		diUpdatePMap(ipimap, ip->i_ino, TRUE, tblk);
+		ipimap->i_state |= I_DIRTY;
+		if (test_and_clear_cflag(COMMIT_Holdlock, ip)) {
+			if (tblk->flag & tblkGC_LAZY)
+				IWRITE_UNLOCK(ip);
+		}
+		iput(ip);
+	}
+}
+
+
+/*
+ *      txAllocPMap()
+ *
+ * function: allocate from persistent map;
+ *
+ * parameter:
+ *      ipbmap  -
+ *      malock -
+ *              xad list:
+ *              pxd:
+ *
+ *      maptype -
+ *              allocate from persistent map;
+ *              free from persistent map;
+ *              (e.g., tmp file - free from working map at releae
+ *               of last reference);
+ *              free from persistent and working map;
+ *
+ *      lsn     - log sequence number;
+ */
+static void txAllocPMap(struct inode *ip, maplock_t * maplock,
+			tblock_t * tblk)
+{
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	xdlistlock_t *xadlistlock;
+	xad_t *xad;
+	s64 xaddr;
+	int xlen;
+	pxdlock_t *pxdlock;
+	xdlistlock_t *pxdlistlock;
+	pxd_t *pxd;
+	int n;
+
+	/*
+	 * allocate from persistent map;
+	 */
+	if (maplock->flag & mlckALLOCXADLIST) {
+		xadlistlock = (xdlistlock_t *) maplock;
+		xad = xadlistlock->xdlist;
+		for (n = 0; n < xadlistlock->count; n++, xad++) {
+			if (xad->flag & (XAD_NEW | XAD_EXTENDED)) {
+				xaddr = addressXAD(xad);
+				xlen = lengthXAD(xad);
+				dbUpdatePMap(ipbmap, FALSE, xaddr,
+					     (s64) xlen, tblk);
+				xad->flag &= ~(XAD_NEW | XAD_EXTENDED);
+				jFYI(1,
+				     ("allocPMap: xaddr:0x%lx xlen:%d\n",
+				      (ulong) xaddr, xlen));
+			}
+		}
+	} else if (maplock->flag & mlckALLOCPXD) {
+		pxdlock = (pxdlock_t *) maplock;
+		xaddr = addressPXD(&pxdlock->pxd);
+		xlen = lengthPXD(&pxdlock->pxd);
+		dbUpdatePMap(ipbmap, FALSE, xaddr, (s64) xlen, tblk);
+		jFYI(1,
+		     ("allocPMap: xaddr:0x%lx xlen:%d\n", (ulong) xaddr,
+		      xlen));
+	} else {		/* (maplock->flag & mlckALLOCPXDLIST) */
+
+		pxdlistlock = (xdlistlock_t *) maplock;
+		pxd = pxdlistlock->xdlist;
+		for (n = 0; n < pxdlistlock->count; n++, pxd++) {
+			xaddr = addressPXD(pxd);
+			xlen = lengthPXD(pxd);
+			dbUpdatePMap(ipbmap, FALSE, xaddr, (s64) xlen,
+				     tblk);
+			jFYI(1,
+			     ("allocPMap: xaddr:0x%lx xlen:%d\n",
+			      (ulong) xaddr, xlen));
+		}
+	}
+}
+
+
+/*
+ *      txFreeMap()
+ *
+ * function:    free from persistent and/or working map;
+ *
+ * todo: optimization
+ */
+void txFreeMap(struct inode *ip,
+	       maplock_t * maplock, tblock_t * tblk, int maptype)
+{
+	struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap;
+	xdlistlock_t *xadlistlock;
+	xad_t *xad;
+	s64 xaddr;
+	int xlen;
+	pxdlock_t *pxdlock;
+	xdlistlock_t *pxdlistlock;
+	pxd_t *pxd;
+	int n;
+
+	jFYI(1,
+	     ("txFreeMap: tblk:0x%p maplock:0x%p maptype:0x%x\n",
+	      tblk, maplock, maptype));
+
+	/*
+	 * free from persistent map;
+	 */
+	if (maptype == COMMIT_PMAP || maptype == COMMIT_PWMAP) {
+		if (maplock->flag & mlckFREEXADLIST) {
+			xadlistlock = (xdlistlock_t *) maplock;
+			xad = xadlistlock->xdlist;
+			for (n = 0; n < xadlistlock->count; n++, xad++) {
+				if (!(xad->flag & XAD_NEW)) {
+					xaddr = addressXAD(xad);
+					xlen = lengthXAD(xad);
+					dbUpdatePMap(ipbmap, TRUE, xaddr,
+						     (s64) xlen, tblk);
+					jFYI(1,
+					     ("freePMap: xaddr:0x%lx xlen:%d\n",
+					      (ulong) xaddr, xlen));
+				}
+			}
+		} else if (maplock->flag & mlckFREEPXD) {
+			pxdlock = (pxdlock_t *) maplock;
+			xaddr = addressPXD(&pxdlock->pxd);
+			xlen = lengthPXD(&pxdlock->pxd);
+			dbUpdatePMap(ipbmap, TRUE, xaddr, (s64) xlen,
+				     tblk);
+			jFYI(1,
+			     ("freePMap: xaddr:0x%lx xlen:%d\n",
+			      (ulong) xaddr, xlen));
+		} else {	/* (maplock->flag & mlckALLOCPXDLIST) */
+
+			pxdlistlock = (xdlistlock_t *) maplock;
+			pxd = pxdlistlock->xdlist;
+			for (n = 0; n < pxdlistlock->count; n++, pxd++) {
+				xaddr = addressPXD(pxd);
+				xlen = lengthPXD(pxd);
+				dbUpdatePMap(ipbmap, TRUE, xaddr,
+					     (s64) xlen, tblk);
+				jFYI(1,
+				     ("freePMap: xaddr:0x%lx xlen:%d\n",
+				      (ulong) xaddr, xlen));
+			}
+		}
+	}
+
+	/*
+	 * free from working map;
+	 */
+	if (maptype == COMMIT_PWMAP || maptype == COMMIT_WMAP) {
+		if (maplock->flag & mlckFREEXADLIST) {
+			xadlistlock = (xdlistlock_t *) maplock;
+			xad = xadlistlock->xdlist;
+			for (n = 0; n < xadlistlock->count; n++, xad++) {
+				xaddr = addressXAD(xad);
+				xlen = lengthXAD(xad);
+				dbFree(ip, xaddr, (s64) xlen);
+				xad->flag = 0;
+				jFYI(1,
+				     ("freeWMap: xaddr:0x%lx xlen:%d\n",
+				      (ulong) xaddr, xlen));
+			}
+		} else if (maplock->flag & mlckFREEPXD) {
+			pxdlock = (pxdlock_t *) maplock;
+			xaddr = addressPXD(&pxdlock->pxd);
+			xlen = lengthPXD(&pxdlock->pxd);
+			dbFree(ip, xaddr, (s64) xlen);
+			jFYI(1,
+			     ("freeWMap: xaddr:0x%lx xlen:%d\n",
+			      (ulong) xaddr, xlen));
+		} else {	/* (maplock->flag & mlckFREEPXDLIST) */
+
+			pxdlistlock = (xdlistlock_t *) maplock;
+			pxd = pxdlistlock->xdlist;
+			for (n = 0; n < pxdlistlock->count; n++, pxd++) {
+				xaddr = addressPXD(pxd);
+				xlen = lengthPXD(pxd);
+				dbFree(ip, xaddr, (s64) xlen);
+				jFYI(1,
+				     ("freeWMap: xaddr:0x%lx xlen:%d\n",
+				      (ulong) xaddr, xlen));
+			}
+		}
+	}
+}
+
+
+/*
+ *      txFreelock()
+ *
+ * function:    remove tlock from inode anonymous locklist
+ */
+void txFreelock(struct inode *ip)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	tlock_t *xtlck, *tlck;
+	lid_t xlid = 0, lid;
+
+	if (!jfs_ip->atlhead)
+		return;
+
+	xtlck = (tlock_t *) &jfs_ip->atlhead;
+
+	while ((lid = xtlck->next)) {
+		tlck = lid_to_tlock(lid);
+		if (tlck->flag & tlckFREELOCK) {
+			xtlck->next = tlck->next;
+			txLockFree(lid);
+		} else {
+			xtlck = tlck;
+			xlid = lid;
+		}
+	}
+
+	if (jfs_ip->atlhead)
+		jfs_ip->atltail = xlid;
+	else {
+		jfs_ip->atltail = 0;
+		/*
+		 * If inode was on anon_list, remove it
+		 */
+		TXN_LOCK();
+		list_del_init(&jfs_ip->anon_inode_list);
+		TXN_UNLOCK();
+	}
+}
+
+
+/*
+ *      txAbort()
+ *
+ * function: abort tx before commit;
+ *
+ * frees line-locks and segment locks for all
+ * segments in comdata structure.
+ * Optionally sets state of file-system to FM_DIRTY in super-block.
+ * log age of page-frames in memory for which caller has
+ * are reset to 0 (to avoid logwarap).
+ */
+void txAbort(tid_t tid, int dirty)
+{
+	lid_t lid, next;
+	metapage_t *mp;
+	tblock_t *tblk = tid_to_tblock(tid);
+
+	jEVENT(1, ("txAbort: tid:%d dirty:0x%x\n", tid, dirty));
+
+	/*
+	 * free tlocks of the transaction
+	 */
+	for (lid = tblk->next; lid; lid = next) {
+		next = lid_to_tlock(lid)->next;
+
+		mp = lid_to_tlock(lid)->mp;
+
+		if (mp) {
+			mp->lid = 0;
+
+			/*
+			 * reset lsn of page to avoid logwarap:
+			 *
+			 * (page may have been previously committed by another
+			 * transaction(s) but has not been paged, i.e.,
+			 * it may be on logsync list even though it has not
+			 * been logged for the current tx.)
+			 */
+			if (mp->xflag & COMMIT_PAGE && mp->lsn)
+				LogSyncRelease(mp);
+		}
+		/* insert tlock at head of freelist */
+		TXN_LOCK();
+		txLockFree(lid);
+		TXN_UNLOCK();
+	}
+
+	/* caller will free the transaction block */
+
+	tblk->next = tblk->last = 0;
+
+	/*
+	 * mark filesystem dirty
+	 */
+	if (dirty)
+		updateSuper(tblk->sb, FM_DIRTY);
+
+	return;
+}
+
+
+/*
+ *      txAbortCommit()
+ *
+ * function: abort commit.
+ *
+ * frees tlocks of transaction; line-locks and segment locks for all
+ * segments in comdata structure. frees malloc storage
+ * sets state of file-system to FM_MDIRTY in super-block.
+ * log age of page-frames in memory for which caller has
+ * are reset to 0 (to avoid logwarap).
+ */
+void txAbortCommit(commit_t * cd, int exval)
+{
+	tblock_t *tblk;
+	tid_t tid;
+	lid_t lid, next;
+	metapage_t *mp;
+
+	assert(exval == EIO || exval == ENOMEM);
+	jEVENT(1, ("txAbortCommit: cd:0x%p\n", cd));
+
+	/*
+	 * free tlocks of the transaction
+	 */
+	tid = cd->tid;
+	tblk = tid_to_tblock(tid);
+	for (lid = tblk->next; lid; lid = next) {
+		next = lid_to_tlock(lid)->next;
+
+		mp = lid_to_tlock(lid)->mp;
+		if (mp) {
+			mp->lid = 0;
+
+			/*
+			 * reset lsn of page to avoid logwarap;
+			 */
+			if (mp->xflag & COMMIT_PAGE)
+				LogSyncRelease(mp);
+		}
+
+		/* insert tlock at head of freelist */
+		TXN_LOCK();
+		txLockFree(lid);
+		TXN_UNLOCK();
+	}
+
+	tblk->next = tblk->last = 0;
+
+	/* free the transaction block */
+	txEnd(tid);
+
+	/*
+	 * mark filesystem dirty
+	 */
+	updateSuper(cd->sb, FM_DIRTY);
+}
+
+
+/*
+ *      txLazyCommit(void)
+ *
+ *	All transactions except those changing ipimap (COMMIT_FORCE) are
+ *	processed by this routine.  This insures that the inode and block
+ *	allocation maps are updated in order.  For synchronous transactions,
+ *	let the user thread finish processing after txUpdateMap() is called.
+ */
+void txLazyCommit(tblock_t * tblk)
+{
+	log_t *log;
+
+	while (((tblk->flag & tblkGC_READY) == 0) &&
+	       ((tblk->flag & tblkGC_UNLOCKED) == 0)) {
+		/* We must have gotten ahead of the user thread
+		 */
+		jFYI(1,
+		     ("jfs_lazycommit: tblk 0x%p not unlocked\n", tblk));
+		schedule();
+	}
+
+	jFYI(1, ("txLazyCommit: processing tblk 0x%p\n", tblk));
+
+	txUpdateMap(tblk);
+
+	log = (log_t *) JFS_SBI(tblk->sb)->log;
+
+	spin_lock_irq(&log->gclock);	// LOGGC_LOCK
+
+	tblk->flag |= tblkGC_COMMITTED;
+
+	if ((tblk->flag & tblkGC_READY) || (tblk->flag & tblkGC_LAZY))
+		log->gcrtc--;
+
+	if (tblk->flag & tblkGC_READY)
+		wake_up(&tblk->gcwait);	// LOGGC_WAKEUP
+
+	spin_unlock_irq(&log->gclock);	// LOGGC_UNLOCK
+
+	if (tblk->flag & tblkGC_LAZY) {
+		txUnlock(tblk, 0);
+		tblk->flag &= ~tblkGC_LAZY;
+		txEnd(tblk - TxBlock);	/* Convert back to tid */
+	}
+
+	jFYI(1, ("txLazyCommit: done: tblk = 0x%p\n", tblk));
+}
+
+/*
+ *      jfs_lazycommit(void)
+ *
+ *	To be run as a kernel daemon.  If lbmIODone is called in an interrupt
+ *	context, or where blocking is not wanted, this routine will process
+ *	committed transactions from the unlock queue.
+ */
+int jfs_lazycommit(void)
+{
+	int WorkDone;
+	tblock_t *tblk;
+	unsigned long flags;
+
+	lock_kernel();
+
+	daemonize();
+	current->tty = NULL;
+	strcpy(current->comm, "jfsCommit");
+
+	unlock_kernel();
+
+	jfsCommitTask = current;
+
+	spin_lock_irq(&current->sigmask_lock);
+	siginitsetinv(&current->blocked,
+		      sigmask(SIGHUP) | sigmask(SIGKILL) | sigmask(SIGSTOP)
+		      | sigmask(SIGCONT));
+	spin_unlock_irq(&current->sigmask_lock);
+
+	LAZY_LOCK_INIT();
+	TxAnchor.unlock_queue = TxAnchor.unlock_tail = 0;
+
+	complete(&jfsIOwait);
+
+	do {
+		LAZY_LOCK(flags);
+restart:
+		WorkDone = 0;
+		while ((tblk = TxAnchor.unlock_queue)) {
+			/*
+			 * We can't get ahead of user thread.  Spinning is
+			 * simpler than blocking/waking.  We shouldn't spin
+			 * very long, since user thread shouldn't be blocking
+			 * between lmGroupCommit & txEnd.
+			 */
+			WorkDone = 1;
+
+			/*
+			 * Remove first transaction from queue
+			 */
+			TxAnchor.unlock_queue = tblk->cqnext;
+			tblk->cqnext = 0;
+			if (TxAnchor.unlock_tail == tblk)
+				TxAnchor.unlock_tail = 0;
+
+			LAZY_UNLOCK(flags);
+			txLazyCommit(tblk);
+
+			/*
+			 * We can be running indefinately if other processors
+			 * are adding transactions to this list
+			 */
+			if (current->need_resched)
+				schedule();
+			LAZY_LOCK(flags);
+		}
+
+		if (WorkDone)
+			goto restart;
+		
+		LAZY_UNLOCK(flags);
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	} while (!jfs_thread_stopped());
+
+	if (TxAnchor.unlock_queue)
+		jERROR(1, ("jfs_lazycommit being killed with pending transactions!\n"));
+	else
+		jFYI(1, ("jfs_lazycommit being killed\n"));
+	complete(&jfsIOwait);
+	return 0;
+}
+
+void txLazyUnlock(tblock_t * tblk)
+{
+	unsigned long flags;
+
+	LAZY_LOCK(flags);
+
+	if (TxAnchor.unlock_tail)
+		TxAnchor.unlock_tail->cqnext = tblk;
+	else
+		TxAnchor.unlock_queue = tblk;
+	TxAnchor.unlock_tail = tblk;
+	tblk->cqnext = 0;
+	LAZY_UNLOCK(flags);
+	wake_up_process(jfsCommitTask);
+}
+
+static void LogSyncRelease(metapage_t * mp)
+{
+	log_t *log = mp->log;
+
+	assert(atomic_read(&mp->nohomeok));
+	assert(log);
+	atomic_dec(&mp->nohomeok);
+
+	if (atomic_read(&mp->nohomeok))
+		return;
+
+	hold_metapage(mp, 0);
+
+	LOGSYNC_LOCK(log);
+	mp->log = NULL;
+	mp->lsn = 0;
+	mp->clsn = 0;
+	log->count--;
+	list_del_init(&mp->synclist);
+	LOGSYNC_UNLOCK(log);
+
+	release_metapage(mp);
+}
+
+/*
+ *      jfs_sync(void)
+ *
+ *	To be run as a kernel daemon.  This is awakened when tlocks run low.
+ *	We write any inodes that have anonymous tlocks so they will become
+ *	available.
+ */
+int jfs_sync(void)
+{
+	struct inode *ip;
+	struct jfs_inode_info *jfs_ip;
+
+	lock_kernel();
+
+	daemonize();
+	current->tty = NULL;
+	strcpy(current->comm, "jfsSync");
+
+	unlock_kernel();
+
+	jfsSyncTask = current;
+
+	spin_lock_irq(&current->sigmask_lock);
+	siginitsetinv(&current->blocked,
+		      sigmask(SIGHUP) | sigmask(SIGKILL) | sigmask(SIGSTOP)
+		      | sigmask(SIGCONT));
+	spin_unlock_irq(&current->sigmask_lock);
+
+	complete(&jfsIOwait);
+
+	do {
+		/*
+		 * write each inode on the anonymous inode list
+		 */
+		TXN_LOCK();
+		while (TlocksLow && !list_empty(&TxAnchor.anon_list)) {
+			jfs_ip = list_entry(TxAnchor.anon_list.next,
+					    struct jfs_inode_info,
+					    anon_inode_list);
+			ip = jfs_ip->inode;
+
+			/*
+			 * We must release the TXN_LOCK since our
+			 * IWRITE_TRYLOCK implementation may still block
+			 */
+			TXN_UNLOCK();
+			if (IWRITE_TRYLOCK(ip)) {
+				/*
+				 * inode will be removed from anonymous list
+				 * when it is committed
+				 */
+				jfs_commit_inode(ip, 0);
+				IWRITE_UNLOCK(ip);
+				/*
+				 * Just to be safe.  I don't know how
+				 * long we can run without blocking
+				 */
+				if (current->need_resched)
+					schedule();
+				TXN_LOCK();
+			} else {
+				/* We can't get the write lock.  It may
+				 * be held by a thread waiting for tlock's
+				 * so let's not block here.  Save it to
+				 * put back on the anon_list.
+				 */
+
+				/*
+				 * We released TXN_LOCK, let's make sure
+				 * this inode is still there
+				 */
+				TXN_LOCK();
+				if (TxAnchor.anon_list.next !=
+				    &jfs_ip->anon_inode_list)
+					continue;
+
+				/* Take off anon_list */
+				list_del(&jfs_ip->anon_inode_list);
+
+				/* Put on anon_list2 */
+				list_add(&jfs_ip->anon_inode_list,
+					 &TxAnchor.anon_list2);
+			}
+		}
+		/* Add anon_list2 back to anon_list */
+		if (!list_empty(&TxAnchor.anon_list2)) {
+			list_splice(&TxAnchor.anon_list2, &TxAnchor.anon_list);
+			INIT_LIST_HEAD(&TxAnchor.anon_list2);
+		}
+		TXN_UNLOCK();
+
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	} while (!jfs_thread_stopped());
+
+	jFYI(1, ("jfs_sync being killed\n"));
+	complete(&jfsIOwait);
+	return 0;
+}
+
+#if CONFIG_PROC_FS
+int jfs_txanchor_read(char *buffer, char **start, off_t offset, int length,
+		      int *eof, void *data)
+{
+	int len = 0;
+	off_t begin;
+	char *freewait;
+	char *freelockwait;
+	char *lowlockwait;
+
+	freewait =
+	    waitqueue_active(&TxAnchor.freewait) ? "active" : "empty";
+	freelockwait =
+	    waitqueue_active(&TxAnchor.freelockwait) ? "active" : "empty";
+	lowlockwait =
+	    waitqueue_active(&TxAnchor.lowlockwait) ? "active" : "empty";
+
+	len += sprintf(buffer,
+		       "JFS TxAnchor\n"
+		       "============\n"
+		       "freetid = %d\n"
+		       "freewait = %s\n"
+		       "freelock = %d\n"
+		       "freelockwait = %s\n"
+		       "lowlockwait = %s\n"
+		       "tlocksInUse = %d\n"
+		       "unlock_queue = 0x%p\n"
+		       "unlock_tail = 0x%p\n",
+		       TxAnchor.freetid,
+		       freewait,
+		       TxAnchor.freelock,
+		       freelockwait,
+		       lowlockwait,
+		       TxAnchor.tlocksInUse,
+		       TxAnchor.unlock_queue,
+		       TxAnchor.unlock_tail);
+
+	begin = offset;
+	*start = buffer + begin;
+	len -= begin;
+
+	if (len > length)
+		len = length;
+	else
+		*eof = 1;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_txnmgr.h linuxppc64_2_4/fs/jfs/jfs_txnmgr.h
--- linux-2.4.19/fs/jfs/jfs_txnmgr.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_txnmgr.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,315 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+/*
+ * Change History :
+ *
+ */
+
+#ifndef _H_JFS_TXNMGR
+#define _H_JFS_TXNMGR
+/*
+ *	jfs_txnmgr.h: transaction manager
+ */
+
+#include "jfs_logmgr.h"
+
+/*
+ * Hide implementation of TxBlock and TxLock
+ */
+#define tid_to_tblock(tid) (&TxBlock[tid])
+
+#define lid_to_tlock(lid) (&TxLock[lid])
+
+/*
+ *	transaction block
+ */
+typedef struct tblock {
+	/*
+	 * tblock_t and jbuf_t common area: struct logsyncblk
+	 *
+	 * the following 5 fields are the same as struct logsyncblk
+	 * which is common to tblock and jbuf to form logsynclist
+	 */
+	u16 xflag;		/* tx commit type */
+	u16 flag;		/* tx commit state */
+	lid_t dummy;		/* Must keep structures common */
+	s32 lsn;		/* recovery lsn */
+	struct list_head synclist;	/* logsynclist link */
+
+	/* lock management */
+	struct super_block *sb;	/* 4: super block */
+	lid_t next;		/* 2: index of first tlock of tid */
+	lid_t last;		/* 2: index of last tlock of tid */
+	wait_queue_head_t waitor;	/* 4: tids waiting on this tid */
+
+	/* log management */
+	u32 logtid;		/* 4: log transaction id */
+				/* (32) */
+
+	/* commit management */
+	struct tblock *cqnext;	/* 4: commit queue link */
+	s32 clsn;		/* 4: commit lsn */
+	struct lbuf *bp;	/* 4: */
+	s32 pn;			/* 4: commit record log page number */
+	s32 eor;		/* 4: commit record eor */
+	wait_queue_head_t gcwait;	/* 4: group commit event list:
+					 *    ready transactions wait on this
+					 *    event for group commit completion.
+					 */
+	struct inode *ip;	/* 4: inode being created or deleted */
+	s32 rsrvd;		/* 4: */
+} tblock_t;			/* (64) */
+
+extern struct tblock *TxBlock;	/* transaction block table */
+
+/* commit flags: tblk->xflag */
+#define	COMMIT_SYNC	0x0001	/* synchronous commit */
+#define	COMMIT_FORCE	0x0002	/* force pageout at end of commit */
+#define	COMMIT_FLUSH	0x0004	/* init flush at end of commit */
+#define COMMIT_MAP	0x00f0
+#define	COMMIT_PMAP	0x0010	/* update pmap */
+#define	COMMIT_WMAP	0x0020	/* update wmap */
+#define	COMMIT_PWMAP	0x0040	/* update pwmap */
+#define	COMMIT_FREE	0x0f00
+#define	COMMIT_DELETE	0x0100	/* inode delete */
+#define	COMMIT_TRUNCATE	0x0200	/* file truncation */
+#define	COMMIT_CREATE	0x0400	/* inode create */
+#define	COMMIT_LAZY	0x0800	/* lazy commit */
+#define COMMIT_PAGE	0x1000	/* Identifies element as metapage */
+#define COMMIT_INODE	0x2000	/* Identifies element as inode */
+
+/* group commit flags tblk->flag: see jfs_logmgr.h */
+
+/*
+ *	transaction lock
+ */
+typedef struct tlock {
+	lid_t next;		/* index next lockword on tid locklist
+				 *          next lockword on freelist
+				 */
+	tid_t tid;		/* transaction id holding lock */
+
+	u16 flag;		/* 2: lock control */
+	u16 type;		/* 2: log type */
+
+	struct metapage *mp;	/* 4: object page buffer locked */
+	struct inode *ip;	/* 4: object */
+	/* (16) */
+
+	s16 lock[24];		/* 48: overlay area */
+} tlock_t;			/* (64) */
+
+extern struct tlock *TxLock;	/* transaction lock table */
+
+/*
+ * tlock flag
+ */
+/* txLock state */
+#define tlckPAGELOCK		0x8000
+#define tlckINODELOCK		0x4000
+#define tlckLINELOCK		0x2000
+#define tlckINLINELOCK		0x1000
+/* lmLog state */
+#define tlckLOG			0x0800
+/* updateMap state */
+#define	tlckUPDATEMAP		0x0080
+/* freeLock state */
+#define tlckFREELOCK		0x0008
+#define tlckWRITEPAGE		0x0004
+#define tlckFREEPAGE		0x0002
+
+/*
+ * tlock type
+ */
+#define	tlckTYPE		0xfe00
+#define	tlckINODE		0x8000
+#define	tlckXTREE		0x4000
+#define	tlckDTREE		0x2000
+#define	tlckMAP			0x1000
+#define	tlckEA			0x0800
+#define	tlckACL			0x0400
+#define	tlckDATA		0x0200
+#define	tlckBTROOT		0x0100
+
+#define	tlckOPERATION		0x00ff
+#define tlckGROW		0x0001	/* file grow */
+#define tlckREMOVE		0x0002	/* file delete */
+#define tlckTRUNCATE		0x0004	/* file truncate */
+#define tlckRELOCATE		0x0008	/* file/directory relocate */
+#define tlckENTRY		0x0001	/* directory insert/delete */
+#define tlckEXTEND		0x0002	/* directory extend in-line */
+#define tlckSPLIT		0x0010	/* splited page */
+#define tlckNEW			0x0020	/* new page from split */
+#define tlckFREE		0x0040	/* free page */
+#define tlckRELINK		0x0080	/* update sibling pointer */
+
+/*
+ *	linelock for lmLog()
+ *
+ * note: linelock_t and its variations are overlaid
+ * at tlock.lock: watch for alignment;
+ */
+typedef struct {
+	u8 offset;		/* 1: */
+	u8 length;		/* 1: */
+} lv_t;				/* (2) */
+
+#define	TLOCKSHORT	20
+#define	TLOCKLONG	28
+
+typedef struct {
+	u16 next;		/* 2: next linelock */
+
+	s8 maxcnt;		/* 1: */
+	s8 index;		/* 1: */
+
+	u16 flag;		/* 2: */
+	u8 type;		/* 1: */
+	u8 l2linesize;		/* 1: log2 of linesize */
+	/* (8) */
+
+	lv_t lv[20];		/* 40: */
+} linelock_t;			/* (48) */
+
+#define dtlock_t	linelock_t
+#define itlock_t	linelock_t
+
+typedef struct {
+	u16 next;		/* 2: */
+
+	s8 maxcnt;		/* 1: */
+	s8 index;		/* 1: */
+
+	u16 flag;		/* 2: */
+	u8 type;		/* 1: */
+	u8 l2linesize;		/* 1: log2 of linesize */
+				/* (8) */
+
+	lv_t header;		/* 2: */
+	lv_t lwm;		/* 2: low water mark */
+	lv_t hwm;		/* 2: high water mark */
+	lv_t twm;		/* 2: */
+				/* (16) */
+
+	s32 pxdlock[8];		/* 32: */
+} xtlock_t;			/* (48) */
+
+
+/*
+ *	maplock for txUpdateMap()
+ *
+ * note: maplock_t and its variations are overlaid
+ * at tlock.lock/linelock: watch for alignment;
+ * N.B. next field may be set by linelock, and should not
+ * be modified by maplock;
+ * N.B. index of the first pxdlock specifies index of next 
+ * free maplock (i.e., number of maplock) in the tlock; 
+ */
+typedef struct {
+	u16 next;		/* 2: */
+
+	u8 maxcnt;		/* 2: */
+	u8 index;		/* 2: next free maplock index */
+
+	u16 flag;		/* 2: */
+	u8 type;		/* 1: */
+	u8 count;		/* 1: number of pxd/xad */
+				/* (8) */
+
+	pxd_t pxd;		/* 8: */
+} maplock_t;			/* (16): */
+
+/* maplock flag */
+#define	mlckALLOC		0x00f0
+#define	mlckALLOCXADLIST	0x0080
+#define	mlckALLOCPXDLIST	0x0040
+#define	mlckALLOCXAD		0x0020
+#define	mlckALLOCPXD		0x0010
+#define	mlckFREE		0x000f
+#define	mlckFREEXADLIST		0x0008
+#define	mlckFREEPXDLIST		0x0004
+#define	mlckFREEXAD		0x0002
+#define	mlckFREEPXD		0x0001
+
+#define	pxdlock_t	maplock_t
+
+typedef struct {
+	u16 next;		/* 2: */
+
+	u8 maxcnt;		/* 2: */
+	u8 index;		/* 2: */
+
+	u16 flag;		/* 2: */
+	u8 type;		/* 1: */
+	u8 count;		/* 1: number of pxd/xad */
+				/* (8) */
+
+	void *xdlist;		/* 4: pxd/xad list */
+	s32 rsrvd;		/* 4: */
+} xdlistlock_t;			/* (16): */
+
+
+/*
+ *	commit
+ *
+ * parameter to the commit manager routines
+ */
+typedef struct commit {
+	tid_t tid;		/* 4: tid = index of tblock */
+	int flag;		/* 4: flags */
+	log_t *log;		/* 4: log */
+	struct super_block *sb;	/* 4: superblock */
+
+	int nip;		/* 4: number of entries in iplist */
+	struct inode **iplist;	/* 4: list of pointers to inodes */
+				/* (32) */
+
+	/* log record descriptor on 64-bit boundary */
+	lrd_t lrd;		/* : log record descriptor */
+} commit_t;
+
+/*
+ * external declarations
+ */
+extern tlock_t *txLock(tid_t tid, struct inode *ip, struct metapage *mp, int flag);
+
+extern tlock_t *txMaplock(tid_t tid, struct inode *ip, int flag);
+
+extern int txCommit(tid_t tid, int nip, struct inode **iplist, int flag);
+
+extern tid_t txBegin(struct super_block *sb, int flag);
+
+extern void txBeginAnon(struct super_block *sb);
+
+extern void txEnd(tid_t tid);
+
+extern void txAbort(tid_t tid, int dirty);
+
+extern linelock_t *txLinelock(linelock_t * tlock);
+
+extern void txFreeMap(struct inode *ip,
+		      maplock_t * maplock, tblock_t * tblk, int maptype);
+
+extern void txEA(tid_t tid, struct inode *ip, dxd_t * oldea, dxd_t * newea);
+
+extern void txFreelock(struct inode *ip);
+
+extern int lmLog(log_t * log, tblock_t * tblk, lrd_t * lrd, tlock_t * tlck);
+
+#endif				/* _H_JFS_TXNMGR */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_types.h linuxppc64_2_4/fs/jfs/jfs_types.h
--- linux-2.4.19/fs/jfs/jfs_types.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_types.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,188 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#ifndef _H_JFS_TYPES
+#define	_H_JFS_TYPES
+
+/*
+ *	jfs_types.h:
+ *
+ * basic type/utility  definitions
+ *
+ * note: this header file must be the 1st include file
+ * of JFS include list in all JFS .c file.
+ */
+
+#include <linux/types.h>
+#include <linux/nls.h>
+
+#include "endian24.h"
+#include "jfs_compat.h"
+
+/*
+ * transaction and lock id's
+ */
+typedef uint tid_t;
+typedef uint lid_t;
+
+/*
+ * Almost identical to Linux's timespec, but not quite
+ */
+struct timestruc_t {
+	u32 tv_sec;
+	u32 tv_nsec;
+};
+
+/*
+ *	handy
+ */
+
+#define LEFTMOSTONE	0x80000000
+#define	HIGHORDER	0x80000000u	/* high order bit on            */
+#define	ONES		0xffffffffu	/* all bit on                   */
+
+typedef int boolean_t;
+#define TRUE 1
+#define FALSE 0
+
+/*
+ *	logical xd (lxd)
+ */
+typedef struct {
+	unsigned len:24;
+	unsigned off1:8;
+	u32 off2;
+} lxd_t;
+
+/* lxd_t field construction */
+#define	LXDlength(lxd, length32)	( (lxd)->len = length32 )
+#define	LXDoffset(lxd, offset64)\
+{\
+	(lxd)->off1 = ((s64)offset64) >> 32;\
+	(lxd)->off2 = (offset64) & 0xffffffff;\
+}
+
+/* lxd_t field extraction */
+#define	lengthLXD(lxd)	( (lxd)->len )
+#define	offsetLXD(lxd)\
+	( ((s64)((lxd)->off1)) << 32 | (lxd)->off2 )
+
+/* lxd list */
+typedef struct {
+	s16 maxnlxd;
+	s16 nlxd;
+	lxd_t *lxd;
+} lxdlist_t;
+
+/*
+ *	physical xd (pxd)
+ */
+typedef struct {
+	unsigned len:24;
+	unsigned addr1:8;
+	u32 addr2;
+} pxd_t;
+
+/* xd_t field construction */
+
+#define	PXDlength(pxd, length32)	((pxd)->len = __cpu_to_le24(length32))
+#define	PXDaddress(pxd, address64)\
+{\
+	(pxd)->addr1 = ((s64)address64) >> 32;\
+	(pxd)->addr2 = __cpu_to_le32((address64) & 0xffffffff);\
+}
+
+/* xd_t field extraction */
+#define	lengthPXD(pxd)	__le24_to_cpu((pxd)->len)
+#define	addressPXD(pxd)\
+	( ((s64)((pxd)->addr1)) << 32 | __le32_to_cpu((pxd)->addr2))
+
+/* pxd list */
+typedef struct {
+	s16 maxnpxd;
+	s16 npxd;
+	pxd_t pxd[8];
+} pxdlist_t;
+
+
+/*
+ *	data extent descriptor (dxd)
+ */
+typedef struct {
+	unsigned flag:8;	/* 1: flags */
+	unsigned rsrvd:24;	/* 3: */
+	u32 size;		/* 4: size in byte */
+	unsigned len:24;	/* 3: length in unit of fsblksize */
+	unsigned addr1:8;	/* 1: address in unit of fsblksize */
+	u32 addr2;		/* 4: address in unit of fsblksize */
+} dxd_t;			/* - 16 - */
+
+/* dxd_t flags */
+#define	DXD_INDEX	0x80	/* B+-tree index */
+#define	DXD_INLINE	0x40	/* in-line data extent */
+#define	DXD_EXTENT	0x20	/* out-of-line single extent */
+#define	DXD_FILE	0x10	/* out-of-line file (inode) */
+#define DXD_CORRUPT	0x08	/* Inconsistency detected */
+
+/* dxd_t field construction
+ *	Conveniently, the PXD macros work for DXD
+ */
+#define	DXDlength	PXDlength
+#define	DXDaddress	PXDaddress
+#define	lengthDXD	lengthPXD
+#define	addressDXD	addressPXD
+
+/*
+ *      directory entry argument
+ */
+typedef struct component_name {
+	int namlen;
+	wchar_t *name;
+} component_t;
+
+
+/*
+ *	DASD limit information - stored in directory inode
+ */
+typedef struct dasd {
+	u8 thresh;		/* Alert Threshold (in percent) */
+	u8 delta;		/* Alert Threshold delta (in percent)   */
+	u8 rsrvd1;
+	u8 limit_hi;		/* DASD limit (in logical blocks)       */
+	u32 limit_lo;		/* DASD limit (in logical blocks)       */
+	u8 rsrvd2[3];
+	u8 used_hi;		/* DASD usage (in logical blocks)       */
+	u32 used_lo;		/* DASD usage (in logical blocks)       */
+} dasd_t;
+
+#define DASDLIMIT(dasdp) \
+	(((u64)((dasdp)->limit_hi) << 32) + __le32_to_cpu((dasdp)->limit_lo))
+#define setDASDLIMIT(dasdp, limit)\
+{\
+	(dasdp)->limit_hi = ((u64)limit) >> 32;\
+	(dasdp)->limit_lo = __cpu_to_le32(limit);\
+}
+#define DASDUSED(dasdp) \
+	(((u64)((dasdp)->used_hi) << 32) + __le32_to_cpu((dasdp)->used_lo))
+#define setDASDUSED(dasdp, used)\
+{\
+	(dasdp)->used_hi = ((u64)used) >> 32;\
+	(dasdp)->used_lo = __cpu_to_le32(used);\
+}
+
+#endif				/* !_H_JFS_TYPES */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_umount.c linuxppc64_2_4/fs/jfs/jfs_umount.c
--- linux-2.4.19/fs/jfs/jfs_umount.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_umount.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,158 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+/*
+ * Change History :
+ */
+
+/*
+ *	jfs_umount.c
+ *
+ * note: file system in transition to aggregate/fileset:
+ * (ref. jfs_mount.c)
+ *
+ * file system unmount is interpreted as mount of the single/only 
+ * fileset in the aggregate and, if unmount of the last fileset, 
+ * as unmount of the aggerate;
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_superblock.h"
+#include "jfs_dmap.h"
+#include "jfs_imap.h"
+#include "jfs_metapage.h"
+#include "jfs_debug.h"
+
+/*
+ * NAME:	jfs_umount(vfsp, flags, crp)
+ *
+ * FUNCTION:	vfs_umount()
+ *
+ * PARAMETERS:	vfsp	- virtual file system pointer
+ *		flags	- unmount for shutdown
+ *		crp	- credential
+ *
+ * RETURN :	EBUSY	- device has open files
+ */
+int jfs_umount(struct super_block *sb)
+{
+	int rc = 0;
+	log_t *log;
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	struct inode *ipbmap = sbi->ipbmap;
+	struct inode *ipimap = sbi->ipimap;
+	struct inode *ipaimap = sbi->ipaimap;
+	struct inode *ipaimap2 = sbi->ipaimap2;
+
+	jFYI(1, ("\n	UnMount JFS: sb:0x%p\n", sb));
+
+	/*
+	 *      update superblock and close log 
+	 *
+	 * if mounted read-write and log based recovery was enabled
+	 */
+	if ((log = sbi->log)) {
+		/*
+		 * close log: 
+		 *
+		 * remove file system from log active file system list.
+		 */
+		log = sbi->log;
+		rc = lmLogClose(sb, log);
+	}
+
+	/*
+	 * close fileset inode allocation map (aka fileset inode)
+	 */
+	jEVENT(0, ("jfs_umount: close ipimap:0x%p\n", ipimap));
+	diUnmount(ipimap, 0);
+
+	diFreeSpecial(ipimap);
+	sbi->ipimap = NULL;
+
+	/*
+	 * close secondary aggregate inode allocation map
+	 */
+	ipaimap2 = sbi->ipaimap2;
+	if (ipaimap2) {
+		jEVENT(0, ("jfs_umount: close ipaimap2:0x%p\n", ipaimap2));
+		diUnmount(ipaimap2, 0);
+		diFreeSpecial(ipaimap2);
+		sbi->ipaimap2 = NULL;
+	}
+
+	/*
+	 * close aggregate inode allocation map
+	 */
+	ipaimap = sbi->ipaimap;
+	jEVENT(0, ("jfs_umount: close ipaimap:0x%p\n", ipaimap));
+	diUnmount(ipaimap, 0);
+	diFreeSpecial(ipaimap);
+	sbi->ipaimap = NULL;
+
+	/*
+	 * close aggregate block allocation map
+	 */
+	jEVENT(0, ("jfs_umount: close ipbmap:%p\n", ipbmap));
+	dbUnmount(ipbmap, 0);
+
+	diFreeSpecial(ipbmap);
+	sbi->ipimap = NULL;
+
+	/*
+	 * ensure all file system file pages are propagated to their
+	 * home blocks on disk (and their in-memory buffer pages are 
+	 * invalidated) BEFORE updating file system superblock state
+	 * (to signify file system is unmounted cleanly, and thus in 
+	 * consistent state) and log superblock active file system 
+	 * list (to signify skip logredo()).
+	 */
+	if (log)		/* log = NULL if read-only mount */
+		rc = updateSuper(sb, FM_CLEAN);
+
+
+	jFYI(0, ("	UnMount JFS Complete: %d\n", rc));
+	return rc;
+}
+
+
+int jfs_umount_rw(struct super_block *sb)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+
+	if (!sbi->log)
+		return 0;
+
+	/*
+	 * close log: 
+	 *
+	 * remove file system from log active file system list.
+	 */
+	lmLogClose(sb, sbi->log);
+
+	dbSync(sbi->ipbmap);
+	diSync(sbi->ipimap);
+
+	sbi->log = 0;
+
+	return updateSuper(sb, FM_CLEAN);
+       
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_unicode.c linuxppc64_2_4/fs/jfs/jfs_unicode.c
--- linux-2.4.19/fs/jfs/jfs_unicode.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_unicode.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,110 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include "jfs_types.h"
+#include "jfs_filsys.h"
+#include "jfs_unicode.h"
+#include "jfs_debug.h"
+
+/*
+ * NAME:	jfs_strfromUCS()
+ *
+ * FUNCTION:	Convert little-endian unicode string to character string
+ *
+ */
+int jfs_strfromUCS_le(char *to, const wchar_t * from,	/* LITTLE ENDIAN */
+		      int len, struct nls_table *codepage)
+{
+	int i;
+	int outlen = 0;
+
+	for (i = 0; (i < len) && from[i]; i++) {
+		int charlen;
+		charlen =
+		    codepage->uni2char(le16_to_cpu(from[i]), &to[outlen],
+				       NLS_MAX_CHARSET_SIZE);
+		if (charlen > 0) {
+			outlen += charlen;
+		} else {
+			to[outlen++] = '?';
+		}
+	}
+	to[outlen] = 0;
+	jEVENT(0, ("jfs_strfromUCS returning %d - '%s'\n", outlen, to));
+	return outlen;
+}
+
+/*
+ * NAME:	jfs_strtoUCS()
+ *
+ * FUNCTION:	Convert character string to unicode string
+ *
+ */
+int jfs_strtoUCS(wchar_t * to,
+		 const char *from, int len, struct nls_table *codepage)
+{
+	int charlen;
+	int i;
+
+	jEVENT(0, ("jfs_strtoUCS - '%s'\n", from));
+
+	for (i = 0; len && *from; i++, from += charlen, len -= charlen) {
+		charlen = codepage->char2uni(from, len, &to[i]);
+		if (charlen < 1) {
+			jERROR(1, ("jfs_strtoUCS: char2uni returned %d.\n",
+				   charlen));
+			jERROR(1, ("charset = %s, char = 0x%x\n",
+				   codepage->charset, (unsigned char) *from));
+			to[i] = 0x003f;	/* a question mark */
+			charlen = 1;
+		}
+	}
+
+	jEVENT(0, (" returning %d\n", i));
+
+	to[i] = 0;
+	return i;
+}
+
+/*
+ * NAME:	get_UCSname()
+ *
+ * FUNCTION:	Allocate and translate to unicode string
+ *
+ */
+int get_UCSname(component_t * uniName, struct dentry *dentry,
+		struct nls_table *nls_tab)
+{
+	int length = dentry->d_name.len;
+
+	if (length > JFS_NAME_MAX)
+		return ENAMETOOLONG;
+
+	uniName->name =
+	    kmalloc((length + 1) * sizeof(wchar_t), GFP_NOFS);
+
+	if (uniName->name == NULL)
+		return ENOSPC;
+
+	uniName->namlen = jfs_strtoUCS(uniName->name, dentry->d_name.name,
+				       length, nls_tab);
+
+	return 0;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_unicode.h linuxppc64_2_4/fs/jfs/jfs_unicode.h
--- linux-2.4.19/fs/jfs/jfs_unicode.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_unicode.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,143 @@
+/*
+ * unistrk:  Unicode kernel case support
+ *
+ * Function:
+ *     Convert a unicode character to upper or lower case using
+ *     compressed tables.
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ *
+ */
+
+#include <asm/byteorder.h>
+#include "jfs_types.h"
+
+typedef struct {
+	wchar_t start;
+	wchar_t end;
+	signed char *table;
+} UNICASERANGE;
+
+extern signed char UniUpperTable[512];
+extern UNICASERANGE UniUpperRange[];
+extern int get_UCSname(component_t *, struct dentry *, struct nls_table *);
+extern int jfs_strfromUCS_le(char *, const wchar_t *, int, struct nls_table *);
+
+#define free_UCSname(COMP) kfree((COMP)->name)
+
+/*
+ * UniStrcpy:  Copy a string
+ */
+static inline wchar_t *UniStrcpy(wchar_t * ucs1, const wchar_t * ucs2)
+{
+	wchar_t *anchor = ucs1;	/* save the start of result string */
+
+	while ((*ucs1++ = *ucs2++));
+	return anchor;
+}
+
+
+
+/*
+ * UniStrncpy:  Copy length limited string with pad
+ */
+static inline wchar_t *UniStrncpy(wchar_t * ucs1, const wchar_t * ucs2,
+				  size_t n)
+{
+	wchar_t *anchor = ucs1;
+
+	while (n-- && *ucs2)	/* Copy the strings */
+		*ucs1++ = *ucs2++;
+
+	n++;
+	while (n--)		/* Pad with nulls */
+		*ucs1++ = 0;
+	return anchor;
+}
+
+/*
+ * UniStrncmp_le:  Compare length limited string - native to little-endian
+ */
+static inline int UniStrncmp_le(const wchar_t * ucs1, const wchar_t * ucs2,
+				size_t n)
+{
+	if (!n)
+		return 0;	/* Null strings are equal */
+	while ((*ucs1 == __le16_to_cpu(*ucs2)) && *ucs1 && --n) {
+		ucs1++;
+		ucs2++;
+	}
+	return (int) *ucs1 - (int) __le16_to_cpu(*ucs2);
+}
+
+/*
+ * UniStrncpy_le:  Copy length limited string with pad to little-endian
+ */
+static inline wchar_t *UniStrncpy_le(wchar_t * ucs1, const wchar_t * ucs2,
+				     size_t n)
+{
+	wchar_t *anchor = ucs1;
+
+	while (n-- && *ucs2)	/* Copy the strings */
+		*ucs1++ = __le16_to_cpu(*ucs2++);
+
+	n++;
+	while (n--)		/* Pad with nulls */
+		*ucs1++ = 0;
+	return anchor;
+}
+
+
+/*
+ * UniToupper:  Convert a unicode character to upper case
+ */
+static inline wchar_t UniToupper(register wchar_t uc)
+{
+	register UNICASERANGE *rp;
+
+	if (uc < sizeof(UniUpperTable)) {	/* Latin characters */
+		return uc + UniUpperTable[uc];	/* Use base tables */
+	} else {
+		rp = UniUpperRange;	/* Use range tables */
+		while (rp->start) {
+			if (uc < rp->start)	/* Before start of range */
+				return uc;	/* Uppercase = input */
+			if (uc <= rp->end)	/* In range */
+				return uc + rp->table[uc - rp->start];
+			rp++;	/* Try next range */
+		}
+	}
+	return uc;		/* Past last range */
+}
+
+
+/*
+ * UniStrupr:  Upper case a unicode string
+ */
+static inline wchar_t *UniStrupr(register wchar_t * upin)
+{
+	register wchar_t *up;
+
+	up = upin;
+	while (*up) {		/* For all characters */
+		*up = UniToupper(*up);
+		up++;
+	}
+	return upin;		/* Return input pointer */
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_uniupr.c linuxppc64_2_4/fs/jfs/jfs_uniupr.c
--- linux-2.4.19/fs/jfs/jfs_uniupr.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_uniupr.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,137 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * jfs_uniupr.c - Unicode compressed case ranges
+ *
+*/
+
+#include <linux/fs.h>
+#include "jfs_unicode.h"
+
+/*
+ * Latin upper case
+ */
+signed char UniUpperTable[512] = {
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 000-00f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 010-01f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 020-02f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 030-03f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 040-04f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 050-05f */
+   0,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* 060-06f */
+ -32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,  0,  0,  0,  0,  0, /* 070-07f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 080-08f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 090-09f */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 0a0-0af */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 0b0-0bf */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 0c0-0cf */
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 0d0-0df */
+ -32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* 0e0-0ef */
+ -32,-32,-32,-32,-32,-32,-32,  0,-32,-32,-32,-32,-32,-32,-32,121, /* 0f0-0ff */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 100-10f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 110-11f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 120-12f */
+   0,  0,  0, -1,  0, -1,  0, -1,  0,  0, -1,  0, -1,  0, -1,  0, /* 130-13f */
+  -1,  0, -1,  0, -1,  0, -1,  0, -1,  0,  0, -1,  0, -1,  0, -1, /* 140-14f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 150-15f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 160-16f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0,  0, -1,  0, -1,  0, -1,  0, /* 170-17f */
+   0,  0,  0, -1,  0, -1,  0,  0, -1,  0,  0,  0, -1,  0,  0,  0, /* 180-18f */
+   0,  0, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0, /* 190-19f */
+   0, -1,  0, -1,  0, -1,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0, /* 1a0-1af */
+  -1,  0,  0,  0, -1,  0, -1,  0,  0, -1,  0,  0,  0, -1,  0,  0, /* 1b0-1bf */
+   0,  0,  0,  0,  0, -1, -2,  0, -1, -2,  0, -1, -2,  0, -1,  0, /* 1c0-1cf */
+  -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,-79,  0, -1, /* 1d0-1df */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e0-1ef */
+   0,  0, -1, -2,  0, -1,  0,  0,  0, -1,  0, -1,  0, -1,  0, -1, /* 1f0-1ff */
+};
+
+/* Upper case range - Greek */
+static signed char UniCaseRangeU03a0[47] = {
+   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,-38,-37,-37,-37, /* 3a0-3af */
+   0,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* 3b0-3bf */
+ -32,-32,-31,-32,-32,-32,-32,-32,-32,-32,-32,-32,-64,-63,-63,
+};
+
+/* Upper case range - Cyrillic */
+static signed char UniCaseRangeU0430[48] = {
+ -32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* 430-43f */
+ -32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* 440-44f */
+   0,-80,-80,-80,-80,-80,-80,-80,-80,-80,-80,-80,-80,  0,-80,-80, /* 450-45f */
+};
+
+/* Upper case range - Extended cyrillic */
+static signed char UniCaseRangeU0490[61] = {
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 490-49f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 4a0-4af */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 4b0-4bf */
+   0,  0, -1,  0, -1,  0,  0,  0, -1,  0,  0,  0, -1,
+};
+
+/* Upper case range - Extended latin and greek */
+static signed char UniCaseRangeU1e00[509] = {
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e00-1e0f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e10-1e1f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e20-1e2f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e30-1e3f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e40-1e4f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e50-1e5f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e60-1e6f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e70-1e7f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1e80-1e8f */
+   0, -1,  0, -1,  0, -1,  0,  0,  0,  0,  0,-59,  0, -1,  0, -1, /* 1e90-1e9f */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1ea0-1eaf */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1eb0-1ebf */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1ec0-1ecf */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1ed0-1edf */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1, /* 1ee0-1eef */
+   0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0, /* 1ef0-1eff */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f00-1f0f */
+   8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f10-1f1f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f20-1f2f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f30-1f3f */
+   8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f40-1f4f */
+   0,  8,  0,  8,  0,  8,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f50-1f5f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f60-1f6f */
+  74, 74, 86, 86, 86, 86,100,100,  0,  0,112,112,126,126,  0,  0, /* 1f70-1f7f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f80-1f8f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1f90-1f9f */
+   8,  8,  8,  8,  8,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0,  0, /* 1fa0-1faf */
+   8,  8,  0,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1fb0-1fbf */
+   0,  0,  0,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1fc0-1fcf */
+   8,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1fd0-1fdf */
+   8,  8,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, /* 1fe0-1fef */
+   0,  0,  0,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,
+};
+
+/* Upper case range - Wide latin */
+static signed char UniCaseRangeUff40[27] = {
+   0,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32, /* ff40-ff4f */
+ -32,-32,-32,-32,-32,-32,-32,-32,-32,-32,-32,
+};
+
+/*
+ * Upper Case Range
+ */
+UNICASERANGE UniUpperRange[] = {
+    { 0x03a0,  0x03ce,  UniCaseRangeU03a0 },
+    { 0x0430,  0x045f,  UniCaseRangeU0430 },
+    { 0x0490,  0x04cc,  UniCaseRangeU0490 },
+    { 0x1e00,  0x1ffc,  UniCaseRangeU1e00 },
+    { 0xff40,  0xff5a,  UniCaseRangeUff40 },
+    { 0, 0, 0 }
+};
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_xtree.c linuxppc64_2_4/fs/jfs/jfs_xtree.c
--- linux-2.4.19/fs/jfs/jfs_xtree.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_xtree.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,4444 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+/*
+ *      jfs_xtree.c: extent allocation descriptor B+-tree manager
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_dmap.h"
+#include "jfs_dinode.h"
+#include "jfs_superblock.h"
+#include "jfs_debug.h"
+
+/*
+ * xtree local flag
+ */
+#define XT_INSERT       0x00000001
+
+/*
+ *       xtree key/entry comparison: extent offset
+ *
+ * return:
+ *      -1: k < start of extent
+ *       0: start_of_extent <= k <= end_of_extent
+ *       1: k > end_of_extent
+ */
+#define XT_CMP(CMP, K, X, OFFSET64)\
+{\
+        OFFSET64 = offsetXAD(X);\
+        (CMP) = ((K) >= OFFSET64 + lengthXAD(X)) ? 1 :\
+              ((K) < OFFSET64) ? -1 : 0;\
+}
+
+/* write a xad entry */
+#define XT_PUTENTRY(XAD, FLAG, OFF, LEN, ADDR)\
+{\
+        (XAD)->flag = (FLAG);\
+        XADoffset((XAD), (OFF));\
+        XADlength((XAD), (LEN));\
+        XADaddress((XAD), (ADDR));\
+}
+
+#define XT_PAGE(IP, MP) BT_PAGE(IP, MP, xtpage_t, i_xtroot)
+
+/* get page buffer for specified block address */
+#define XT_GETPAGE(IP, BN, MP, SIZE, P, RC)\
+{\
+        BT_GETPAGE(IP, BN, MP, xtpage_t, SIZE, P, RC, i_xtroot)\
+        if (!(RC))\
+        {\
+                if ((le16_to_cpu((P)->header.nextindex) < XTENTRYSTART) ||\
+                    (le16_to_cpu((P)->header.nextindex) > le16_to_cpu((P)->header.maxentry)) ||\
+                    (le16_to_cpu((P)->header.maxentry) > (((BN)==0)?XTROOTMAXSLOT:PSIZE>>L2XTSLOTSIZE)))\
+                {\
+                        jERROR(1,("XT_GETPAGE: xtree page corrupt\n"));\
+			BT_PUTPAGE(MP);\
+			updateSuper((IP)->i_sb, FM_DIRTY);\
+			MP = NULL;\
+                        RC = EIO;\
+                }\
+        }\
+}
+
+/* for consistency */
+#define XT_PUTPAGE(MP) BT_PUTPAGE(MP)
+
+#define XT_GETSEARCH(IP, LEAF, BN, MP,  P, INDEX) \
+	BT_GETSEARCH(IP, LEAF, BN, MP, xtpage_t, P, INDEX, i_xtroot)
+/* xtree entry parameter descriptor */
+typedef struct {
+	metapage_t *mp;
+	s16 index;
+	u8 flag;
+	s64 off;
+	s64 addr;
+	int len;
+	pxdlist_t *pxdlist;
+} xtsplit_t;
+
+
+/*
+ *      statistics
+ */
+#ifdef CONFIG_JFS_STATISTICS
+static struct {
+	uint search;
+	uint fastSearch;
+	uint split;
+} xtStat;
+#endif
+
+
+/*
+ * forward references
+ */
+static int xtSearch(struct inode *ip,
+		    s64 xoff, int *cmpp, btstack_t * btstack, int flag);
+
+static int xtSplitUp(tid_t tid,
+		     struct inode *ip,
+		     xtsplit_t * split, btstack_t * btstack);
+
+static int xtSplitPage(tid_t tid,
+		       struct inode *ip,
+		       xtsplit_t * split, metapage_t ** rmpp, s64 * rbnp);
+
+static int xtSplitRoot(tid_t tid,
+		       struct inode *ip,
+		       xtsplit_t * split, metapage_t ** rmpp);
+
+#ifdef _STILL_TO_PORT
+static int xtDeleteUp(tid_t tid,
+		      struct inode *ip,
+		      metapage_t * fmp,
+		      xtpage_t * fp, btstack_t * btstack);
+
+static int xtSearchNode(struct inode *ip,
+			xad_t * xad,
+			int *cmpp, btstack_t * btstack, int flag);
+
+static int xtRelink(tid_t tid, struct inode *ip, xtpage_t * fp);
+#endif				/*  _STILL_TO_PORT */
+
+/* External references */
+
+/*
+ *      debug control
+ */
+/*      #define _JFS_DEBUG_XTREE        1 */
+
+
+/*
+ *      xtLookup()
+ *
+ * function: map a single page into a physical extent;
+ */
+int xtLookup(struct inode *ip, s64 lstart,
+	     s64 llen, int *pflag, s64 * paddr, s32 * plen, int no_check)
+{
+	int rc = 0;
+	btstack_t btstack;
+	int cmp;
+	s64 bn;
+	metapage_t *mp;
+	xtpage_t *p;
+	int index;
+	xad_t *xad;
+	s64 size, xoff, xend;
+	int xlen;
+	s64 xaddr;
+
+	*plen = 0;
+
+	if (!no_check) {
+		/* is lookup offset beyond eof ? */
+		size = ((u64) ip->i_size + (JFS_SBI(ip->i_sb)->bsize - 1)) >>
+		    JFS_SBI(ip->i_sb)->l2bsize;
+		if (lstart >= size) {
+			jERROR(1,
+			       ("xtLookup: lstart (0x%lx) >= size (0x%lx)\n",
+				(ulong) lstart, (ulong) size));
+			return 0;
+		}
+	}
+
+	/*
+	 * search for the xad entry covering the logical extent
+	 */
+//search:
+	if ((rc = xtSearch(ip, lstart, &cmp, &btstack, 0))) {
+		jERROR(1, ("xtLookup: xtSearch returned %d\n", rc));
+		return rc;
+	}
+
+	/*
+	 *      compute the physical extent covering logical extent
+	 *
+	 * N.B. search may have failed (e.g., hole in sparse file),
+	 * and returned the index of the next entry.
+	 */
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/* is xad found covering start of logical extent ?
+	 * lstart is a page start address,
+	 * i.e., lstart cannot start in a hole;
+	 */
+	if (cmp) {
+		jFYI(1, ("xtLookup: cmp = %d\n", cmp));
+		goto out;
+	}
+
+	/*
+	 * lxd covered by xad
+	 */
+	xad = &p->xad[index];
+	xoff = offsetXAD(xad);
+	xlen = lengthXAD(xad);
+	xend = xoff + xlen;
+	xaddr = addressXAD(xad);
+
+	jEVENT(0,
+	       ("index = %d, xoff = 0x%lx, xlen = 0x%x, xaddr = 0x%lx\n",
+		index, (ulong) xoff, xlen, (ulong) xaddr));
+
+	/* initialize new pxd */
+	*pflag = xad->flag;
+	*paddr = xaddr + (lstart - xoff);
+	/* a page must be fully covered by an xad */
+	*plen = min(xend - lstart, llen);
+
+      out:
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *      xtLookupList()
+ *
+ * function: map a single logical extent into a list of physical extent;
+ *
+ * parameter:
+ *      struct inode    *ip,
+ *      lxdlist_t       *lxdlist,       lxd list (in)
+ *      xadlist_t       *xadlist,       xad list (in/out)
+ *      int		flag)
+ *
+ * coverage of lxd by xad under assumption of
+ * . lxd's are ordered and disjoint.
+ * . xad's are ordered and disjoint.
+ *
+ * return:
+ *      0:      success
+ *
+ * note: a page being written (even a single byte) is backed fully,
+ *      except the last page which is only backed with blocks
+ *      required to cover the last byte;
+ *      the extent backing a page is fully contained within an xad;
+ */
+int xtLookupList(struct inode *ip, lxdlist_t * lxdlist,	/* lxd list (in) */
+		 xadlist_t * xadlist,	/* xad list (in/out) */
+		 int flag)
+{
+	int rc = 0;
+	btstack_t btstack;
+	int cmp;
+	s64 bn;
+	metapage_t *mp;
+	xtpage_t *p;
+	int index;
+	lxd_t *lxd;
+	xad_t *xad, *pxd;
+	s64 size, lstart, lend, xstart, xend, pstart;
+	s64 llen, xlen, plen;
+	s64 xaddr, paddr;
+	int nlxd, npxd, maxnpxd;
+
+	npxd = xadlist->nxad = 0;
+	maxnpxd = xadlist->maxnxad;
+	pxd = xadlist->xad;
+
+	nlxd = lxdlist->nlxd;
+	lxd = lxdlist->lxd;
+
+	lstart = offsetLXD(lxd);
+	llen = lengthLXD(lxd);
+	lend = lstart + llen;
+
+	size = (ip->i_size + (JFS_SBI(ip->i_sb)->bsize - 1)) >>
+	    JFS_SBI(ip->i_sb)->l2bsize;
+
+	/*
+	 * search for the xad entry covering the logical extent
+	 */
+      search:
+	if (lstart >= size)
+		return 0;
+
+	if ((rc = xtSearch(ip, lstart, &cmp, &btstack, 0)))
+		return rc;
+
+	/*
+	 *      compute the physical extent covering logical extent
+	 *
+	 * N.B. search may have failed (e.g., hole in sparse file),
+	 * and returned the index of the next entry.
+	 */
+//map:
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/* is xad on the next sibling page ? */
+	if (index == le16_to_cpu(p->header.nextindex)) {
+		if (p->header.flag & BT_ROOT)
+			goto mapend;
+
+		if ((bn = le64_to_cpu(p->header.next)) == 0)
+			goto mapend;
+
+		XT_PUTPAGE(mp);
+
+		/* get next sibling page */
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		index = XTENTRYSTART;
+	}
+
+	xad = &p->xad[index];
+
+	/*
+	 * is lxd covered by xad ?
+	 */
+      compare:
+	xstart = offsetXAD(xad);
+	xlen = lengthXAD(xad);
+	xend = xstart + xlen;
+	xaddr = addressXAD(xad);
+
+      compare1:
+	if (xstart < lstart)
+		goto compare2;
+
+	/* (lstart <= xstart) */
+
+	/* lxd is NOT covered by xad */
+	if (lend <= xstart) {
+		/*
+		 * get next lxd
+		 */
+		if (--nlxd == 0)
+			goto mapend;
+		lxd++;
+
+		lstart = offsetLXD(lxd);
+		llen = lengthLXD(lxd);
+		lend = lstart + llen;
+		if (lstart >= size)
+			goto mapend;
+
+		/* compare with the current xad  */
+		goto compare1;
+	}
+	/* lxd is covered by xad */
+	else {			/* (xstart < lend) */
+
+		/* initialize new pxd */
+		pstart = xstart;
+		plen = min(lend - xstart, xlen);
+		paddr = xaddr;
+
+		goto cover;
+	}
+
+	/* (xstart < lstart) */
+      compare2:
+	/* lxd is covered by xad */
+	if (lstart < xend) {
+		/* initialize new pxd */
+		pstart = lstart;
+		plen = min(xend - lstart, llen);
+		paddr = xaddr + (lstart - xstart);
+
+		goto cover;
+	}
+	/* lxd is NOT covered by xad */
+	else {			/* (xend <= lstart) */
+
+		/*
+		 * get next xad
+		 *
+		 * linear search next xad covering lxd on
+		 * the current xad page, and then tree search
+		 */
+		if (index == le16_to_cpu(p->header.nextindex) - 1) {
+			if (p->header.flag & BT_ROOT)
+				goto mapend;
+
+			XT_PUTPAGE(mp);
+			goto search;
+		} else {
+			index++;
+			xad++;
+
+			/* compare with new xad */
+			goto compare;
+		}
+	}
+
+	/*
+	 * lxd is covered by xad and a new pxd has been initialized
+	 * (lstart <= xstart < lend) or (xstart < lstart < xend)
+	 */
+      cover:
+	/* finalize pxd corresponding to current xad */
+	XT_PUTENTRY(pxd, xad->flag, pstart, plen, paddr);
+
+	if (++npxd >= maxnpxd)
+		goto mapend;
+	pxd++;
+
+	/*
+	 * lxd is fully covered by xad
+	 */
+	if (lend <= xend) {
+		/*
+		 * get next lxd
+		 */
+		if (--nlxd == 0)
+			goto mapend;
+		lxd++;
+
+		lstart = offsetLXD(lxd);
+		llen = lengthLXD(lxd);
+		lend = lstart + llen;
+		if (lstart >= size)
+			goto mapend;
+
+		/*
+		 * test for old xad covering new lxd
+		 * (old xstart < new lstart)
+		 */
+		goto compare2;
+	}
+	/*
+	 * lxd is partially covered by xad
+	 */
+	else {			/* (xend < lend)  */
+
+		/*
+		 * get next xad
+		 *
+		 * linear search next xad covering lxd on
+		 * the current xad page, and then next xad page search
+		 */
+		if (index == le16_to_cpu(p->header.nextindex) - 1) {
+			if (p->header.flag & BT_ROOT)
+				goto mapend;
+
+			if ((bn = le64_to_cpu(p->header.next)) == 0)
+				goto mapend;
+
+			XT_PUTPAGE(mp);
+
+			/* get next sibling page */
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+			if (rc)
+				return rc;
+
+			index = XTENTRYSTART;
+			xad = &p->xad[index];
+		} else {
+			index++;
+			xad++;
+		}
+
+		/*
+		 * test for new xad covering old lxd
+		 * (old lstart < new xstart)
+		 */
+		goto compare;
+	}
+
+      mapend:
+	xadlist->nxad = npxd;
+
+//out:
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *      xtSearch()
+ *
+ * function:    search for the xad entry covering specified offset.
+ *
+ * parameters:
+ *      ip      - file object;
+ *      xoff    - extent offset;
+ *      cmpp    - comparison result:
+ *      btstack - traverse stack;
+ *      flag    - search process flag (XT_INSERT);
+ *
+ * returns:
+ *      btstack contains (bn, index) of search path traversed to the entry.
+ *      *cmpp is set to result of comparison with the entry returned.
+ *      the page containing the entry is pinned at exit.
+ */
+static int xtSearch(struct inode *ip, s64 xoff,	/* offset of extent */
+		    int *cmpp, btstack_t * btstack, int flag)
+{
+	struct jfs_inode_info *jfs_ip = JFS_IP(ip);
+	int rc = 0;
+	int cmp = 1;		/* init for empty page */
+	s64 bn;			/* block number */
+	metapage_t *mp;		/* page buffer */
+	xtpage_t *p;		/* page */
+	xad_t *xad;
+	int base, index, lim, btindex;
+	btframe_t *btsp;
+	int nsplit = 0;		/* number of pages to split */
+	s64 t64;
+
+	INCREMENT(xtStat.search);
+
+	BT_CLR(btstack);
+
+	btstack->nsplit = 0;
+
+	/*
+	 *      search down tree from root:
+	 *
+	 * between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
+	 * internal page, child page Pi contains entry with k, Ki <= K < Kj.
+	 *
+	 * if entry with search key K is not found
+	 * internal page search find the entry with largest key Ki
+	 * less than K which point to the child page to search;
+	 * leaf page search find the entry with smallest key Kj
+	 * greater than K so that the returned index is the position of
+	 * the entry to be shifted right for insertion of new entry.
+	 * for empty tree, search key is greater than any key of the tree.
+	 *
+	 * by convention, root bn = 0.
+	 */
+	for (bn = 0;;) {
+		/* get/pin the page to search */
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/* try sequential access heuristics with the previous
+		 * access entry in target leaf page:
+		 * once search narrowed down into the target leaf,
+		 * key must either match an entry in the leaf or
+		 * key entry does not exist in the tree;
+		 */
+//fastSearch:
+		if ((jfs_ip->btorder & BT_SEQUENTIAL) &&
+		    (p->header.flag & BT_LEAF) &&
+		    (index = jfs_ip->btindex) <
+		    le16_to_cpu(p->header.nextindex)) {
+			xad = &p->xad[index];
+			t64 = offsetXAD(xad);
+			if (xoff < t64 + lengthXAD(xad)) {
+				if (xoff >= t64) {
+					*cmpp = 0;
+					goto out;
+				}
+
+				/* stop sequential access heuristics */
+				goto binarySearch;
+			} else {	/* (t64 + lengthXAD(xad)) <= xoff */
+
+				/* try next sequential entry */
+				index++;
+				if (index <
+				    le16_to_cpu(p->header.nextindex)) {
+					xad++;
+					t64 = offsetXAD(xad);
+					if (xoff < t64 + lengthXAD(xad)) {
+						if (xoff >= t64) {
+							*cmpp = 0;
+							goto out;
+						}
+
+						/* miss: key falls between
+						 * previous and this entry
+						 */
+						*cmpp = 1;
+						goto out;
+					}
+
+					/* (xoff >= t64 + lengthXAD(xad));
+					 * matching entry may be further out:
+					 * stop heuristic search
+					 */
+					/* stop sequential access heuristics */
+					goto binarySearch;
+				}
+
+				/* (index == p->header.nextindex);
+				 * miss: key entry does not exist in
+				 * the target leaf/tree
+				 */
+				*cmpp = 1;
+				goto out;
+			}
+
+			/*
+			 * if hit, return index of the entry found, and
+			 * if miss, where new entry with search key is
+			 * to be inserted;
+			 */
+		      out:
+			/* compute number of pages to split */
+			if (flag & XT_INSERT) {
+				if (p->header.nextindex ==	/* little-endian */
+				    p->header.maxentry)
+					nsplit++;
+				else
+					nsplit = 0;
+				btstack->nsplit = nsplit;
+			}
+
+			/* save search result */
+			btsp = btstack->top;
+			btsp->bn = bn;
+			btsp->index = index;
+			btsp->mp = mp;
+
+			/* update sequential access heuristics */
+			jfs_ip->btindex = index;
+
+			INCREMENT(xtStat.fastSearch);
+			return 0;
+		}
+
+		/* well, ... full search now */
+	      binarySearch:
+		lim = le16_to_cpu(p->header.nextindex) - XTENTRYSTART;
+
+		/*
+		 * binary search with search key K on the current page
+		 */
+		for (base = XTENTRYSTART; lim; lim >>= 1) {
+			index = base + (lim >> 1);
+
+			XT_CMP(cmp, xoff, &p->xad[index], t64);
+			if (cmp == 0) {
+				/*
+				 *      search hit
+				 */
+				/* search hit - leaf page:
+				 * return the entry found
+				 */
+				if (p->header.flag & BT_LEAF) {
+					*cmpp = cmp;
+
+					/* compute number of pages to split */
+					if (flag & XT_INSERT) {
+						if (p->header.nextindex ==
+						    p->header.maxentry)
+							nsplit++;
+						else
+							nsplit = 0;
+						btstack->nsplit = nsplit;
+					}
+
+					/* save search result */
+					btsp = btstack->top;
+					btsp->bn = bn;
+					btsp->index = index;
+					btsp->mp = mp;
+
+					/* init sequential access heuristics */
+					btindex = jfs_ip->btindex;
+					if (index == btindex ||
+					    index == btindex + 1)
+						jfs_ip->btorder = BT_SEQUENTIAL;
+					else
+						jfs_ip->btorder = BT_RANDOM;
+					jfs_ip->btindex = index;
+
+					return 0;
+				}
+
+				/* search hit - internal page:
+				 * descend/search its child page
+				 */
+				goto next;
+			}
+
+			if (cmp > 0) {
+				base = index + 1;
+				--lim;
+			}
+		}
+
+		/*
+		 *      search miss
+		 *
+		 * base is the smallest index with key (Kj) greater than
+		 * search key (K) and may be zero or maxentry index.
+		 */
+		/*
+		 * search miss - leaf page:
+		 *
+		 * return location of entry (base) where new entry with
+		 * search key K is to be inserted.
+		 */
+		if (p->header.flag & BT_LEAF) {
+			*cmpp = cmp;
+
+			/* compute number of pages to split */
+			if (flag & XT_INSERT) {
+				if (p->header.nextindex ==
+				    p->header.maxentry)
+					nsplit++;
+				else
+					nsplit = 0;
+				btstack->nsplit = nsplit;
+			}
+
+			/* save search result */
+			btsp = btstack->top;
+			btsp->bn = bn;
+			btsp->index = base;
+			btsp->mp = mp;
+
+			/* init sequential access heuristics */
+			btindex = jfs_ip->btindex;
+			if (base == btindex || base == btindex + 1)
+				jfs_ip->btorder = BT_SEQUENTIAL;
+			else
+				jfs_ip->btorder = BT_RANDOM;
+			jfs_ip->btindex = base;
+
+			return 0;
+		}
+
+		/*
+		 * search miss - non-leaf page:
+		 *
+		 * if base is non-zero, decrement base by one to get the parent
+		 * entry of the child page to search.
+		 */
+		index = base ? base - 1 : base;
+
+		/*
+		 * go down to child page
+		 */
+	      next:
+		/* update number of pages to split */
+		if (p->header.nextindex == p->header.maxentry)
+			nsplit++;
+		else
+			nsplit = 0;
+
+		/* push (bn, index) of the parent page/entry */
+		BT_PUSH(btstack, bn, index);
+
+		/* get the child page block number */
+		bn = addressXAD(&p->xad[index]);
+
+		/* unpin the parent page */
+		XT_PUTPAGE(mp);
+	}
+}
+
+/*
+ *      xtInsert()
+ *
+ * function:
+ *
+ * parameter:
+ *      tid     - transaction id;
+ *      ip      - file object;
+ *      xflag   - extent flag (XAD_NOTRECORDED):
+ *      xoff    - extent offset;
+ *      xlen    - extent length;
+ *      xaddrp  - extent address pointer (in/out):
+ *              if (*xaddrp)
+ *                      caller allocated data extent at *xaddrp;
+ *              else
+ *                      allocate data extent and return its xaddr;
+ *      flag    -
+ *
+ * return:
+ */
+int xtInsert(tid_t tid,		/* transaction id */
+	     struct inode *ip, int xflag, s64 xoff, s32 xlen, s64 * xaddrp,
+	     int flag)
+{
+	int rc = 0;
+	s64 xaddr, hint;
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* base B+-tree index page */
+	s64 bn;
+	int index, nextindex;
+	btstack_t btstack;	/* traverse stack */
+	xtsplit_t split;	/* split information */
+	xad_t *xad;
+	int cmp;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+
+	jFYI(1,
+	     ("xtInsert: nxoff:0x%lx nxlen:0x%x\n", (ulong) xoff, xlen));
+
+	/*
+	 *      search for the entry location at which to insert:
+	 *
+	 * xtFastSearch() and xtSearch() both returns (leaf page
+	 * pinned, index at which to insert).
+	 * n.b. xtSearch() may return index of maxentry of
+	 * the full page.
+	 */
+	if ((rc = xtSearch(ip, xoff, &cmp, &btstack, XT_INSERT)))
+		return rc;
+
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/* This test must follow XT_GETSEARCH since mp must be valid if
+	 * we branch to out: */
+	if (cmp == 0) {
+		rc = EEXIST;
+		goto out;
+	}
+
+	/*
+	 * allocate data extent requested
+	 *
+	 * allocation hint: last xad
+	 */
+	if ((xaddr = *xaddrp) == 0) {
+		if (index > XTENTRYSTART) {
+			xad = &p->xad[index - 1];
+			hint = addressXAD(xad) + lengthXAD(xad) - 1;
+		} else
+			hint = 0;
+		if ((rc = dbAlloc(ip, hint, (s64) xlen, &xaddr)))
+			goto out;
+	}
+
+	/*
+	 *      insert entry for new extent
+	 */
+	xflag |= XAD_NEW;
+
+	/*
+	 *      if the leaf page is full, split the page and
+	 *      propagate up the router entry for the new page from split
+	 *
+	 * The xtSplitUp() will insert the entry and unpin the leaf page.
+	 */
+	nextindex = le16_to_cpu(p->header.nextindex);
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+		split.mp = mp;
+		split.index = index;
+		split.flag = xflag;
+		split.off = xoff;
+		split.len = xlen;
+		split.addr = xaddr;
+		split.pxdlist = NULL;
+		if ((rc = xtSplitUp(tid, ip, &split, &btstack))) {
+			/* undo data extent allocation */
+			if (*xaddrp == 0)
+				dbFree(ip, xaddr, (s64) xlen);
+			return rc;
+		}
+
+		*xaddrp = xaddr;
+		return 0;
+	}
+
+	/*
+	 *      insert the new entry into the leaf page
+	 */
+	/*
+	 * acquire a transaction lock on the leaf page;
+	 *
+	 * action: xad insertion/extension;
+	 */
+	BT_MARK_DIRTY(mp, ip);
+
+	/* if insert into middle, shift right remaining entries. */
+	if (index < nextindex)
+		memmove(&p->xad[index + 1], &p->xad[index],
+			(nextindex - index) * sizeof(xad_t));
+
+	/* insert the new entry: mark the entry NEW */
+	xad = &p->xad[index];
+	XT_PUTENTRY(xad, xflag, xoff, xlen, xaddr);
+
+	/* advance next available entry index */
+	p->header.nextindex =
+	    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+
+	/* Don't log it if there are no links to the file */
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+		xtlck = (xtlock_t *) & tlck->lock;
+		xtlck->lwm.offset =
+		    (xtlck->lwm.offset) ? min(index,
+					      (int)xtlck->lwm.offset) : index;
+		xtlck->lwm.length =
+		    le16_to_cpu(p->header.nextindex) - xtlck->lwm.offset;
+	}
+
+	*xaddrp = xaddr;
+
+      out:
+	/* unpin the leaf page */
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *      xtSplitUp()
+ *
+ * function:
+ *      split full pages as propagating insertion up the tree
+ *
+ * parameter:
+ *      tid     - transaction id;
+ *      ip      - file object;
+ *      split   - entry parameter descriptor;
+ *      btstack - traverse stack from xtSearch()
+ *
+ * return:
+ */
+static int
+xtSplitUp(tid_t tid,
+	  struct inode *ip, xtsplit_t * split, btstack_t * btstack)
+{
+	int rc = 0;
+	metapage_t *smp;
+	xtpage_t *sp;		/* split page */
+	metapage_t *rmp;
+	s64 rbn;		/* new right page block number */
+	metapage_t *rcmp;
+	xtpage_t *rcp;		/* right child page */
+	s64 rcbn;		/* right child page block number */
+	int skip;		/* index of entry of insertion */
+	int nextindex;		/* next available entry index of p */
+	btframe_t *parent;	/* parent page entry on traverse stack */
+	xad_t *xad;
+	s64 xaddr;
+	int xlen;
+	int nsplit;		/* number of pages split */
+	pxdlist_t pxdlist;
+	pxd_t *pxd;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+
+	smp = split->mp;
+	sp = XT_PAGE(ip, smp);
+
+	/* is inode xtree root extension/inline EA area free ? */
+	if ((sp->header.flag & BT_ROOT) && (!S_ISDIR(ip->i_mode)) &&
+	    (sp->header.maxentry < cpu_to_le16(XTROOTMAXSLOT)) &&
+	    (JFS_IP(ip)->mode2 & INLINEEA)) {
+		sp->header.maxentry = cpu_to_le16(XTROOTMAXSLOT);
+		JFS_IP(ip)->mode2 &= ~INLINEEA;
+
+		BT_MARK_DIRTY(smp, ip);
+		/*
+		 * acquire a transaction lock on the leaf page;
+		 *
+		 * action: xad insertion/extension;
+		 */
+
+		/* if insert into middle, shift right remaining entries. */
+		skip = split->index;
+		nextindex = le16_to_cpu(sp->header.nextindex);
+		if (skip < nextindex)
+			memmove(&sp->xad[skip + 1], &sp->xad[skip],
+				(nextindex - skip) * sizeof(xad_t));
+
+		/* insert the new entry: mark the entry NEW */
+		xad = &sp->xad[skip];
+		XT_PUTENTRY(xad, split->flag, split->off, split->len,
+			    split->addr);
+
+		/* advance next available entry index */
+		sp->header.nextindex =
+		    cpu_to_le16(le16_to_cpu(sp->header.nextindex) + 1);
+
+		/* Don't log it if there are no links to the file */
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			tlck = txLock(tid, ip, smp, tlckXTREE | tlckGROW);
+			xtlck = (xtlock_t *) & tlck->lock;
+			xtlck->lwm.offset = (xtlck->lwm.offset) ?
+			    min(skip, (int)xtlck->lwm.offset) : skip;
+			xtlck->lwm.length =
+			    le16_to_cpu(sp->header.nextindex) -
+			    xtlck->lwm.offset;
+		}
+
+		return 0;
+	}
+
+	/*
+	 * allocate new index blocks to cover index page split(s)
+	 *
+	 * allocation hint: ?
+	 */
+	if (split->pxdlist == NULL) {
+		nsplit = btstack->nsplit;
+		split->pxdlist = &pxdlist;
+		pxdlist.maxnpxd = pxdlist.npxd = 0;
+		pxd = &pxdlist.pxd[0];
+		xlen = JFS_SBI(ip->i_sb)->nbperpage;
+		for (; nsplit > 0; nsplit--, pxd++) {
+			if ((rc = dbAlloc(ip, (s64) 0, (s64) xlen, &xaddr))
+			    == 0) {
+				PXDaddress(pxd, xaddr);
+				PXDlength(pxd, xlen);
+
+				pxdlist.maxnpxd++;
+
+				continue;
+			}
+
+			/* undo allocation */
+
+			XT_PUTPAGE(smp);
+			return rc;
+		}
+	}
+
+	/*
+	 * Split leaf page <sp> into <sp> and a new right page <rp>.
+	 *
+	 * The split routines insert the new entry into the leaf page,
+	 * and acquire txLock as appropriate.
+	 * return <rp> pinned and its block number <rpbn>.
+	 */
+	rc = (sp->header.flag & BT_ROOT) ?
+	    xtSplitRoot(tid, ip, split, &rmp) :
+	    xtSplitPage(tid, ip, split, &rmp, &rbn);
+	if (rc)
+		return EIO;
+
+	XT_PUTPAGE(smp);
+
+	/*
+	 * propagate up the router entry for the leaf page just split
+	 *
+	 * insert a router entry for the new page into the parent page,
+	 * propagate the insert/split up the tree by walking back the stack
+	 * of (bn of parent page, index of child page entry in parent page)
+	 * that were traversed during the search for the page that split.
+	 *
+	 * the propagation of insert/split up the tree stops if the root
+	 * splits or the page inserted into doesn't have to split to hold
+	 * the new entry.
+	 *
+	 * the parent entry for the split page remains the same, and
+	 * a new entry is inserted at its right with the first key and
+	 * block number of the new right page.
+	 *
+	 * There are a maximum of 3 pages pinned at any time:
+	 * right child, left parent and right parent (when the parent splits)
+	 * to keep the child page pinned while working on the parent.
+	 * make sure that all pins are released at exit.
+	 */
+	while ((parent = BT_POP(btstack)) != NULL) {
+		/* parent page specified by stack frame <parent> */
+
+		/* keep current child pages <rcp> pinned */
+		rcmp = rmp;
+		rcbn = rbn;
+		rcp = XT_PAGE(ip, rcmp);
+
+		/*
+		 * insert router entry in parent for new right child page <rp>
+		 */
+		/* get/pin the parent page <sp> */
+		XT_GETPAGE(ip, parent->bn, smp, PSIZE, sp, rc);
+		if (rc)
+			goto errout2;
+
+		/*
+		 * The new key entry goes ONE AFTER the index of parent entry,
+		 * because the split was to the right.
+		 */
+		skip = parent->index + 1;
+
+		/*
+		 * split or shift right remaining entries of the parent page
+		 */
+		nextindex = le16_to_cpu(sp->header.nextindex);
+		/*
+		 * parent page is full - split the parent page
+		 */
+		if (nextindex == le16_to_cpu(sp->header.maxentry)) {
+			/* init for parent page split */
+			split->mp = smp;
+			split->index = skip;	/* index at insert */
+			split->flag = XAD_NEW;
+			split->off = offsetXAD(&rcp->xad[XTENTRYSTART]);
+			split->len = JFS_SBI(ip->i_sb)->nbperpage;
+			split->addr = rcbn;
+
+			/* unpin previous right child page */
+			XT_PUTPAGE(rcmp);
+
+			/* The split routines insert the new entry,
+			 * and acquire txLock as appropriate.
+			 * return <rp> pinned and its block number <rpbn>.
+			 */
+			rc = (sp->header.flag & BT_ROOT) ?
+			    xtSplitRoot(tid, ip, split, &rmp) :
+			    xtSplitPage(tid, ip, split, &rmp, &rbn);
+			if (rc)
+				goto errout1;
+
+			XT_PUTPAGE(smp);
+			/* keep new child page <rp> pinned */
+		}
+		/*
+		 * parent page is not full - insert in parent page
+		 */
+		else {
+			/*
+			 * insert router entry in parent for the right child
+			 * page from the first entry of the right child page:
+			 */
+			/*
+			 * acquire a transaction lock on the parent page;
+			 *
+			 * action: router xad insertion;
+			 */
+			BT_MARK_DIRTY(smp, ip);
+
+			/*
+			 * if insert into middle, shift right remaining entries
+			 */
+			if (skip < nextindex)
+				memmove(&sp->xad[skip + 1], &sp->xad[skip],
+					(nextindex -
+					 skip) << L2XTSLOTSIZE);
+
+			/* insert the router entry */
+			xad = &sp->xad[skip];
+			XT_PUTENTRY(xad, XAD_NEW,
+				    offsetXAD(&rcp->xad[XTENTRYSTART]),
+				    JFS_SBI(ip->i_sb)->nbperpage, rcbn);
+
+			/* advance next available entry index. */
+			sp->header.nextindex =
+			    cpu_to_le16(le16_to_cpu(sp->header.nextindex) +
+					1);
+
+			/* Don't log it if there are no links to the file */
+			if (!test_cflag(COMMIT_Nolink, ip)) {
+				tlck = txLock(tid, ip, smp,
+					      tlckXTREE | tlckGROW);
+				xtlck = (xtlock_t *) & tlck->lock;
+				xtlck->lwm.offset = (xtlck->lwm.offset) ?
+				    min(skip, (int)xtlck->lwm.offset) : skip;
+				xtlck->lwm.length =
+				    le16_to_cpu(sp->header.nextindex) -
+				    xtlck->lwm.offset;
+			}
+
+			/* unpin parent page */
+			XT_PUTPAGE(smp);
+
+			/* exit propagate up */
+			break;
+		}
+	}
+
+	/* unpin current right page */
+	XT_PUTPAGE(rmp);
+
+	return 0;
+
+	/*
+	 * If something fails in the above loop we were already walking back
+	 * up the tree and the tree is now inconsistent.
+	 * release all pages we're holding.
+	 */
+      errout1:
+	XT_PUTPAGE(smp);
+
+      errout2:
+	XT_PUTPAGE(rcmp);
+
+	return rc;
+}
+
+
+/*
+ *      xtSplitPage()
+ *
+ * function:
+ *      split a full non-root page into
+ *      original/split/left page and new right page
+ *      i.e., the original/split page remains as left page.
+ *
+ * parameter:
+ *      int		tid,
+ *      struct inode    *ip,
+ *      xtsplit_t       *split,
+ *      metapage_t	**rmpp,
+ *      u64		*rbnp,
+ *
+ * return:
+ *      Pointer to page in which to insert or NULL on error.
+ */
+static int
+xtSplitPage(tid_t tid, struct inode *ip,
+	    xtsplit_t * split, metapage_t ** rmpp, s64 * rbnp)
+{
+	int rc = 0;
+	metapage_t *smp;
+	xtpage_t *sp;
+	metapage_t *rmp;
+	xtpage_t *rp;		/* new right page allocated */
+	s64 rbn;		/* new right page block number */
+	metapage_t *mp;
+	xtpage_t *p;
+	s64 nextbn;
+	int skip, maxentry, middle, righthalf, n;
+	xad_t *xad;
+	pxdlist_t *pxdlist;
+	pxd_t *pxd;
+	tlock_t *tlck;
+	xtlock_t *sxtlck = 0, *rxtlck = 0;
+
+	smp = split->mp;
+	sp = XT_PAGE(ip, smp);
+
+	INCREMENT(xtStat.split);
+
+	/*
+	 * allocate the new right page for the split
+	 */
+	pxdlist = split->pxdlist;
+	pxd = &pxdlist->pxd[pxdlist->npxd];
+	pxdlist->npxd++;
+	rbn = addressPXD(pxd);
+	rmp = get_metapage(ip, rbn, PSIZE, 1);
+	if (rmp == NULL)
+		return EIO;
+
+	jEVENT(0,
+	       ("xtSplitPage: ip:0x%p smp:0x%p rmp:0x%p\n", ip, smp, rmp));
+
+	BT_MARK_DIRTY(rmp, ip);
+	/*
+	 * action: new page;
+	 */
+
+	rp = (xtpage_t *) rmp->data;
+	rp->header.self = *pxd;
+	rp->header.flag = sp->header.flag & BT_TYPE;
+	rp->header.maxentry = sp->header.maxentry;	/* little-endian */
+	rp->header.nextindex = cpu_to_le16(XTENTRYSTART);
+
+	BT_MARK_DIRTY(smp, ip);
+	/* Don't log it if there are no links to the file */
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		/*
+		 * acquire a transaction lock on the new right page;
+		 */
+		tlck = txLock(tid, ip, rmp, tlckXTREE | tlckNEW);
+		rxtlck = (xtlock_t *) & tlck->lock;
+		rxtlck->lwm.offset = XTENTRYSTART;
+		/*
+		 * acquire a transaction lock on the split page
+		 */
+		tlck = txLock(tid, ip, smp, tlckXTREE | tlckGROW);
+		sxtlck = (xtlock_t *) & tlck->lock;
+	}
+
+	/*
+	 * initialize/update sibling pointers of <sp> and <rp>
+	 */
+	nextbn = le64_to_cpu(sp->header.next);
+	rp->header.next = cpu_to_le64(nextbn);
+	rp->header.prev = cpu_to_le64(addressPXD(&sp->header.self));
+	sp->header.next = cpu_to_le64(rbn);
+
+	skip = split->index;
+
+	/*
+	 *      sequential append at tail (after last entry of last page)
+	 *
+	 * if splitting the last page on a level because of appending
+	 * a entry to it (skip is maxentry), it's likely that the access is
+	 * sequential. adding an empty page on the side of the level is less
+	 * work and can push the fill factor much higher than normal.
+	 * if we're wrong it's no big deal -  we will do the split the right
+	 * way next time.
+	 * (it may look like it's equally easy to do a similar hack for
+	 * reverse sorted data, that is, split the tree left, but it's not.
+	 * Be my guest.)
+	 */
+	if (nextbn == 0 && skip == le16_to_cpu(sp->header.maxentry)) {
+		/*
+		 * acquire a transaction lock on the new/right page;
+		 *
+		 * action: xad insertion;
+		 */
+		/* insert entry at the first entry of the new right page */
+		xad = &rp->xad[XTENTRYSTART];
+		XT_PUTENTRY(xad, split->flag, split->off, split->len,
+			    split->addr);
+
+		rp->header.nextindex = cpu_to_le16(XTENTRYSTART + 1);
+
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			/* rxtlck->lwm.offset = XTENTRYSTART; */
+			rxtlck->lwm.length = 1;
+		}
+
+		*rmpp = rmp;
+		*rbnp = rbn;
+
+		ip->i_blocks += LBLK2PBLK(ip->i_sb, lengthPXD(pxd));
+
+		jEVENT(0, ("xtSplitPage: sp:0x%p rp:0x%p\n", sp, rp));
+		return 0;
+	}
+
+	/*
+	 *      non-sequential insert (at possibly middle page)
+	 */
+
+	/*
+	 * update previous pointer of old next/right page of <sp>
+	 */
+	if (nextbn != 0) {
+		XT_GETPAGE(ip, nextbn, mp, PSIZE, p, rc);
+		if (rc) {
+			XT_PUTPAGE(rmp);
+			return rc;
+		}
+
+		BT_MARK_DIRTY(mp, ip);
+		/*
+		 * acquire a transaction lock on the next page;
+		 *
+		 * action:sibling pointer update;
+		 */
+		if (!test_cflag(COMMIT_Nolink, ip))
+			tlck = txLock(tid, ip, mp, tlckXTREE | tlckRELINK);
+
+		p->header.prev = cpu_to_le64(rbn);
+
+		/* sibling page may have been updated previously, or
+		 * it may be updated later;
+		 */
+
+		XT_PUTPAGE(mp);
+	}
+
+	/*
+	 * split the data between the split and new/right pages
+	 */
+	maxentry = le16_to_cpu(sp->header.maxentry);
+	middle = maxentry >> 1;
+	righthalf = maxentry - middle;
+
+	/*
+	 * skip index in old split/left page - insert into left page:
+	 */
+	if (skip <= middle) {
+		/* move right half of split page to the new right page */
+		memmove(&rp->xad[XTENTRYSTART], &sp->xad[middle],
+			righthalf << L2XTSLOTSIZE);
+
+		/* shift right tail of left half to make room for new entry */
+		if (skip < middle)
+			memmove(&sp->xad[skip + 1], &sp->xad[skip],
+				(middle - skip) << L2XTSLOTSIZE);
+
+		/* insert new entry */
+		xad = &sp->xad[skip];
+		XT_PUTENTRY(xad, split->flag, split->off, split->len,
+			    split->addr);
+
+		/* update page header */
+		sp->header.nextindex = cpu_to_le16(middle + 1);
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			sxtlck->lwm.offset = (sxtlck->lwm.offset) ?
+			    min(skip, (int)sxtlck->lwm.offset) : skip;
+		}
+
+		rp->header.nextindex =
+		    cpu_to_le16(XTENTRYSTART + righthalf);
+	}
+	/*
+	 * skip index in new right page - insert into right page:
+	 */
+	else {
+		/* move left head of right half to right page */
+		n = skip - middle;
+		memmove(&rp->xad[XTENTRYSTART], &sp->xad[middle],
+			n << L2XTSLOTSIZE);
+
+		/* insert new entry */
+		n += XTENTRYSTART;
+		xad = &rp->xad[n];
+		XT_PUTENTRY(xad, split->flag, split->off, split->len,
+			    split->addr);
+
+		/* move right tail of right half to right page */
+		if (skip < maxentry)
+			memmove(&rp->xad[n + 1], &sp->xad[skip],
+				(maxentry - skip) << L2XTSLOTSIZE);
+
+		/* update page header */
+		sp->header.nextindex = cpu_to_le16(middle);
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			sxtlck->lwm.offset = (sxtlck->lwm.offset) ?
+			    min(middle, (int)sxtlck->lwm.offset) : middle;
+		}
+
+		rp->header.nextindex = cpu_to_le16(XTENTRYSTART +
+						   righthalf + 1);
+	}
+
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		sxtlck->lwm.length = le16_to_cpu(sp->header.nextindex) -
+		    sxtlck->lwm.offset;
+
+		/* rxtlck->lwm.offset = XTENTRYSTART; */
+		rxtlck->lwm.length = le16_to_cpu(rp->header.nextindex) -
+		    XTENTRYSTART;
+	}
+
+	*rmpp = rmp;
+	*rbnp = rbn;
+
+	ip->i_blocks += LBLK2PBLK(ip->i_sb, lengthPXD(pxd));
+
+	jEVENT(0, ("xtSplitPage: sp:0x%p rp:0x%p\n", sp, rp));
+	return rc;
+}
+
+
+/*
+ *      xtSplitRoot()
+ *
+ * function:
+ *      split the full root page into
+ *      original/root/split page and new right page
+ *      i.e., root remains fixed in tree anchor (inode) and
+ *      the root is copied to a single new right child page
+ *      since root page << non-root page, and
+ *      the split root page contains a single entry for the
+ *      new right child page.
+ *
+ * parameter:
+ *      int		tid,
+ *      struct inode    *ip,
+ *      xtsplit_t       *split,
+ *      metapage_t	**rmpp)
+ *
+ * return:
+ *      Pointer to page in which to insert or NULL on error.
+ */
+static int
+xtSplitRoot(tid_t tid,
+	    struct inode *ip, xtsplit_t * split, metapage_t ** rmpp)
+{
+	xtpage_t *sp;
+	metapage_t *rmp;
+	xtpage_t *rp;
+	s64 rbn;
+	int skip, nextindex;
+	xad_t *xad;
+	pxd_t *pxd;
+	pxdlist_t *pxdlist;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+
+	sp = &JFS_IP(ip)->i_xtroot;
+
+	INCREMENT(xtStat.split);
+
+	/*
+	 *      allocate a single (right) child page
+	 */
+	pxdlist = split->pxdlist;
+	pxd = &pxdlist->pxd[pxdlist->npxd];
+	pxdlist->npxd++;
+	rbn = addressPXD(pxd);
+	rmp = get_metapage(ip, rbn, PSIZE, 1);
+	if (rmp == NULL)
+		return EIO;
+
+	jEVENT(0, ("xtSplitRoot: ip:0x%p rmp:0x%p\n", ip, rmp));
+
+	/*
+	 * acquire a transaction lock on the new right page;
+	 *
+	 * action: new page;
+	 */
+	BT_MARK_DIRTY(rmp, ip);
+
+	rp = (xtpage_t *) rmp->data;
+	rp->header.flag =
+	    (sp->header.flag & BT_LEAF) ? BT_LEAF : BT_INTERNAL;
+	rp->header.self = *pxd;
+	rp->header.nextindex = cpu_to_le16(XTENTRYSTART);
+	rp->header.maxentry = cpu_to_le16(PSIZE >> L2XTSLOTSIZE);
+
+	/* initialize sibling pointers */
+	rp->header.next = 0;
+	rp->header.prev = 0;
+
+	/*
+	 * copy the in-line root page into new right page extent
+	 */
+	nextindex = le16_to_cpu(sp->header.maxentry);
+	memmove(&rp->xad[XTENTRYSTART], &sp->xad[XTENTRYSTART],
+		(nextindex - XTENTRYSTART) << L2XTSLOTSIZE);
+
+	/*
+	 * insert the new entry into the new right/child page
+	 * (skip index in the new right page will not change)
+	 */
+	skip = split->index;
+	/* if insert into middle, shift right remaining entries */
+	if (skip != nextindex)
+		memmove(&rp->xad[skip + 1], &rp->xad[skip],
+			(nextindex - skip) * sizeof(xad_t));
+
+	xad = &rp->xad[skip];
+	XT_PUTENTRY(xad, split->flag, split->off, split->len, split->addr);
+
+	/* update page header */
+	rp->header.nextindex = cpu_to_le16(nextindex + 1);
+
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, rmp, tlckXTREE | tlckNEW);
+		xtlck = (xtlock_t *) & tlck->lock;
+		xtlck->lwm.offset = XTENTRYSTART;
+		xtlck->lwm.length = le16_to_cpu(rp->header.nextindex) -
+		    XTENTRYSTART;
+	}
+
+	/*
+	 *      reset the root
+	 *
+	 * init root with the single entry for the new right page
+	 * set the 1st entry offset to 0, which force the left-most key
+	 * at any level of the tree to be less than any search key.
+	 */
+	/*
+	 * acquire a transaction lock on the root page (in-memory inode);
+	 *
+	 * action: root split;
+	 */
+	BT_MARK_DIRTY(split->mp, ip);
+
+	xad = &sp->xad[XTENTRYSTART];
+	XT_PUTENTRY(xad, XAD_NEW, 0, JFS_SBI(ip->i_sb)->nbperpage, rbn);
+
+	/* update page header of root */
+	sp->header.flag &= ~BT_LEAF;
+	sp->header.flag |= BT_INTERNAL;
+
+	sp->header.nextindex = cpu_to_le16(XTENTRYSTART + 1);
+
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, split->mp, tlckXTREE | tlckGROW);
+		xtlck = (xtlock_t *) & tlck->lock;
+		xtlck->lwm.offset = XTENTRYSTART;
+		xtlck->lwm.length = 1;
+	}
+
+	*rmpp = rmp;
+
+	ip->i_blocks += LBLK2PBLK(ip->i_sb, lengthPXD(pxd));
+
+	jEVENT(0, ("xtSplitRoot: sp:0x%p rp:0x%p\n", sp, rp));
+	return 0;
+}
+
+
+/*
+ *      xtExtend()
+ *
+ * function: extend in-place;
+ *
+ * note: existing extent may or may not have been committed.
+ * caller is responsible for pager buffer cache update, and
+ * working block allocation map update;
+ * update pmap: alloc whole extended extent;
+ */
+int xtExtend(tid_t tid,		/* transaction id */
+	     struct inode *ip, s64 xoff,	/* delta extent offset */
+	     s32 xlen,		/* delta extent length */
+	     int flag)
+{
+	int rc = 0;
+	int cmp;
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* base B+-tree index page */
+	s64 bn;
+	int index, nextindex, len;
+	btstack_t btstack;	/* traverse stack */
+	xtsplit_t split;	/* split information */
+	xad_t *xad;
+	s64 xaddr;
+	tlock_t *tlck;
+	xtlock_t *xtlck = 0;
+	int rootsplit = 0;
+
+	jFYI(1,
+	     ("xtExtend: nxoff:0x%lx nxlen:0x%x\n", (ulong) xoff, xlen));
+
+	/* there must exist extent to be extended */
+	if ((rc = xtSearch(ip, xoff - 1, &cmp, &btstack, XT_INSERT)))
+		return rc;
+	assert(cmp == 0);
+
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/* extension must be contiguous */
+	xad = &p->xad[index];
+	jFYI(0, ("xtExtend: xoff:0x%lx xlen:0x%x xaddr:0x%lx\n",
+		 (ulong) offsetXAD(xad), lengthXAD(xad),
+		 (ulong) addressXAD(xad)));
+	assert((offsetXAD(xad) + lengthXAD(xad)) == xoff);
+
+	/*
+	 * acquire a transaction lock on the leaf page;
+	 *
+	 * action: xad insertion/extension;
+	 */
+	BT_MARK_DIRTY(mp, ip);
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+		xtlck = (xtlock_t *) & tlck->lock;
+	}
+
+	/* extend will overflow extent ? */
+	xlen = lengthXAD(xad) + xlen;
+	if ((len = xlen - MAXXLEN) <= 0)
+		goto extendOld;
+
+	/*
+	 *      extent overflow: insert entry for new extent
+	 */
+//insertNew:
+	xoff = offsetXAD(xad) + MAXXLEN;
+	xaddr = addressXAD(xad) + MAXXLEN;
+	nextindex = le16_to_cpu(p->header.nextindex);
+
+	/*
+	 *      if the leaf page is full, insert the new entry and
+	 *      propagate up the router entry for the new page from split
+	 *
+	 * The xtSplitUp() will insert the entry and unpin the leaf page.
+	 */
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+		rootsplit = p->header.flag & BT_ROOT;
+
+		/* xtSpliUp() unpins leaf pages */
+		split.mp = mp;
+		split.index = index + 1;
+		split.flag = XAD_NEW;
+		split.off = xoff;	/* split offset */
+		split.len = len;
+		split.addr = xaddr;
+		split.pxdlist = NULL;
+		if ((rc = xtSplitUp(tid, ip, &split, &btstack)))
+			return rc;
+
+		/*
+		 * if leaf root has been split, original root has been
+		 * copied to new child page, i.e., original entry now
+		 * resides on the new child page;
+		 */
+		if (rootsplit) {
+			if (p->header.nextindex ==
+			    cpu_to_le16(XTENTRYSTART + 1)) {
+				xad = &p->xad[XTENTRYSTART];
+				bn = addressXAD(xad);
+
+				/* get new child page */
+				XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+				BT_MARK_DIRTY(mp, ip);
+				if (!test_cflag(COMMIT_Nolink, ip)) {
+					tlck = txLock(tid, ip, mp,
+						      tlckXTREE |
+						      tlckGROW);
+					xtlck = (xtlock_t *) & tlck->lock;
+				}
+			}
+		} else
+			/* get back old page */
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	}
+	/*
+	 *      insert the new entry into the leaf page
+	 */
+	else {
+		/* insert the new entry: mark the entry NEW */
+		xad = &p->xad[index + 1];
+		XT_PUTENTRY(xad, XAD_NEW, xoff, len, xaddr);
+
+		/* advance next available entry index */
+		p->header.nextindex =
+		    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+	}
+
+	/* get back old entry */
+	xad = &p->xad[index];
+	xlen = MAXXLEN;
+
+	/*
+	 * extend old extent
+	 */
+      extendOld:
+	XADlength(xad, xlen);
+	if (!(xad->flag & XAD_NEW))
+		xad->flag |= XAD_EXTENDED;
+
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		xtlck->lwm.offset =
+		    (xtlck->lwm.offset) ? min(index,
+					      (int)xtlck->lwm.offset) : index;
+		xtlck->lwm.length =
+		    le16_to_cpu(p->header.nextindex) - xtlck->lwm.offset;
+	}
+
+	/* unpin the leaf page */
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *      xtTailgate()
+ *
+ * function: split existing 'tail' extent
+ *      (split offset >= start offset of tail extent), and
+ *      relocate and extend the split tail half;
+ *
+ * note: existing extent may or may not have been committed.
+ * caller is responsible for pager buffer cache update, and
+ * working block allocation map update;
+ * update pmap: free old split tail extent, alloc new extent;
+ */
+int xtTailgate(tid_t tid,		/* transaction id */
+	       struct inode *ip, s64 xoff,	/* split/new extent offset */
+	       s32 xlen,	/* new extent length */
+	       s64 xaddr,	/* new extent address */
+	       int flag)
+{
+	int rc = 0;
+	int cmp;
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* base B+-tree index page */
+	s64 bn;
+	int index, nextindex, llen, rlen;
+	btstack_t btstack;	/* traverse stack */
+	xtsplit_t split;	/* split information */
+	xad_t *xad;
+	tlock_t *tlck;
+	xtlock_t *xtlck = 0;
+	tlock_t *mtlck;
+	maplock_t *pxdlock;
+	int rootsplit = 0;
+
+/*
+printf("xtTailgate: nxoff:0x%lx nxlen:0x%x nxaddr:0x%lx\n",
+        (ulong)xoff, xlen, (ulong)xaddr);
+*/
+
+	/* there must exist extent to be tailgated */
+	if ((rc = xtSearch(ip, xoff, &cmp, &btstack, XT_INSERT)))
+		return rc;
+	assert(cmp == 0);
+
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	/* entry found must be last entry */
+	nextindex = le16_to_cpu(p->header.nextindex);
+	assert(index == nextindex - 1);
+
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire tlock of the leaf page containing original entry
+	 */
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+		xtlck = (xtlock_t *) & tlck->lock;
+	}
+
+	/* completely replace extent ? */
+	xad = &p->xad[index];
+/*
+printf("xtTailgate: xoff:0x%lx xlen:0x%x xaddr:0x%lx\n",
+        (ulong)offsetXAD(xad), lengthXAD(xad), (ulong)addressXAD(xad));
+*/
+	if ((llen = xoff - offsetXAD(xad)) == 0)
+		goto updateOld;
+
+	/*
+	 *      partially replace extent: insert entry for new extent
+	 */
+//insertNew:
+	/*
+	 *      if the leaf page is full, insert the new entry and
+	 *      propagate up the router entry for the new page from split
+	 *
+	 * The xtSplitUp() will insert the entry and unpin the leaf page.
+	 */
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+		rootsplit = p->header.flag & BT_ROOT;
+
+		/* xtSpliUp() unpins leaf pages */
+		split.mp = mp;
+		split.index = index + 1;
+		split.flag = XAD_NEW;
+		split.off = xoff;	/* split offset */
+		split.len = xlen;
+		split.addr = xaddr;
+		split.pxdlist = NULL;
+		if ((rc = xtSplitUp(tid, ip, &split, &btstack)))
+			return rc;
+
+		/*
+		 * if leaf root has been split, original root has been
+		 * copied to new child page, i.e., original entry now
+		 * resides on the new child page;
+		 */
+		if (rootsplit) {
+			if (p->header.nextindex ==
+			    cpu_to_le16(XTENTRYSTART + 1)) {
+				xad = &p->xad[XTENTRYSTART];
+				bn = addressXAD(xad);
+
+				/* get new child page */
+				XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+				BT_MARK_DIRTY(mp, ip);
+				if (!test_cflag(COMMIT_Nolink, ip)) {
+					tlck = txLock(tid, ip, mp,
+						      tlckXTREE |
+						      tlckGROW);
+					xtlck = (xtlock_t *) & tlck->lock;
+				}
+			}
+		} else
+			/* get back old page */
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	}
+	/*
+	 *      insert the new entry into the leaf page
+	 */
+	else {
+		/* insert the new entry: mark the entry NEW */
+		xad = &p->xad[index + 1];
+		XT_PUTENTRY(xad, XAD_NEW, xoff, xlen, xaddr);
+
+		/* advance next available entry index */
+		p->header.nextindex =
+		    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+	}
+
+	/* get back old XAD */
+	xad = &p->xad[index];
+
+	/*
+	 * truncate/relocate old extent at split offset
+	 */
+      updateOld:
+	/* update dmap for old/committed/truncated extent */
+	rlen = lengthXAD(xad) - llen;
+	if (!(xad->flag & XAD_NEW)) {
+		/* free from PWMAP at commit */
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			mtlck = txMaplock(tid, ip, tlckMAP);
+			pxdlock = (maplock_t *) & mtlck->lock;
+			pxdlock->flag = mlckFREEPXD;
+			PXDaddress(&pxdlock->pxd, addressXAD(xad) + llen);
+			PXDlength(&pxdlock->pxd, rlen);
+			pxdlock->index = 1;
+		}
+		jEVENT(0,
+		       ("xtTailgate: free extent xaddr:0x%lx xlen:0x%x\n",
+			(ulong) addressPXD(&pxdlock->pxd),
+			lengthPXD(&pxdlock->pxd)));
+	} else
+		/* free from WMAP */
+		dbFree(ip, addressXAD(xad) + llen, (s64) rlen);
+
+	if (llen)
+		/* truncate */
+		XADlength(xad, llen);
+	else
+		/* replace */
+		XT_PUTENTRY(xad, XAD_NEW, xoff, xlen, xaddr);
+
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		xtlck->lwm.offset = (xtlck->lwm.offset) ?
+		    min(index, (int)xtlck->lwm.offset) : index;
+		xtlck->lwm.length = le16_to_cpu(p->header.nextindex) -
+		    xtlck->lwm.offset;
+	}
+
+	/* unpin the leaf page */
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/*
+ *      xtUpdate()
+ *
+ * function: update XAD;
+ *
+ *      update extent for allocated_but_not_recorded or
+ *      compressed extent;
+ *
+ * parameter:
+ *      nxad    - new XAD;
+ *                logical extent of the specified XAD must be completely
+ *                contained by an existing XAD;
+ */
+int xtUpdate(tid_t tid, struct inode *ip, xad_t * nxad)
+{				/* new XAD */
+	int rc = 0;
+	int cmp;
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* base B+-tree index page */
+	s64 bn;
+	int index0, index, newindex, nextindex;
+	btstack_t btstack;	/* traverse stack */
+	xtsplit_t split;	/* split information */
+	xad_t *xad, *lxad, *rxad;
+	int xflag;
+	s64 nxoff, xoff;
+	int nxlen, xlen, lxlen, rxlen;
+	s64 nxaddr, xaddr;
+	tlock_t *tlck;
+	xtlock_t *xtlck = 0;
+	int rootsplit = 0, newpage = 0;
+
+	/* there must exist extent to be tailgated */
+	nxoff = offsetXAD(nxad);
+	nxlen = lengthXAD(nxad);
+	nxaddr = addressXAD(nxad);
+/*
+printf("xtUpdate: nxflag:0x%x nxoff:0x%lx nxlen:0x%x nxaddr:0x%lx\n",
+        nxad->flag, (ulong)nxoff, nxlen, (ulong)nxaddr);
+*/
+	if ((rc = xtSearch(ip, nxoff, &cmp, &btstack, XT_INSERT)))
+		return rc;
+	assert(cmp == 0);
+
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index0);
+
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire tlock of the leaf page containing original entry
+	 */
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+		xtlck = (xtlock_t *) & tlck->lock;
+	}
+
+	xad = &p->xad[index0];
+	xflag = xad->flag;
+	xoff = offsetXAD(xad);
+	xlen = lengthXAD(xad);
+	xaddr = addressXAD(xad);
+/*
+printf("xtUpdate: xflag:0x%x xoff:0x%lx xlen:0x%x xaddr:0x%lx\n",
+        xflag, (ulong)xoff, xlen, (ulong)xaddr);
+*/
+
+	/* nXAD must be completely contained within XAD */
+	assert(xoff <= nxoff);
+	assert(nxoff + nxlen <= xoff + xlen);
+
+	index = index0;
+	newindex = index + 1;
+	nextindex = le16_to_cpu(p->header.nextindex);
+
+#ifdef  _JFS_WIP_NOCOALESCE
+	if (xoff < nxoff)
+		goto updateRight;
+
+	/*
+	 * replace XAD with nXAD
+	 */
+      replace:			/* (nxoff == xoff) */
+	if (nxlen == xlen) {
+		/* replace XAD with nXAD:recorded */
+		*xad = *nxad;
+		xad->flag = xflag & ~XAD_NOTRECORDED;
+
+		goto out;
+	} else			/* (nxlen < xlen) */
+		goto updateLeft;
+#endif				/* _JFS_WIP_NOCOALESCE */
+
+/* #ifdef _JFS_WIP_COALESCE */
+	if (xoff < nxoff)
+		goto coalesceRight;
+
+	/*
+	 * coalesce with left XAD
+	 */
+//coalesceLeft: /* (xoff == nxoff) */
+	/* is XAD first entry of page ? */
+	if (index == XTENTRYSTART)
+		goto replace;
+
+	/* is nXAD logically and physically contiguous with lXAD ? */
+	lxad = &p->xad[index - 1];
+	lxlen = lengthXAD(lxad);
+	if (!(lxad->flag & XAD_NOTRECORDED) &&
+	    (nxoff == offsetXAD(lxad) + lxlen) &&
+	    (nxaddr == addressXAD(lxad) + lxlen) &&
+	    (lxlen + nxlen < MAXXLEN)) {
+		/* extend right lXAD */
+		index0 = index - 1;
+		XADlength(lxad, lxlen + nxlen);
+
+		/* If we just merged two extents together, need to make sure the
+		 * right extent gets logged.  If the left one is marked XAD_NEW,
+		 * then we know it will be logged.  Otherwise, mark as
+		 * XAD_EXTENDED
+		 */
+		if (!(lxad->flag & XAD_NEW))
+			lxad->flag |= XAD_EXTENDED;
+
+		if (xlen > nxlen) {
+			/* truncate XAD */
+			XADoffset(xad, xoff + nxlen);
+			XADlength(xad, xlen - nxlen);
+			XADaddress(xad, xaddr + nxlen);
+			goto out;
+		} else {	/* (xlen == nxlen) */
+
+			/* remove XAD */
+			if (index < nextindex - 1)
+				memmove(&p->xad[index], &p->xad[index + 1],
+					(nextindex - index -
+					 1) << L2XTSLOTSIZE);
+
+			p->header.nextindex =
+			    cpu_to_le16(le16_to_cpu(p->header.nextindex) -
+					1);
+
+			index = index0;
+			newindex = index + 1;
+			nextindex = le16_to_cpu(p->header.nextindex);
+			xoff = nxoff = offsetXAD(lxad);
+			xlen = nxlen = lxlen + nxlen;
+			xaddr = nxaddr = addressXAD(lxad);
+			goto coalesceRight;
+		}
+	}
+
+	/*
+	 * replace XAD with nXAD
+	 */
+      replace:			/* (nxoff == xoff) */
+	if (nxlen == xlen) {
+		/* replace XAD with nXAD:recorded */
+		*xad = *nxad;
+		xad->flag = xflag & ~XAD_NOTRECORDED;
+
+		goto coalesceRight;
+	} else			/* (nxlen < xlen) */
+		goto updateLeft;
+
+	/*
+	 * coalesce with right XAD
+	 */
+      coalesceRight:		/* (xoff <= nxoff) */
+	/* is XAD last entry of page ? */
+	if (newindex == nextindex) {
+		if (xoff == nxoff)
+			goto out;
+		goto updateRight;
+	}
+
+	/* is nXAD logically and physically contiguous with rXAD ? */
+	rxad = &p->xad[index + 1];
+	rxlen = lengthXAD(rxad);
+	if (!(rxad->flag & XAD_NOTRECORDED) &&
+	    (nxoff + nxlen == offsetXAD(rxad)) &&
+	    (nxaddr + nxlen == addressXAD(rxad)) &&
+	    (rxlen + nxlen < MAXXLEN)) {
+		/* extend left rXAD */
+		XADoffset(rxad, nxoff);
+		XADlength(rxad, rxlen + nxlen);
+		XADaddress(rxad, nxaddr);
+
+		/* If we just merged two extents together, need to make sure
+		 * the left extent gets logged.  If the right one is marked
+		 * XAD_NEW, then we know it will be logged.  Otherwise, mark as
+		 * XAD_EXTENDED
+		 */
+		if (!(rxad->flag & XAD_NEW))
+			rxad->flag |= XAD_EXTENDED;
+
+		if (xlen > nxlen)
+			/* truncate XAD */
+			XADlength(xad, xlen - nxlen);
+		else {		/* (xlen == nxlen) */
+
+			/* remove XAD */
+			memmove(&p->xad[index], &p->xad[index + 1],
+				(nextindex - index - 1) << L2XTSLOTSIZE);
+
+			p->header.nextindex =
+			    cpu_to_le16(le16_to_cpu(p->header.nextindex) -
+					1);
+		}
+
+		goto out;
+	} else if (xoff == nxoff)
+		goto out;
+
+	assert(xoff < nxoff);
+/* #endif _JFS_WIP_COALESCE */
+
+	/*
+	 * split XAD into (lXAD, nXAD):
+	 *
+	 *          |---nXAD--->
+	 * --|----------XAD----------|--
+	 *   |-lXAD-|
+	 */
+      updateRight:		/* (xoff < nxoff) */
+	/* truncate old XAD as lXAD:not_recorded */
+	xad = &p->xad[index];
+	XADlength(xad, nxoff - xoff);
+
+	/* insert nXAD:recorded */
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+/*
+printf("xtUpdate.updateRight.split p:0x%p\n", p);
+*/
+		rootsplit = p->header.flag & BT_ROOT;
+
+		/* xtSpliUp() unpins leaf pages */
+		split.mp = mp;
+		split.index = newindex;
+		split.flag = xflag & ~XAD_NOTRECORDED;
+		split.off = nxoff;
+		split.len = nxlen;
+		split.addr = nxaddr;
+		split.pxdlist = NULL;
+		if ((rc = xtSplitUp(tid, ip, &split, &btstack)))
+			return rc;
+
+		/*
+		 * if leaf root has been split, original root has been
+		 * copied to new child page, i.e., original entry now
+		 * resides on the new child page;
+		 */
+		if (rootsplit) {
+			if (p->header.nextindex ==
+			    cpu_to_le16(XTENTRYSTART + 1)) {
+				xad = &p->xad[XTENTRYSTART];
+				bn = addressXAD(xad);
+
+				/* get new child page */
+				XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+				BT_MARK_DIRTY(mp, ip);
+				if (!test_cflag(COMMIT_Nolink, ip)) {
+					tlck = txLock(tid, ip, mp,
+						      tlckXTREE |
+						      tlckGROW);
+					xtlck = (xtlock_t *) & tlck->lock;
+				}
+			}
+		} else {
+			/* get back old page */
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+			/* is nXAD on new page ? */
+			if (newindex >
+			    (le16_to_cpu(p->header.maxentry) >> 1)) {
+				newindex =
+				    newindex -
+				    le16_to_cpu(p->header.nextindex) +
+				    XTENTRYSTART;
+				newpage = 1;
+			}
+		}
+	} else {
+		/* if insert into middle, shift right remaining entries */
+		if (newindex < nextindex)
+			memmove(&p->xad[newindex + 1], &p->xad[newindex],
+				(nextindex - newindex) << L2XTSLOTSIZE);
+
+		/* insert the entry */
+		xad = &p->xad[newindex];
+		*xad = *nxad;
+		xad->flag = xflag & ~XAD_NOTRECORDED;
+
+		/* advance next available entry index. */
+		p->header.nextindex =
+		    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+	}
+
+	/*
+	 * does nXAD force 3-way split ?
+	 *
+	 *          |---nXAD--->|
+	 * --|----------XAD-------------|--
+	 *   |-lXAD-|           |-rXAD -|
+	 */
+	if (nxoff + nxlen == xoff + xlen)
+		goto out;
+
+	/* reorient nXAD as XAD for further split XAD into (nXAD, rXAD) */
+	if (newpage) {
+		/* close out old page */
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			xtlck->lwm.offset = (xtlck->lwm.offset) ?
+			    min(index0, (int)xtlck->lwm.offset) : index0;
+			xtlck->lwm.length =
+			    le16_to_cpu(p->header.nextindex) -
+			    xtlck->lwm.offset;
+		}
+
+		bn = le64_to_cpu(p->header.next);
+		XT_PUTPAGE(mp);
+
+		/* get new right page */
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+		BT_MARK_DIRTY(mp, ip);
+		if (!test_cflag(COMMIT_Nolink, ip)) {
+			tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+			xtlck = (xtlock_t *) & tlck->lock;
+		}
+
+		index0 = index = newindex;
+	} else
+		index++;
+
+	newindex = index + 1;
+	nextindex = le16_to_cpu(p->header.nextindex);
+	xlen = xlen - (nxoff - xoff);
+	xoff = nxoff;
+	xaddr = nxaddr;
+
+	/* recompute split pages */
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+/*
+printf("xtUpdate: updateRight+Left recompute split pages: p:0x%p\n", p);
+*/
+		XT_PUTPAGE(mp);
+
+		if ((rc = xtSearch(ip, nxoff, &cmp, &btstack, XT_INSERT)))
+			return rc;
+		assert(cmp == 0);
+
+		/* retrieve search result */
+		XT_GETSEARCH(ip, btstack.top, bn, mp, p, index0);
+		assert(index0 == index);
+	}
+
+	/*
+	 * split XAD into (nXAD, rXAD)
+	 *
+	 *          ---nXAD---|
+	 * --|----------XAD----------|--
+	 *                    |-rXAD-|
+	 */
+      updateLeft:		/* (nxoff == xoff) && (nxlen < xlen) */
+	/* update old XAD with nXAD:recorded */
+	xad = &p->xad[index];
+	*xad = *nxad;
+	xad->flag = xflag & ~XAD_NOTRECORDED;
+
+	/* insert rXAD:not_recorded */
+	xoff = xoff + nxlen;
+	xlen = xlen - nxlen;
+	xaddr = xaddr + nxlen;
+	if (nextindex == le16_to_cpu(p->header.maxentry)) {
+		rootsplit = p->header.flag & BT_ROOT;
+
+/*
+printf("xtUpdate.updateLeft.split p:0x%p\n", p);
+*/
+		/* xtSpliUp() unpins leaf pages */
+		split.mp = mp;
+		split.index = newindex;
+		split.flag = xflag;
+		split.off = xoff;
+		split.len = xlen;
+		split.addr = xaddr;
+		split.pxdlist = NULL;
+		if ((rc = xtSplitUp(tid, ip, &split, &btstack)))
+			return rc;
+
+		/*
+		 * if leaf root has been split, original root has been
+		 * copied to new child page, i.e., original entry now
+		 * resides on the new child page;
+		 */
+		if (rootsplit) {
+			if (p->header.nextindex ==
+			    cpu_to_le16(XTENTRYSTART + 1)) {
+				xad = &p->xad[XTENTRYSTART];
+				bn = addressXAD(xad);
+
+				/* get new child page */
+				XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+
+				BT_MARK_DIRTY(mp, ip);
+				if (!test_cflag(COMMIT_Nolink, ip)) {
+					tlck = txLock(tid, ip, mp,
+						      tlckXTREE |
+						      tlckGROW);
+					xtlck = (xtlock_t *) & tlck->lock;
+				}
+			}
+		} else
+			/* get back old page */
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	} else {
+		/* if insert into middle, shift right remaining entries */
+		if (newindex < nextindex)
+			memmove(&p->xad[newindex + 1], &p->xad[newindex],
+				(nextindex - newindex) << L2XTSLOTSIZE);
+
+		/* insert the entry */
+		xad = &p->xad[newindex];
+		XT_PUTENTRY(xad, xflag, xoff, xlen, xaddr);
+
+		/* advance next available entry index. */
+		p->header.nextindex =
+		    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+	}
+
+      out:
+	if (!test_cflag(COMMIT_Nolink, ip)) {
+		xtlck->lwm.offset = (xtlck->lwm.offset) ?
+		    min(index0, (int)xtlck->lwm.offset) : index0;
+		xtlck->lwm.length = le16_to_cpu(p->header.nextindex) -
+		    xtlck->lwm.offset;
+	}
+
+	/* unpin the leaf page */
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+#ifdef _STILL_TO_PORT
+/*
+ *      xtAppend()
+ *
+ * function: grow in append mode from contiguous region specified ;
+ *
+ * parameter:
+ *      tid             - transaction id;
+ *      ip              - file object;
+ *      xflag           - extent flag:
+ *      xoff            - extent offset;
+ *      maxblocks       - max extent length;
+ *      xlen            - extent length (in/out);
+ *      xaddrp          - extent address pointer (in/out):
+ *      flag            -
+ *
+ * return:
+ */
+int xtAppend(tid_t tid,		/* transaction id */
+	     struct inode *ip, int xflag, s64 xoff, s32 maxblocks,	/* @GD1 */
+	     s32 * xlenp,	/* (in/out) */
+	     s64 * xaddrp,	/* (in/out) */
+	     int flag)
+{
+	int rc = 0;
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* base B+-tree index page */
+	s64 bn, xaddr;
+	int index, nextindex;
+	btstack_t btstack;	/* traverse stack */
+	xtsplit_t split;	/* split information */
+	xad_t *xad;
+	int cmp;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+	int nsplit, nblocks, xlen;
+	pxdlist_t pxdlist;
+	pxd_t *pxd;
+
+	xaddr = *xaddrp;
+	xlen = *xlenp;
+	jEVENT(0,
+	       ("xtAppend: xoff:0x%lx maxblocks:%d xlen:%d xaddr:0x%lx\n",
+		(ulong) xoff, maxblocks, xlen, (ulong) xaddr));
+
+	/*
+	 *      search for the entry location at which to insert:
+	 *
+	 * xtFastSearch() and xtSearch() both returns (leaf page
+	 * pinned, index at which to insert).
+	 * n.b. xtSearch() may return index of maxentry of
+	 * the full page.
+	 */
+	if ((rc = xtSearch(ip, xoff, &cmp, &btstack, XT_INSERT)))
+		return rc;
+
+	/* retrieve search result */
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+
+	if (cmp == 0) {
+		rc = EEXIST;
+		goto out;
+	}
+//insert:
+	/*
+	 *      insert entry for new extent
+	 */
+	xflag |= XAD_NEW;
+
+	/*
+	 *      if the leaf page is full, split the page and
+	 *      propagate up the router entry for the new page from split
+	 *
+	 * The xtSplitUp() will insert the entry and unpin the leaf page.
+	 */
+	nextindex = le16_to_cpu(p->header.nextindex);
+	if (nextindex < le16_to_cpu(p->header.maxentry))
+		goto insertLeaf;
+
+	/*
+	 * allocate new index blocks to cover index page split(s)
+	 */
+	nsplit = btstack.nsplit;
+	split.pxdlist = &pxdlist;
+	pxdlist.maxnpxd = pxdlist.npxd = 0;
+	pxd = &pxdlist.pxd[0];
+	nblocks = JFS_SBI(ip->i_sb)->nbperpage;
+	for (; nsplit > 0; nsplit--, pxd++, xaddr += nblocks, maxblocks -= nblocks) {	/* @GD1 */
+		if ((rc = dbAllocBottomUp(ip, xaddr, (s64) nblocks)) == 0) {
+			PXDaddress(pxd, xaddr);
+			PXDlength(pxd, nblocks);
+
+			pxdlist.maxnpxd++;
+
+			continue;
+		}
+
+		/* undo allocation */
+
+		goto out;
+	}
+
+	xlen = min(xlen, maxblocks);	/* @GD1 */
+
+	/*
+	 * allocate data extent requested
+	 */
+	if ((rc = dbAllocBottomUp(ip, xaddr, (s64) xlen)))
+		goto out;
+
+	split.mp = mp;
+	split.index = index;
+	split.flag = xflag;
+	split.off = xoff;
+	split.len = xlen;
+	split.addr = xaddr;
+	if ((rc = xtSplitUp(tid, ip, &split, &btstack))) {
+		/* undo data extent allocation */
+		dbFree(ip, *xaddrp, (s64) * xlenp);
+
+		return rc;
+	}
+
+	*xaddrp = xaddr;
+	*xlenp = xlen;
+	return 0;
+
+	/*
+	 *      insert the new entry into the leaf page
+	 */
+      insertLeaf:
+	/*
+	 * allocate data extent requested
+	 */
+	if ((rc = dbAllocBottomUp(ip, xaddr, (s64) xlen)))
+		goto out;
+
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire a transaction lock on the leaf page;
+	 *
+	 * action: xad insertion/extension;
+	 */
+	tlck = txLock(tid, ip, mp, tlckXTREE | tlckGROW);
+	xtlck = (xtlock_t *) & tlck->lock;
+
+	/* insert the new entry: mark the entry NEW */
+	xad = &p->xad[index];
+	XT_PUTENTRY(xad, xflag, xoff, xlen, xaddr);
+
+	/* advance next available entry index */
+	p->header.nextindex =
+	    cpu_to_le16(le16_to_cpu(p->header.nextindex) + 1);
+
+	xtlck->lwm.offset =
+	    (xtlck->lwm.offset) ? min(index, xtlck->lwm.offset) : index;
+	xtlck->lwm.length = le16_to_cpu(p->header.nextindex) -
+	    xtlck->lwm.offset;
+
+	*xaddrp = xaddr;
+	*xlenp = xlen;
+
+      out:
+	/* unpin the leaf page */
+	XT_PUTPAGE(mp);
+
+	return rc;
+}
+
+
+/* - TBD for defragmentaion/reorganization -
+ *
+ *      xtDelete()
+ *
+ * function:
+ *      delete the entry with the specified key.
+ *
+ *      N.B.: whole extent of the entry is assumed to be deleted.
+ *
+ * parameter:
+ *
+ * return:
+ *       ENOENT: if the entry is not found.
+ *
+ * exception:
+ */
+int xtDelete(tid_t tid, struct inode *ip, s64 xoff, s32 xlen, int flag)
+{
+	int rc = 0;
+	btstack_t btstack;
+	int cmp;
+	s64 bn;
+	metapage_t *mp;
+	xtpage_t *p;
+	int index, nextindex;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+
+	/*
+	 * find the matching entry; xtSearch() pins the page
+	 */
+	if ((rc = xtSearch(ip, xoff, &cmp, &btstack, 0)))
+		return rc;
+
+	XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+	if (cmp) {
+		/* unpin the leaf page */
+		XT_PUTPAGE(mp);
+		return ENOENT;
+	}
+
+	/*
+	 * delete the entry from the leaf page
+	 */
+	nextindex = le16_to_cpu(p->header.nextindex);
+	p->header.nextindex =
+	    cpu_to_le16(le16_to_cpu(p->header.nextindex) - 1);
+
+	/*
+	 * if the leaf page bocome empty, free the page
+	 */
+	if (p->header.nextindex == cpu_to_le16(XTENTRYSTART))
+		return (xtDeleteUp(tid, ip, mp, p, &btstack));
+
+	BT_MARK_DIRTY(mp, ip);
+	/*
+	 * acquire a transaction lock on the leaf page;
+	 *
+	 * action:xad deletion;
+	 */
+	tlck = txLock(tid, ip, mp, tlckXTREE);
+	xtlck = (xtlock_t *) & tlck->lock;
+	xtlck->lwm.offset =
+	    (xtlck->lwm.offset) ? min(index, xtlck->lwm.offset) : index;
+
+	/* if delete from middle, shift left/compact the remaining entries */
+	if (index < nextindex - 1)
+		memmove(&p->xad[index], &p->xad[index + 1],
+			(nextindex - index - 1) * sizeof(xad_t));
+
+	XT_PUTPAGE(mp);
+
+	return 0;
+}
+
+
+/* - TBD for defragmentaion/reorganization -
+ *
+ *      xtDeleteUp()
+ *
+ * function:
+ *      free empty pages as propagating deletion up the tree
+ *
+ * parameter:
+ *
+ * return:
+ */
+static int
+xtDeleteUp(tid_t tid,
+	   struct inode *ip,
+	   metapage_t * fmp, xtpage_t * fp, btstack_t * btstack)
+{
+	int rc = 0;
+	metapage_t *mp;
+	xtpage_t *p;
+	int index, nextindex;
+	s64 xaddr;
+	int xlen;
+	btframe_t *parent;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+
+	/*
+	 * keep root leaf page which has become empty
+	 */
+	if (fp->header.flag & BT_ROOT) {
+		/* keep the root page */
+		fp->header.flag &= ~BT_INTERNAL;
+		fp->header.flag |= BT_LEAF;
+		fp->header.nextindex = cpu_to_le16(XTENTRYSTART);
+
+		/* XT_PUTPAGE(fmp); */
+
+		return 0;
+	}
+
+	/*
+	 * free non-root leaf page
+	 */
+	if ((rc = xtRelink(tid, ip, fp)))
+		return rc;
+
+	xaddr = addressPXD(&fp->header.self);
+	xlen = lengthPXD(&fp->header.self);
+	/* free the page extent */
+	dbFree(ip, xaddr, (s64) xlen);
+
+	/* free the buffer page */
+	discard_metapage(fmp);
+
+	/*
+	 * propagate page deletion up the index tree
+	 *
+	 * If the delete from the parent page makes it empty,
+	 * continue all the way up the tree.
+	 * stop if the root page is reached (which is never deleted) or
+	 * if the entry deletion does not empty the page.
+	 */
+	while ((parent = BT_POP(btstack)) != NULL) {
+		/* get/pin the parent page <sp> */
+		XT_GETPAGE(ip, parent->bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		index = parent->index;
+
+		/* delete the entry for the freed child page from parent.
+		 */
+		nextindex = le16_to_cpu(p->header.nextindex);
+
+		/*
+		 * the parent has the single entry being deleted:
+		 * free the parent page which has become empty.
+		 */
+		if (nextindex == 1) {
+			if (p->header.flag & BT_ROOT) {
+				/* keep the root page */
+				p->header.flag &= ~BT_INTERNAL;
+				p->header.flag |= BT_LEAF;
+				p->header.nextindex =
+				    cpu_to_le16(XTENTRYSTART);
+
+				/* XT_PUTPAGE(fmp); */
+
+				break;
+			} else {
+				/* free the parent page */
+				if ((rc = xtRelink(tid, ip, p)))
+					return rc;
+
+				xaddr = addressPXD(&p->header.self);
+				/* free the page extent */
+				dbFree(ip, xaddr,
+				       (s64) JFS_SBI(ip->i_sb)->nbperpage);
+
+				/* unpin/free the buffer page */
+				discard_metapage(fmp);
+
+				/* propagate up */
+				continue;
+			}
+		}
+		/*
+		 * the parent has other entries remaining:
+		 * delete the router entry from the parent page.
+		 */
+		else {
+			BT_MARK_DIRTY(mp, ip);
+			/*
+			 * acquire a transaction lock on the leaf page;
+			 *
+			 * action:xad deletion;
+			 */
+			tlck = txLock(tid, ip, mp, tlckXTREE);
+			xtlck = (xtlock_t *) & tlck->lock;
+			xtlck->lwm.offset =
+			    (xtlck->lwm.offset) ? min(index,
+						      xtlck->lwm.
+						      offset) : index;
+
+			/* if delete from middle,
+			 * shift left/compact the remaining entries in the page
+			 */
+			if (index < nextindex - 1)
+				memmove(&p->xad[index], &p->xad[index + 1],
+					(nextindex - index -
+					 1) << L2XTSLOTSIZE);
+
+			p->header.nextindex =
+			    cpu_to_le16(le16_to_cpu(p->header.nextindex) -
+					1);
+			jEVENT(0,
+			       ("xtDeleteUp(entry): 0x%lx[%d]\n",
+				(ulong) parent->bn, index));
+		}
+
+		/* unpin the parent page */
+		XT_PUTPAGE(mp);
+
+		/* exit propagation up */
+		break;
+	}
+
+	return 0;
+}
+
+
+/*
+ * NAME:        xtRelocate()
+ *
+ * FUNCTION:    relocate xtpage or data extent of regular file;
+ *              This function is mainly used by defragfs utility.
+ *
+ * NOTE:        This routine does not have the logic to handle
+ *              uncommitted allocated extent. The caller should call
+ *              txCommit() to commit all the allocation before call
+ *              this routine.
+ */
+xtRelocate(tid_t tid, struct inode * ip, xad_t * oxad,	/* old XAD */
+	   s64 nxaddr,		/* new xaddr */
+	   int xtype)
+{				/* extent type: XTPAGE or DATAEXT */
+	int rc = 0;
+	tblock_t *tblk;
+	tlock_t *tlck;
+	xtlock_t *xtlck;
+	metapage_t *mp, *pmp, *lmp, *rmp;	/* meta-page buffer */
+	xtpage_t *p, *pp, *rp, *lp;	/* base B+-tree index page */
+	xad_t *xad;
+	pxd_t *pxd;
+	s64 xoff, xsize;
+	int xlen;
+	s64 oxaddr, sxaddr, dxaddr, nextbn, prevbn;
+	cbuf_t *cp;
+	s64 offset, nbytes, nbrd, pno;
+	int nb, npages, nblks;
+	s64 bn;
+	int cmp;
+	int index;
+	pxdlock_t *pxdlock;
+	btstack_t btstack;	/* traverse stack */
+
+	xtype = xtype & EXTENT_TYPE;
+
+	xoff = offsetXAD(oxad);
+	oxaddr = addressXAD(oxad);
+	xlen = lengthXAD(oxad);
+
+	/* validate extent offset */
+	offset = xoff << JFS_SBI(ip->i_sb)->l2bsize;
+	if (offset >= ip->i_size)
+		return ESTALE;	/* stale extent */
+
+	jEVENT(0,
+	       ("xtRelocate: xtype:%d xoff:0x%lx xlen:0x%x xaddr:0x%lx:0x%lx\n",
+		xtype, (ulong) xoff, xlen, (ulong) oxaddr,
+		(ulong) nxaddr));
+
+	/*
+	 *      1. get and validate the parent xtpage/xad entry
+	 *      covering the source extent to be relocated;
+	 */
+	if (xtype == DATAEXT) {
+		/* search in leaf entry */
+		rc = xtSearch(ip, xoff, &cmp, &btstack, 0);
+		if (rc)
+			return rc;
+		if (cmp) {
+			XT_PUTPAGE(pmp);
+			return ESTALE;
+		}
+
+		/* retrieve search result */
+		XT_GETSEARCH(ip, btstack.top, bn, pmp, pp, index);
+
+		/* validate for exact match with a single entry */
+		xad = &pp->xad[index];
+		if (addressXAD(xad) != oxaddr || lengthXAD(xad) != xlen) {
+			XT_PUTPAGE(pmp);
+			return ESTALE;
+		}
+	} else {		/* (xtype == XTPAGE) */
+
+		/* search in internal entry */
+		rc = xtSearchNode(ip, oxad, &cmp, &btstack, 0);
+		if (rc)
+			return rc;
+		if (cmp) {
+			XT_PUTPAGE(pmp);
+			return ESTALE;
+		}
+
+		/* retrieve search result */
+		XT_GETSEARCH(ip, btstack.top, bn, pmp, pp, index);
+
+		/* xtSearchNode() validated for exact match with a single entry
+		 */
+		xad = &pp->xad[index];
+	}
+	jEVENT(0, ("xtRelocate: parent xad entry validated.\n"));
+
+	/*
+	 *      2. relocate the extent
+	 */
+	if (xtype == DATAEXT) {
+		/* if the extent is allocated-but-not-recorded
+		 * there is no real data to be moved in this extent,
+		 */
+		if (xad->flag & XAD_NOTRECORDED)
+			goto out;
+		else
+			/* release xtpage for cmRead()/xtLookup() */
+			XT_PUTPAGE(pmp);
+
+		/*
+		 *      cmRelocate()
+		 *
+		 * copy target data pages to be relocated;
+		 *
+		 * data extent must start at page boundary and
+		 * multiple of page size (except the last data extent);
+		 * read in each page of the source data extent into cbuf,
+		 * update the cbuf extent descriptor of the page to be
+		 * homeward bound to new dst data extent
+		 * copy the data from the old extent to new extent.
+		 * copy is essential for compressed files to avoid problems
+		 * that can arise if there was a change in compression
+		 * algorithms.
+		 * it is a good strategy because it may disrupt cache
+		 * policy to keep the pages in memory afterwards.
+		 */
+		offset = xoff << JFS_SBI(ip->i_sb)->l2bsize;
+		assert((offset & CM_OFFSET) == 0);
+		nbytes = xlen << JFS_SBI(ip->i_sb)->l2bsize;
+		pno = offset >> CM_L2BSIZE;
+		npages = (nbytes + (CM_BSIZE - 1)) >> CM_L2BSIZE;
+/*
+                npages = ((offset + nbytes - 1) >> CM_L2BSIZE) -
+                         (offset >> CM_L2BSIZE) + 1;
+*/
+		sxaddr = oxaddr;
+		dxaddr = nxaddr;
+
+		/* process the request one cache buffer at a time */
+		for (nbrd = 0; nbrd < nbytes; nbrd += nb,
+		     offset += nb, pno++, npages--) {
+			/* compute page size */
+			nb = min(nbytes - nbrd, CM_BSIZE);
+
+			/* get the cache buffer of the page */
+			if (rc = cmRead(ip, offset, npages, &cp))
+				break;
+
+			assert(addressPXD(&cp->cm_pxd) == sxaddr);
+			assert(!cp->cm_modified);
+
+			/* bind buffer with the new extent address */
+			nblks = nb >> JFS_IP(ip->i_sb)->l2bsize;
+			cmSetXD(ip, cp, pno, dxaddr, nblks);
+
+			/* release the cbuf, mark it as modified */
+			cmPut(cp, TRUE);
+
+			dxaddr += nblks;
+			sxaddr += nblks;
+		}
+
+		/* get back parent page */
+		rc = xtSearch(ip, xoff, &cmp, &btstack, 0);
+		XT_GETSEARCH(ip, btstack.top, bn, pmp, pp, index);
+		jEVENT(0, ("xtRelocate: target data extent relocated.\n"));
+	} else {		/* (xtype  == XTPAGE) */
+
+		/*
+		 * read in the target xtpage from the source extent;
+		 */
+		XT_GETPAGE(ip, oxaddr, mp, PSIZE, p, rc);
+		if (rc) {
+			XT_PUTPAGE(pmp);
+			return rc;
+		}
+
+		/*
+		 * read in sibling pages if any to update sibling pointers;
+		 */
+		rmp = NULL;
+		if (p->header.next) {
+			nextbn = le64_to_cpu(p->header.next);
+			XT_GETPAGE(ip, nextbn, rmp, PSIZE, rp, rc);
+			if (rc) {
+				XT_PUTPAGE(pmp);
+				XT_PUTPAGE(mp);
+				return (rc);
+			}
+		}
+
+		lmp = NULL;
+		if (p->header.prev) {
+			prevbn = le64_to_cpu(p->header.prev);
+			XT_GETPAGE(ip, prevbn, lmp, PSIZE, lp, rc);
+			if (rc) {
+				XT_PUTPAGE(pmp);
+				XT_PUTPAGE(mp);
+				if (rmp)
+					XT_PUTPAGE(rmp);
+				return (rc);
+			}
+		}
+
+		/* at this point, all xtpages to be updated are in memory */
+
+		/*
+		 * update sibling pointers of sibling xtpages if any;
+		 */
+		if (lmp) {
+			BT_MARK_DIRTY(lmp, ip);
+			tlck =
+			    txLock(tid, ip, lmp, tlckXTREE | tlckRELINK);
+			lp->header.next = cpu_to_le64(nxaddr);
+			XT_PUTPAGE(lmp);
+		}
+
+		if (rmp) {
+			BT_MARK_DIRTY(rmp, ip);
+			tlck =
+			    txLock(tid, ip, rmp, tlckXTREE | tlckRELINK);
+			rp->header.prev = cpu_to_le64(nxaddr);
+			XT_PUTPAGE(rmp);
+		}
+
+		/*
+		 * update the target xtpage to be relocated
+		 *
+		 * update the self address of the target page
+		 * and write to destination extent;
+		 * redo image covers the whole xtpage since it is new page
+		 * to the destination extent;
+		 * update of bmap for the free of source extent
+		 * of the target xtpage itself:
+		 * update of bmap for the allocation of destination extent
+		 * of the target xtpage itself:
+		 * update of bmap for the extents covered by xad entries in
+		 * the target xtpage is not necessary since they are not
+		 * updated;
+		 * if not committed before this relocation,
+		 * target page may contain XAD_NEW entries which must
+		 * be scanned for bmap update (logredo() always
+		 * scan xtpage REDOPAGE image for bmap update);
+		 * if committed before this relocation (tlckRELOCATE),
+		 * scan may be skipped by commit() and logredo();
+		 */
+		BT_MARK_DIRTY(mp, ip);
+		/* tlckNEW init  xtlck->lwm.offset = XTENTRYSTART; */
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckNEW);
+		xtlck = (xtlock_t *) & tlck->lock;
+
+		/* update the self address in the xtpage header */
+		pxd = &p->header.self;
+		PXDaddress(pxd, nxaddr);
+
+		/* linelock for the after image of the whole page */
+		xtlck->lwm.length =
+		    le16_to_cpu(p->header.nextindex) - xtlck->lwm.offset;
+
+		/* update the buffer extent descriptor of target xtpage */
+		xsize = xlen << JFS_SBI(ip->i_sb)->l2bsize;
+		bmSetXD(mp, nxaddr, xsize);
+
+		/* unpin the target page to new homeward bound */
+		XT_PUTPAGE(mp);
+		jEVENT(0, ("xtRelocate: target xtpage relocated.\n"));
+	}
+
+	/*
+	 *      3. acquire maplock for the source extent to be freed;
+	 *
+	 * acquire a maplock saving the src relocated extent address;
+	 * to free of the extent at commit time;
+	 */
+      out:
+	/* if DATAEXT relocation, write a LOG_UPDATEMAP record for
+	 * free PXD of the source data extent (logredo() will update
+	 * bmap for free of source data extent), and update bmap for
+	 * free of the source data extent;
+	 */
+	if (xtype == DATAEXT)
+		tlck = txMaplock(tid, ip, tlckMAP);
+	/* if XTPAGE relocation, write a LOG_NOREDOPAGE record
+	 * for the source xtpage (logredo() will init NoRedoPage
+	 * filter and will also update bmap for free of the source
+	 * xtpage), and update bmap for free of the source xtpage;
+	 * N.B. We use tlckMAP instead of tlkcXTREE because there
+	 *      is no buffer associated with this lock since the buffer
+	 *      has been redirected to the target location.
+	 */
+	else			/* (xtype  == XTPAGE) */
+		tlck = txMaplock(tid, ip, tlckMAP | tlckRELOCATE);
+
+	pxdlock = (pxdlock_t *) & tlck->lock;
+	pxdlock->flag = mlckFREEPXD;
+	PXDaddress(&pxdlock->pxd, oxaddr);
+	PXDlength(&pxdlock->pxd, xlen);
+	pxdlock->index = 1;
+
+	/*
+	 *      4. update the parent xad entry for relocation;
+	 *
+	 * acquire tlck for the parent entry with XAD_NEW as entry
+	 * update which will write LOG_REDOPAGE and update bmap for
+	 * allocation of XAD_NEW destination extent;
+	 */
+	jEVENT(0, ("xtRelocate: update parent xad entry.\n"));
+	BT_MARK_DIRTY(pmp, ip);
+	tlck = txLock(tid, ip, pmp, tlckXTREE | tlckGROW);
+	xtlck = (xtlock_t *) & tlck->lock;
+
+	/* update the XAD with the new destination extent; */
+	xad = &pp->xad[index];
+	xad->flag |= XAD_NEW;
+	XADaddress(xad, nxaddr);
+
+	xtlck->lwm.offset = min(index, xtlck->lwm.offset);
+	xtlck->lwm.length = le16_to_cpu(pp->header.nextindex) -
+	    xtlck->lwm.offset;
+
+	/* unpin the parent xtpage */
+	XT_PUTPAGE(pmp);
+
+	return rc;
+}
+
+
+/*
+ *      xtSearchNode()
+ *
+ * function:    search for the internal xad entry covering specified extent.
+ *              This function is mainly used by defragfs utility.
+ *
+ * parameters:
+ *      ip      - file object;
+ *      xad     - extent to find;
+ *      cmpp    - comparison result:
+ *      btstack - traverse stack;
+ *      flag    - search process flag;
+ *
+ * returns:
+ *      btstack contains (bn, index) of search path traversed to the entry.
+ *      *cmpp is set to result of comparison with the entry returned.
+ *      the page containing the entry is pinned at exit.
+ */
+static int xtSearchNode(struct inode *ip, xad_t * xad,	/* required XAD entry */
+			int *cmpp, btstack_t * btstack, int flag)
+{
+	int rc = 0;
+	s64 xoff, xaddr;
+	int xlen;
+	int cmp = 1;		/* init for empty page */
+	s64 bn;			/* block number */
+	metapage_t *mp;		/* meta-page buffer */
+	xtpage_t *p;		/* page */
+	int base, index, lim;
+	btframe_t *btsp;
+	s64 t64;
+
+	BT_CLR(btstack);
+
+	xoff = offsetXAD(xad);
+	xlen = lengthXAD(xad);
+	xaddr = addressXAD(xad);
+
+	/*
+	 *      search down tree from root:
+	 *
+	 * between two consecutive entries of <Ki, Pi> and <Kj, Pj> of
+	 * internal page, child page Pi contains entry with k, Ki <= K < Kj.
+	 *
+	 * if entry with search key K is not found
+	 * internal page search find the entry with largest key Ki
+	 * less than K which point to the child page to search;
+	 * leaf page search find the entry with smallest key Kj
+	 * greater than K so that the returned index is the position of
+	 * the entry to be shifted right for insertion of new entry.
+	 * for empty tree, search key is greater than any key of the tree.
+	 *
+	 * by convention, root bn = 0.
+	 */
+	for (bn = 0;;) {
+		/* get/pin the page to search */
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+		if (p->header.flag & BT_LEAF)
+			return ESTALE;
+
+		lim = le16_to_cpu(p->header.nextindex) - XTENTRYSTART;
+
+		/*
+		 * binary search with search key K on the current page
+		 */
+		for (base = XTENTRYSTART; lim; lim >>= 1) {
+			index = base + (lim >> 1);
+
+			XT_CMP(cmp, xoff, &p->xad[index], t64);
+			if (cmp == 0) {
+				/*
+				 *      search hit
+				 *
+				 * verify for exact match;
+				 */
+				if (xaddr == addressXAD(&p->xad[index]) &&
+				    xoff == offsetXAD(&p->xad[index])) {
+					*cmpp = cmp;
+
+					/* save search result */
+					btsp = btstack->top;
+					btsp->bn = bn;
+					btsp->index = index;
+					btsp->mp = mp;
+
+					return 0;
+				}
+
+				/* descend/search its child page */
+				goto next;
+			}
+
+			if (cmp > 0) {
+				base = index + 1;
+				--lim;
+			}
+		}
+
+		/*
+		 *      search miss - non-leaf page:
+		 *
+		 * base is the smallest index with key (Kj) greater than
+		 * search key (K) and may be zero or maxentry index.
+		 * if base is non-zero, decrement base by one to get the parent
+		 * entry of the child page to search.
+		 */
+		index = base ? base - 1 : base;
+
+		/*
+		 * go down to child page
+		 */
+	      next:
+		/* get the child page block number */
+		bn = addressXAD(&p->xad[index]);
+
+		/* unpin the parent page */
+		XT_PUTPAGE(mp);
+	}
+}
+
+
+/*
+ *      xtRelink()
+ *
+ * function:
+ *      link around a freed page.
+ *
+ * Parameter:
+ *      int           tid,
+ *      struct inode    *ip,
+ *      xtpage_t        *p)
+ *
+ * returns:
+ */
+static int xtRelink(tid_t tid, struct inode *ip, xtpage_t * p)
+{
+	int rc = 0;
+	metapage_t *mp;
+	s64 nextbn, prevbn;
+	tlock_t *tlck;
+
+	nextbn = le64_to_cpu(p->header.next);
+	prevbn = le64_to_cpu(p->header.prev);
+
+	/* update prev pointer of the next page */
+	if (nextbn != 0) {
+		XT_GETPAGE(ip, nextbn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/*
+		 * acquire a transaction lock on the page;
+		 *
+		 * action: update prev pointer;
+		 */
+		BT_MARK_DIRTY(mp, ip);
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckRELINK);
+
+		/* the page may already have been tlock'd */
+
+		p->header.prev = cpu_to_le64(prevbn);
+
+		XT_PUTPAGE(mp);
+	}
+
+	/* update next pointer of the previous page */
+	if (prevbn != 0) {
+		XT_GETPAGE(ip, prevbn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+
+		/*
+		 * acquire a transaction lock on the page;
+		 *
+		 * action: update next pointer;
+		 */
+		BT_MARK_DIRTY(mp, ip);
+		tlck = txLock(tid, ip, mp, tlckXTREE | tlckRELINK);
+
+		/* the page may already have been tlock'd */
+
+		p->header.next = le64_to_cpu(nextbn);
+
+		XT_PUTPAGE(mp);
+	}
+
+	return 0;
+}
+#endif				/*  _STILL_TO_PORT */
+
+
+/*
+ *      xtInitRoot()
+ *
+ * initialize file root (inline in inode)
+ */
+void xtInitRoot(tid_t tid, struct inode *ip)
+{
+	xtpage_t *p;
+	tlock_t *tlck;
+
+	/*
+	 * acquire a transaction lock on the root
+	 *
+	 * action:
+	 */
+	tlck = txLock(tid, ip, (metapage_t *) &JFS_IP(ip)->bxflag,
+		      tlckXTREE | tlckNEW);
+	p = &JFS_IP(ip)->i_xtroot;
+
+	p->header.flag = DXD_INDEX | BT_ROOT | BT_LEAF;
+	p->header.nextindex = cpu_to_le16(XTENTRYSTART);
+
+	if (S_ISDIR(ip->i_mode))
+		p->header.maxentry = cpu_to_le16(XTROOTINITSLOT_DIR);
+	else {
+		p->header.maxentry = cpu_to_le16(XTROOTINITSLOT);
+		ip->i_size = 0;
+	}
+
+
+	return;
+}
+
+
+/*
+ * We can run into a deadlock truncating a file with a large number of
+ * xtree pages (large fragmented file).  A robust fix would entail a
+ * reservation system where we would reserve a number of metadata pages
+ * and tlocks which we would be guaranteed without a deadlock.  Without
+ * this, a partial fix is to limit number of metadata pages we will lock
+ * in a single transaction.  Currently we will truncate the file so that
+ * no more than 50 leaf pages will be locked.  The caller of xtTruncate
+ * will be responsible for ensuring that the current transaction gets
+ * committed, and that subsequent transactions are created to truncate
+ * the file further if needed.
+ */
+#define MAX_TRUNCATE_LEAVES 50
+
+/*
+ *      xtTruncate()
+ *
+ * function:
+ *      traverse for truncation logging backward bottom up;
+ *      terminate at the last extent entry at the current subtree
+ *      root page covering new down size.
+ *      truncation may occur within the last extent entry.
+ *
+ * parameter:
+ *      int           tid,
+ *      struct inode    *ip,
+ *      s64           newsize,
+ *      int           type)   {PWMAP, PMAP, WMAP; DELETE, TRUNCATE}
+ *
+ * return:
+ *
+ * note:
+ *      PWMAP:
+ *       1. truncate (non-COMMIT_NOLINK file)
+ *          by jfs_truncate() or jfs_open(O_TRUNC):
+ *          xtree is updated;
+ *	 2. truncate index table of directory when last entry removed
+ *       map update via tlock at commit time;
+ *      PMAP:
+ *	 Call xtTruncate_pmap instead
+ *      WMAP:
+ *       1. remove (free zero link count) on last reference release
+ *          (pmap has been freed at commit zero link count);
+ *       2. truncate (COMMIT_NOLINK file, i.e., tmp file):
+ *          xtree is updated;
+ *       map update directly at truncation time;
+ *
+ *      if (DELETE)
+ *              no LOG_NOREDOPAGE is required (NOREDOFILE is sufficient);
+ *      else if (TRUNCATE)
+ *              must write LOG_NOREDOPAGE for deleted index page;
+ *
+ * pages may already have been tlocked by anonymous transactions
+ * during file growth (i.e., write) before truncation;
+ *
+ * except last truncated entry, deleted entries remains as is
+ * in the page (nextindex is updated) for other use
+ * (e.g., log/update allocation map): this avoid copying the page
+ * info but delay free of pages;
+ *
+ */
+s64 xtTruncate(tid_t tid, struct inode *ip, s64 newsize, int flag)
+{
+	int rc = 0;
+	s64 teof;
+	metapage_t *mp;
+	xtpage_t *p;
+	s64 bn;
+	int index, nextindex;
+	xad_t *xad;
+	s64 xoff, xaddr;
+	int xlen, len, freexlen;
+	btstack_t btstack;
+	btframe_t *parent;
+	tblock_t *tblk = 0;
+	tlock_t *tlck = 0;
+	xtlock_t *xtlck = 0;
+	xdlistlock_t xadlock;	/* maplock for COMMIT_WMAP */
+	pxdlock_t *pxdlock;	/* maplock for COMMIT_WMAP */
+	s64 nfreed;
+	int freed, log;
+	int locked_leaves = 0;
+
+	/* save object truncation type */
+	if (tid) {
+		tblk = tid_to_tblock(tid);
+		tblk->xflag |= flag;
+	}
+
+	nfreed = 0;
+
+	flag &= COMMIT_MAP;
+	assert(flag != COMMIT_PMAP);
+
+	if (flag == COMMIT_PWMAP)
+		log = 1;
+	else {
+		log = 0;
+		xadlock.flag = mlckFREEXADLIST;
+		xadlock.index = 1;
+	}
+
+	/*
+	 * if the newsize is not an integral number of pages,
+	 * the file between newsize and next page boundary will
+	 * be cleared.
+	 * if truncating into a file hole, it will cause
+	 * a full block to be allocated for the logical block.
+	 */
+
+	/*
+	 * release page blocks of truncated region <teof, eof>
+	 *
+	 * free the data blocks from the leaf index blocks.
+	 * delete the parent index entries corresponding to
+	 * the freed child data/index blocks.
+	 * free the index blocks themselves which aren't needed
+	 * in new sized file.
+	 *
+	 * index blocks are updated only if the blocks are to be
+	 * retained in the new sized file.
+	 * if type is PMAP, the data and index pages are NOT
+	 * freed, and the data and index blocks are NOT freed
+	 * from  working map.
+	 * (this will allow continued access of data/index of
+	 * temporary file (zerolink count file truncated to zero-length)).
+	 */
+	teof = (newsize + (JFS_SBI(ip->i_sb)->bsize - 1)) >>
+	    JFS_SBI(ip->i_sb)->l2bsize;
+
+	/* clear stack */
+	BT_CLR(&btstack);
+
+	/*
+	 * start with root
+	 *
+	 * root resides in the inode
+	 */
+	bn = 0;
+
+	/*
+	 * first access of each page:
+	 */
+      getPage:
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return -rc;
+
+	/* process entries backward from last index */
+	index = le16_to_cpu(p->header.nextindex) - 1;
+
+	if (p->header.flag & BT_INTERNAL)
+		goto getChild;
+
+	/*
+	 *      leaf page
+	 */
+
+	/* Since this is the rightmost leaf, and we may have already freed
+	 * a page that was formerly to the right, let's make sure that the
+	 * next pointer is zero.
+	 */
+	p->header.next = 0;
+
+	freed = 0;
+
+	/* does region covered by leaf page precede Teof ? */
+	xad = &p->xad[index];
+	xoff = offsetXAD(xad);
+	xlen = lengthXAD(xad);
+	if (teof >= xoff + xlen) {
+		XT_PUTPAGE(mp);
+		goto getParent;
+	}
+
+	/* (re)acquire tlock of the leaf page */
+	if (log) {
+		if (++locked_leaves > MAX_TRUNCATE_LEAVES) {
+			/*
+			 * We need to limit the size of the transaction
+			 * to avoid exhausting pagecache & tlocks
+			 */
+			XT_PUTPAGE(mp);
+			newsize = (xoff + xlen) << JFS_SBI(ip->i_sb)->l2bsize;
+			goto getParent;
+		}
+		tlck = txLock(tid, ip, mp, tlckXTREE);
+		tlck->type = tlckXTREE | tlckTRUNCATE;
+		xtlck = (xtlock_t *) & tlck->lock;
+		xtlck->hwm.offset = le16_to_cpu(p->header.nextindex) - 1;
+	}
+	BT_MARK_DIRTY(mp, ip);
+
+	/*
+	 * scan backward leaf page entries
+	 */
+	for (; index >= XTENTRYSTART; index--) {
+		xad = &p->xad[index];
+		xoff = offsetXAD(xad);
+		xlen = lengthXAD(xad);
+		xaddr = addressXAD(xad);
+
+		/*
+		 * entry beyond eof: continue scan of current page
+		 *          xad
+		 * ---|---=======------->
+		 *   eof
+		 */
+		if (teof < xoff) {
+			nfreed += xlen;
+			continue;
+		}
+
+		/*
+		 * (xoff <= teof): last entry to be deleted from page;
+		 * If other entries remain in page: keep and update the page.
+		 */
+
+		/*
+		 * eof == entry_start: delete the entry
+		 *           xad
+		 * -------|=======------->
+		 *       eof
+		 *
+		 */
+		if (teof == xoff) {
+			nfreed += xlen;
+
+			if (index == XTENTRYSTART)
+				break;
+
+			nextindex = index;
+		}
+		/*
+		 * eof within the entry: truncate the entry.
+		 *          xad
+		 * -------===|===------->
+		 *          eof
+		 */
+		else if (teof < xoff + xlen) {
+			/* update truncated entry */
+			len = teof - xoff;
+			freexlen = xlen - len;
+			XADlength(xad, len);
+
+			/* save pxd of truncated extent in tlck */
+			xaddr += len;
+			if (log) {	/* COMMIT_PWMAP */
+				xtlck->lwm.offset = (xtlck->lwm.offset) ?
+				    min(index, (int)xtlck->lwm.offset) : index;
+				xtlck->lwm.length = index + 1 -
+				    xtlck->lwm.offset;
+				pxdlock = (pxdlock_t *) & xtlck->pxdlock;
+				pxdlock->flag = mlckFREEPXD;
+				PXDaddress(&pxdlock->pxd, xaddr);
+				PXDlength(&pxdlock->pxd, freexlen);
+			}
+			/* free truncated extent */
+			else {	/* COMMIT_WMAP */
+
+				pxdlock = (pxdlock_t *) & xadlock;
+				pxdlock->flag = mlckFREEPXD;
+				PXDaddress(&pxdlock->pxd, xaddr);
+				PXDlength(&pxdlock->pxd, freexlen);
+				txFreeMap(ip, pxdlock, 0, COMMIT_WMAP);
+
+				/* reset map lock */
+				xadlock.flag = mlckFREEXADLIST;
+			}
+
+			/* current entry is new last entry; */
+			nextindex = index + 1;
+
+			nfreed += freexlen;
+		}
+		/*
+		 * eof beyond the entry:
+		 *          xad
+		 * -------=======---|--->
+		 *                 eof
+		 */
+		else {		/* (xoff + xlen < teof) */
+
+			nextindex = index + 1;
+		}
+
+		if (nextindex < le16_to_cpu(p->header.nextindex)) {
+			if (!log) {	/* COMMIT_WAMP */
+				xadlock.xdlist = &p->xad[nextindex];
+				xadlock.count =
+				    le16_to_cpu(p->header.nextindex) -
+				    nextindex;
+				txFreeMap(ip, (maplock_t *) & xadlock, 0,
+					  COMMIT_WMAP);
+			}
+			p->header.nextindex = cpu_to_le16(nextindex);
+		}
+
+		XT_PUTPAGE(mp);
+
+		/* assert(freed == 0); */
+		goto getParent;
+	}			/* end scan of leaf page entries */
+
+	freed = 1;
+
+	/*
+	 * leaf page become empty: free the page if type != PMAP
+	 */
+	if (log) {		/* COMMIT_PWMAP */
+		/* txCommit() with tlckFREE:
+		 * free data extents covered by leaf [XTENTRYSTART:hwm);
+		 * invalidate leaf if COMMIT_PWMAP;
+		 * if (TRUNCATE), will write LOG_NOREDOPAGE;
+		 */
+		tlck->type = tlckXTREE | tlckFREE;
+	} else {		/* COMMIT_WAMP */
+
+		/* free data extents covered by leaf */
+		xadlock.xdlist = &p->xad[XTENTRYSTART];
+		xadlock.count =
+		    le16_to_cpu(p->header.nextindex) - XTENTRYSTART;
+		txFreeMap(ip, (maplock_t *) & xadlock, 0, COMMIT_WMAP);
+	}
+
+	if (p->header.flag & BT_ROOT) {
+		p->header.flag &= ~BT_INTERNAL;
+		p->header.flag |= BT_LEAF;
+		p->header.nextindex = cpu_to_le16(XTENTRYSTART);
+
+		XT_PUTPAGE(mp);	/* debug */
+		goto out;
+	} else {
+		if (log) {	/* COMMIT_PWMAP */
+			/* page will be invalidated at tx completion
+			 */
+			XT_PUTPAGE(mp);
+		} else {	/* COMMIT_WMAP */
+
+			if (mp->lid)
+				lid_to_tlock(mp->lid)->flag |= tlckFREELOCK;
+
+			/* invalidate empty leaf page */
+			discard_metapage(mp);
+		}
+	}
+
+	/*
+	 * the leaf page become empty: delete the parent entry
+	 * for the leaf page if the parent page is to be kept
+	 * in the new sized file.
+	 */
+
+	/*
+	 * go back up to the parent page
+	 */
+      getParent:
+	/* pop/restore parent entry for the current child page */
+	if ((parent = BT_POP(&btstack)) == NULL)
+		/* current page must have been root */
+		goto out;
+
+	/* get back the parent page */
+	bn = parent->bn;
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return -rc;
+
+	index = parent->index;
+
+	/*
+	 * child page was not empty:
+	 */
+	if (freed == 0) {
+		/* has any entry deleted from parent ? */
+		if (index < le16_to_cpu(p->header.nextindex) - 1) {
+			/* (re)acquire tlock on the parent page */
+			if (log) {	/* COMMIT_PWMAP */
+				/* txCommit() with tlckTRUNCATE:
+				 * free child extents covered by parent [);
+				 */
+				tlck = txLock(tid, ip, mp, tlckXTREE);
+				xtlck = (xtlock_t *) & tlck->lock;
+				xtlck->twm.offset = index;
+				if (!(tlck->type & tlckTRUNCATE)) {
+					xtlck->hwm.offset =
+					    le16_to_cpu(p->header.
+							nextindex) - 1;
+					tlck->type =
+					    tlckXTREE | tlckTRUNCATE;
+				}
+			} else {	/* COMMIT_WMAP */
+
+				/* free child extents covered by parent */
+				xadlock.xdlist = &p->xad[index + 1];
+				xadlock.count =
+				    le16_to_cpu(p->header.nextindex) -
+				    index - 1;
+				txFreeMap(ip, (maplock_t *) & xadlock, 0,
+					  COMMIT_WMAP);
+			}
+			BT_MARK_DIRTY(mp, ip);
+
+			p->header.nextindex = cpu_to_le16(index + 1);
+		}
+		XT_PUTPAGE(mp);
+		goto getParent;
+	}
+
+	/*
+	 * child page was empty:
+	 */
+	nfreed += lengthXAD(&p->xad[index]);
+
+	/*
+	 * During working map update, child page's tlock must be handled
+	 * before parent's.  This is because the parent's tlock will cause
+	 * the child's disk space to be marked available in the wmap, so
+	 * it's important that the child page be released by that time.
+	 *
+	 * ToDo:  tlocks should be on doubly-linked list, so we can
+	 * quickly remove it and add it to the end.
+	 */
+
+	/*
+	 * Move parent page's tlock to the end of the tid's tlock list
+	 */
+	if (log && mp->lid && (tblk->last != mp->lid) &&
+	    lid_to_tlock(mp->lid)->tid) {
+		lid_t lid = mp->lid;
+		tlock_t *prev;
+
+		tlck = lid_to_tlock(lid);
+
+		if (tblk->next == lid)
+			tblk->next = tlck->next;
+		else {
+			for (prev = lid_to_tlock(tblk->next);
+			     prev->next != lid;
+			     prev = lid_to_tlock(prev->next)) {
+				assert(prev->next);
+			}
+			prev->next = tlck->next;
+		}
+		lid_to_tlock(tblk->last)->next = lid;
+		tlck->next = 0;
+		tblk->last = lid;
+	}
+
+	/*
+	 * parent page become empty: free the page
+	 */
+	if (index == XTENTRYSTART) {
+		if (log) {	/* COMMIT_PWMAP */
+			/* txCommit() with tlckFREE:
+			 * free child extents covered by parent;
+			 * invalidate parent if COMMIT_PWMAP;
+			 */
+			tlck = txLock(tid, ip, mp, tlckXTREE);
+			xtlck = (xtlock_t *) & tlck->lock;
+			xtlck->twm.offset = index;
+			xtlck->hwm.offset =
+			    le16_to_cpu(p->header.nextindex) - 1;
+			tlck->type = tlckXTREE | tlckFREE;
+		} else {	/* COMMIT_WMAP */
+
+			/* free child extents covered by parent */
+			xadlock.xdlist = &p->xad[XTENTRYSTART];
+			xadlock.count =
+			    le16_to_cpu(p->header.nextindex) -
+			    XTENTRYSTART;
+			txFreeMap(ip, (maplock_t *) & xadlock, 0,
+				  COMMIT_WMAP);
+		}
+		BT_MARK_DIRTY(mp, ip);
+
+		if (p->header.flag & BT_ROOT) {
+			p->header.flag &= ~BT_INTERNAL;
+			p->header.flag |= BT_LEAF;
+			p->header.nextindex = cpu_to_le16(XTENTRYSTART);
+			if (le16_to_cpu(p->header.maxentry) == XTROOTMAXSLOT) {
+				/*
+				 * Shrink root down to allow inline
+				 * EA (otherwise fsck complains)
+				 */
+				p->header.maxentry =
+				    cpu_to_le16(XTROOTINITSLOT);
+				JFS_IP(ip)->mode2 |= INLINEEA;
+			}
+
+			XT_PUTPAGE(mp);	/* debug */
+			goto out;
+		} else {
+			if (log) {	/* COMMIT_PWMAP */
+				/* page will be invalidated at tx completion
+				 */
+				XT_PUTPAGE(mp);
+			} else {	/* COMMIT_WMAP */
+
+				if (mp->lid)
+					lid_to_tlock(mp->lid)->flag |=
+						tlckFREELOCK;
+
+				/* invalidate parent page */
+				discard_metapage(mp);
+			}
+
+			/* parent has become empty and freed:
+			 * go back up to its parent page
+			 */
+			/* freed = 1; */
+			goto getParent;
+		}
+	}
+	/*
+	 * parent page still has entries for front region;
+	 */
+	else {
+		/* try truncate region covered by preceding entry
+		 * (process backward)
+		 */
+		index--;
+
+		/* go back down to the child page corresponding
+		 * to the entry
+		 */
+		goto getChild;
+	}
+
+	/*
+	 *      internal page: go down to child page of current entry
+	 */
+      getChild:
+	/* save current parent entry for the child page */
+	BT_PUSH(&btstack, bn, index);
+
+	/* get child page */
+	xad = &p->xad[index];
+	bn = addressXAD(xad);
+
+	/*
+	 * first access of each internal entry:
+	 */
+	/* release parent page */
+	XT_PUTPAGE(mp);
+
+	/* process the child page */
+	goto getPage;
+
+      out:
+	/*
+	 * update file resource stat
+	 */
+	/* set size
+	 */
+	if (S_ISDIR(ip->i_mode) && !newsize)
+		ip->i_size = 1;	/* fsck hates zero-length directories */
+	else
+		ip->i_size = newsize;
+
+	/* update nblocks to reflect freed blocks */
+	ip->i_blocks -= LBLK2PBLK(ip->i_sb, nfreed);
+
+	/*
+	 * free tlock of invalidated pages
+	 */
+	if (flag == COMMIT_WMAP)
+		txFreelock(ip);
+
+	return newsize;
+}
+
+
+/*
+ *      xtTruncate_pmap()
+ *
+ * function:
+ *	Perform truncate to zero lenghth for deleted file, leaving the
+ *	the xtree and working map untouched.  This allows the file to
+ *	be accessed via open file handles, while the delete of the file
+ *	is committed to disk.
+ *
+ * parameter:
+ *      tid_t		tid,
+ *      struct inode	*ip,
+ *      s64		committed_size)
+ *
+ * return: new committed size
+ *
+ * note:
+ *
+ *	To avoid deadlock by holding too many transaction locks, the
+ *	truncation may be broken up into multiple transactions.
+ *	The committed_size keeps track of part of the file has been
+ *	freed from the pmaps.
+ */
+s64 xtTruncate_pmap(tid_t tid, struct inode *ip, s64 committed_size)
+{
+	s64 bn;
+	btstack_t btstack;
+	int cmp;
+	int index;
+	int locked_leaves = 0;
+	metapage_t *mp;
+	xtpage_t *p;
+	btframe_t *parent;
+	int rc;
+	tblock_t *tblk;
+	tlock_t *tlck = 0;
+	xad_t *xad;
+	int xlen;
+	s64 xoff;
+	xtlock_t *xtlck = 0;
+
+	/* save object truncation type */
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_PMAP;
+
+	/* clear stack */
+	BT_CLR(&btstack);
+
+	if (committed_size) {
+		xoff = (committed_size >> JFS_SBI(ip->i_sb)->l2bsize) - 1;
+		rc = xtSearch(ip, xoff, &cmp, &btstack, 0);
+		if (rc)
+			return -rc;
+		assert(cmp == 0);
+		XT_GETSEARCH(ip, btstack.top, bn, mp, p, index);
+	} else {
+		/*
+		 * start with root
+		 *
+		 * root resides in the inode
+		 */
+		bn = 0;
+
+		/*
+		 * first access of each page:
+		 */
+      getPage:
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return -rc;
+
+		/* process entries backward from last index */
+		index = le16_to_cpu(p->header.nextindex) - 1;
+
+		if (p->header.flag & BT_INTERNAL)
+			goto getChild;
+	}
+
+	/*
+	 *      leaf page
+	 */
+
+	if (++locked_leaves > MAX_TRUNCATE_LEAVES) {
+		/*
+		 * We need to limit the size of the transaction
+		 * to avoid exhausting pagecache & tlocks
+		 */
+		xad = &p->xad[index];
+		xoff = offsetXAD(xad);
+		xlen = lengthXAD(xad);
+		XT_PUTPAGE(mp);
+		return  (xoff + xlen) << JFS_SBI(ip->i_sb)->l2bsize;
+	}
+	tlck = txLock(tid, ip, mp, tlckXTREE);
+	tlck->type = tlckXTREE | tlckTRUNCATE;
+	xtlck = (xtlock_t *) & tlck->lock;
+	xtlck->hwm.offset = index;
+
+	tlck->type = tlckXTREE | tlckFREE;
+
+	XT_PUTPAGE(mp);
+
+	/*
+	 * go back up to the parent page
+	 */
+      getParent:
+	/* pop/restore parent entry for the current child page */
+	if ((parent = BT_POP(&btstack)) == NULL)
+		/* current page must have been root */
+		goto out;
+
+	/* get back the parent page */
+	bn = parent->bn;
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return -rc;
+
+	index = parent->index;
+
+	/*
+	 * parent page become empty: free the page
+	 */
+	if (index == XTENTRYSTART) {
+		/* txCommit() with tlckFREE:
+		 * free child extents covered by parent;
+		 * invalidate parent if COMMIT_PWMAP;
+		 */
+		tlck = txLock(tid, ip, mp, tlckXTREE);
+		xtlck = (xtlock_t *) & tlck->lock;
+		xtlck->twm.offset = index;
+		xtlck->hwm.offset =
+		    le16_to_cpu(p->header.nextindex) - 1;
+		tlck->type = tlckXTREE | tlckFREE;
+
+		XT_PUTPAGE(mp);
+
+		if (p->header.flag & BT_ROOT) {
+
+			goto out;
+		} else {
+			goto getParent;
+		}
+	}
+	/*
+	 * parent page still has entries for front region;
+	 */
+	else
+		index--;
+	/*
+	 *      internal page: go down to child page of current entry
+	 */
+      getChild:
+	/* save current parent entry for the child page */
+	BT_PUSH(&btstack, bn, index);
+
+	/* get child page */
+	xad = &p->xad[index];
+	bn = addressXAD(xad);
+
+	/*
+	 * first access of each internal entry:
+	 */
+	/* release parent page */
+	XT_PUTPAGE(mp);
+
+	/* process the child page */
+	goto getPage;
+
+      out:
+
+	return 0;
+}
+
+
+#ifdef _JFS_DEBUG_XTREE
+/*
+ *      xtDisplayTree()
+ *
+ * function: traverse forward
+ */
+int xtDisplayTree(struct inode *ip)
+{
+	int rc = 0;
+	metapage_t *mp;
+	xtpage_t *p;
+	s64 bn, pbn;
+	int index, lastindex, v, h;
+	xad_t *xad;
+	btstack_t btstack;
+	btframe_t *btsp;
+	btframe_t *parent;
+
+	printk("display B+-tree.\n");
+
+	/* clear stack */
+	btsp = btstack.stack;
+
+	/*
+	 * start with root
+	 *
+	 * root resides in the inode
+	 */
+	bn = 0;
+	v = h = 0;
+
+	/*
+	 * first access of each page:
+	 */
+      getPage:
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/* process entries forward from first index */
+	index = XTENTRYSTART;
+	lastindex = le16_to_cpu(p->header.nextindex) - 1;
+
+	if (p->header.flag & BT_INTERNAL) {
+		/*
+		 * first access of each internal page
+		 */
+		goto getChild;
+	} else {		/* (p->header.flag & BT_LEAF) */
+
+		/*
+		 * first access of each leaf page
+		 */
+		printf("leaf page ");
+		xtDisplayPage(ip, bn, p);
+
+		/* unpin the leaf page */
+		XT_PUTPAGE(mp);
+	}
+
+	/*
+	 * go back up to the parent page
+	 */
+      getParent:
+	/* pop/restore parent entry for the current child page */
+	if ((parent = (btsp == btstack.stack ? NULL : --btsp)) == NULL)
+		/* current page must have been root */
+		return;
+
+	/*
+	 * parent page scan completed
+	 */
+	if ((index = parent->index) == (lastindex = parent->lastindex)) {
+		/* go back up to the parent page */
+		goto getParent;
+	}
+
+	/*
+	 * parent page has entries remaining
+	 */
+	/* get back the parent page */
+	bn = parent->bn;
+	/* v = parent->level; */
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/* get next parent entry */
+	index++;
+
+	/*
+	 * internal page: go down to child page of current entry
+	 */
+      getChild:
+	/* push/save current parent entry for the child page */
+	btsp->bn = pbn = bn;
+	btsp->index = index;
+	btsp->lastindex = lastindex;
+	/* btsp->level = v; */
+	/* btsp->node = h; */
+	++btsp;
+
+	/* get child page */
+	xad = &p->xad[index];
+	bn = addressXAD(xad);
+
+	/*
+	 * first access of each internal entry:
+	 */
+	/* release parent page */
+	XT_PUTPAGE(mp);
+
+	printk("traverse down 0x%lx[%d]->0x%lx\n", (ulong) pbn, index,
+	       (ulong) bn);
+	v++;
+	h = index;
+
+	/* process the child page */
+	goto getPage;
+}
+
+
+/*
+ *      xtDisplayPage()
+ *
+ * function: display page
+ */
+int xtDisplayPage(struct inode *ip, s64 bn, xtpage_t * p)
+{
+	int rc = 0;
+	metapage_t *mp;
+	xad_t *xad;
+	s64 xaddr, xoff;
+	int xlen, i, j;
+
+	if (p == NULL) {
+		XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+		if (rc)
+			return rc;
+	}
+
+	/* display page control */
+	printf("bn:0x%lx flag:0x%x nextindex:%d\n",
+	       (ulong) bn, p->header.flag,
+	       le16_to_cpu(p->header.nextindex));
+
+	/* display entries */
+	xad = &p->xad[XTENTRYSTART];
+		for (i = XTENTRYSTART, j = 1; i < le16_to_cpu(p->header.nextindex);
+		     i++, xad++, j++) {
+			xoff = offsetXAD(xad);
+			xaddr = addressXAD(xad);
+			xlen = lengthXAD(xad);
+			printf("\t[%d] 0x%lx:0x%lx(0x%x)", i, (ulong) xoff,
+			       (ulong) xaddr, xlen);
+
+			if (j == 4) {
+				printf("\n");
+				j = 0;
+		}
+	}
+
+	printf("\n");
+}
+#endif				/* _JFS_DEBUG_XTREE */
+
+
+#ifdef _JFS_WIP
+/*
+ *      xtGather()
+ *
+ * function:
+ *      traverse for allocation acquiring tlock at commit time
+ *      (vs at the time of update) logging backward top down
+ *
+ * note:
+ *      problem - establishing that all new allocation have been
+ *      processed both for append and random write in sparse file
+ *      at the current entry at the current subtree root page
+ *
+ */
+int xtGather(t)
+btree_t *t;
+{
+	int rc = 0;
+	xtpage_t *p;
+	u64 bn;
+	int index;
+	btentry_t *e;
+	btstack_t btstack;
+	struct btsf *parent;
+
+	/* clear stack */
+	BT_CLR(&btstack);
+
+	/*
+	 * start with root
+	 *
+	 * root resides in the inode
+	 */
+	bn = 0;
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/* new root is NOT pointed by a new entry
+	   if (p->header.flag & NEW)
+	   allocate new page lock;
+	   write a NEWPAGE log;
+	 */
+
+      dopage:
+	/*
+	 * first access of each page:
+	 */
+	/* process entries backward from last index */
+	index = le16_to_cpu(p->header.nextindex) - 1;
+
+	if (p->header.flag & BT_LEAF) {
+		/*
+		 * first access of each leaf page
+		 */
+		/* process leaf page entries backward */
+		for (; index >= XTENTRYSTART; index--) {
+			e = &p->xad[index];
+			/*
+			 * if newpage, log NEWPAGE.
+			 *
+			 if (e->flag & XAD_NEW) {
+			 nfound =+ entry->length;
+			 update current page lock for the entry;
+			 newpage(entry);
+			 *
+			 * if moved, log move.
+			 *
+			 } else if (e->flag & XAD_MOVED) {
+			 reset flag;
+			 update current page lock for the entry;
+			 }
+			 */
+		}
+
+		/* unpin the leaf page */
+		XT_PUTPAGE(mp);
+
+		/*
+		 * go back up to the parent page
+		 */
+	      getParent:
+		/* restore parent entry for the current child page */
+		if ((parent = BT_POP(&btstack)) == NULL)
+			/* current page must have been root */
+			return 0;
+
+		if ((index = parent->index) == XTENTRYSTART) {
+			/*
+			 * parent page scan completed
+			 */
+			/* go back up to the parent page */
+			goto getParent;
+		} else {
+			/*
+			 * parent page has entries remaining
+			 */
+			/* get back the parent page */
+			bn = parent->bn;
+			XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+			if (rc)
+				return EIO;
+
+			/* first subroot page which
+			 * covers all new allocated blocks
+			 * itself not new/modified.
+			 * (if modified from split of descendent,
+			 * go down path of split page)
+
+			 if (nfound == nnew &&
+			 !(p->header.flag & (NEW | MOD)))
+			 exit scan;
+			 */
+
+			/* process parent page entries backward */
+			index--;
+		}
+	} else {
+		/*
+		 * first access of each internal page
+		 */
+	}
+
+	/*
+	 * internal page: go down to child page of current entry
+	 */
+
+	/* save current parent entry for the child page */
+	BT_PUSH(&btstack, bn, index);
+
+	/* get current entry for the child page */
+	e = &p->xad[index];
+
+	/*
+	 * first access of each internal entry:
+	 */
+	/*
+	 * if new entry, log btree_tnewentry.
+	 *
+	 if (e->flag & XAD_NEW)
+	 update parent page lock for the entry;
+	 */
+
+	/* release parent page */
+	XT_PUTPAGE(mp);
+
+	/* get child page */
+	bn = e->bn;
+	XT_GETPAGE(ip, bn, mp, PSIZE, p, rc);
+	if (rc)
+		return rc;
+
+	/*
+	 * first access of each non-root page:
+	 */
+	/*
+	 * if new, log btree_newpage.
+	 *
+	 if (p->header.flag & NEW)
+	 allocate new page lock;
+	 write a NEWPAGE log (next, prev);
+	 */
+
+	/* process the child page */
+	goto dopage;
+
+      out:
+	return 0;
+}
+#endif				/* _JFS_WIP */
+
+
+#ifdef CONFIG_JFS_STATISTICS
+int jfs_xtstat_read(char *buffer, char **start, off_t offset, int length,
+		    int *eof, void *data)
+{
+	int len = 0;
+	off_t begin;
+
+	len += sprintf(buffer,
+		       "JFS Xtree statistics\n"
+		       "====================\n"
+		       "searches = %d\n"
+		       "fast searches = %d\n"
+		       "splits = %d\n",
+		       xtStat.search,
+		       xtStat.fastSearch,
+		       xtStat.split);
+
+	begin = offset;
+	*start = buffer + begin;
+	len -= begin;
+
+	if (len > length)
+		len = length;
+	else
+		*eof = 1;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/jfs_xtree.h linuxppc64_2_4/fs/jfs/jfs_xtree.h
--- linux-2.4.19/fs/jfs/jfs_xtree.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/jfs_xtree.h	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,143 @@
+/*
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+*/
+/*
+ * Change History :
+ *
+*/
+
+#ifndef _H_JFS_XTREE
+#define _H_JFS_XTREE
+
+/*
+ *      jfs_xtree.h: extent allocation descriptor B+-tree manager
+ */
+
+#include "jfs_btree.h"
+
+
+/*
+ *      extent allocation descriptor (xad)
+ */
+typedef struct xad {
+	unsigned flag:8;	/* 1: flag */
+	unsigned rsvrd:16;	/* 2: reserved */
+	unsigned off1:8;	/* 1: offset in unit of fsblksize */
+	u32 off2;		/* 4: offset in unit of fsblksize */
+	unsigned len:24;	/* 3: length in unit of fsblksize */
+	unsigned addr1:8;	/* 1: address in unit of fsblksize */
+	u32 addr2;		/* 4: address in unit of fsblksize */
+} xad_t;			/* (16) */
+
+#define MAXXLEN         ((1 << 24) - 1)
+
+#define XTSLOTSIZE      16
+#define L2XTSLOTSIZE    4
+
+/* xad_t field construction */
+#define XADoffset(xad, offset64)\
+{\
+        (xad)->off1 = ((u64)offset64) >> 32;\
+        (xad)->off2 = __cpu_to_le32((offset64) & 0xffffffff);\
+}
+#define XADaddress(xad, address64)\
+{\
+        (xad)->addr1 = ((u64)address64) >> 32;\
+        (xad)->addr2 = __cpu_to_le32((address64) & 0xffffffff);\
+}
+#define XADlength(xad, length32)        (xad)->len = __cpu_to_le24(length32)
+
+/* xad_t field extraction */
+#define offsetXAD(xad)\
+        ( ((s64)((xad)->off1)) << 32 | __le32_to_cpu((xad)->off2))
+#define addressXAD(xad)\
+        ( ((s64)((xad)->addr1)) << 32 | __le32_to_cpu((xad)->addr2))
+#define lengthXAD(xad)  __le24_to_cpu((xad)->len)
+
+/* xad list */
+typedef struct {
+	s16 maxnxad;
+	s16 nxad;
+	xad_t *xad;
+} xadlist_t;
+
+/* xad_t flags */
+#define XAD_NEW         0x01	/* new */
+#define XAD_EXTENDED    0x02	/* extended */
+#define XAD_COMPRESSED  0x04	/* compressed with recorded length */
+#define XAD_NOTRECORDED 0x08	/* allocated but not recorded */
+#define XAD_COW         0x10	/* copy-on-write */
+
+
+/* possible values for maxentry */
+#define XTROOTINITSLOT_DIR  6
+#define XTROOTINITSLOT  10
+#define XTROOTMAXSLOT   18
+#define XTPAGEMAXSLOT   256
+#define XTENTRYSTART    2
+
+/*
+ *      xtree page:
+ */
+typedef union {
+	struct xtheader {
+		s64 next;	/* 8: */
+		s64 prev;	/* 8: */
+
+		u8 flag;	/* 1: */
+		u8 rsrvd1;	/* 1: */
+		s16 nextindex;	/* 2: next index = number of entries */
+		s16 maxentry;	/* 2: max number of entries */
+		s16 rsrvd2;	/* 2: */
+
+		pxd_t self;	/* 8: self */
+	} header;		/* (32) */
+
+	xad_t xad[XTROOTMAXSLOT];	/* 16 * maxentry: xad array */
+} xtpage_t;
+
+/*
+ *      external declaration
+ */
+extern int xtLookup(struct inode *ip, s64 lstart, s64 llen,
+		    int *pflag, s64 * paddr, int *plen, int flag);
+extern int xtLookupList(struct inode *ip, lxdlist_t * lxdlist,
+			xadlist_t * xadlist, int flag);
+extern void xtInitRoot(tid_t tid, struct inode *ip);
+extern int xtInsert(tid_t tid, struct inode *ip,
+		    int xflag, s64 xoff, int xlen, s64 * xaddrp, int flag);
+extern int xtExtend(tid_t tid, struct inode *ip, s64 xoff, int xlen,
+		    int flag);
+extern int xtTailgate(tid_t tid, struct inode *ip,
+		      s64 xoff, int xlen, s64 xaddr, int flag);
+extern int xtUpdate(tid_t tid, struct inode *ip, struct xad *nxad);
+extern int xtDelete(tid_t tid, struct inode *ip, s64 xoff, int xlen,
+		    int flag);
+extern s64 xtTruncate(tid_t tid, struct inode *ip, s64 newsize, int type);
+extern s64 xtTruncate_pmap(tid_t tid, struct inode *ip, s64 committed_size);
+extern int xtRelocate(tid_t tid, struct inode *ip,
+		      xad_t * oxad, s64 nxaddr, int xtype);
+extern int xtAppend(tid_t tid,
+		    struct inode *ip, int xflag, s64 xoff, int maxblocks,
+		    int *xlenp, s64 * xaddrp, int flag);
+
+#ifdef  _JFS_DEBUG_XTREE
+extern int xtDisplayTree(struct inode *ip);
+extern int xtDisplayPage(struct inode *ip, s64 bn, xtpage_t * p);
+#endif				/* _JFS_DEBUG_XTREE */
+
+#endif				/* !_H_JFS_XTREE */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/namei.c linuxppc64_2_4/fs/jfs/namei.c
--- linux-2.4.19/fs/jfs/namei.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/namei.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,1461 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Module: jfs/namei.c
+ *
+ */
+
+/*
+ * Change History :
+ *
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include "jfs_incore.h"
+#include "jfs_inode.h"
+#include "jfs_dinode.h"
+#include "jfs_dmap.h"
+#include "jfs_unicode.h"
+#include "jfs_metapage.h"
+#include "jfs_debug.h"
+
+extern struct inode_operations jfs_file_inode_operations;
+extern struct inode_operations jfs_symlink_inode_operations;
+extern struct file_operations jfs_file_operations;
+extern struct address_space_operations jfs_aops;
+
+extern int jfs_fsync(struct file *, struct dentry *, int);
+extern void jfs_truncate_nolock(struct inode *, loff_t);
+
+/*
+ * forward references
+ */
+struct inode_operations jfs_dir_inode_operations;
+struct file_operations jfs_dir_operations;
+
+s64 commitZeroLink(tid_t, struct inode *);
+
+/*
+ * NAME:	jfs_create(dip, dentry, mode)
+ *
+ * FUNCTION:	create a regular file in the parent directory <dip>
+ *		with name = <from dentry> and mode = <mode>
+ *
+ * PARAMETER:	dip 	- parent directory vnode
+ *		dentry	- dentry of new file
+ *		mode	- create mode (rwxrwxrwx).
+ *
+ * RETURN:	Errors from subroutines
+ *
+ */
+int jfs_create(struct inode *dip, struct dentry *dentry, int mode)
+{
+	int rc = 0;
+	tid_t tid;		/* transaction id */
+	struct inode *ip = NULL;	/* child directory inode */
+	ino_t ino;
+	component_t dname;	/* child directory name */
+	btstack_t btstack;
+	struct inode *iplist[2];
+	tblock_t *tblk;
+
+	jFYI(1, ("jfs_create: dip:0x%p name:%s\n", dip, dentry->d_name.name));
+
+	IWRITE_LOCK(dip);
+
+	/*
+	 * search parent directory for entry/freespace
+	 * (dtSearch() returns parent directory page pinned)
+	 */
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+		goto out1;
+
+	/*
+	 * Either iAlloc() or txBegin() may block.  Deadlock can occur if we
+	 * block there while holding dtree page, so we allocate the inode &
+	 * begin the transaction before we search the directory.
+	 */
+	ip = ialloc(dip, mode);
+	if (ip == NULL) {
+		rc = ENOSPC;
+		goto out2;
+	}
+
+	tid = txBegin(dip->i_sb, 0);
+
+	if ((rc = dtSearch(dip, &dname, &ino, &btstack, JFS_CREATE))) {
+		jERROR(1, ("jfs_create: dtSearch returned %d\n", rc));
+		ip->i_nlink = 0;
+		iput(ip);
+		txEnd(tid);
+		goto out2;
+	}
+
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_CREATE;
+	tblk->ip = ip;
+
+	iplist[0] = dip;
+	iplist[1] = ip;
+
+	/*
+	 * initialize the child XAD tree root in-line in inode
+	 */
+	xtInitRoot(tid, ip);
+
+	/*
+	 * create entry in parent directory for child directory
+	 * (dtInsert() releases parent directory page)
+	 */
+	ino = ip->i_ino;
+	if ((rc = dtInsert(tid, dip, &dname, &ino, &btstack))) {
+		jERROR(1, ("jfs_create: dtInsert returned %d\n", rc));
+		/* discard new inode */
+		ip->i_nlink = 0;
+		iput(ip);
+
+		if (rc == EIO)
+			txAbort(tid, 1);	/* Marks Filesystem dirty */
+		else
+			txAbort(tid, 0);	/* Filesystem full */
+		txEnd(tid);
+		goto out2;
+	}
+
+	ip->i_op = &jfs_file_inode_operations;
+	ip->i_fop = &jfs_file_operations;
+	ip->i_mapping->a_ops = &jfs_aops;
+
+	insert_inode_hash(ip);
+	mark_inode_dirty(ip);
+	d_instantiate(dentry, ip);
+
+	dip->i_version = ++event;
+	dip->i_ctime = dip->i_mtime = CURRENT_TIME;
+
+	mark_inode_dirty(dip);
+
+	rc = txCommit(tid, 2, &iplist[0], 0);
+	txEnd(tid);
+
+      out2:
+	free_UCSname(&dname);
+
+      out1:
+
+	IWRITE_UNLOCK(dip);
+	jFYI(1, ("jfs_create: rc:%d\n", -rc));
+	return -rc;
+}
+
+
+/*
+ * NAME:	jfs_mkdir(dip, dentry, mode)
+ *
+ * FUNCTION:	create a child directory in the parent directory <dip>
+ *		with name = <from dentry> and mode = <mode>
+ *
+ * PARAMETER:	dip 	- parent directory vnode
+ *		dentry	- dentry of child directory
+ *		mode	- create mode (rwxrwxrwx).
+ *
+ * RETURN:	Errors from subroutines
+ *
+ * note:
+ * EACCESS: user needs search+write permission on the parent directory
+ */
+int jfs_mkdir(struct inode *dip, struct dentry *dentry, int mode)
+{
+	int rc = 0;
+	tid_t tid;		/* transaction id */
+	struct inode *ip = NULL;	/* child directory inode */
+	ino_t ino;
+	component_t dname;	/* child directory name */
+	btstack_t btstack;
+	struct inode *iplist[2];
+	tblock_t *tblk;
+
+	jFYI(1, ("jfs_mkdir: dip:0x%p name:%s\n", dip, dentry->d_name.name));
+
+	IWRITE_LOCK(dip);
+
+	/* link count overflow on parent directory ? */
+	if (dip->i_nlink == JFS_LINK_MAX) {
+		rc = EMLINK;
+		goto out1;
+	}
+
+	/*
+	 * search parent directory for entry/freespace
+	 * (dtSearch() returns parent directory page pinned)
+	 */
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+		goto out1;
+
+	/*
+	 * Either iAlloc() or txBegin() may block.  Deadlock can occur if we
+	 * block there while holding dtree page, so we allocate the inode &
+	 * begin the transaction before we search the directory.
+	 */
+	ip = ialloc(dip, S_IFDIR | mode);
+	if (ip == NULL) {
+		rc = ENOSPC;
+		goto out2;
+	}
+
+	tid = txBegin(dip->i_sb, 0);
+
+	if ((rc = dtSearch(dip, &dname, &ino, &btstack, JFS_CREATE))) {
+		jERROR(1, ("jfs_mkdir: dtSearch returned %d\n", rc));
+		ip->i_nlink = 0;
+		iput(ip);
+		txEnd(tid);
+		goto out2;
+	}
+
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_CREATE;
+	tblk->ip = ip;
+
+	iplist[0] = dip;
+	iplist[1] = ip;
+
+	/*
+	 * initialize the child directory in-line in inode
+	 */
+	dtInitRoot(tid, ip, dip->i_ino);
+
+	/*
+	 * create entry in parent directory for child directory
+	 * (dtInsert() releases parent directory page)
+	 */
+	ino = ip->i_ino;
+	if ((rc = dtInsert(tid, dip, &dname, &ino, &btstack))) {
+		jERROR(1, ("jfs_mkdir: dtInsert returned %d\n", rc));
+		/* discard new directory inode */
+		ip->i_nlink = 0;
+		iput(ip);
+
+		if (rc == EIO)
+			txAbort(tid, 1);	/* Marks Filesystem dirty */
+		else
+			txAbort(tid, 0);	/* Filesystem full */
+		txEnd(tid);
+		goto out2;
+	}
+
+	ip->i_nlink = 2;	/* for '.' */
+	ip->i_op = &jfs_dir_inode_operations;
+	ip->i_fop = &jfs_dir_operations;
+	ip->i_mapping->a_ops = &jfs_aops;
+	ip->i_mapping->gfp_mask = GFP_NOFS;
+
+	insert_inode_hash(ip);
+	mark_inode_dirty(ip);
+	d_instantiate(dentry, ip);
+
+	/* update parent directory inode */
+	dip->i_nlink++;		/* for '..' from child directory */
+	dip->i_version = ++event;
+	dip->i_ctime = dip->i_mtime = CURRENT_TIME;
+	mark_inode_dirty(dip);
+
+	rc = txCommit(tid, 2, &iplist[0], 0);
+	txEnd(tid);
+
+      out2:
+	free_UCSname(&dname);
+
+      out1:
+
+	IWRITE_UNLOCK(dip);
+
+	jFYI(1, ("jfs_mkdir: rc:%d\n", -rc));
+	return -rc;
+}
+
+/*
+ * NAME:	jfs_rmdir(dip, dentry)
+ *
+ * FUNCTION:	remove a link to child directory
+ *
+ * PARAMETER:	dip 	- parent inode
+ *		dentry	- child directory dentry
+ *
+ * RETURN:	EINVAL	- if name is . or ..
+ *		EINVAL  - if . or .. exist but are invalid.
+ *		errors from subroutines
+ *
+ * note:
+ * if other threads have the directory open when the last link 
+ * is removed, the "." and ".." entries, if present, are removed before 
+ * rmdir() returns and no new entries may be created in the directory, 
+ * but the directory is not removed until the last reference to 
+ * the directory is released (cf.unlink() of regular file).
+ */
+int jfs_rmdir(struct inode *dip, struct dentry *dentry)
+{
+	int rc;
+	tid_t tid;		/* transaction id */
+	struct inode *ip = dentry->d_inode;
+	ino_t ino;
+	component_t dname;
+	struct inode *iplist[2];
+	tblock_t *tblk;
+
+	jFYI(1, ("jfs_rmdir: dip:0x%p name:%s\n", dip, dentry->d_name.name));
+
+	IWRITE_LOCK_LIST(2, dip, ip);
+
+	/* directory must be empty to be removed */
+	if (!dtEmpty(ip)) {
+		IWRITE_UNLOCK(ip);
+		IWRITE_UNLOCK(dip);
+		rc = ENOTEMPTY;
+		goto out;
+	}
+
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dip->i_sb)->nls_tab))) {
+		IWRITE_UNLOCK(ip);
+		IWRITE_UNLOCK(dip);
+		goto out;
+	}
+
+	tid = txBegin(dip->i_sb, 0);
+
+	iplist[0] = dip;
+	iplist[1] = ip;
+
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_DELETE;
+	tblk->ip = ip;
+
+	/*
+	 * delete the entry of target directory from parent directory
+	 */
+	ino = ip->i_ino;
+	if ((rc = dtDelete(tid, dip, &dname, &ino, JFS_REMOVE))) {
+		jERROR(1, ("jfs_rmdir: dtDelete returned %d\n", rc));
+		if (rc == EIO)
+			txAbort(tid, 1);
+		txEnd(tid);
+
+		IWRITE_UNLOCK(ip);
+		IWRITE_UNLOCK(dip);
+
+		goto out2;
+	}
+
+	/* update parent directory's link count corresponding
+	 * to ".." entry of the target directory deleted
+	 */
+	dip->i_nlink--;
+	dip->i_ctime = dip->i_mtime = CURRENT_TIME;
+	dip->i_version = ++event;
+	mark_inode_dirty(dip);
+
+	/*
+	 * OS/2 could have created EA and/or ACL
+	 */
+	/* free EA from both persistent and working map */
+	if (JFS_IP(ip)->ea.flag & DXD_EXTENT) {
+		/* free EA pages */
+		txEA(tid, ip, &JFS_IP(ip)->ea, NULL);
+	}
+	JFS_IP(ip)->ea.flag = 0;
+
+	/* free ACL from both persistent and working map */
+	if (JFS_IP(ip)->acl.flag & DXD_EXTENT) {
+		/* free ACL pages */
+		txEA(tid, ip, &JFS_IP(ip)->acl, NULL);
+	}
+	JFS_IP(ip)->acl.flag = 0;
+
+	/* mark the target directory as deleted */
+	ip->i_nlink = 0;
+	mark_inode_dirty(ip);
+
+	rc = txCommit(tid, 2, &iplist[0], 0);
+
+	txEnd(tid);
+
+	IWRITE_UNLOCK(ip);
+
+	/*
+	 * Truncating the directory index table is not guaranteed.  It
+	 * may need to be done iteratively
+	 */
+	if (test_cflag(COMMIT_Stale, dip)) {
+		if (dip->i_size > 1)
+			jfs_truncate_nolock(dip, 0);
+
+		clear_cflag(COMMIT_Stale, dip);
+	}
+
+	IWRITE_UNLOCK(dip);
+
+	d_delete(dentry);
+
+      out2:
+	free_UCSname(&dname);
+
+      out:
+	jFYI(1, ("jfs_rmdir: rc:%d\n", rc));
+	return -rc;
+}
+
+/*
+ * NAME:	jfs_unlink(dip, dentry)
+ *
+ * FUNCTION:	remove a link to object <vp> named by <name> 
+ *		from parent directory <dvp>
+ *
+ * PARAMETER:	dip 	- inode of parent directory
+ *		dentry 	- dentry of object to be removed
+ *
+ * RETURN:	errors from subroutines
+ *
+ * note:
+ * temporary file: if one or more processes have the file open
+ * when the last link is removed, the link will be removed before
+ * unlink() returns, but the removal of the file contents will be
+ * postponed until all references to the files are closed.
+ *
+ * JFS does NOT support unlink() on directories.
+ *
+ */
+int jfs_unlink(struct inode *dip, struct dentry *dentry)
+{
+	int rc;
+	tid_t tid;		/* transaction id */
+	struct inode *ip = dentry->d_inode;
+	ino_t ino;
+	component_t dname;	/* object name */
+	struct inode *iplist[2];
+	tblock_t *tblk;
+	s64 new_size = 0;
+	int commit_flag;
+
+	jFYI(1, ("jfs_unlink: dip:0x%p name:%s\n", dip, dentry->d_name.name));
+
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+		goto out;
+
+	IWRITE_LOCK_LIST(2, ip, dip);
+
+	tid = txBegin(dip->i_sb, 0);
+
+	iplist[0] = dip;
+	iplist[1] = ip;
+
+	/*
+	 * delete the entry of target file from parent directory
+	 */
+	ino = ip->i_ino;
+	if ((rc = dtDelete(tid, dip, &dname, &ino, JFS_REMOVE))) {
+		jERROR(1, ("jfs_unlink: dtDelete returned %d\n", rc));
+		if (rc == EIO)
+			txAbort(tid, 1);	/* Marks FS Dirty */
+		txEnd(tid);
+		IWRITE_UNLOCK(ip);
+		IWRITE_UNLOCK(dip);
+		goto out1;
+	}
+
+	ASSERT(ip->i_nlink);
+
+	ip->i_ctime = dip->i_ctime = dip->i_mtime = CURRENT_TIME;
+	dip->i_version = ++event;
+	mark_inode_dirty(dip);
+
+	/* update target's inode */
+	ip->i_nlink--;
+	mark_inode_dirty(ip);
+
+	/*
+	 *      commit zero link count object
+	 */
+	if (ip->i_nlink == 0) {
+		assert(!test_cflag(COMMIT_Nolink, ip));
+		/* free block resources */
+		if ((new_size = commitZeroLink(tid, ip)) < 0) {
+			txAbort(tid, 1);	/* Marks FS Dirty */
+			txEnd(tid);
+			IWRITE_UNLOCK(ip);
+			IWRITE_UNLOCK(dip);
+			rc = -new_size;		/* We return -rc */
+			goto out1;
+		}
+		tblk = tid_to_tblock(tid);
+		tblk->xflag |= COMMIT_DELETE;
+		tblk->ip = ip;
+	}
+
+	/*
+	 * Incomplete truncate of file data can
+	 * result in timing problems unless we synchronously commit the
+	 * transaction.
+	 */
+	if (new_size)
+		commit_flag = COMMIT_SYNC;
+	else
+		commit_flag = 0;
+
+	/*
+	 * If xtTruncate was incomplete, commit synchronously to avoid
+	 * timing complications
+	 */
+	rc = txCommit(tid, 2, &iplist[0], commit_flag);
+
+	txEnd(tid);
+
+	while (new_size && (rc == 0)) {
+		tid = txBegin(dip->i_sb, 0);
+		new_size = xtTruncate_pmap(tid, ip, new_size);
+		if (new_size < 0) {
+			txAbort(tid, 1);	/* Marks FS Dirty */
+			rc = -new_size;		/* We return -rc */
+		} else
+			rc = txCommit(tid, 2, &iplist[0], COMMIT_SYNC);
+		txEnd(tid);
+	}
+
+	if (!test_cflag(COMMIT_Holdlock, ip))
+		IWRITE_UNLOCK(ip);
+
+	/*
+	 * Truncating the directory index table is not guaranteed.  It
+	 * may need to be done iteratively
+	 */
+	if (test_cflag(COMMIT_Stale, dip)) {
+		if (dip->i_size > 1)
+			jfs_truncate_nolock(dip, 0);
+
+		clear_cflag(COMMIT_Stale, dip);
+	}
+
+	IWRITE_UNLOCK(dip);
+
+	d_delete(dentry);
+
+      out1:
+	free_UCSname(&dname);
+      out:
+	jFYI(1, ("jfs_unlink: rc:%d\n", -rc));
+	return -rc;
+}
+
+/*
+ * NAME:	commitZeroLink()
+ *
+ * FUNCTION:    for non-directory, called by jfs_remove(),
+ *		truncate a regular file, directory or symbolic
+ *		link to zero length. return 0 if type is not 
+ *		one of these.
+ *
+ *		if the file is currently associated with a VM segment
+ *		only permanent disk and inode map resources are freed,
+ *		and neither the inode nor indirect blocks are modified
+ *		so that the resources can be later freed in the work
+ *		map by ctrunc1.
+ *		if there is no VM segment on entry, the resources are
+ *		freed in both work and permanent map.
+ *		(? for temporary file - memory object is cached even 
+ *		after no reference:
+ *		reference count > 0 -   )
+ *
+ * PARAMETERS:	cd	- pointer to commit data structure.
+ *			  current inode is the one to truncate.
+ *
+ * RETURN :	Errors from subroutines
+ */
+s64 commitZeroLink(tid_t tid, struct inode *ip)
+{
+	int filetype;
+	tblock_t *tblk;
+
+	jFYI(1, ("commitZeroLink: tid = %d, ip = 0x%p\n", tid, ip));
+
+	filetype = ip->i_mode & S_IFMT;
+	switch (filetype) {
+	case S_IFREG:
+		break;
+	case S_IFLNK:
+		/* fast symbolic link */
+		if (ip->i_size <= 256) {
+			ip->i_size = 0;
+			return 0;
+		}
+		break;
+	default:
+		assert(filetype != S_IFDIR);
+		return 0;
+	}
+
+	set_cflag(COMMIT_Freewmap, ip);
+
+	/* mark transaction of block map update type */
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_PMAP;
+
+	/*
+	 * free EA
+	 */
+	if (JFS_IP(ip)->ea.flag & DXD_EXTENT)
+		/* acquire maplock on EA to be freed from block map */
+		txEA(tid, ip, &JFS_IP(ip)->ea, NULL);
+
+	/*
+	 * free ACL
+	 */
+	if (JFS_IP(ip)->acl.flag & DXD_EXTENT)
+		/* acquire maplock on EA to be freed from block map */
+		txEA(tid, ip, &JFS_IP(ip)->acl, NULL);
+
+	/*
+	 * free xtree/data (truncate to zero length):
+	 * free xtree/data pages from cache if COMMIT_PWMAP, 
+	 * free xtree/data blocks from persistent block map, and
+	 * free xtree/data blocks from working block map if COMMIT_PWMAP;
+	 */
+	if (ip->i_size)
+		return xtTruncate_pmap(tid, ip, 0);
+
+	return 0;
+}
+
+
+/*
+ * NAME:	freeZeroLink()
+ *
+ * FUNCTION:    for non-directory, called by iClose(),
+ *		free resources of a file from cache and WORKING map 
+ *		for a file previously committed with zero link count
+ *		while associated with a pager object,
+ *
+ * PARAMETER:	ip	- pointer to inode of file.
+ *
+ * RETURN:	0 -ok
+ */
+int freeZeroLink(struct inode *ip)
+{
+	int rc = 0;
+	int type;
+
+	jFYI(1, ("freeZeroLink: ip = 0x%p\n", ip));
+
+	/* return if not reg or symbolic link or if size is
+	 * already ok.
+	 */
+	type = ip->i_mode & S_IFMT;
+
+	switch (type) {
+	case S_IFREG:
+		break;
+	case S_IFLNK:
+		/* if its contained in inode nothing to do */
+		if (ip->i_size <= 256)
+			return 0;
+		break;
+	default:
+		return 0;
+	}
+
+	/*
+	 * free EA
+	 */
+	if (JFS_IP(ip)->ea.flag & DXD_EXTENT) {
+		s64 xaddr;
+		int xlen;
+		maplock_t maplock;	/* maplock for COMMIT_WMAP */
+		pxdlock_t *pxdlock;	/* maplock for COMMIT_WMAP */
+
+		/* free EA pages from cache */
+		xaddr = addressDXD(&JFS_IP(ip)->ea);
+		xlen = lengthDXD(&JFS_IP(ip)->ea);
+#ifdef _STILL_TO_PORT
+		bmExtentInvalidate(ip, xaddr, xlen);
+#endif
+
+		/* free EA extent from working block map */
+		maplock.index = 1;
+		pxdlock = (pxdlock_t *) & maplock;
+		pxdlock->flag = mlckFREEPXD;
+		PXDaddress(&pxdlock->pxd, xaddr);
+		PXDlength(&pxdlock->pxd, xlen);
+		txFreeMap(ip, pxdlock, 0, COMMIT_WMAP);
+	}
+
+	/*
+	 * free ACL
+	 */
+	if (JFS_IP(ip)->acl.flag & DXD_EXTENT) {
+		s64 xaddr;
+		int xlen;
+		maplock_t maplock;	/* maplock for COMMIT_WMAP */
+		pxdlock_t *pxdlock;	/* maplock for COMMIT_WMAP */
+
+		/* free ACL pages from cache */
+		xaddr = addressDXD(&JFS_IP(ip)->acl);
+		xlen = lengthDXD(&JFS_IP(ip)->acl);
+#ifdef _STILL_TO_PORT
+		bmExtentInvalidate(ip, xaddr, xlen);
+#endif
+
+		/* free ACL extent from working block map */
+		maplock.index = 1;
+		pxdlock = (pxdlock_t *) & maplock;
+		pxdlock->flag = mlckFREEPXD;
+		PXDaddress(&pxdlock->pxd, xaddr);
+		PXDlength(&pxdlock->pxd, xlen);
+		txFreeMap(ip, pxdlock, 0, COMMIT_WMAP);
+	}
+
+	/*
+	 * free xtree/data (truncate to zero length):
+	 * free xtree/data pages from cache, and
+	 * free xtree/data blocks from working block map;
+	 */
+	if (ip->i_size)
+		rc = xtTruncate(0, ip, 0, COMMIT_WMAP);
+
+	return rc;
+}
+
+/*
+ * NAME:	jfs_link(vp, dvp, name, crp)
+ *
+ * FUNCTION:	create a link to <vp> by the name = <name>
+ *		in the parent directory <dvp>
+ *
+ * PARAMETER:	vp 	- target object
+ *		dvp	- parent directory of new link
+ *		name	- name of new link to target object
+ *		crp	- credential
+ *
+ * RETURN:	Errors from subroutines
+ *
+ * note:
+ * JFS does NOT support link() on directories (to prevent circular
+ * path in the directory hierarchy);
+ * EPERM: the target object is a directory, and either the caller
+ * does not have appropriate privileges or the implementation prohibits
+ * using link() on directories [XPG4.2].
+ *
+ * JFS does NOT support links between file systems:
+ * EXDEV: target object and new link are on different file systems and
+ * implementation does not support links between file systems [XPG4.2].
+ */
+int jfs_link(struct dentry *old_dentry,
+	     struct inode *dir, struct dentry *dentry)
+{
+	int rc;
+	tid_t tid;
+	struct inode *ip = old_dentry->d_inode;
+	ino_t ino;
+	component_t dname;
+	btstack_t btstack;
+	struct inode *iplist[2];
+
+	jFYI(1,
+	     ("jfs_link: %s %s\n", old_dentry->d_name.name,
+	      dentry->d_name.name));
+
+	/* JFS does NOT support link() on directories */
+	if (S_ISDIR(ip->i_mode))
+		return -EPERM;
+
+	IWRITE_LOCK_LIST(2, dir, ip);
+
+	tid = txBegin(ip->i_sb, 0);
+
+	if (ip->i_nlink == JFS_LINK_MAX) {
+		rc = EMLINK;
+		goto out;
+	}
+
+	/*
+	 * scan parent directory for entry/freespace
+	 */
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(ip->i_sb)->nls_tab)))
+		goto out;
+
+	if ((rc = dtSearch(dir, &dname, &ino, &btstack, JFS_CREATE)))
+		goto out;
+
+	/*
+	 * create entry for new link in parent directory
+	 */
+	ino = ip->i_ino;
+	if ((rc = dtInsert(tid, dir, &dname, &ino, &btstack)))
+		goto out;
+
+	dir->i_version = ++event;
+
+	/* update object inode */
+	ip->i_nlink++;		/* for new link */
+	ip->i_ctime = CURRENT_TIME;
+	mark_inode_dirty(dir);
+	atomic_inc(&ip->i_count);
+	d_instantiate(dentry, ip);
+
+	iplist[0] = ip;
+	iplist[1] = dir;
+	rc = txCommit(tid, 2, &iplist[0], 0);
+
+      out:
+	IWRITE_UNLOCK(dir);
+	IWRITE_UNLOCK(ip);
+
+	txEnd(tid);
+
+	jFYI(1, ("jfs_link: rc:%d\n", rc));
+	return -rc;
+}
+
+/*
+ * NAME:	jfs_symlink(dip, dentry, name)
+ *
+ * FUNCTION:	creates a symbolic link to <symlink> by name <name>
+ *		        in directory <dip>
+ *
+ * PARAMETER:	dip	    - parent directory vnode
+ *		        dentry 	- dentry of symbolic link
+ *		        name    - the path name of the existing object 
+ *			              that will be the source of the link
+ *
+ * RETURN:	errors from subroutines
+ *
+ * note:
+ * ENAMETOOLONG: pathname resolution of a symbolic link produced
+ * an intermediate result whose length exceeds PATH_MAX [XPG4.2]
+*/
+
+int jfs_symlink(struct inode *dip, struct dentry *dentry, const char *name)
+{
+	int rc;
+	tid_t tid;
+	ino_t ino = 0;
+	component_t dname;
+	int ssize;		/* source pathname size */
+	btstack_t btstack;
+	struct inode *ip = dentry->d_inode;
+	unchar *i_fastsymlink;
+	s64 xlen = 0;
+	int bmask = 0, xsize;
+	s64 xaddr;
+	metapage_t *mp;
+	struct super_block *sb;
+	tblock_t *tblk;
+
+	struct inode *iplist[2];
+
+	jFYI(1, ("jfs_symlink: dip:0x%p name:%s\n", dip, name));
+
+	IWRITE_LOCK(dip);
+
+	ssize = strlen(name) + 1;
+
+	tid = txBegin(dip->i_sb, 0);
+
+	/*
+	 * search parent directory for entry/freespace
+	 * (dtSearch() returns parent directory page pinned)
+	 */
+
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+		goto out1;
+
+	if ((rc = dtSearch(dip, &dname, &ino, &btstack, JFS_CREATE)))
+		goto out2;
+
+
+
+	/*
+	 * allocate on-disk/in-memory inode for symbolic link:
+	 * (iAlloc() returns new, locked inode)
+	 */
+
+	ip = ialloc(dip, S_IFLNK | 0777);
+	if (ip == NULL) {
+		BT_PUTSEARCH(&btstack);
+		rc = ENOSPC;
+		goto out2;
+	}
+
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_CREATE;
+	tblk->ip = ip;
+
+	/*
+	 * create entry for symbolic link in parent directory
+	 */
+
+	ino = ip->i_ino;
+
+
+
+	if ((rc = dtInsert(tid, dip, &dname, &ino, &btstack))) {
+		jERROR(1, ("jfs_symlink: dtInsert returned %d\n", rc));
+		/* discard ne inode */
+		ip->i_nlink = 0;
+		iput(ip);
+		goto out2;
+
+	}
+
+	/* fix symlink access permission
+	 * (dir_create() ANDs in the u.u_cmask, 
+	 * but symlinks really need to be 777 access)
+	 */
+	ip->i_mode |= 0777;
+
+	/*
+	   *       write symbolic link target path name
+	 */
+	xtInitRoot(tid, ip);
+
+	/*
+	 * write source path name inline in on-disk inode (fast symbolic link)
+	 */
+
+	if (ssize <= IDATASIZE) {
+		ip->i_op = &jfs_symlink_inode_operations;
+
+		i_fastsymlink = JFS_IP(ip)->i_inline;
+		memcpy(i_fastsymlink, name, ssize);
+		ip->i_size = ssize - 1;
+		jFYI(1,
+		     ("jfs_symlink: fast symlink added  ssize:%d name:%s \n",
+		      ssize, name));
+	}
+	/*
+	 * write source path name in a single extent
+	 */
+	else {
+		jFYI(1, ("jfs_symlink: allocate extent ip:0x%p\n", ip));
+
+		ip->i_op = &page_symlink_inode_operations;
+		ip->i_mapping->a_ops = &jfs_aops;
+
+		/*
+		 * even though the data of symlink object (source 
+		 * path name) is treated as non-journaled user data,
+		 * it is read/written thru buffer cache for performance.
+		 */
+		sb = ip->i_sb;
+		bmask = JFS_SBI(sb)->bsize - 1;
+		xsize = (ssize + bmask) & ~bmask;
+		xaddr = 0;
+		xlen = xsize >> JFS_SBI(sb)->l2bsize;
+		if ((rc = xtInsert(tid, ip, 0, 0, xlen, &xaddr, 0)) == 0) {
+			ip->i_size = ssize - 1;
+			while (ssize) {
+				int copy_size = min(ssize, PSIZE);
+
+				mp = get_metapage(ip, xaddr, PSIZE, 1);
+
+				if (mp == NULL) {
+					dtDelete(tid, dip, &dname, &ino,
+						 JFS_REMOVE);
+					ip->i_nlink = 0;
+					iput(ip);
+					rc = EIO;
+					goto out2;
+				}
+				memcpy(mp->data, name, copy_size);
+				flush_metapage(mp);
+#if 0
+				mark_buffer_uptodate(bp, 1);
+				mark_buffer_dirty(bp, 1);
+				if (IS_SYNC(dip)) {
+					ll_rw_block(WRITE, 1, &bp);
+					wait_on_buffer(bp);
+				}
+				brelse(bp);
+#endif				/* 0 */
+				ssize -= copy_size;
+				xaddr += JFS_SBI(sb)->nbperpage;
+			}
+			ip->i_blocks = LBLK2PBLK(sb, xlen);
+		} else {
+			dtDelete(tid, dip, &dname, &ino, JFS_REMOVE);
+			ip->i_nlink = 0;
+			iput(ip);
+			rc = ENOSPC;
+			goto out2;
+		}
+	}
+	dip->i_version = ++event;
+
+	insert_inode_hash(ip);
+	mark_inode_dirty(ip);
+	d_instantiate(dentry, ip);
+
+	/*
+	 * commit update of parent directory and link object
+	 *
+	 * if extent allocation failed (ENOSPC),
+	 * the parent inode is committed regardless to avoid
+	 * backing out parent directory update (by dtInsert())
+	 * and subsequent dtDelete() which is harmless wrt 
+	 * integrity concern.  
+	 * the symlink inode will be freed by iput() at exit
+	 * as it has a zero link count (by dtDelete()) and 
+	 * no permanant resources. 
+	 */
+
+	iplist[0] = dip;
+	if (rc == 0) {
+		iplist[1] = ip;
+		rc = txCommit(tid, 2, &iplist[0], 0);
+	} else
+		rc = txCommit(tid, 1, &iplist[0], 0);
+
+      out2:
+
+	free_UCSname(&dname);
+      out1:
+	IWRITE_UNLOCK(dip);
+
+	txEnd(tid);
+
+	jFYI(1, ("jfs_symlink: rc:%d\n", -rc));
+	return -rc;
+}
+
+
+/*
+ * NAME:        jfs_rename
+ *
+ * FUNCTION:    rename a file or directory
+ */
+int jfs_rename(struct inode *old_dir, struct dentry *old_dentry,
+	       struct inode *new_dir, struct dentry *new_dentry)
+{
+	btstack_t btstack;
+	ino_t ino;
+	component_t new_dname;
+	struct inode *new_ip;
+	component_t old_dname;
+	struct inode *old_ip;
+	int rc;
+	tid_t tid;
+	tlock_t *tlck;
+	dtlock_t *dtlck;
+	lv_t *lv;
+	int ipcount;
+	struct inode *iplist[4];
+	tblock_t *tblk;
+	s64 new_size = 0;
+	int commit_flag;
+
+
+	jFYI(1,
+	     ("jfs_rename: %s %s\n", old_dentry->d_name.name,
+	      new_dentry->d_name.name));
+
+	old_ip = old_dentry->d_inode;
+	new_ip = new_dentry->d_inode;
+
+	if (old_dir == new_dir) {
+		if (new_ip)
+			IWRITE_LOCK_LIST(3, old_dir, old_ip, new_ip);
+		else
+			IWRITE_LOCK_LIST(2, old_dir, old_ip);
+	} else {
+		if (new_ip)
+			IWRITE_LOCK_LIST(4, old_dir, new_dir, old_ip,
+					 new_ip);
+		else
+			IWRITE_LOCK_LIST(3, old_dir, new_dir, old_ip);
+	}
+
+	if ((rc = get_UCSname(&old_dname, old_dentry,
+			      JFS_SBI(old_dir->i_sb)->nls_tab)))
+		goto out1;
+
+	if ((rc = get_UCSname(&new_dname, new_dentry,
+			      JFS_SBI(old_dir->i_sb)->nls_tab)))
+		goto out2;
+
+	/*
+	 * Make sure source inode number is what we think it is
+	 */
+	rc = dtSearch(old_dir, &old_dname, &ino, &btstack, JFS_LOOKUP);
+	if (rc || (ino != old_ip->i_ino)) {
+		rc = ENOENT;
+		goto out3;
+	}
+
+	/*
+	 * Make sure dest inode number (if any) is what we think it is
+	 */
+	rc = dtSearch(new_dir, &new_dname, &ino, &btstack, JFS_LOOKUP);
+	if (rc == 0) {
+		if ((new_ip == 0) || (ino != new_ip->i_ino)) {
+			rc = ESTALE;
+			goto out3;
+		}
+	} else if (rc != ENOENT)
+		goto out3;
+	else if (new_ip) {
+		/* no entry exists, but one was expected */
+		rc = ESTALE;
+		goto out3;
+	}
+
+	if (S_ISDIR(old_ip->i_mode)) {
+		if (new_ip) {
+			if (!dtEmpty(new_ip)) {
+				rc = ENOTEMPTY;
+				goto out3;
+			}
+		} else if ((new_dir != old_dir) &&
+			   (new_dir->i_nlink == JFS_LINK_MAX)) {
+			rc = EMLINK;
+			goto out3;
+		}
+	}
+
+	/*
+	 * The real work starts here
+	 */
+	tid = txBegin(new_dir->i_sb, 0);
+
+	if (new_ip) {
+		/*
+		 * Change existing directory entry to new inode number
+		 */
+		ino = new_ip->i_ino;
+		rc = dtModify(tid, new_dir, &new_dname, &ino,
+			      old_ip->i_ino, JFS_RENAME);
+		if (rc)
+			goto out4;
+		new_ip->i_nlink--;
+		if (S_ISDIR(new_ip->i_mode)) {
+			new_ip->i_nlink--;
+			assert(new_ip->i_nlink == 0);
+			tblk = tid_to_tblock(tid);
+			tblk->xflag |= COMMIT_DELETE;
+			tblk->ip = new_ip;
+		} else if (new_ip->i_nlink == 0) {
+			assert(!test_cflag(COMMIT_Nolink, new_ip));
+			/* free block resources */
+			if ((new_size = commitZeroLink(tid, new_ip)) < 0) {
+				txAbort(tid, 1);	/* Marks FS Dirty */
+				rc = -new_size;		/* We return -rc */
+				goto out4;
+			}
+			tblk = tid_to_tblock(tid);
+			tblk->xflag |= COMMIT_DELETE;
+			tblk->ip = new_ip;
+		} else {
+			new_ip->i_ctime = CURRENT_TIME;
+			mark_inode_dirty(new_ip);
+		}
+	} else {
+		/*
+		 * Add new directory entry
+		 */
+		rc = dtSearch(new_dir, &new_dname, &ino, &btstack,
+			      JFS_CREATE);
+		if (rc) {
+			jERROR(1,
+			       ("jfs_rename didn't expect dtSearch to fail w/rc = %d\n",
+				rc));
+			goto out4;
+		}
+
+		ino = old_ip->i_ino;
+		rc = dtInsert(tid, new_dir, &new_dname, &ino, &btstack);
+		if (rc) {
+			jERROR(1,
+			       ("jfs_rename: dtInsert failed w/rc = %d\n",
+				rc));
+			goto out4;
+		}
+		if (S_ISDIR(old_ip->i_mode))
+			new_dir->i_nlink++;
+	}
+	/*
+	 * Remove old directory entry
+	 */
+
+	ino = old_ip->i_ino;
+	rc = dtDelete(tid, old_dir, &old_dname, &ino, JFS_REMOVE);
+	if (rc) {
+		jERROR(1,
+		       ("jfs_rename did not expect dtDelete to return rc = %d\n",
+			rc));
+		txAbort(tid, 1);	/* Marks Filesystem dirty */
+		goto out4;
+	}
+	if (S_ISDIR(old_ip->i_mode)) {
+		old_dir->i_nlink--;
+		if (old_dir != new_dir) {
+			/*
+			 * Change inode number of parent for moved directory
+			 */
+
+			JFS_IP(old_ip)->i_dtroot.header.idotdot =
+				cpu_to_le32(new_dir->i_ino);
+
+			/* Linelock header of dtree */
+			tlck = txLock(tid, old_ip,
+				      (metapage_t *) & JFS_IP(old_ip)->bxflag,
+				      tlckDTREE | tlckBTROOT);
+			dtlck = (dtlock_t *) & tlck->lock;
+			ASSERT(dtlck->index == 0);
+			lv = (lv_t *) & dtlck->lv[0];
+			lv->offset = 0;
+			lv->length = 1;
+			dtlck->index++;
+		}
+	}
+
+	/*
+	 * Update ctime on changed/moved inodes & mark dirty
+	 */
+	old_ip->i_ctime = CURRENT_TIME;
+	mark_inode_dirty(old_ip);
+
+	new_dir->i_version = ++event;
+	new_dir->i_ctime = CURRENT_TIME;
+	mark_inode_dirty(new_dir);
+
+	/* Build list of inodes modified by this transaction */
+	ipcount = 0;
+	iplist[ipcount++] = old_ip;
+	if (new_ip)
+		iplist[ipcount++] = new_ip;
+	iplist[ipcount++] = old_dir;
+
+	if (old_dir != new_dir) {
+		iplist[ipcount++] = new_dir;
+		old_dir->i_version = ++event;
+		old_dir->i_ctime = CURRENT_TIME;
+		mark_inode_dirty(old_dir);
+	}
+
+	/*
+	 * Incomplete truncate of file data can
+	 * result in timing problems unless we synchronously commit the
+	 * transaction.
+	 */
+	if (new_size)
+		commit_flag = COMMIT_SYNC;
+	else
+		commit_flag = 0;
+
+	rc = txCommit(tid, ipcount, iplist, commit_flag);
+
+	/*
+	 * Don't unlock new_ip if COMMIT_HOLDLOCK is set
+	 */
+	if (new_ip && test_cflag(COMMIT_Holdlock, new_ip))
+		new_ip = 0;
+
+      out4:
+	txEnd(tid);
+
+	while (new_size && (rc == 0)) {
+		tid = txBegin(new_ip->i_sb, 0);
+		new_size = xtTruncate_pmap(tid, new_ip, new_size);
+		if (new_size < 0) {
+			txAbort(tid, 1);
+			rc = -new_size;		/* We return -rc */
+		} else
+			rc = txCommit(tid, 1, &new_ip, COMMIT_SYNC);
+		txEnd(tid);
+	}
+      out3:
+	free_UCSname(&new_dname);
+      out2:
+	free_UCSname(&old_dname);
+      out1:
+	IWRITE_UNLOCK(old_ip);
+	if (old_dir != new_dir)
+		IWRITE_UNLOCK(new_dir);
+	if (new_ip)
+		IWRITE_UNLOCK(new_ip);
+
+	/*
+	 * Truncating the directory index table is not guaranteed.  It
+	 * may need to be done iteratively
+	 */
+	if (test_cflag(COMMIT_Stale, old_dir)) {
+		if (old_dir->i_size > 1)
+			jfs_truncate_nolock(old_dir, 0);
+
+		clear_cflag(COMMIT_Stale, old_dir);
+	}
+
+	IWRITE_UNLOCK(old_dir);
+
+	jFYI(1, ("jfs_rename: returning %d\n", rc));
+	return -rc;
+}
+
+
+/*
+ * NAME:        jfs_mknod
+ *
+ * FUNCTION:    Create a special file (device)
+ */
+int jfs_mknod(struct inode *dir, struct dentry *dentry, int mode, int rdev)
+{
+	btstack_t btstack;
+	component_t dname;
+	ino_t ino;
+	struct inode *ip;
+	struct inode *iplist[2];
+	int rc;
+	tid_t tid;
+	tblock_t *tblk;
+
+	jFYI(1, ("jfs_mknod: %s\n", dentry->d_name.name));
+
+	if ((rc = get_UCSname(&dname, dentry, JFS_SBI(dir->i_sb)->nls_tab)))
+		goto out;
+
+	IWRITE_LOCK(dir);
+
+	ip = ialloc(dir, mode);
+	if (ip == NULL) {
+		rc = ENOSPC;
+		goto out1;
+	}
+
+	tid = txBegin(dir->i_sb, 0);
+
+	if ((rc = dtSearch(dir, &dname, &ino, &btstack, JFS_CREATE))) {
+		ip->i_nlink = 0;
+		iput(ip);
+		txEnd(tid);
+		goto out1;
+	}
+
+	tblk = tid_to_tblock(tid);
+	tblk->xflag |= COMMIT_CREATE;
+	tblk->ip = ip;
+
+	ino = ip->i_ino;
+	if ((rc = dtInsert(tid, dir, &dname, &ino, &btstack))) {
+		ip->i_nlink = 0;
+		iput(ip);
+		txEnd(tid);
+		goto out1;
+	}
+
+	if (S_ISREG(ip->i_mode)) {
+		ip->i_op = &jfs_file_inode_operations;
+		ip->i_fop = &jfs_file_operations;
+		ip->i_mapping->a_ops = &jfs_aops;
+	} else
+		init_special_inode(ip, ip->i_mode, rdev);
+
+	insert_inode_hash(ip);
+	mark_inode_dirty(ip);
+	d_instantiate(dentry, ip);
+
+	dir->i_version = ++event;
+	dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+
+	mark_inode_dirty(dir);
+
+	iplist[0] = dir;
+	iplist[1] = ip;
+	rc = txCommit(tid, 2, iplist, 0);
+	txEnd(tid);
+
+      out1:
+	IWRITE_UNLOCK(dir);
+	free_UCSname(&dname);
+
+      out:
+	jFYI(1, ("jfs_mknod: returning %d\n", rc));
+	return -rc;
+}
+
+static struct dentry *jfs_lookup(struct inode *dip, struct dentry *dentry)
+{
+	btstack_t btstack;
+	ino_t inum;
+	struct inode *ip;
+	component_t key;
+	const char *name = dentry->d_name.name;
+	int len = dentry->d_name.len;
+	int rc;
+
+	jFYI(1, ("jfs_lookup: name = %s\n", name));
+
+
+	if ((name[0] == '.') && (len == 1))
+		inum = dip->i_ino;
+	else if (strcmp(name, "..") == 0)
+		inum = PARENT(dip);
+	else {
+		if ((rc =
+		     get_UCSname(&key, dentry, JFS_SBI(dip->i_sb)->nls_tab)))
+			return ERR_PTR(-rc);
+		IREAD_LOCK(dip);
+		rc = dtSearch(dip, &key, &inum, &btstack, JFS_LOOKUP);
+		IREAD_UNLOCK(dip);
+		free_UCSname(&key);
+		if (rc == ENOENT) {
+			d_add(dentry, NULL);
+			return ERR_PTR(0);
+		} else if (rc) {
+			jERROR(1,
+			       ("jfs_lookup: dtSearch returned %d\n", rc));
+			return ERR_PTR(-rc);
+		}
+	}
+
+	ip = iget(dip->i_sb, inum);
+	if (ip == NULL) {
+		jERROR(1,
+		       ("jfs_lookup: iget failed on inum %d\n",
+			(uint) inum));
+		return ERR_PTR(-EACCES);
+	}
+
+	d_add(dentry, ip);
+
+	return ERR_PTR(0);
+}
+
+struct inode_operations jfs_dir_inode_operations = {
+	create:		jfs_create,
+	lookup:		jfs_lookup,
+	link:		jfs_link,
+	unlink:		jfs_unlink,
+	symlink:	jfs_symlink,
+	mkdir:		jfs_mkdir,
+	rmdir:		jfs_rmdir,
+	mknod:		jfs_mknod,
+	rename:		jfs_rename,
+};
+
+struct file_operations jfs_dir_operations = {
+	read:		generic_read_dir,
+	readdir:	jfs_readdir,
+	fsync:		jfs_fsync,
+};
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/super.c linuxppc64_2_4/fs/jfs/super.c
--- linux-2.4.19/fs/jfs/super.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/super.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,481 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include <linux/config.h>
+#include <linux/module.h>
+#include <asm/uaccess.h>
+#include "jfs_incore.h"
+#include "jfs_filsys.h"
+#include "jfs_metapage.h"
+#include "jfs_superblock.h"
+#include "jfs_dmap.h"
+#include "jfs_imap.h"
+#include "jfs_debug.h"
+
+MODULE_DESCRIPTION("The Journaled Filesystem (JFS)");
+MODULE_AUTHOR("Steve Best/Dave Kleikamp/Barry Arndt, IBM");
+MODULE_LICENSE("GPL");
+
+static int in_shutdown;
+static pid_t jfsIOthread;
+static pid_t jfsCommitThread;
+static pid_t jfsSyncThread;
+struct task_struct *jfsIOtask;
+struct task_struct *jfsCommitTask;
+struct task_struct *jfsSyncTask;
+DECLARE_COMPLETION(jfsIOwait);
+
+#ifdef CONFIG_JFS_DEBUG
+int jfsloglevel = 1;
+MODULE_PARM(jfsloglevel, "i");
+MODULE_PARM_DESC(jfsloglevel, "Specify JFS loglevel (0, 1 or 2)");
+#endif
+
+/*
+ * External declarations
+ */
+extern int jfs_mount(struct super_block *);
+extern int jfs_mount_rw(struct super_block *, int);
+extern int jfs_umount(struct super_block *);
+extern int jfs_umount_rw(struct super_block *);
+
+extern int jfsIOWait(void *);
+extern int jfs_lazycommit(void *);
+extern int jfs_sync(void *);
+extern void jfs_put_inode(struct inode *inode);
+extern void jfs_read_inode(struct inode *inode);
+extern void jfs_dirty_inode(struct inode *inode);
+extern void jfs_delete_inode(struct inode *inode);
+extern void jfs_write_inode(struct inode *inode, int wait);
+
+#if defined(CONFIG_JFS_DEBUG) && defined(CONFIG_PROC_FS)
+extern void jfs_proc_init(void);
+extern void jfs_proc_clean(void);
+#endif
+
+int jfs_thread_stopped(void)
+{
+	unsigned long signr;
+	siginfo_t info;
+
+	spin_lock_irq(&current->sigmask_lock);
+	signr = dequeue_signal(&current->blocked, &info);
+	spin_unlock_irq(&current->sigmask_lock);
+
+	if (signr == SIGKILL && in_shutdown)
+		return 1;
+	return 0;
+}
+
+static int jfs_statfs(struct super_block *sb, struct statfs *buf)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	s64 maxinodes;
+	imap_t *imap = JFS_IP(sbi->ipimap)->i_imap;
+
+	jFYI(1, ("In jfs_statfs\n"));
+	buf->f_type = JFS_SUPER_MAGIC;
+	buf->f_bsize = sbi->bsize;
+	buf->f_blocks = sbi->bmap->db_mapsize;
+	buf->f_bfree = sbi->bmap->db_nfree;
+	buf->f_bavail = sbi->bmap->db_nfree;
+	/*
+	 * If we really return the number of allocated & free inodes, some
+	 * applications will fail because they won't see enough free inodes.
+	 * We'll try to calculate some guess as to how may inodes we can
+	 * really allocate
+	 *
+	 * buf->f_files = atomic_read(&imap->im_numinos);
+	 * buf->f_ffree = atomic_read(&imap->im_numfree);
+	 */
+	maxinodes = min((s64) atomic_read(&imap->im_numinos) +
+			((sbi->bmap->db_nfree >> imap->im_l2nbperiext)
+			 << L2INOSPEREXT), (s64)0xffffffffLL);
+	buf->f_files = maxinodes;
+	buf->f_ffree = maxinodes - (atomic_read(&imap->im_numinos) -
+				    atomic_read(&imap->im_numfree));
+
+	buf->f_namelen = JFS_NAME_MAX;
+	return 0;
+}
+
+static void jfs_put_super(struct super_block *sb)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+	int rc;
+
+	jFYI(1, ("In jfs_put_super\n"));
+	rc = jfs_umount(sb);
+	if (rc) {
+		jERROR(1, ("jfs_umount failed with return code %d\n", rc));
+	}
+	unload_nls(sbi->nls_tab);
+	sbi->nls_tab = NULL;
+
+	/*
+	 * We need to clean out the direct_inode pages since this inode
+	 * is not in the inode hash.
+	 */
+	fsync_inode_data_buffers(sbi->direct_inode);
+	truncate_inode_pages(sbi->direct_mapping, 0);
+	iput(sbi->direct_inode);
+	sbi->direct_inode = NULL;
+	sbi->direct_mapping = NULL;
+
+	JFS_SBI(sb) = 0;
+	kfree(sbi);
+}
+
+static int parse_options (char * options, struct jfs_sb_info *sbi)
+{
+	void *nls_map = NULL;
+	char * this_char;
+	char * value;
+
+	if (!options)
+		return 1;
+	while ((this_char = strsep (&options, ",")) != NULL) {
+		if (!*this_char)
+			continue;
+		if ((value = strchr (this_char, '=')) != NULL)
+			*value++ = 0;
+		if (!strcmp (this_char, "iocharset")) {
+			if (!value || !*value)
+				goto needs_arg;
+			if (nls_map)	/* specified iocharset twice! */
+				unload_nls(nls_map);
+			nls_map = load_nls(value);
+			if (!nls_map) {
+				printk(KERN_ERR "JFS: charset not found\n");
+				goto cleanup;
+			}
+		/* Silently ignore the quota options */
+		} else if (!strcmp (this_char, "grpquota")
+		         || !strcmp (this_char, "noquota")
+		         || !strcmp (this_char, "quota")
+		         || !strcmp (this_char, "usrquota"))
+			/* Don't do anything ;-) */ ;
+		else {
+			printk ("jfs: Unrecognized mount option %s\n", this_char);
+			goto cleanup;
+		}
+	}
+	if (nls_map) {
+		/* Discard old (if remount) */
+		if (sbi->nls_tab)
+			unload_nls(sbi->nls_tab);
+		sbi->nls_tab = nls_map;
+	}
+	return 1;
+needs_arg:
+	printk(KERN_ERR "JFS: %s needs an argument\n", this_char);
+cleanup:
+	if (nls_map)
+		unload_nls(nls_map);
+	return 0;
+}
+
+int jfs_remount(struct super_block *sb, int *flags, char *data)
+{
+	struct jfs_sb_info *sbi = JFS_SBI(sb);
+
+	if (!parse_options(data, sbi)) {
+		return -EINVAL;
+	}
+
+	if ((sb->s_flags & MS_RDONLY) && !(*flags & MS_RDONLY)) {
+		/*
+		 * Invalidate any previously read metadata.  fsck may
+		 * have changed the on-disk data since we mounted r/o
+		 */
+		truncate_inode_pages(sbi->direct_mapping, 0);
+
+		return jfs_mount_rw(sb, 1);
+	} else if ((!(sb->s_flags & MS_RDONLY)) && (*flags & MS_RDONLY))
+		return jfs_umount_rw(sb);
+
+	return 0;
+}
+
+static struct super_operations jfs_sops = {
+	read_inode:	jfs_read_inode,
+	dirty_inode:	jfs_dirty_inode,
+	write_inode:	jfs_write_inode,
+	put_inode:	jfs_put_inode,
+	delete_inode:	jfs_delete_inode,
+	put_super:	jfs_put_super,
+	statfs:		jfs_statfs,
+	remount_fs:	jfs_remount,
+	clear_inode:	diClearExtension,
+};
+
+static struct super_block *jfs_read_super(struct super_block *sb,
+					  void *data, int silent)
+{
+	struct jfs_sb_info *sbi;
+	struct inode *inode;
+	int rc;
+
+	jFYI(1,
+	     ("In jfs_read_super s_dev=0x%x s_flags=0x%lx\n", sb->s_dev,
+	      sb->s_flags));
+
+	sbi = kmalloc(sizeof(struct jfs_sb_info), GFP_KERNEL);
+	JFS_SBI(sb) = sbi;
+	if (!sbi)
+		return NULL;
+	memset(sbi, 0, sizeof(struct jfs_sb_info));
+
+	if (!parse_options((char *)data, sbi)) {
+		kfree(sbi);
+		return NULL;
+	}
+
+	/*
+	 * Initialize blocksize to 4K.
+	 */
+	sb->s_blocksize = PSIZE;
+	sb->s_blocksize_bits = L2PSIZE;
+	set_blocksize(sb->s_dev, PSIZE);
+
+	/*
+	 * Initialize direct-mapping inode/address-space
+	 */
+	inode = new_inode(sb);
+	if (inode == NULL)
+		goto out_kfree;
+	inode->i_ino = 0;
+	inode->i_nlink = 1;
+	inode->i_size = 0x0000010000000000LL;
+	inode->i_mapping->a_ops = &direct_aops;
+	inode->i_mapping->gfp_mask = GFP_NOFS;
+
+	sbi->direct_inode = inode;
+	sbi->direct_mapping = inode->i_mapping;
+
+	rc = alloc_jfs_inode(inode);
+	if (rc)
+		goto out_free_inode;
+
+	sb->s_op = &jfs_sops;
+	rc = jfs_mount(sb);
+	if (rc) {
+		if (!silent) {
+			jERROR(1,
+			       ("jfs_mount failed w/return code = %d\n",
+				rc));
+		}
+		goto out_mount_failed;
+	}
+	if (sb->s_flags & MS_RDONLY)
+		sbi->log = 0;
+	else {
+		rc = jfs_mount_rw(sb, 0);
+		if (rc) {
+			if (!silent) {
+				jERROR(1,
+				       ("jfs_mount_rw failed w/return code = %d\n",
+					rc));
+			}
+			goto out_no_rw;
+		}
+	}
+
+	sb->s_magic = JFS_SUPER_MAGIC;
+
+	inode = iget(sb, ROOT_I);
+	if (!inode || is_bad_inode(inode))
+		goto out_no_root;
+	sb->s_root = d_alloc_root(inode);
+	if (!sb->s_root)
+		goto out_no_root;
+
+	if (!sbi->nls_tab)
+		sbi->nls_tab = load_nls_default();
+
+	sb->s_maxbytes = ((u64) sb->s_blocksize) << 40;
+#if BITS_PER_LONG == 32
+	sb->s_maxbytes = min((u64)PAGE_CACHE_SIZE << 32, sb->s_maxbytes);
+#endif
+
+	return sb;
+
+out_no_root:
+	jEVENT(1, ("jfs_read_super: get root inode failed\n"));
+	if (inode)
+		iput(inode);
+
+out_no_rw:
+	rc = jfs_umount(sb);
+	if (rc) {
+		jERROR(1, ("jfs_umount failed with return code %d\n", rc));
+	}
+out_mount_failed:
+	fsync_inode_data_buffers(sbi->direct_inode);
+	truncate_inode_pages(sbi->direct_mapping, 0);
+	sb->s_op = NULL;
+
+	free_jfs_inode(inode);
+
+out_free_inode:
+	iput(sbi->direct_inode);
+	sbi->direct_inode = NULL;
+	sbi->direct_mapping = NULL;
+out_kfree:
+	if (sbi->nls_tab)
+		unload_nls(sbi->nls_tab);
+	kfree(sbi);
+	return NULL;
+}
+
+static DECLARE_FSTYPE_DEV(jfs_fs_type, "jfs", jfs_read_super);
+
+extern int metapage_init(void);
+extern int txInit(void);
+extern void txExit(void);
+extern void metapage_exit(void);
+
+static void init_once(void * foo, kmem_cache_t * cachep, unsigned long flags)
+{
+      struct jfs_inode_info *jfs_ip = (struct jfs_inode_info *) foo;
+
+      if ((flags & (SLAB_CTOR_VERIFY|SLAB_CTOR_CONSTRUCTOR)) ==
+          SLAB_CTOR_CONSTRUCTOR) {
+              INIT_LIST_HEAD(&jfs_ip->anon_inode_list);
+              INIT_LIST_HEAD(&jfs_ip->mp_list);
+              RDWRLOCK_INIT(&jfs_ip->rdwrlock);
+      }
+}
+
+static int __init init_jfs_fs(void)
+{
+	int rc;
+
+	printk("JFS development version: $Name:  $\n");
+
+	jfs_inode_cachep =
+	    kmem_cache_create("jfs_ip",
+	                    sizeof(struct jfs_inode_info),
+                            0, 0, init_once, NULL);
+	if (jfs_inode_cachep == NULL)
+		return -ENOMEM;
+
+	/*
+	 * Metapage initialization
+	 */
+	rc = metapage_init();
+	if (rc) {
+		jERROR(1, ("metapage_init failed w/rc = %d\n", rc));
+		goto free_slab;
+	}
+
+	/*
+	 * Transaction Manager initialization
+	 */
+	rc = txInit();
+	if (rc) {
+		jERROR(1, ("txInit failed w/rc = %d\n", rc));
+		goto free_metapage;
+	}
+
+	/*
+	 * I/O completion thread (endio)
+	 */
+	jfsIOthread = kernel_thread(jfsIOWait, 0,
+				    CLONE_FS | CLONE_FILES |
+				    CLONE_SIGHAND);
+	if (jfsIOthread < 0) {
+		jERROR(1,
+		       ("init_jfs_fs: fork failed w/rc = %d\n",
+			jfsIOthread));
+		goto end_txmngr;
+	}
+	wait_for_completion(&jfsIOwait);	/* Wait until IO thread starts */
+
+	jfsCommitThread = kernel_thread(jfs_lazycommit, 0,
+					CLONE_FS | CLONE_FILES |
+					CLONE_SIGHAND);
+	if (jfsCommitThread < 0) {
+		jERROR(1,
+		       ("init_jfs_fs: fork failed w/rc = %d\n",
+			jfsCommitThread));
+		goto kill_iotask;
+	}
+	wait_for_completion(&jfsIOwait);	/* Wait until IO thread starts */
+
+	jfsSyncThread = kernel_thread(jfs_sync, 0,
+				      CLONE_FS | CLONE_FILES |
+				      CLONE_SIGHAND);
+	if (jfsSyncThread < 0) {
+		jERROR(1,
+		       ("init_jfs_fs: fork failed w/rc = %d\n",
+			jfsSyncThread));
+		goto kill_committask;
+	}
+	wait_for_completion(&jfsIOwait);	/* Wait until IO thread starts */
+
+#if defined(CONFIG_JFS_DEBUG) && defined(CONFIG_PROC_FS)
+	jfs_proc_init();
+#endif
+
+	return register_filesystem(&jfs_fs_type);
+
+
+kill_committask:
+	send_sig(SIGKILL, jfsCommitTask, 1);
+	wait_for_completion(&jfsIOwait);	/* Wait until Commit thread exits */
+kill_iotask:
+	send_sig(SIGKILL, jfsIOtask, 1);
+	wait_for_completion(&jfsIOwait);	/* Wait until IO thread exits */
+end_txmngr:
+	txExit();
+free_metapage:
+	metapage_exit();
+free_slab:
+	kmem_cache_destroy(jfs_inode_cachep);
+	return -rc;
+}
+
+static void __exit exit_jfs_fs(void)
+{
+	jFYI(1, ("exit_jfs_fs called\n"));
+
+	in_shutdown = 1;
+	txExit();
+	metapage_exit();
+	send_sig(SIGKILL, jfsIOtask, 1);
+	wait_for_completion(&jfsIOwait);	/* Wait until IO thread exits */
+	send_sig(SIGKILL, jfsCommitTask, 1);
+	wait_for_completion(&jfsIOwait);	/* Wait until Commit thread exits */
+	send_sig(SIGKILL, jfsSyncTask, 1);
+	wait_for_completion(&jfsIOwait);	/* Wait until Sync thread exits */
+#if defined(CONFIG_JFS_DEBUG) && defined(CONFIG_PROC_FS)
+	jfs_proc_clean();
+#endif
+	unregister_filesystem(&jfs_fs_type);
+	kmem_cache_destroy(jfs_inode_cachep);
+}
+
+
+EXPORT_NO_SYMBOLS;
+
+module_init(init_jfs_fs)
+module_exit(exit_jfs_fs)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/fs/jfs/symlink.c linuxppc64_2_4/fs/jfs/symlink.c
--- linux-2.4.19/fs/jfs/symlink.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/fs/jfs/symlink.c	Tue Apr 23 11:14:25 2002
@@ -0,0 +1,47 @@
+
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ *  JFS fast symlink handling code
+ */
+
+#include <linux/fs.h>
+#include "jfs_incore.h"
+
+static int jfs_readlink(struct dentry *, char *buffer, int buflen);
+static int jfs_follow_link(struct dentry *dentry, struct nameidata *nd);
+
+/*
+ * symlinks can't do much...
+ */
+struct inode_operations jfs_symlink_inode_operations = {
+	readlink:	jfs_readlink,
+	follow_link:	jfs_follow_link,
+};
+
+static int jfs_follow_link(struct dentry *dentry, struct nameidata *nd)
+{
+	char *s = JFS_IP(dentry->d_inode)->i_inline;
+	return vfs_follow_link(nd, s);
+}
+
+static int jfs_readlink(struct dentry *dentry, char *buffer, int buflen)
+{
+	char *s = JFS_IP(dentry->d_inode)->i_inline;
+	return vfs_readlink(dentry, buffer, buflen, s);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/dec/kn02ba.h linuxppc64_2_4/include/asm-mips/dec/kn02ba.h
--- linux-2.4.19/include/asm-mips/dec/kn02ba.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/dec/kn02ba.h	Wed Dec 31 18:00:00 1969
@@ -1,53 +0,0 @@
-/*
- *	include/asm-mips/dec/kn02ba.h
- *
- *	DECstation 5000/1xx (3min or KN02-BA) definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS_DEC_KN02BA_H 
-#define __ASM_MIPS_DEC_KN02BA_H 
-
-#include <asm/dec/kn02xa.h>		/* For common definitions. */
-
-/*
- * Some port addresses...
- */
-#define KN02BA_IOASIC_BASE	KN02XA_IOASIC_BASE	/* I/O ASIC */
-#define KN02BA_RTC_BASE		KN02XA_RTC_BASE		/* RTC */
-
-/*
- * CPU interrupt bits.
- */
-#define KN02BA_CPU_INR_HALT	6	/* HALT button */
-#define KN02BA_CPU_INR_CASCADE	5	/* I/O ASIC cascade */
-#define KN02BA_CPU_INR_TC2	4	/* TURBOchannel slot #2 */
-#define KN02BA_CPU_INR_TC1	3	/* TURBOchannel slot #1 */
-#define KN02BA_CPU_INR_TC0	2	/* TURBOchannel slot #0 */
-
-/*
- * I/O ASIC interrupt bits.  Star marks denote non-IRQ status bits.
- */
-#define KN02BA_IO_INR_RES_15	15	/* unused */
-#define KN02BA_IO_INR_NVRAM	14	/* (*) NVRAM clear jumper */
-#define KN02BA_IO_INR_RES_13	13	/* unused */
-#define KN02BA_IO_INR_MEMORY	12	/* memory, I/O bus write errors */
-#define KN02BA_IO_INR_RES_11	11	/* unused */
-#define KN02BA_IO_INR_NRMOD	10	/* (*) NRMOD manufacturing jumper */
-#define KN02BA_IO_INR_ASC	9	/* ASC (NCR53C94) SCSI */
-#define KN02BA_IO_INR_LANCE	8	/* LANCE (Am7990) Ethernet */
-#define KN02BA_IO_INR_SCC1	7	/* SCC (Z85C30) serial #1 */
-#define KN02BA_IO_INR_SCC0	6	/* SCC (Z85C30) serial #0 */
-#define KN02BA_IO_INR_RTC	5	/* DS1287 RTC */
-#define KN02BA_IO_INR_PSU	4	/* power supply unit warning */
-#define KN02BA_IO_INR_RES_3	3	/* unused */
-#define KN02BA_IO_INR_ASC_DATA	2	/* SCSI data ready (discouraged?) */
-#define KN02BA_IO_INR_PBNC	1	/* HALT button debouncer */
-#define KN02BA_IO_INR_PBNO	0	/* ~HALT button debouncer */
-
-#endif /* __ASM_MIPS_DEC_KN02BA_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/dec/kn02ca.h linuxppc64_2_4/include/asm-mips/dec/kn02ca.h
--- linux-2.4.19/include/asm-mips/dec/kn02ca.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/dec/kn02ca.h	Wed Dec 31 18:00:00 1969
@@ -1,53 +0,0 @@
-/*
- *	include/asm-mips/dec/kn02ca.h
- *
- *	Personal DECstation 5000/xx (Maxine or KN02-CA) definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS_DEC_KN02CA_H 
-#define __ASM_MIPS_DEC_KN02CA_H 
-
-#include <asm/dec/kn02xa.h>		/* For common definitions. */
-
-/*
- * Some port addresses...
- */
-#define KN02CA_IOASIC_BASE	KN02XA_IOASIC_BASE	/* I/O ASIC */
-#define KN02CA_RTC_BASE		KN02XA_RTC_BASE		/* RTC */
-
-/*
- * CPU interrupt bits.
- */
-#define KN02CA_CPU_INR_HALT	6	/* HALT from ACCESS.Bus */
-#define KN02CA_CPU_INR_CASCADE	5	/* I/O ASIC cascade */
-#define KN02CA_CPU_INR_MEMORY	4	/* memory, I/O bus write errors */
-#define KN02CA_CPU_INR_RTC	3	/* DS1287 RTC */
-#define KN02CA_CPU_INR_TIMER	2	/* ARC periodic timer */
-
-/*
- * I/O ASIC interrupt bits.  Star marks denote non-IRQ status bits.
- */
-#define KN02CA_IO_INR_FLOPPY	15	/* 82077 FDC */
-#define KN02CA_IO_INR_NVRAM	14	/* (*) NVRAM clear jumper */
-#define KN02CA_IO_INR_POWERON	13	/* (*) power-on reset */
-#define KN02CA_IO_INR_TC0	12	/* TURBOchannel slot #0 */
-#define KN02CA_IO_INR_ISDN	11	/* Am79C30A ISDN */
-#define KN02CA_IO_INR_NRMOD	10	/* (*) NRMOD manufacturing jumper */
-#define KN02CA_IO_INR_ASC	9	/* ASC (NCR53C94) SCSI */
-#define KN02CA_IO_INR_LANCE	8	/* LANCE (Am7990) Ethernet */
-#define KN02CA_IO_INR_HDFLOPPY	7	/* (*) HD (1.44MB) floppy status */
-#define KN02CA_IO_INR_SCC0	6	/* SCC (Z85C30) serial #0 */
-#define KN02CA_IO_INR_TC1	5	/* TURBOchannel slot #1 */
-#define KN02CA_IO_INR_XDFLOPPY	4	/* (*) XD (2.88MB) floppy status */
-#define KN02CA_IO_INR_VIDEO	3	/* framebuffer */
-#define KN02CA_IO_INR_XVIDEO	2	/* ~framebuffer */
-#define KN02CA_IO_INR_AB_XMIT	1	/* ACCESS.bus transmit */
-#define KN02CA_IO_INR_AB_RECV	0	/* ACCESS.bus receive */
-
-#endif /* __ASM_MIPS_DEC_KN02CA_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/dec/kn05.h linuxppc64_2_4/include/asm-mips/dec/kn05.h
--- linux-2.4.19/include/asm-mips/dec/kn05.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/dec/kn05.h	Wed Dec 31 18:00:00 1969
@@ -1,70 +0,0 @@
-/*
- *	include/asm-mips/dec/kn05.h
- *
- *	DECstation 5000/260 (4max+ or KN05) and DECsystem 5900-260
- *	definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- *
- *	WARNING!  All this information is pure guesswork based on the
- *	ROM.  It is provided here in hope it will give someone some
- *	food for thought.  No documentation for the KN05 module has
- *	been located so far.
- */
-#ifndef __ASM_MIPS_DEC_KN05_H
-#define __ASM_MIPS_DEC_KN05_H
-
-#include <asm/dec/ioasic_addrs.h>
-
-/*
- * The oncard MB (Memory Buffer) ASIC provides an additional address
- * decoder.  Certain address ranges within the "high" 16 slots are
- * passed to the I/O ASIC's decoder like with the KN03.  Others are
- * handled locally.  "Low" slots are always passed.
- */
-#define KN05_MB_ROM	(16*CHUNK_SIZE)		/* KN05 card ROM */
-#define KN05_IOCTL	(17*CHUNK_SIZE)		/* I/O ASIC */
-#define KN05_ESAR	(18*CHUNK_SIZE)		/* LANCE MAC address chip */
-#define KN05_LANCE	(19*CHUNK_SIZE)		/* LANCE Ethernet */
-#define KN05_MB_INT	(20*CHUNK_SIZE)		/* MB interrupt register? */
-#define KN05_MB_UNKN_0	(21*CHUNK_SIZE)		/* MB unknown register */
-#define KN05_MB_UNKN_1	(22*CHUNK_SIZE)		/* MB unknown register */
-#define KN05_MB_CSR	(23*CHUNK_SIZE)		/* MB control & status */
-#define KN05_RESERVED_0	(24*CHUNK_SIZE)		/* unused? */
-#define KN05_RESERVED_1	(25*CHUNK_SIZE)		/* unused? */
-#define KN05_RESERVED_2	(26*CHUNK_SIZE)		/* unused? */
-#define KN05_RESERVED_3	(27*CHUNK_SIZE)		/* unused? */
-#define KN05_SCSI	(28*CHUNK_SIZE)		/* ASC SCSI */
-#define KN05_RESERVED_4	(29*CHUNK_SIZE)		/* unused? */
-#define KN05_RESERVED_5	(30*CHUNK_SIZE)		/* unused? */
-#define KN05_RESERVED_6	(31*CHUNK_SIZE)		/* unused? */
-
-/*
- * Bits for the MB interrupt (?) register.
- * The register appears read-only.
- */
-#define KN05_MB_INT_TC		(1<<0)		/* TURBOchannel? */
-#define KN05_MB_INT_RTC		(1<<1)		/* RTC? */
-
-/*
- * Bits for the MB control & status register.
- * Set to 0x00bf8001 on my system by the ROM.
- */
-#define KN05_MB_CSR_PF		(1<<0)		/* ??? */
-#define KN05_MB_CSR_F		(1<<1)		/* ??? */
-#define KN05_MB_CSR_ECC		(0xff<<2)	/* ??? */
-#define KN05_MB_CSR_OD		(1<<10)		/* ??? */
-#define KN05_MB_CSR_CP		(1<<11)		/* ??? */
-#define KN05_MB_CSR_UNC		(1<<12)		/* ??? */
-#define KN05_MB_CSR_IM		(1<<13)		/* ??? */
-#define KN05_MB_CSR_NC		(1<<14)		/* ??? */
-#define KN05_MB_CSR_EE		(1<<15)		/* (bus) Exception Enable? */
-#define KN05_MB_CSR_MSK		(0x1f<<16)	/* ??? */
-#define KN05_MB_CSR_FW		(1<<21)		/* ??? */
-
-#endif /* __ASM_MIPS_DEC_KN05_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/dec/kn230.h linuxppc64_2_4/include/asm-mips/dec/kn230.h
--- linux-2.4.19/include/asm-mips/dec/kn230.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/dec/kn230.h	Wed Dec 31 18:00:00 1969
@@ -1,26 +0,0 @@
-/*
- *	include/asm-mips/dec/kn230.h
- *
- *	DECstation 5100 (MIPSmate or KN230) definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS_DEC_KN230_H 
-#define __ASM_MIPS_DEC_KN230_H 
-
-/*
- * CPU interrupt bits.
- */
-#define KN230_CPU_INR_HALT	6	/* HALT button */
-#define KN230_CPU_INR_MEMORY	5	/* memory, I/O bus write errors */
-#define KN230_CPU_INR_RTC	4	/* DS1287 RTC */
-#define KN230_CPU_INR_SII	3	/* SII (DC7061) SCSI */
-#define KN230_CPU_INR_LANCE	3	/* LANCE (Am7990) Ethernet */
-#define KN230_CPU_INR_DZ11	2	/* DZ11 (DC7085) serial */
-
-#endif /* __ASM_MIPS_DEC_KN230_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/cntmr.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/cntmr.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/cntmr.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/cntmr.h	Wed Dec 31 18:00:00 1969
@@ -1,42 +0,0 @@
-/* cntmr.h - Timer/Counter interface header file */
-
-/* Copyright - Galileo technology */
-
-#ifndef __INCtimerCounterDrvh 
-#define __INCtimerCounterDrvh
-
-/* includes */
-
-#include "core.h"
-
-/* defines */
-
-#define FIRST_CNTMR   0
-#define LAST_CNTMR    3
-
-#define CNTMR0_READ(pData)\
-        GT_REG_READ(CNTMR0, pData)
-
-#define CNTMR1_READ(pData)\
-        GT_REG_READ(CNTMR1, pData)
-
-#define CNTMR2_READ(pData)\
-        GT_REG_READ(CNTMR2, pData)
-
-#define CNTMR3_READ(pData)\
-        GT_REG_READ(CNTMR3, pData)
-
-/* typedefs */
-
-typedef enum counterTimer{CNTMR_0,CNTMR_1,CNTMR_2,CNTMR_3} CNTMR_NUM;
-typedef enum cntTmrOpModes{COUNTER, TIMER} CNT_TMR_OP_MODES;
-
-bool    cntTmrLoad(unsigned int countNum, unsigned int value);	
-bool    cntTmrSetMode(CNTMR_NUM countNum, CNT_TMR_OP_MODES opMode);
-bool    cntTmrEnable(CNTMR_NUM countNum);  
-bool    cntTmrStart (CNTMR_NUM countNum,unsigned int countValue,
-                   CNT_TMR_OP_MODES opMode);
-unsigned int    cntTmrDisable(CNTMR_NUM countNum);              
-unsigned int    cntTmrRead(CNTMR_NUM countNum);
-
-#endif /* __INCtimerCounterDrvh */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/core.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/core.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/core.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/core.h	Wed Dec 31 18:00:00 1969
@@ -1,182 +0,0 @@
-/*
- * This header file contains simple Read/Write macros for addressing the SDRAM,
- * devices, GT`s internal registers and PCI (using the PCI`s address space).
- *
- * Copyright Galileo Technology.
- */
-#ifndef __INCcoreh 
-#define __INCcoreh
-
-#include <linux/types.h>
-#include <asm/byteorder.h>
-#include <asm/gt64120.h>
-
-#define INTERNAL_REG_BASE_ADDR 0x14000000
-
-#define NO_BIT          0x00000000
-#define BIT0            0x00000001
-#define BIT1            0x00000002
-#define BIT2            0x00000004
-#define BIT3            0x00000008
-#define BIT4            0x00000010
-#define BIT5            0x00000020
-#define BIT6            0x00000040
-#define BIT7            0x00000080
-#define BIT8            0x00000100
-#define BIT9            0x00000200
-#define BIT10           0x00000400
-#define BIT11           0x00000800
-#define BIT12           0x00001000
-#define BIT13           0x00002000
-#define BIT14           0x00004000
-#define BIT15           0x00008000
-#define BIT16           0x00010000
-#define BIT17           0x00020000
-#define BIT18           0x00040000
-#define BIT19           0x00080000
-#define BIT20           0x00100000
-#define BIT21           0x00200000
-#define BIT22           0x00400000
-#define BIT23           0x00800000
-#define BIT24           0x01000000
-#define BIT25           0x02000000
-#define BIT26           0x04000000
-#define BIT27           0x08000000
-#define BIT28           0x10000000
-#define BIT29           0x20000000
-#define BIT30           0x40000000
-#define BIT31           0x80000000
-
-#define _1K             0x00000400
-#define _2K             0x00000800
-#define _4K             0x00001000
-#define _8K             0x00002000
-#define _16K            0x00004000
-#define _32K            0x00008000
-#define _64K            0x00010000
-#define _128K           0x00020000
-#define _256K           0x00040000
-#define _512K           0x00080000
-
-#define _1M             0x00100000
-#define _2M             0x00200000
-#define _3M             0x00300000
-#define _4M             0x00400000
-#define _5M             0x00500000
-#define _6M             0x00600000
-#define _7M             0x00700000
-#define _8M             0x00800000
-#define _9M             0x00900000
-#define _10M            0x00a00000
-#define _11M            0x00b00000
-#define _12M            0x00c00000
-#define _13M            0x00d00000
-#define _14M            0x00e00000
-#define _15M            0x00f00000
-#define _16M            0x01000000
-
-typedef enum _bool{false,true} bool;
-
-#ifndef NULL
-#define NULL 0                                  
-#endif
-
-/* The two following defines are according to MIPS architecture. */
-#define NONE_CACHEABLE			0xa0000000
-#define MIPS_CACHEABLE			0x80000000
-
-/* Read/Write to/from GT`s internal registers */
-#define GT_REG_READ(offset, pData)					\
-do {									\
-	*pData = (*((u32 *)(NONE_CACHEABLE |				\
-		INTERNAL_REG_BASE_ADDR | (offset))));			\
-	*pData = cpu_to_le32(*pData);					\
-} while(0)
-
-#define GT_REG_WRITE(offset, data)					\
-	(*((u32 *)(NONE_CACHEABLE | INTERNAL_REG_BASE_ADDR |		\
-		(offset))) = cpu_to_le32(data))
-
-#define VIRTUAL_TO_PHY(y)	((u32)(y) & (u32)0x5fffffff)
-#define PHY_TO_VIRTUAL(y)	((u32)(y) | NONE_CACHEABLE)
-
-/* Write 32/16/8 bit Non-Cache-able */
-#define WRITE_CHAR(address, data)					\
-	(*((u8 *)((address) | NONE_CACHEABLE)) = (data))
-#define WRITE_SHORT(address, data)					\
-	(*((u16 *)((address) | NONE_CACHEABLE)) = (u16) data)
-#define WRITE_WORD(address, data)					\
-	(*((u32 *)((address) | NONE_CACHEABLE)) = (u32) data)
-
-/* Write 32/16/8 bits Cacheable */
-#define WRITE_CHAR_CACHEABLE(address, data)				\
-	(*((u8 *)((address) | MIPS_CACHEABLE)) = (data))
-                
-#define WRITE_SHORT_CACHEABLE(address, data)				\
-	(*((u16 *)((address) | MIPS_CACHEABLE)) = (u16) data)
-
-#define WRITE_WORD_CACHEABLE(address, data)				\
-	(*((u32 *)((address) | MIPS_CACHEABLE )) = (u32) data)
-
-/* Read 32/16/8 bits NonCacheable - returns data in variable. */
-#define READ_CHAR(address,pData)					\
-	(*(pData) = *((u8 *)((address) | NONE_CACHEABLE)))
-
-#define READ_SHORT(address,pData)					\
-	(*(pData) = *((u16 *)((address) | NONE_CACHEABLE)))
-
-#define READ_WORD(address,pData)					\
-	(*(pData) = *((u32 *)((address) | NONE_CACHEABLE)))
-
-/* Read 32/16/8 bit NonCacheable - returns data direct. */
-#define READCHAR(address)						\
-	(*((u8 *)((address) | NONE_CACHEABLE)))
-
-#define READSHORT(address)						\
-	(*((u16 *)((address) | NONE_CACHEABLE)))
-
-#define READWORD(address)						\
-        (*((u32 *)((address) | NONE_CACHEABLE)))
-
-/* Read 32/16/8 bit Cacheable - returns data in variable. */
-#define READ_CHAR_CACHEABLE(address,pData)				\
-        (*(pData) = *((u8 *)((address) | MIPS_CACHEABLE)))
-
-#define READ_SHORT_CACHEABLE(address,pData)				\
-        (*(pData) = *((u16 *)((address) | MIPS_CACHEABLE)))
-#define READ_WORD_CACHEABLE(address,pData)				\
-        (*(pData) = *((u32 *)((address) | MIPS_CACHEABLE)))
-
-/* Read 32/16/8 bit Cacheable - returns data direct. */
-#define READCHAR_CACHEABLE(address)					\
-	(*((u8 *)((address) | MIPS_CACHEABLE)))
-
-#define READSHORT_CACHEABLE(address)					\
-	(*((u16 *)((address) | MIPS_CACHEABLE)))
-
-#define READWORD_CACHEABLE(address)					\
-	(*((u32 *)((address) | MIPS_CACHEABLE)))
-
-/*
- * SET_REG_BITS(regOffset,bits) - 
- * gets register offset and bits: a 32bit value. It set to logic '1' in the  
- * internal register the bits which given as an input example:
- * SET_REG_BITS(0x840,BIT3 | BIT24 | BIT30) - set bits: 3,24 and 30 to logic
- * '1' in register 0x840 while the other bits stays as is.
- */
-#define SET_REG_BITS(regOffset,bits)					\
-	(*(u32*)(NONE_CACHEABLE | INTERNAL_REG_BASE_ADDR |		\
-		(regOffset)) |= (u32)cpu_to_le32(bits))
-
-/*
- * RESET_REG_BITS(regOffset,bits) -
- * gets register offset and bits: a 32bit value. It set to logic '0' in the  
- * internal register the bits which given as an input example:
- * RESET_REG_BITS(0x840,BIT3 | BIT24 | BIT30) - set bits: 3,24 and 30 to logic
- * '0' in register 0x840 while the other bits stays as is.
- */
-#define RESET_REG_BITS(regOffset,bits)					\
-	(*(u32 *)(NONE_CACHEABLE | INTERNAL_REG_BASE_ADDR |		\
-		(regOffset)) &= ~((u32)cpu_to_le32(bits)))
-
-#endif /* __INCcoreh */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/dma.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/dma.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/dma.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/dma.h	Wed Dec 31 18:00:00 1969
@@ -1,80 +0,0 @@
-/* DMA.h - DMA functions and definitions*/
-
-/* Copyright Galileo Technology. */
-
-#ifndef __INCdmah 
-#define __INCdmah
-
-/* includes */
-
-#include "core.h"
-
-/* defines */
-
-#define FIRST_DMA_ENGINE   0
-#define LAST_DMA_ENGINE    3
-
-#define FLY_BY						BIT0
-#define RD_WR_FLY					BIT1
-#define DECREMENT_SOURCE_ADDRESS	BIT2
-#define HOLD_SOURCE_ADDRESS			BIT3
-#define DECREMENT_DEST_ADDRESS		BIT4
-#define HOLD_DEST_ADDRESS			BIT5
-#define DTL_1BYTE					BIT6 | BIT8
-#define DTL_2BYTES					BIT7 | BIT8
-#define DTL_4BYTES					BIT7
-#define DTL_8BYTES					NO_BIT
-#define DTL_16BYTES					BIT6
-#define DTL_32BYTES					BIT6 | BIT7
-#define DTL_64BYTES					BIT6 | BIT7 | BIT8
-#define NON_CHAIN_MOD				BIT9
-#define INT_EVERY_NULL_POINTER		BIT10
-#define BLOCK_TRANSFER_MODE			BIT11
-#define CHANNEL_ENABLE				BIT12
-#define FETCH_NEXT_RECORED			BIT13
-#define DMA_ACTIVITY_STATUS         BIT14
-#define ALIGN_TOWARD_DEST			BIT15
-#define MASK_DMA_REQ				BIT16
-#define ENABLE_DESCRIPTOR			BIT17
-#define ENABLE_EOT					BIT18
-#define ENABLE_EOT_INTERRUPT		BIT19
-#define ABORT_DMA					BIT20
-#define SOURCE_ADDR_IN_PCI0			BIT21
-#define SOURCE_ADDR_IN_PCI1			BIT22
-#define DEST_ADDR_IN_PCI0			BIT23
-#define DEST_ADDR_IN_PCI1			BIT24
-#define REC_ADDR_IN_PCI0			BIT25
-#define REC_ADDR_IN_PCI1			BIT26
-#define REQ_FROM_TIMER_COUNTER		BIT28
-
-/* typedefs */
-
-typedef enum dmaEngine{DMA_ENG_0,DMA_ENG_1,DMA_ENG_2,DMA_ENG_3} DMA_ENGINE;
-
-/* priority definitions */
-typedef enum prioChan01{ROUND_ROBIN01,CH_1,CH_0} PRIO_CHAN_0_1;
-typedef enum prioChan23{ROUND_ROBIN23,CH_3,CH_2} PRIO_CHAN_2_3;
-typedef enum prioGroup{ROUND_ROBIN,CH_2_3,CH_0_1} PRIO_GROUP;
-typedef enum prioOpt{RETURN_BUS,KEEP_BUS} PRIO_OPT; 
-
-typedef struct dmaRecored
-{
-    unsigned int    ByteCnt;                                            
-    unsigned int    SrcAdd;
-    unsigned int    DestAdd;
-    unsigned int    NextRecPtr;
-} DMA_RECORED;
-
-typedef enum __dma_status{CHANNEL_BUSY,NO_SUCH_CHANNEL,DMA_OK,
-                            GENERAL_ERROR} DMA_STATUS;
-
-DMA_STATUS dmaTransfer (DMA_ENGINE engine,unsigned int sourceAddr,
-                        unsigned int destAddr,unsigned int numOfBytes,
-                        unsigned int command,DMA_RECORED * nextRecoredPointer);
-bool	dmaCommand (DMA_ENGINE channel,unsigned int command);
-bool    isDmaChannelActive (DMA_ENGINE channel);
-
-bool    changeDmaPriority(PRIO_CHAN_0_1 prio_01, PRIO_CHAN_2_3 prio_23, 
-                          PRIO_GROUP prioGrp, PRIO_OPT prioOpt);
-
-#endif /* __INCdmah */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/eeprom_param.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/eeprom_param.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/eeprom_param.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/eeprom_param.h	Wed Dec 31 18:00:00 1969
@@ -1,53 +0,0 @@
-#ifndef EEPROM_PARAM_
-#define EEPROM_PARAM_
-#define SDRAM_REGS 0xbf000000
-
-unsigned int galileo_dl(void);
-void (*boot_addr)(int argc, char **argv, char **envp);
-
-#define NETWORK_BT_BIN 0
-#define FLASH_BT 1
-#define SERIAL_BT 2
-#define NETWORK_BT_SREC 3
-
-#define LINUX_OS 0
-#define OTHER_OS 1
-
-/********************************************************************
- *eeprom_parameters -
- *
- *This structure holds the eeprom parameters (usually stored on flash
- *memory)
- *The structure is all stored in flash memory except memory_size which
- *is probed each boot time for the real size of memory on the
- *evaluation board.
- *
- *The structure also holds information that is not used by all
- *evaluation board, such as the eth?_mac, which holds the MAC addresses
- *of the built in ethernet ports in the EVB96100 for example, but is
- *never used by EVB64120A.
- *
- *********************************************************************/
-
-struct eeprom_parameters {
-  unsigned int boot_source;
-  unsigned int operating_system;
-  
-  /* network loader parametrs */
-  unsigned int host_ip;
-  unsigned int server_ip;
-  char bootimage[64];
-  
-  /* Board parameters */
-  char eth0_mac[6];
-  char eth1_mac[6];
-  char eth2_mac[6];
-  char eth3_mac[6];
-
-  /* Command Line (usually needed for Linux) */
-  char os_command_line[256];
-  unsigned int entry_point;
-  unsigned memory_size;
-};
-
-#endif /* EEPROM_PARAM_ */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/flashdrv.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/flashdrv.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/flashdrv.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/flashdrv.h	Wed Dec 31 18:00:00 1969
@@ -1,81 +0,0 @@
-/* flashdrv.h - FLASH memory interface header file */
-
-/* Copyright Galileo Technology. */
-
-#ifndef __INCflashdrvh 
-#define __INCflashdrvh
-
-/* includes */
-
-#include "core.h"
-
-/* defines */
-
-/* Supported Flash Manufactures */ 
-
-#define AMD_FLASH       0x01
-#define ST_FLASH        0x20
-#define INTEL_FLASH     0x89
-#define MICRON_FLASH    0x89
-
-/* Supported Flash Devices */
-
-/* AMD Devices */
-#define AM29F400BT      0x2223    
-#define AM29F400BB      0x22AB
-#define AM29LV800BT     0x22DA
-#define AM29LV400BT     0x22B9
-#define AM29LV400BB     0x22BA
-#define AM29LV040B      0x4f
-/* ST Devices */
-#define M29W040         0xE3
-/* INTEL Devices - We have added I before the name defintion.*/
-#define I28F320J3A      0x16
-#define I28F640J3A      0x17
-#define I28F128J3A      0x18
-#define I28F320B3_B     0x8897
-#define I28F320B3_T     0x8896 
-#define I28F160B3_B     0x8891
-#define I28F160B3_T     0x8890
-
-#define POINTER_TO_FLASH           flashParametrs[0] 
-#define FLASH_BASE_ADDRESS         flashParametrs[1] 
-#define FLASH_WIDTH                flashParametrs[2] /* In Bytes */
-#define FLASH_MODE                 flashParametrs[3] /* In bits  */
-#define MANUFACTOR_ID              POINTER_TO_FLASH + 0
-#define VENDOR_ID                  POINTER_TO_FLASH + 1
-#define NUMBER_OF_SECTORS          POINTER_TO_FLASH + 2
-#define FIRST_SECTOR_SIZE          POINTER_TO_FLASH + 3
-#define NUM_OF_DEVICES             FLASH_WIDTH / (FLASH_MODE / 8)
- 
-/* typedefs */
-
-typedef enum _FlashMode {PURE8,X8 = 8,X16 = 16} FLASHmode;
-/* PURE8 - when using a flash device whice can be configurated only as  
-            8 bit device. */
-/* X8    - when using a flash device which is 16 bit wide but configured to
-           operate in 8 bit mode.*/
-/* X16   - when using a flash device which is 16 bit wide */
-
-bool    flashErase(void);
-bool    flashEraseSector(unsigned int sectorNumber);
-bool    flashWriteWord(unsigned int offset,unsigned int data);
-bool    flashWriteShort(unsigned int offset,unsigned short sdata);
-bool    flashWriteChar(unsigned int offset,unsigned char cdata);
-void    flashReset(void);
-unsigned int    flashInWhichSector(unsigned int offset);
-unsigned int    flashGetSectorSize(unsigned int sectorNumber);
-unsigned int    flashInit(unsigned int baseAddress,unsigned int flashWidth,
-                          FLASHmode FlashMode);
-unsigned int    flashGetNumOfSectors(void);
-unsigned int    flashGetSize(void);
-unsigned int    flashGetSectorOffset(unsigned int sectorNum);
-unsigned int    flashWriteBlock(unsigned int offset,unsigned int numOfByte,
-                        unsigned char * blockAddress);
-unsigned int    flashReadWord(unsigned int offset);
-unsigned char   flashReadChar(unsigned int offset);
-unsigned short  flashReadShort(unsigned int offset);
-unsigned int    flashReadBlock(unsigned int offset,unsigned int numOfByte,
-                               unsigned char * blockAddress);
-#endif /* __INCflashdrvh */
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/i2o.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/i2o.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/i2o.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/i2o.h	Wed Dec 31 18:00:00 1969
@@ -1,61 +0,0 @@
-/* i2o.h - Header file for the I2O`s interface */
-
-/* Copyright - Galileo technology. */
-
-#ifndef __INCi2oh
-#define __INCi2oh
-
-/* includes */            
-            
-#include "core.h"
-
-/* typedefs */           
-
-typedef enum _i2oMessageReg{MESSAGE_REG_0,MESSAGE_REG_1} I2O_MESSAGE_REG;
-typedef enum _cirQueSize{I20_16K = 0x1,I20_32K = 0x2,I20_64K = 0x4,\
-                  I20_128K = 0x8,I20_256K = 0xc} CIRCULAR_QUEUE_SIZE;
-
-/* Message handle Functions */
-unsigned int    getInBoundMassege(I2O_MESSAGE_REG messageRegNum);
-bool  sendOutBoundMassege(I2O_MESSAGE_REG messageRegNum,unsigned int message);
-bool  checkInBoundIntAndClear(I2O_MESSAGE_REG messageRegNum);
-bool  outBoundMessageAcknowledge(I2O_MESSAGE_REG messageRegNum);
-bool  maskInBoundMessageInterrupt(I2O_MESSAGE_REG messageRegNum);
-bool  enableInBoundMessageInterrupt(I2O_MESSAGE_REG messageRegNum);
-bool  maskOutBoundMessageInterrupt(I2O_MESSAGE_REG messageRegNum);
-bool  enableOutBoundMessageInterrupt(I2O_MESSAGE_REG messageRegNum);
-
-/* Doorbell handle Functions */
-unsigned int    readInBoundDoorBellInt(void);
-bool  initiateOutBoundDoorBellInt(unsigned int interruptBits);
-bool  clearInBoundDoorBellInt(unsigned int interruptBits);
-bool  isInBoundDoorBellInterruptSet(void);
-bool  isOutBoundDoorBellInterruptSet(void); /* For acknowledge */
-bool  maskInBoundDoorBellInterrupt(void);
-bool  enableInBoundDoorBellInterrupt(void);
-bool  maskOutBoundDoorBellInterrupt(void);
-bool  enableOutBoundDoorBellInterrupt(void);
-
-/* I2O - Circular Queues handle Functions */
-
-/* initialization */
-bool  circularQueueEnable(CIRCULAR_QUEUE_SIZE cirQueSize,
-                          unsigned int queueBaseAddr);
-
-/* Inbound Post Queue */
-unsigned int    inBoundPostQueuePop(void);
-bool    isInBoundPostQueueInterruptSet(void);
-bool  clearInBoundPostQueueInterrupt(void);
-void    maskInBoundPostQueueInterrupt(void);
-void    enableInBoundPostQueueInterrupt(void);
-/* Outbound Post Queue */
-bool  outBoundPostQueuePush(unsigned int data);
-bool    isOutBoundPostQueueEmpty(void);
-/* Inbound Free Queue */
-bool  inBoundFreeQueuePush(unsigned int data);
-bool    isInBoundFreeQueueEmpty(void);
-/* Outbound Free Queue */
-unsigned int    outBoundFreeQueuePop(void);
-
-#endif  /* __INCi2oh */
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/memory.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/memory.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/memory.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/memory.h	Wed Dec 31 18:00:00 1969
@@ -1,57 +0,0 @@
-/* Memory.h - Memory mappings and remapping functions declarations */
-
-/* Copyright - Galileo technology. */
-
-#ifndef __INCmemoryh
-#define __INCmemoryh
-
-/* includes */
-
-#include "core.h"
-
-/* defines */
-
-#define DONT_MODIFY     0xffffffff
-#define PARITY_SUPPORT  0x40000000
-
-#define _8BIT           0x00000000
-#define _16BIT          0x00010000
-#define _32BIT          0x00020000
-#define _64BIT          0x00030000
-
-/* typedefs */
-
-typedef enum __memBank{BANK0,BANK1,BANK2,BANK3} MEMORY_BANK;
-typedef enum __device{DEVICE0,DEVICE1,DEVICE2,DEVICE3,BOOT_DEVICE} DEVICE;
-
-
-unsigned int   getMemoryBankBaseAddress(MEMORY_BANK bank);
-unsigned int   getDeviceBaseAddress(DEVICE device);
-unsigned int   getMemoryBankSize(MEMORY_BANK bank);
-unsigned int   getDeviceSize(DEVICE device);
-unsigned int   getDeviceWidth(DEVICE device);
-
-bool mapMemoryBanks0and1(unsigned int bank0Base,unsigned int bank0Length,
-                         unsigned int bank1Base,unsigned int bank1Length);
-bool mapMemoryBanks2and3(unsigned int bank2Base,unsigned int bank2Length,
-                         unsigned int bank3Base,unsigned int bank3Length);
-bool mapDevices0_1and2MemorySpace(unsigned int device0Base,
-                                  unsigned int device0Length,
-                                  unsigned int device1Base,
-                                  unsigned int device1Length,
-                                  unsigned int device2Base,
-                                  unsigned int device2Length);
-bool mapDevices3andBootMemorySpace(unsigned int device3Base,
-                                   unsigned int device3Length,
-                                   unsigned int bootDeviceBase,
-                                   unsigned int bootDeviceLength);
-bool mapInternalRegistersMemorySpace(unsigned int internalRegBase);
-bool modifyDeviceParameters(DEVICE device,unsigned int TurnOff,
-                            unsigned int AccToFirst,unsigned int AccToNext,
-                            unsigned int ALEtoWr, unsigned int WrActive,
-                            unsigned int WrHigh,unsigned int Width,
-                            bool ParitySupport);
-
-bool remapAddress(unsigned int remapReg, unsigned int remapValue);
-#endif /* __INCmemoryh */
-
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/pci.h linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/pci.h
--- linux-2.4.19/include/asm-mips/galileo-boards/evb64120A/pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/galileo-boards/evb64120A/pci.h	Wed Dec 31 18:00:00 1969
@@ -1,165 +0,0 @@
-/* PCI.h - PCI functions header file */
-
-/* Copyright - Galileo technology. */
-
-#ifndef  __INCpcih
-#define  __INCpcih
-
-/* includes */
-
-#include"core.h"
-
-/* defines */
-
-#define PCI0_MASTER_ENABLE(deviceNumber) pci0WriteConfigReg(                  \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MASTER_ENABLE |                \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI0_MASTER_DISABLE(deviceNumber) pci0WriteConfigReg(                 \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,~MASTER_ENABLE &               \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI1_MASTER_ENABLE(deviceNumber) pci1WriteConfigReg(                  \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MASTER_ENABLE |                \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI1_MASTER_DISABLE(deviceNumber) pci1WriteConfigReg(                 \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,~MASTER_ENABLE &               \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI0_MEMORY_ENABLE(deviceNumber) pci0WriteConfigReg(                  \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MEMORY_ENABLE |                \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )           
-
-#define PCI1_MEMORY_ENABLE(deviceNumber) pci1WriteConfigReg(                  \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MEMORY_ENABLE |                \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI0_IO_ENABLE(deviceNumber) pci0WriteConfigReg(                      \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,I_O_ENABLE |                   \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI1_IO_ENABLE(deviceNumber) pci1WriteConfigReg(                      \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,I_O_ENABLE |                   \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI0_SLAVE_ENABLE(deviceNumber) pci0WriteConfigReg(                   \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MEMORY_ENABLE | I_O_ENABLE |   \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI1_SLAVE_ENABLE(deviceNumber) pci1WriteConfigReg(                   \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,MEMORY_ENABLE | I_O_ENABLE |   \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber) )
-
-#define PCI0_DISABLE(deviceNumber) pci0WriteConfigReg(                        \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,0xfffffff8  &                  \
-          pci0ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber))
-
-#define PCI1_DISABLE(deviceNumber) pci1WriteConfigReg(                        \
-          PCI_0STATUS_AND_COMMAND,deviceNumber,0xfffffff8  &                  \
-          pci1ReadConfigReg(PCI_0STATUS_AND_COMMAND,deviceNumber))
-
-#define 	MASTER_ENABLE			BIT2  
-#define		MEMORY_ENABLE			BIT1  
-#define		I_O_ENABLE  			BIT0  
-#define     SELF                    0
-/* Agent on the PCI bus may have up to 6 BARS. */
-#define     BAR0                    0x10
-#define     BAR1                    0x14
-#define     BAR2                    0x18
-#define     BAR3                    0x1c
-#define     BAR4                    0x20
-#define     BAR5                    0x24
-   
-
-/* typedefs */
-
-typedef struct pciDevice
-{
-    char            type[20];
-    unsigned int    deviceNum;
-    unsigned int    venID;                                            
-    unsigned int    deviceID;
-    unsigned int    bar0Base;
-    unsigned int    bar0Size;
-    unsigned int    bar0Type;
-    unsigned int    bar1Base;
-    unsigned int    bar1Size;
-    unsigned int    bar1Type;
-    unsigned int    bar2Base;
-    unsigned int    bar2Size;
-    unsigned int    bar2Type;
-    unsigned int    bar3Base;
-    unsigned int    bar3Size;
-    unsigned int    bar3Type;
-    unsigned int    bar4Base;
-    unsigned int    bar4Size;
-    unsigned int    bar4Type;
-    unsigned int    bar5Base;
-    unsigned int    bar5Size;
-    unsigned int    bar5Type;
-} PCI_DEVICE;
-
-void    pci0WriteConfigReg(unsigned int regOffset,unsigned int pciDevNum,
-                           unsigned int data);
-void    pci1WriteConfigReg(unsigned int regOffset,unsigned int pciDevNum,
-                           unsigned int data);
-void    pci0ScanDevices(PCI_DEVICE *pci0Detect,unsigned int numberOfElment);
-void    pci1ScanDevices(PCI_DEVICE *pci1Detect,unsigned int numberOfElment);
-unsigned int    pci0ReadConfigReg (unsigned int regOffset,
-                                   unsigned int pciDevNum);
-unsigned int    pci1ReadConfigReg (unsigned int regOffset,
-                                   unsigned int pciDevNum);
-
-/*      Master`s memory space   */
-
-void    pci0MapIOspace(unsigned int pci0IoBase,unsigned int pci0IoLength);
-void    pci0MapMemory0space(unsigned int pci0Mem0Base,
-                            unsigned int pci0Mem0Length);
-void    pci0MapMemory1space(unsigned int pci0Mem1Base,
-                            unsigned int pci0Mem1Length);
-
-void    pci1MapIOspace(unsigned int pci1IoBase,unsigned int pci1IoLength);
-void    pci1MapMemory0space(unsigned int pci1Mem0Base,
-                            unsigned int pci1Mem0Length);
-void    pci1MapMemory1space(unsigned int pci1Mem1Base,
-                            unsigned int pci1Mem1Length);
-
-unsigned int    pci0GetIOspaceBase(void);
-unsigned int    pci0GetIOspaceSize(void);
-unsigned int    pci0GetMemory0Base(void);
-unsigned int    pci0GetMemory0Size(void);
-unsigned int    pci0GetMemory1Base(void);
-unsigned int    pci0GetMemory1Size(void);
-
-unsigned int    pci1GetIOspaceBase(void);
-unsigned int    pci1GetIOspaceSize(void);
-unsigned int    pci1GetMemory0Base(void);
-unsigned int    pci1GetMemory0Size(void);
-unsigned int    pci1GetMemory1Base(void);
-unsigned int    pci1GetMemory1Size(void);
-
-/*      Slave`s memory space   */
-void    pci0MapInternalRegSpace(unsigned int pci0InternalBase);
-void    pci0MapInternalRegIOSpace(unsigned int pci0InternalBase);
-void    pci0MapMemoryBanks0_1(unsigned int pci0Dram0_1Base,
-                              unsigned int pci0Dram0_1Size);
-void    pci0MapMemoryBanks2_3(unsigned int pci0Dram2_3Base,
-                              unsigned int pci0Dram2_3Size);
-void    pci0MapDevices0_1and2MemorySpace(unsigned int pci0Dev012Base,
-                                         unsigned int pci0Dev012Length);
-void    pci0MapDevices3andBootMemorySpace(unsigned int pci0Dev3andBootBase,
-                                          unsigned int pci0Dev3andBootLength);
-
-void    pci1MapInternalRegSpace(unsigned int pci1InternalBase);
-void    pci1MapInternalRegIOSpace(unsigned int pci1InternalBase);
-void    pci1MapMemoryBanks0_1(unsigned int pci1Dram0_1Base,
-                              unsigned int pci1Dram0_1Size);
-void    pci1MapMemoryBanks2_3(unsigned int pci1Dram2_3Base,
-                              unsigned int pci1Dram2_3Size);
-void    pci1MapDevices0_1and2MemorySpace(unsigned int pci1Dev012Base,
-                                         unsigned int pci1Dev012Length);
-void    pci1MapDevices3andBootMemorySpace(unsigned int pci1Dev3andBootBase,
-                                          unsigned int pci1Dev3andBootLength);
-
-#endif  /* __INCpcih */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/irq_cpu.h linuxppc64_2_4/include/asm-mips/irq_cpu.h
--- linux-2.4.19/include/asm-mips/irq_cpu.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/irq_cpu.h	Wed Dec 31 18:00:00 1969
@@ -1,18 +0,0 @@
-/*
- *	include/asm-mips/irq_cpu.h
- *
- *	MIPS CPU interrupt definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS_IRQ_CPU_H
-#define __ASM_MIPS_IRQ_CPU_H
-
-extern void mips_cpu_irq_init(int irq_base);
-
-#endif /* __ASM_MIPS_IRQ_CPU_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/mips-boards/bonito64.h linuxppc64_2_4/include/asm-mips/mips-boards/bonito64.h
--- linux-2.4.19/include/asm-mips/mips-boards/bonito64.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/mips-boards/bonito64.h	Wed Dec 31 18:00:00 1969
@@ -1,432 +0,0 @@
-/*
- * bonito.h
- *
- * Carsten Langgaard, carstenl@mips.com
- * Copyright (C) 2001 MIPS Technologies, Inc.  All rights reserved.
- *
- * ########################################################################
- *
- * This file is the original bonito.h from Algorithmics with minor changes
- * to fit into linux.
- */
-
-/*
- * Bonito Register Map 
- * Copyright (c) 1999 Algorithmics Ltd
- *
- * Algorithmics gives permission for anyone to use and modify this file
- * without any obligation or license condition except that you retain
- * this copyright message in any source redistribution in whole or part.
- *
- * Updated copies of this and other files can be found at
- * ftp://ftp.algor.co.uk/pub/bonito/
- * 
- * Users of the Bonito controller are warmly recommended to contribute
- * any useful changes back to Algorithmics (mail to bonito@algor.co.uk).
- */
-
-/* Revision 1.48 autogenerated on 08/17/99 15:20:01 */
-/* This bonito64 version editted from bonito.h Revision 1.48 on 11/09/00 */
-
-#ifndef _ASM_MIPS_BOARDS_BONITO64_H
-#define _ASM_MIPS_BOARDS_BONITO64_H
-
-#ifdef __ASSEMBLY__
-
-/* offsets from base register */
-#define BONITO(x)	(x)
-
-#else /* !__ASSEMBLY__ */
-
-/* offsets from base pointer, this construct allows optimisation */
-/* static char * const _bonito = PA_TO_KVA1(BONITO_BASE); */
-#define BONITO(x)		*(volatile u32 *)(_bonito + (x))
-
-#endif /* __ASSEMBLY__ */
-
-
-#define BONITO_BOOT_BASE		0x1fc00000
-#define BONITO_BOOT_SIZE		0x00100000
-#define BONITO_BOOT_TOP 		(BONITO_BOOT_BASE+BONITO_BOOT_SIZE-1)
-#define BONITO_FLASH_BASE		0x1c000000
-#define BONITO_FLASH_SIZE		0x03000000
-#define BONITO_FLASH_TOP		(BONITO_FLASH_BASE+BONITO_FLASH_SIZE-1)
-#define BONITO_SOCKET_BASE		0x1f800000
-#define BONITO_SOCKET_SIZE		0x00400000
-#define BONITO_SOCKET_TOP		(BONITO_SOCKET_BASE+BONITO_SOCKET_SIZE-1)
-#define BONITO_REG_BASE 		0x1fe00000
-#define BONITO_REG_SIZE 		0x00040000
-#define BONITO_REG_TOP			(BONITO_REG_BASE+BONITO_REG_SIZE-1)
-#define BONITO_DEV_BASE 		0x1ff00000
-#define BONITO_DEV_SIZE 		0x00100000
-#define BONITO_DEV_TOP			(BONITO_DEV_BASE+BONITO_DEV_SIZE-1)
-#define BONITO_PCILO_BASE		0x10000000
-#define BONITO_PCILO_SIZE		0x0c000000
-#define BONITO_PCILO_TOP		(BONITO_PCILO_BASE+BONITO_PCILO_SIZE-1)
-#define BONITO_PCILO0_BASE		0x10000000
-#define BONITO_PCILO1_BASE		0x14000000
-#define BONITO_PCILO2_BASE		0x18000000
-#define BONITO_PCIHI_BASE		0x20000000
-#define BONITO_PCIHI_SIZE		0x20000000
-#define BONITO_PCIHI_TOP		(BONITO_PCIHI_BASE+BONITO_PCIHI_SIZE-1)
-#define BONITO_PCIIO_BASE		0x1fd00000
-#define BONITO_PCIIO_SIZE		0x00100000
-#define BONITO_PCIIO_TOP		(BONITO_PCIIO_BASE+BONITO_PCIIO_SIZE-1)
-#define BONITO_PCICFG_BASE		0x1fe80000
-#define BONITO_PCICFG_SIZE		0x00080000
-#define BONITO_PCICFG_TOP		(BONITO_PCICFG_BASE+BONITO_PCICFG_SIZE-1)
- 
-
-/* Bonito Register Bases */
-
-#define BONITO_PCICONFIGBASE		0x00
-#define BONITO_REGBASE			0x100
-
-
-/* PCI Configuration  Registers */
-
-#define BONITO_PCI_REG(x)               BONITO(BONITO_PCICONFIGBASE + (x))
-#define BONITO_PCIDID			BONITO_PCI_REG(0x00)
-#define BONITO_PCICMD			BONITO_PCI_REG(0x04)
-#define BONITO_PCICLASS 		BONITO_PCI_REG(0x08)
-#define BONITO_PCILTIMER		BONITO_PCI_REG(0x0c)
-#define BONITO_PCIBASE0 		BONITO_PCI_REG(0x10)
-#define BONITO_PCIBASE1 		BONITO_PCI_REG(0x14)
-#define BONITO_PCIBASE2 		BONITO_PCI_REG(0x18)
-#define BONITO_PCIEXPRBASE		BONITO_PCI_REG(0x30)
-#define BONITO_PCIINT			BONITO_PCI_REG(0x3c)
-
-#define BONITO_PCICMD_PERR_CLR		0x80000000
-#define BONITO_PCICMD_SERR_CLR		0x40000000
-#define BONITO_PCICMD_MABORT_CLR	0x20000000
-#define BONITO_PCICMD_MTABORT_CLR	0x10000000
-#define BONITO_PCICMD_TABORT_CLR	0x08000000
-#define BONITO_PCICMD_MPERR_CLR 	0x01000000
-#define BONITO_PCICMD_PERRRESPEN	0x00000040
-#define BONITO_PCICMD_ASTEPEN		0x00000080
-#define BONITO_PCICMD_SERREN		0x00000100
-#define BONITO_PCILTIMER_BUSLATENCY	0x0000ff00
-#define BONITO_PCILTIMER_BUSLATENCY_SHIFT	8
-
-
-
-
-/* 1. Bonito h/w Configuration */
-/* Power on register */
-
-#define BONITO_BONPONCFG		BONITO(BONITO_REGBASE + 0x00)
-
-#define BONITO_BONPONCFG_SYSCONTROLLERRD	0x00040000
-#define BONITO_BONPONCFG_ROMCS1SAMP	0x00020000
-#define BONITO_BONPONCFG_ROMCS0SAMP	0x00010000
-#define BONITO_BONPONCFG_CPUBIGEND	0x00004000
-/* Added by RPF 11-9-00 */
-#define BONITO_BONPONCFG_BURSTORDER	0x00001000
-/* --- */
-#define BONITO_BONPONCFG_CPUPARITY	0x00002000
-#define BONITO_BONPONCFG_CPUTYPE	0x00000007
-#define BONITO_BONPONCFG_CPUTYPE_SHIFT	0
-#define BONITO_BONPONCFG_PCIRESET_OUT	0x00000008
-#define BONITO_BONPONCFG_IS_ARBITER	0x00000010
-#define BONITO_BONPONCFG_ROMBOOT	0x000000c0
-#define BONITO_BONPONCFG_ROMBOOT_SHIFT	6
-
-#define BONITO_BONPONCFG_ROMBOOT_FLASH	(0x0<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_SOCKET (0x1<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_SDRAM	(0x2<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_CPURESET	(0x3<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-
-#define BONITO_BONPONCFG_ROMCS0WIDTH	0x00000100
-#define BONITO_BONPONCFG_ROMCS1WIDTH	0x00000200
-#define BONITO_BONPONCFG_ROMCS0FAST	0x00000400
-#define BONITO_BONPONCFG_ROMCS1FAST	0x00000800
-#define BONITO_BONPONCFG_CONFIG_DIS	0x00000020
-
-
-/* Other Bonito configuration */
-
-#define BONITO_BONGENCFG_OFFSET         0x4
-#define BONITO_BONGENCFG		BONITO(BONITO_REGBASE + BONITO_BONGENCFG_OFFSET)
-
-#define BONITO_BONGENCFG_DEBUGMODE	0x00000001
-#define BONITO_BONGENCFG_SNOOPEN	0x00000002
-#define BONITO_BONGENCFG_CPUSELFRESET	0x00000004
-
-#define BONITO_BONGENCFG_FORCE_IRQA	0x00000008
-#define BONITO_BONGENCFG_IRQA_ISOUT	0x00000010
-#define BONITO_BONGENCFG_IRQA_FROM_INT1 0x00000020
-#define BONITO_BONGENCFG_BYTESWAP	0x00000040
-
-#define BONITO_BONGENCFG_UNCACHED	0x00000080
-#define BONITO_BONGENCFG_PREFETCHEN	0x00000100
-#define BONITO_BONGENCFG_WBEHINDEN	0x00000200
-#define BONITO_BONGENCFG_CACHEALG	0x00000c00
-#define BONITO_BONGENCFG_CACHEALG_SHIFT 10
-#define BONITO_BONGENCFG_PCIQUEUE	0x00001000
-#define BONITO_BONGENCFG_CACHESTOP	0x00002000
-#define BONITO_BONGENCFG_MSTRBYTESWAP	0x00004000
-#define BONITO_BONGENCFG_BUSERREN	0x00008000
-#define BONITO_BONGENCFG_NORETRYTIMEOUT 0x00010000
-#define BONITO_BONGENCFG_SHORTCOPYTIMEOUT	0x00020000
-
-/* 2. IO & IDE configuration */
-
-#define BONITO_IODEVCFG 		BONITO(BONITO_REGBASE + 0x08)
-
-/* 3. IO & IDE configuration */
-
-#define BONITO_SDCFG			BONITO(BONITO_REGBASE + 0x0c)
-
-/* 4. PCI address map control */
-
-#define BONITO_PCIMAP			BONITO(BONITO_REGBASE + 0x10)
-#define BONITO_PCIMEMBASECFG		BONITO(BONITO_REGBASE + 0x14)
-#define BONITO_PCIMAP_CFG		BONITO(BONITO_REGBASE + 0x18)
-
-/* 5. ICU & GPIO regs */
- 
-/* GPIO Regs - r/w */
-
-#define BONITO_GPIODATA_OFFSET          0x1c
-#define BONITO_GPIODATA 		BONITO(BONITO_REGBASE + BONITO_GPIODATA_OFFSET)
-#define BONITO_GPIOIE			BONITO(BONITO_REGBASE + 0x20)
-
-/* ICU Configuration Regs - r/w */
-
-#define BONITO_INTEDGE			BONITO(BONITO_REGBASE + 0x24)
-#define BONITO_INTSTEER 		BONITO(BONITO_REGBASE + 0x28)
-#define BONITO_INTPOL			BONITO(BONITO_REGBASE + 0x2c)
-
-/* ICU Enable Regs - IntEn & IntISR are r/o. */
-
-#define BONITO_INTENSET 		BONITO(BONITO_REGBASE + 0x30)
-#define BONITO_INTENCLR 		BONITO(BONITO_REGBASE + 0x34)
-#define BONITO_INTEN			BONITO(BONITO_REGBASE + 0x38)
-#define BONITO_INTISR			BONITO(BONITO_REGBASE + 0x3c)
-
-/* PCI mail boxes */
-
-#define BONITO_PCIMAIL0_OFFSET          0x40
-#define BONITO_PCIMAIL1_OFFSET          0x44
-#define BONITO_PCIMAIL2_OFFSET          0x48
-#define BONITO_PCIMAIL3_OFFSET          0x4c
-#define BONITO_PCIMAIL0 		BONITO(BONITO_REGBASE + 0x40)
-#define BONITO_PCIMAIL1 		BONITO(BONITO_REGBASE + 0x44)
-#define BONITO_PCIMAIL2 		BONITO(BONITO_REGBASE + 0x48)
-#define BONITO_PCIMAIL3 		BONITO(BONITO_REGBASE + 0x4c)
-
-
-/* 6. PCI cache */
-
-#define BONITO_PCICACHECTRL		BONITO(BONITO_REGBASE + 0x50)
-#define BONITO_PCICACHETAG		BONITO(BONITO_REGBASE + 0x54)
-
-#define BONITO_PCIBADADDR		BONITO(BONITO_REGBASE + 0x58)
-#define BONITO_PCIMSTAT 		BONITO(BONITO_REGBASE + 0x5c)
-
-
-/*
-#define BONITO_PCIRDPOST		BONITO(BONITO_REGBASE + 0x60)
-#define BONITO_PCIDATA			BONITO(BONITO_REGBASE + 0x64)
-*/
-
-/* 7. IDE DMA & Copier */
- 
-#define BONITO_CONFIGBASE		0x000
-#define BONITO_BONITOBASE		0x100
-#define BONITO_LDMABASE 		0x200
-#define BONITO_COPBASE			0x300
-#define BONITO_REG_BLOCKMASK		0x300
-
-#define BONITO_LDMACTRL 		BONITO(BONITO_LDMABASE + 0x0)
-#define BONITO_LDMASTAT 		BONITO(BONITO_LDMABASE + 0x0)
-#define BONITO_LDMAADDR 		BONITO(BONITO_LDMABASE + 0x4)
-#define BONITO_LDMAGO			BONITO(BONITO_LDMABASE + 0x8)
-#define BONITO_LDMADATA 		BONITO(BONITO_LDMABASE + 0xc)
-
-#define BONITO_COPCTRL			BONITO(BONITO_COPBASE + 0x0)
-#define BONITO_COPSTAT			BONITO(BONITO_COPBASE + 0x0)
-#define BONITO_COPPADDR 		BONITO(BONITO_COPBASE + 0x4)
-#define BONITO_COPDADDR 		BONITO(BONITO_COPBASE + 0x8)
-#define BONITO_COPGO			BONITO(BONITO_COPBASE + 0xc)
-
-
-/* ###### Bit Definitions for individual Registers #### */
-
-/* Gen DMA. */
-
-#define BONITO_IDECOPDADDR_DMA_DADDR	0x0ffffffc
-#define BONITO_IDECOPDADDR_DMA_DADDR_SHIFT	2
-#define BONITO_IDECOPPADDR_DMA_PADDR	0xfffffffc
-#define BONITO_IDECOPPADDR_DMA_PADDR_SHIFT	2
-#define BONITO_IDECOPGO_DMA_SIZE	0x0000fffe
-#define BONITO_IDECOPGO_DMA_SIZE_SHIFT	0
-#define BONITO_IDECOPGO_DMA_WRITE	0x00010000
-#define BONITO_IDECOPGO_DMAWCOUNT	0x000f0000
-#define BONITO_IDECOPGO_DMAWCOUNT_SHIFT	16
-
-#define BONITO_IDECOPCTRL_DMA_STARTBIT	0x80000000
-#define BONITO_IDECOPCTRL_DMA_RSTBIT	0x40000000
-
-/* DRAM - sdCfg */
-
-#define BONITO_SDCFG_AROWBITS		0x00000003
-#define BONITO_SDCFG_AROWBITS_SHIFT	0
-#define BONITO_SDCFG_ACOLBITS		0x0000000c
-#define BONITO_SDCFG_ACOLBITS_SHIFT	2
-#define BONITO_SDCFG_ABANKBIT		0x00000010
-#define BONITO_SDCFG_ASIDES		0x00000020
-#define BONITO_SDCFG_AABSENT		0x00000040
-#define BONITO_SDCFG_AWIDTH64		0x00000080
-
-#define BONITO_SDCFG_BROWBITS		0x00000300
-#define BONITO_SDCFG_BROWBITS_SHIFT	8
-#define BONITO_SDCFG_BCOLBITS		0x00000c00
-#define BONITO_SDCFG_BCOLBITS_SHIFT	10
-#define BONITO_SDCFG_BBANKBIT		0x00001000
-#define BONITO_SDCFG_BSIDES		0x00002000
-#define BONITO_SDCFG_BABSENT		0x00004000
-#define BONITO_SDCFG_BWIDTH64		0x00008000
-
-#define BONITO_SDCFG_EXTRDDATA		0x00010000
-#define BONITO_SDCFG_EXTRASCAS		0x00020000
-#define BONITO_SDCFG_EXTPRECH		0x00040000
-#define BONITO_SDCFG_EXTRASWIDTH	0x00180000
-#define BONITO_SDCFG_EXTRASWIDTH_SHIFT	19
-/* Changed by RPF 11-9-00 */
-#define BONITO_SDCFG_DRAMMODESET	0x00200000
-/* --- */
-#define BONITO_SDCFG_DRAMEXTREGS	0x00400000
-#define BONITO_SDCFG_DRAMPARITY 	0x00800000
-/* Added by RPF 11-9-00 */
-#define BONITO_SDCFG_DRAMBURSTLEN 	0x03000000
-#define BONITO_SDCFG_DRAMBURSTLEN_SHIFT	24
-#define BONITO_SDCFG_DRAMMODESET_DONE 	0x80000000
-/* --- */
-
-/* PCI Cache - pciCacheCtrl */
-
-#define BONITO_PCICACHECTRL_CACHECMD	0x00000007
-#define BONITO_PCICACHECTRL_CACHECMD_SHIFT	0
-#define BONITO_PCICACHECTRL_CACHECMDLINE	0x00000018
-#define BONITO_PCICACHECTRL_CACHECMDLINE_SHIFT	3
-#define BONITO_PCICACHECTRL_CMDEXEC	0x00000020
-
-#define BONITO_IODEVCFG_BUFFBIT_CS0	0x00000001
-#define BONITO_IODEVCFG_SPEEDBIT_CS0	0x00000002
-#define BONITO_IODEVCFG_MOREABITS_CS0	0x00000004
-
-#define BONITO_IODEVCFG_BUFFBIT_CS1	0x00000008
-#define BONITO_IODEVCFG_SPEEDBIT_CS1	0x00000010
-#define BONITO_IODEVCFG_MOREABITS_CS1	0x00000020
-
-#define BONITO_IODEVCFG_BUFFBIT_CS2	0x00000040
-#define BONITO_IODEVCFG_SPEEDBIT_CS2	0x00000080
-#define BONITO_IODEVCFG_MOREABITS_CS2	0x00000100
-
-#define BONITO_IODEVCFG_BUFFBIT_CS3	0x00000200
-#define BONITO_IODEVCFG_SPEEDBIT_CS3	0x00000400
-#define BONITO_IODEVCFG_MOREABITS_CS3	0x00000800
-
-#define BONITO_IODEVCFG_BUFFBIT_IDE	0x00001000
-#define BONITO_IODEVCFG_SPEEDBIT_IDE	0x00002000
-#define BONITO_IODEVCFG_WORDSWAPBIT_IDE 0x00004000
-#define BONITO_IODEVCFG_MODEBIT_IDE	0x00008000
-#define BONITO_IODEVCFG_DMAON_IDE	0x001f0000
-#define BONITO_IODEVCFG_DMAON_IDE_SHIFT 16
-#define BONITO_IODEVCFG_DMAOFF_IDE	0x01e00000
-#define BONITO_IODEVCFG_DMAOFF_IDE_SHIFT	21
-#define BONITO_IODEVCFG_EPROMSPLIT	0x02000000
-/* Added by RPF 11-9-00 */
-#define BONITO_IODEVCFG_CPUCLOCKPERIOD	0xfc000000
-#define BONITO_IODEVCFG_CPUCLOCKPERIOD_SHIFT 26
-/* --- */
-
-/* gpio */
-#define BONITO_GPIO_GPIOW		0x000003ff
-#define BONITO_GPIO_GPIOW_SHIFT 	0
-#define BONITO_GPIO_GPIOR		0x01ff0000
-#define BONITO_GPIO_GPIOR_SHIFT 	16
-#define BONITO_GPIO_GPINR		0xfe000000
-#define BONITO_GPIO_GPINR_SHIFT 	25
-#define BONITO_GPIO_IOW(N)		(1<<(BONITO_GPIO_GPIOW_SHIFT+(N)))
-#define BONITO_GPIO_IOR(N)		(1<<(BONITO_GPIO_GPIOR_SHIFT+(N)))
-#define BONITO_GPIO_INR(N)		(1<<(BONITO_GPIO_GPINR_SHIFT+(N)))
-
-/* ICU */
-#define BONITO_ICU_MBOXES		0x0000000f
-#define BONITO_ICU_MBOXES_SHIFT 	0
-#define BONITO_ICU_DMARDY		0x00000010
-#define BONITO_ICU_DMAEMPTY		0x00000020
-#define BONITO_ICU_COPYRDY		0x00000040
-#define BONITO_ICU_COPYEMPTY		0x00000080
-#define BONITO_ICU_COPYERR		0x00000100
-#define BONITO_ICU_PCIIRQ		0x00000200
-#define BONITO_ICU_MASTERERR		0x00000400
-#define BONITO_ICU_SYSTEMERR		0x00000800
-#define BONITO_ICU_DRAMPERR		0x00001000
-#define BONITO_ICU_RETRYERR		0x00002000
-#define BONITO_ICU_GPIOS		0x01ff0000
-#define BONITO_ICU_GPIOS_SHIFT		16
-#define BONITO_ICU_GPINS		0x7e000000
-#define BONITO_ICU_GPINS_SHIFT		25
-#define BONITO_ICU_MBOX(N)		(1<<(BONITO_ICU_MBOXES_SHIFT+(N)))
-#define BONITO_ICU_GPIO(N)		(1<<(BONITO_ICU_GPIOS_SHIFT+(N)))
-#define BONITO_ICU_GPIN(N)		(1<<(BONITO_ICU_GPINS_SHIFT+(N)))
-
-/* pcimap */
-
-#define BONITO_PCIMAP_PCIMAP_LO0	0x0000003f
-#define BONITO_PCIMAP_PCIMAP_LO0_SHIFT	0
-#define BONITO_PCIMAP_PCIMAP_LO1	0x00000fc0
-#define BONITO_PCIMAP_PCIMAP_LO1_SHIFT	6
-#define BONITO_PCIMAP_PCIMAP_LO2	0x0003f000
-#define BONITO_PCIMAP_PCIMAP_LO2_SHIFT	12
-#define BONITO_PCIMAP_PCIMAP_2		0x00040000
-#define BONITO_PCIMAP_WIN(WIN,ADDR)	((((ADDR)>>26) & BONITO_PCIMAP_PCIMAP_LO0) << ((WIN)*6))
-
-#define BONITO_PCIMAP_WINSIZE           (1<<26)
-#define BONITO_PCIMAP_WINOFFSET(ADDR)	((ADDR) & (BONITO_PCIMAP_WINSIZE - 1))
-#define BONITO_PCIMAP_WINBASE(ADDR)	((ADDR) << 26)
-
-/* pcimembaseCfg */
-
-#define BONITO_PCIMEMBASECFG_MASK               0xf0000000
-#define BONITO_PCIMEMBASECFG_MEMBASE0_MASK	0x0000001f
-#define BONITO_PCIMEMBASECFG_MEMBASE0_MASK_SHIFT	0
-#define BONITO_PCIMEMBASECFG_MEMBASE0_TRANS	0x000003e0
-#define BONITO_PCIMEMBASECFG_MEMBASE0_TRANS_SHIFT	5
-#define BONITO_PCIMEMBASECFG_MEMBASE0_CACHED	0x00000400
-#define BONITO_PCIMEMBASECFG_MEMBASE0_IO	0x00000800
-
-#define BONITO_PCIMEMBASECFG_MEMBASE1_MASK	0x0001f000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_MASK_SHIFT	12
-#define BONITO_PCIMEMBASECFG_MEMBASE1_TRANS	0x003e0000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_TRANS_SHIFT	17
-#define BONITO_PCIMEMBASECFG_MEMBASE1_CACHED	0x00400000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_IO	0x00800000
-
-#define BONITO_PCIMEMBASECFG_ASHIFT	23
-#define BONITO_PCIMEMBASECFG_AMASK              0x007fffff
-#define BONITO_PCIMEMBASECFGSIZE(WIN,SIZE)	(((~((SIZE)-1))>>(BONITO_PCIMEMBASECFG_ASHIFT-BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK)
-#define BONITO_PCIMEMBASECFGBASE(WIN,BASE)	(((BASE)>>(BONITO_PCIMEMBASECFG_ASHIFT-BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS_SHIFT)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS)
-
-#define BONITO_PCIMEMBASECFG_SIZE(WIN,CFG)  (((((~(CFG)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK)) << (BONITO_PCIMEMBASECFG_ASHIFT - BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT)) | BONITO_PCIMEMBASECFG_AMASK)
-
-
-#define BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)  ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-#define BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)  ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-#define BONITO_PCIMEMBASECFG_ADDRTRANS(WIN,CFG) ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-
-#define BONITO_PCITOPHYS(WIN,ADDR,CFG)          ( \
-                                                  (((ADDR) & (~(BONITO_PCIMEMBASECFG_MASK))) & (~(BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)))) | \
-                                                  (BONITO_PCIMEMBASECFG_ADDRTRANS(WIN,CFG)) \
-                                                )
-
-/* PCICmd */
-
-#define BONITO_PCICMD_MEMEN		0x00000002
-#define BONITO_PCICMD_MSTREN		0x00000004
-
-
-#endif /* _ASM_MIPS_BOARDS_BONITO64_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/mips-boards/msc01_pci.h linuxppc64_2_4/include/asm-mips/mips-boards/msc01_pci.h
--- linux-2.4.19/include/asm-mips/mips-boards/msc01_pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/mips-boards/msc01_pci.h	Wed Dec 31 18:00:00 1969
@@ -1,244 +0,0 @@
-/*
- * mcs01_pci.h
- *
- * Carsten Langgaard, carstenl@mips.com
- * Copyright (C) 2002 MIPS Technologies, Inc.  All rights reserved.
- *
- * ########################################################################
- *
- * PCI Register definitions for the MIPS System Controller.
- */
-#ifndef MSC01_PCI_H
-#define MSC01_PCI_H
-
-/*****************************************************************************
- * Register offset addresses
- ****************************************************************************/
-
-#define MSC01_PCI_ID_OFS		0x0000
-#define MSC01_PCI_SC2PMBASL_OFS		0x0208
-#define MSC01_PCI_SC2PMMSKL_OFS		0x0218
-#define MSC01_PCI_SC2PMMAPL_OFS		0x0228
-#define MSC01_PCI_SC2PIOBASL_OFS	0x0248
-#define MSC01_PCI_SC2PIOMSKL_OFS	0x0258
-#define MSC01_PCI_SC2PIOMAPL_OFS	0x0268
-#define MSC01_PCI_P2SCMSKL_OFS		0x0308
-#define MSC01_PCI_P2SCMAPL_OFS		0x0318
-#define MSC01_PCI_INTCFG_OFS		0x0600
-#define MSC01_PCI_INTSTAT_OFS		0x0608
-#define MSC01_PCI_CFGADDR_OFS		0x0610
-#define MSC01_PCI_CFGDATA_OFS		0x0618
-#define MSC01_PCI_IACK_OFS		0x0620
-#define MSC01_PCI_HEAD0_OFS		0x2000  /* DevID, VendorID */
-#define MSC01_PCI_HEAD1_OFS		0x2008  /* Status, Command */
-#define MSC01_PCI_HEAD2_OFS		0x2010  /* Class code, RevID */
-#define MSC01_PCI_HEAD3_OFS		0x2018  /* bist, header, latency */
-#define MSC01_PCI_HEAD4_OFS		0x2020  /* BAR 0 */
-#define MSC01_PCI_HEAD5_OFS		0x2028  /* BAR 1 */
-#define MSC01_PCI_HEAD6_OFS		0x2030  /* BAR 2 */
-#define MSC01_PCI_HEAD7_OFS		0x2038  /* BAR 3 */
-#define MSC01_PCI_HEAD8_OFS		0x2040  /* BAR 4 */
-#define MSC01_PCI_HEAD9_OFS		0x2048  /* BAR 5 */
-#define MSC01_PCI_HEAD10_OFS		0x2050  /* CardBus CIS Ptr */
-#define MSC01_PCI_HEAD11_OFS		0x2058  /* SubSystem ID, -VendorID */
-#define MSC01_PCI_HEAD12_OFS		0x2060  /* ROM BAR */
-#define MSC01_PCI_HEAD13_OFS		0x2068  /* Capabilities ptr */
-#define MSC01_PCI_HEAD14_OFS		0x2070  /* reserved */
-#define MSC01_PCI_HEAD15_OFS		0x2078  /* Maxl, ming, intpin, int */
-#define MSC01_PCI_BAR0_OFS		0x2220
-#define MSC01_PCI_CFG_OFS		0x2380
-#define MSC01_PCI_SWAP_OFS		0x2388
-
-
-/*****************************************************************************
- * Register encodings
- ****************************************************************************/
-
-#define MSC01_PCI_ID_ID_SHF		16
-#define MSC01_PCI_ID_ID_MSK		0x00ff0000
-#define MSC01_PCI_ID_ID_HOSTBRIDGE	82
-#define MSC01_PCI_ID_MAR_SHF		8
-#define MSC01_PCI_ID_MAR_MSK		0x0000ff00
-#define MSC01_PCI_ID_MIR_SHF		0
-#define MSC01_PCI_ID_MIR_MSK		0x000000ff
-
-#define MSC01_PCI_SC2PMBASL_BAS_SHF	24
-#define MSC01_PCI_SC2PMBASL_BAS_MSK	0xff000000
-
-#define MSC01_PCI_SC2PMMSKL_MSK_SHF	24
-#define MSC01_PCI_SC2PMMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_SC2PMMAPL_MAP_SHF	24
-#define MSC01_PCI_SC2PMMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOBASL_BAS_SHF	24
-#define MSC01_PCI_SC2PIOBASL_BAS_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOMSKL_MSK_SHF	24
-#define MSC01_PCI_SC2PIOMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOMAPL_MAP_SHF	24
-#define MSC01_PCI_SC2PIOMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_P2SCMSKL_MSK_SHF	24
-#define MSC01_PCI_P2SCMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_P2SCMAPL_MAP_SHF	24
-#define MSC01_PCI_P2SCMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_INTCFG_RST_SHF        10
-#define MSC01_PCI_INTCFG_RST_MSK        0x00000400
-#define MSC01_PCI_INTCFG_RST_BIT        0x00000400
-#define MSC01_PCI_INTCFG_MWE_SHF        9
-#define MSC01_PCI_INTCFG_MWE_MSK        0x00000200
-#define MSC01_PCI_INTCFG_MWE_BIT        0x00000200
-#define MSC01_PCI_INTCFG_DTO_SHF        8
-#define MSC01_PCI_INTCFG_DTO_MSK        0x00000100
-#define MSC01_PCI_INTCFG_DTO_BIT        0x00000100 
-#define MSC01_PCI_INTCFG_MA_SHF         7
-#define MSC01_PCI_INTCFG_MA_MSK         0x00000080
-#define MSC01_PCI_INTCFG_MA_BIT         0x00000080
-#define MSC01_PCI_INTCFG_TA_SHF         6
-#define MSC01_PCI_INTCFG_TA_MSK         0x00000040
-#define MSC01_PCI_INTCFG_TA_BIT         0x00000040
-#define MSC01_PCI_INTCFG_RTY_SHF        5
-#define MSC01_PCI_INTCFG_RTY_MSK        0x00000020
-#define MSC01_PCI_INTCFG_RTY_BIT        0x00000020
-#define MSC01_PCI_INTCFG_MWP_SHF        4
-#define MSC01_PCI_INTCFG_MWP_MSK        0x00000010
-#define MSC01_PCI_INTCFG_MWP_BIT        0x00000010
-#define MSC01_PCI_INTCFG_MRP_SHF        3
-#define MSC01_PCI_INTCFG_MRP_MSK        0x00000008
-#define MSC01_PCI_INTCFG_MRP_BIT        0x00000008
-#define MSC01_PCI_INTCFG_SWP_SHF        2
-#define MSC01_PCI_INTCFG_SWP_MSK        0x00000004
-#define MSC01_PCI_INTCFG_SWP_BIT        0x00000004
-#define MSC01_PCI_INTCFG_SRP_SHF        1
-#define MSC01_PCI_INTCFG_SRP_MSK        0x00000002
-#define MSC01_PCI_INTCFG_SRP_BIT        0x00000002
-#define MSC01_PCI_INTCFG_SE_SHF         0
-#define MSC01_PCI_INTCFG_SE_MSK         0x00000001
-#define MSC01_PCI_INTCFG_SE_BIT         0x00000001
-
-#define MSC01_PCI_INTSTAT_RST_SHF       10
-#define MSC01_PCI_INTSTAT_RST_MSK       0x00000400
-#define MSC01_PCI_INTSTAT_RST_BIT       0x00000400
-#define MSC01_PCI_INTSTAT_MWE_SHF       9
-#define MSC01_PCI_INTSTAT_MWE_MSK       0x00000200
-#define MSC01_PCI_INTSTAT_MWE_BIT       0x00000200
-#define MSC01_PCI_INTSTAT_DTO_SHF       8
-#define MSC01_PCI_INTSTAT_DTO_MSK       0x00000100
-#define MSC01_PCI_INTSTAT_DTO_BIT       0x00000100 
-#define MSC01_PCI_INTSTAT_MA_SHF        7
-#define MSC01_PCI_INTSTAT_MA_MSK        0x00000080
-#define MSC01_PCI_INTSTAT_MA_BIT        0x00000080
-#define MSC01_PCI_INTSTAT_TA_SHF        6
-#define MSC01_PCI_INTSTAT_TA_MSK        0x00000040
-#define MSC01_PCI_INTSTAT_TA_BIT        0x00000040
-#define MSC01_PCI_INTSTAT_RTY_SHF       5
-#define MSC01_PCI_INTSTAT_RTY_MSK       0x00000020
-#define MSC01_PCI_INTSTAT_RTY_BIT       0x00000020
-#define MSC01_PCI_INTSTAT_MWP_SHF       4
-#define MSC01_PCI_INTSTAT_MWP_MSK       0x00000010
-#define MSC01_PCI_INTSTAT_MWP_BIT       0x00000010
-#define MSC01_PCI_INTSTAT_MRP_SHF       3
-#define MSC01_PCI_INTSTAT_MRP_MSK       0x00000008
-#define MSC01_PCI_INTSTAT_MRP_BIT       0x00000008
-#define MSC01_PCI_INTSTAT_SWP_SHF       2
-#define MSC01_PCI_INTSTAT_SWP_MSK       0x00000004
-#define MSC01_PCI_INTSTAT_SWP_BIT       0x00000004
-#define MSC01_PCI_INTSTAT_SRP_SHF       1
-#define MSC01_PCI_INTSTAT_SRP_MSK       0x00000002
-#define MSC01_PCI_INTSTAT_SRP_BIT       0x00000002
-#define MSC01_PCI_INTSTAT_SE_SHF        0
-#define MSC01_PCI_INTSTAT_SE_MSK        0x00000001
-#define MSC01_PCI_INTSTAT_SE_BIT        0x00000001
-
-#define MSC01_PCI_CFGADDR_BNUM_SHF	16
-#define MSC01_PCI_CFGADDR_BNUM_MSK	0x00ff0000
-#define MSC01_PCI_CFGADDR_DNUM_SHF	11
-#define MSC01_PCI_CFGADDR_DNUM_MSK	0x0000f800
-#define MSC01_PCI_CFGADDR_FNUM_SHF	8
-#define MSC01_PCI_CFGADDR_FNUM_MSK	0x00000700
-#define MSC01_PCI_CFGADDR_RNUM_SHF	2
-#define MSC01_PCI_CFGADDR_RNUM_MSK	0x000000fc
-
-#define MSC01_PCI_CFGDATA_DATA_SHF	0
-#define MSC01_PCI_CFGDATA_DATA_MSK	0xffffffff
-
-/* The defines below are ONLY valid for a MEM bar! */
-#define MSC01_PCI_BAR0_SIZE_SHF	        4
-#define MSC01_PCI_BAR0_SIZE_MSK	        0xfffffff0
-#define MSC01_PCI_BAR0_P_SHF	        3
-#define MSC01_PCI_BAR0_P_MSK	        0x00000008
-#define MSC01_PCI_BAR0_P_BIT	        MSC01_PCI_BAR0_P_MSK
-#define MSC01_PCI_BAR0_D_SHF	        1
-#define MSC01_PCI_BAR0_D_MSK	        0x00000006
-#define MSC01_PCI_BAR0_T_SHF	        0
-#define MSC01_PCI_BAR0_T_MSK	        0x00000001
-#define MSC01_PCI_BAR0_T_BIT	        MSC01_PCI_BAR0_T_MSK
-
-
-#define MSC01_PCI_CFG_RA_SHF	        17
-#define MSC01_PCI_CFG_RA_MSK	        0x00020000
-#define MSC01_PCI_CFG_RA_BIT	        MSC01_PCI_CFG_RA_MSK
-#define MSC01_PCI_CFG_G_SHF	        16
-#define MSC01_PCI_CFG_G_MSK	        0x00010000
-#define MSC01_PCI_CFG_G_BIT	        MSC01_PCI_CFG_G_MSK
-#define MSC01_PCI_CFG_EN_SHF	        15
-#define MSC01_PCI_CFG_EN_MSK	        0x00008000
-#define MSC01_PCI_CFG_EN_BIT	        MSC01_PCI_CFG_EN_MSK
-#define MSC01_PCI_CFG_MAXRTRY_SHF       0
-#define MSC01_PCI_CFG_MAXRTRY_MSK       0x000000ff
-
-#define MSC01_PCI_SWAP_IO_SHF		18
-#define MSC01_PCI_SWAP_IO_MSK		0x000c0000
-#define MSC01_PCI_SWAP_MEM_SHF		16
-#define MSC01_PCI_SWAP_MEM_MSK		0x00030000
-#define MSC01_PCI_SWAP_BAR0_SHF		0
-#define MSC01_PCI_SWAP_BAR0_MSK		0x00000003
-#define MSC01_PCI_SWAP_NOSWAP		0
-#define MSC01_PCI_SWAP_BYTESWAP		1
-
-/*****************************************************************************
- * Registers absolute addresses
- ****************************************************************************/
-
-#define MSC01_PCI_ID            (MSC01_PCI_REG_BASE + MSC01_PCI_ID_OFS)
-#define MSC01_PCI_SC2PMBASL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMBASL_OFS)
-#define MSC01_PCI_SC2PMMSKL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMMSKL_OFS)
-#define MSC01_PCI_SC2PMMAPL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMMAPL_OFS)
-#define MSC01_PCI_SC2PIOBASL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOBASL_OFS)
-#define MSC01_PCI_SC2PIOMSKL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOMSKL_OFS)
-#define MSC01_PCI_SC2PIOMAPL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOMAPL_OFS)
-#define MSC01_PCI_P2SCMSKL      (MSC01_PCI_REG_BASE + MSC01_PCI_P2SCMSKL_OFS)
-#define MSC01_PCI_P2SCMAPL      (MSC01_PCI_REG_BASE + MSC01_PCI_P2SCMAPL_OFS)
-#define MSC01_PCI_INTCFG        (MSC01_PCI_REG_BASE + MSC01_PCI_INTCFG_OFS)
-#define MSC01_PCI_INTSTAT       (MSC01_PCI_REG_BASE + MSC01_PCI_INTSTAT_OFS)
-#define MSC01_PCI_CFGADDR       (MSC01_PCI_REG_BASE + MSC01_PCI_CFGADDR_OFS)
-#define MSC01_PCI_CFGDATA       (MSC01_PCI_REG_BASE + MSC01_PCI_CFGDATA_OFS)
-#define MSC01_PCI_IACK		(MSC01_PCI_REG_BASE + MSC01_PCI_IACK_OFS)
-#define MSC01_PCI_HEAD0		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD0_OFS)
-#define MSC01_PCI_HEAD1		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD1_OFS)
-#define MSC01_PCI_HEAD2		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD2_OFS)
-#define MSC01_PCI_HEAD3		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD3_OFS)
-#define MSC01_PCI_HEAD4		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD4_OFS)
-#define MSC01_PCI_HEAD5		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD5_OFS)
-#define MSC01_PCI_HEAD6		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD6_OFS)
-#define MSC01_PCI_HEAD7		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD7_OFS)
-#define MSC01_PCI_HEAD8		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD8_OFS)
-#define MSC01_PCI_HEAD9		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD9_OFS)
-#define MSC01_PCI_HEAD10	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD10_OFS)
-#define MSC01_PCI_HEAD11	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD12	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD13	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD14	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD15        (MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_BAR0		(MSC01_PCI_REG_BASE + MSC01_PCI_BAR0_OFS)
-#define MSC01_PCI_CFG		(MSC01_PCI_REG_BASE + MSC01_PCI_CFG_OFS)
-#define MSC01_PCI_SWAP		(MSC01_PCI_REG_BASE + MSC01_PCI_SWAP_OFS)
-
-#endif
-/*****************************************************************************
- *  End of msc01_pci.h
- *****************************************************************************/
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/pgtable-bits.h linuxppc64_2_4/include/asm-mips/pgtable-bits.h
--- linux-2.4.19/include/asm-mips/pgtable-bits.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/pgtable-bits.h	Wed Dec 31 18:00:00 1969
@@ -1,98 +0,0 @@
-/*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
- *
- * Copyright (C) 1994, 95, 96, 97, 98, 99, 2000 by Ralf Baechle at alii
- * Copyright (C) 2001, 2002 by Ralf Baechle
- * Copyright (C) 1999 Silicon Graphics, Inc.
- */
-#ifndef _ASM_CACHINGMODES_H
-#define _ASM_CACHINGMODES_H
-
-#include <linux/config.h>
-
-/* Note that we shift the lower 32bits of each EntryLo[01] entry
- * 6 bits to the left. That way we can convert the PFN into the
- * physical address by a single 'and' operation and gain 6 additional
- * bits for storing information which isn't present in a normal
- * MIPS page table.
- *
- * Similar to the Alpha port, we need to keep track of the ref
- * and mod bits in software.  We have a software "yeah you can read
- * from this page" bit, and a hardware one which actually lets the
- * process read from the page.  On the same token we have a software
- * writable bit and the real hardware one which actually lets the
- * process write to the page, this keeps a mod bit via the hardware
- * dirty bit.
- *
- * Certain revisions of the R4000 and R5000 have a bug where if a
- * certain sequence occurs in the last 3 instructions of an executable
- * page, and the following page is not mapped, the cpu can do
- * unpredictable things.  The code (when it is written) to deal with
- * this problem will be in the update_mmu_cache() code for the r4k.
- */
-#define _PAGE_PRESENT               (1<<0)  /* implemented in software */
-#define _PAGE_READ                  (1<<1)  /* implemented in software */
-#define _PAGE_WRITE                 (1<<2)  /* implemented in software */
-#define _PAGE_ACCESSED              (1<<3)  /* implemented in software */
-#define _PAGE_MODIFIED              (1<<4)  /* implemented in software */
-
-#if defined(CONFIG_CPU_R3000) || defined(CONFIG_CPU_TX39XX)
-
-#define _PAGE_GLOBAL                (1<<8)
-#define _PAGE_VALID                 (1<<9)
-#define _PAGE_SILENT_READ           (1<<9)  /* synonym                 */
-#define _PAGE_DIRTY                 (1<<10) /* The MIPS dirty bit      */
-#define _PAGE_SILENT_WRITE          (1<<10)
-#define _CACHE_UNCACHED             (1<<11)
-#define _CACHE_MASK                 (1<<11)
-#define _CACHE_CACHABLE_NONCOHERENT 0
-
-#else
-#define _PAGE_R4KBUG                (1<<5)  /* workaround for r4k bug  */
-#define _PAGE_GLOBAL                (1<<6)
-#define _PAGE_VALID                 (1<<7)
-#define _PAGE_SILENT_READ           (1<<7)  /* synonym                 */
-#define _PAGE_DIRTY                 (1<<8)  /* The MIPS dirty bit      */
-#define _PAGE_SILENT_WRITE          (1<<8)
-#define _CACHE_MASK                 (7<<9)
-
-#if defined(CONFIG_CPU_SB1)
-
-/* No penalty for being coherent on the SB1, so just
-   use it for "noncoherent" spaces, too.  Shouldn't hurt. */
-
-#define _CACHE_UNCACHED             (2<<9)  
-#define _CACHE_CACHABLE_COW         (5<<9)  
-#define _CACHE_CACHABLE_NONCOHERENT (5<<9)  
-#define _CACHE_UNCACHED_ACCELERATED (7<<9)  
-
-#else
-
-#define _CACHE_CACHABLE_NO_WA       (0<<9)  /* R4600 only              */
-#define _CACHE_CACHABLE_WA          (1<<9)  /* R4600 only              */
-#define _CACHE_UNCACHED             (2<<9)  /* R4[0246]00              */
-#define _CACHE_CACHABLE_NONCOHERENT (3<<9)  /* R4[0246]00              */
-#define _CACHE_CACHABLE_CE          (4<<9)  /* R4[04]00 only           */
-#define _CACHE_CACHABLE_COW         (5<<9)  /* R4[04]00 only           */
-#define _CACHE_CACHABLE_CUW         (6<<9)  /* R4[04]00 only           */
-#define _CACHE_UNCACHED_ACCELERATED (7<<9)  /* R10000 only             */
-
-#endif
-#endif
-
-#define __READABLE	(_PAGE_READ | _PAGE_SILENT_READ | _PAGE_ACCESSED)
-#define __WRITEABLE	(_PAGE_WRITE | _PAGE_SILENT_WRITE | _PAGE_MODIFIED)
-
-#define _PAGE_CHG_MASK  (PAGE_MASK | _PAGE_ACCESSED | _PAGE_MODIFIED | _CACHE_MASK)
-
-#ifdef CONFIG_MIPS_UNCACHED
-#define PAGE_CACHABLE_DEFAULT	_CACHE_UNCACHED
-#elif CONFIG_CPU_SB1
-#define PAGE_CACHABLE_DEFAULT	_CACHE_CACHABLE_COW
-#else
-#define PAGE_CACHABLE_DEFAULT	_CACHE_CACHABLE_NONCOHERENT
-#endif
-
-#endif /* _ASM_CACHINGMODES_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/sgi/sgigio.h linuxppc64_2_4/include/asm-mips/sgi/sgigio.h
--- linux-2.4.19/include/asm-mips/sgi/sgigio.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/sgi/sgigio.h	Wed Dec 31 18:00:00 1969
@@ -1,69 +0,0 @@
-/*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
- *
- * sgigio.h: Definitions for GIO bus found on SGI IP22 (and others by linux
- *           unsupported) machines.
- *
- * Copyright (C) 2002 Ladislav Michl
- */
-#ifndef _ASM_SGI_SGIGIO_H
-#define _ASM_SGI_SGIGIO_H
-
-/*
- * There is 10MB of GIO address space for GIO64 slot devices
- * slot#   slot type address range            size
- * -----   --------- ----------------------- -----
- *   0     GFX       0x1f000000 - 0x1f3fffff   4MB
- *   1     EXP0      0x1f400000 - 0x1f5fffff   2MB
- *   2     EXP1      0x1f600000 - 0x1f9fffff   4MB
- *
- * There are un-slotted devices, HPC, I/O and misc devices, which are grouped 
- * into the HPC address space.
- *   -     MISC      0x1fb00000 - 0x1fbfffff   1MB
- *      
- * Following space is reserved and unused
- *   -     RESERVED  0x18000000 - 0x1effffff 112MB
- *
- * The GIO specification tends to use slot numbers while the MC specification 
- * tends to use slot types.
- *
- * slot0  - the "graphics" (GFX) slot but there is no requirement that 
- *          a graphics dev may only use this slot
- * slot1  - this is the "expansion"-slot 0 (EXP0), do not confuse with 
- *          slot 0 (GFX).
- * slot2  - this is the "expansion"-slot 1 (EXP1), do not confuse with 
- *          slot 1 (EXP0).
- */
-
-#define GIO_SLOT_GFX	0
-#define GIO_SLOT_GIO1	1
-#define GIO_SLOT_GIO2	2
-#define GIO_NUM_SLOTS	3
-
-#define GIO_ANY_ID	0xff
-
-#define GIO_VALID_ID_ONLY	0x01
-#define GIO_IFACE_64		0x02
-#define GIO_HAS_ROM		0x04
-
-struct gio_dev {
-	unsigned char	device;
-	unsigned char	revision;
-	unsigned short	vendor;
-	unsigned char	flags;
-
-	unsigned char	slot_number;
-	unsigned long	base_addr;
-	unsigned int	map_size;
-	
-	char		*name;
-	char		slot_name[5];
-};
-
-extern struct gio_dev* gio_find_device(unsigned char device, const struct gio_dev *from);
-
-extern void sgigio_init(void);
-
-#endif /* _ASM_SGI_SGIGIO_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/traps.h linuxppc64_2_4/include/asm-mips/traps.h
--- linux-2.4.19/include/asm-mips/traps.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/traps.h	Wed Dec 31 18:00:00 1969
@@ -1,27 +0,0 @@
-/*
- *	include/asm-mips/traps.h
- *
- *	Trap handling definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS_TRAPS_H
-#define __ASM_MIPS_TRAPS_H
-
-/*
- * Possible status responses for a be_board_handler backend.
- */
-#define MIPS_BE_DISCARD	0		/* return with no action */
-#define MIPS_BE_FIXUP	1		/* return to the fixup code */
-#define MIPS_BE_FATAL	2		/* treat as an unrecoverable error */
-
-extern int (*be_board_handler)(struct pt_regs *regs, int is_fixup);
-
-extern void bus_error_init(void);
-
-#endif /* __ASM_MIPS_TRAPS_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips/war.h linuxppc64_2_4/include/asm-mips/war.h
--- linux-2.4.19/include/asm-mips/war.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips/war.h	Wed Dec 31 18:00:00 1969
@@ -1,53 +0,0 @@
-/*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
- *
- * Copyright (C) 2002 by Ralf Baechle
- */
-#ifndef _ASM_WAR_H
-#define _ASM_WAR_H
-
-/*
- * Pleassures of the R4600 V1.x.  Cite from the IDT R4600 V1.7 errata:
- *
- *  18. The CACHE instructions Hit_Writeback_Invalidate_D, Hit_Writeback_D,
- *      Hit_Invalidate_D and Create_Dirty_Excl_D should only be
- *      executed if there is no other dcache activity. If the dcache is
- *      accessed for another instruction immeidately preceding when these
- *      cache instructions are executing, it is possible that the dcache 
- *      tag match outputs used by these cache instructions will be 
- *      incorrect. These cache instructions should be preceded by at least
- *      four instructions that are not any kind of load or store 
- *      instruction.
- *
- *      This is not allowed:    lw
- *                              nop
- *                              nop
- *                              nop
- *                              cache       Hit_Writeback_Invalidate_D
- *
- *      This is allowed:        lw
- *                              nop
- *                              nop
- *                              nop
- *                              nop
- *                              cache       Hit_Writeback_Invalidate_D
- */
-#define R4600_V1_HIT_DCACHE_WAR
-
-
-/*
- * Writeback and invalidate the primary cache dcache before DMA.
- *
- * R4600 v2.0 bug: "The CACHE instructions Hit_Writeback_Inv_D,
- * Hit_Writeback_D, Hit_Invalidate_D and Create_Dirty_Exclusive_D will only
- * operate correctly if the internal data cache refill buffer is empty.  These
- * CACHE instructions should be separated from any potential data cache miss
- * by a load instruction to an uncached address to empty the response buffer."
- * (Revision 2.0 device errata from IDT available on http://www.idt.com/
- * in .pdf format.)
- */
-#define R4600_V2_HIT_CACHEOP_WAR
-
-#endif /* _ASM_WAR_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips64/fpu_emulator.h linuxppc64_2_4/include/asm-mips64/fpu_emulator.h
--- linux-2.4.19/include/asm-mips64/fpu_emulator.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips64/fpu_emulator.h	Wed Dec 31 18:00:00 1969
@@ -1,38 +0,0 @@
-/*
- *  This program is free software; you can distribute it and/or modify it
- *  under the terms of the GNU General Public License (Version 2) as
- *  published by the Free Software Foundation.
- *
- *  This program is distributed in the hope it will be useful, but WITHOUT
- *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
- *  for more details.
- *
- *  You should have received a copy of the GNU General Public License along
- *  with this program; if not, write to the Free Software Foundation, Inc.,
- *  59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
- *
- * Further private data for which no space exists in mips_fpu_soft_struct.
- * This should be subsumed into the mips_fpu_soft_struct structure as
- * defined in processor.h as soon as the absurd wired absolute assembler
- * offsets become dynamic at compile time.
- *
- * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com
- * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.
- */
-#ifndef _ASM_FPU_EMULATOR_H
-#define _ASM_FPU_EMULATOR_H
-
-struct mips_fpu_emulator_private {
-	unsigned int eir;
-	struct {
-		unsigned int emulated;
-		unsigned int loads;
-		unsigned int stores;
-		unsigned int cp1ops;
-		unsigned int cp1xops;
-		unsigned int errors;
-	} stats;
-};
-
-#endif /* _ASM_FPU_EMULATOR_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips64/irq_cpu.h linuxppc64_2_4/include/asm-mips64/irq_cpu.h
--- linux-2.4.19/include/asm-mips64/irq_cpu.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips64/irq_cpu.h	Wed Dec 31 18:00:00 1969
@@ -1,18 +0,0 @@
-/*
- *	include/asm-mips/irq_cpu.h
- *
- *	MIPS CPU interrupt definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS64_IRQ_CPU_H
-#define __ASM_MIPS64_IRQ_CPU_H
-
-extern void mips_cpu_irq_init(int irq_base);
-
-#endif /* __ASM_MIPS64_IRQ_CPU_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips64/mips-boards/bonito64.h linuxppc64_2_4/include/asm-mips64/mips-boards/bonito64.h
--- linux-2.4.19/include/asm-mips64/mips-boards/bonito64.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips64/mips-boards/bonito64.h	Wed Dec 31 18:00:00 1969
@@ -1,432 +0,0 @@
-/*
- * bonito.h
- *
- * Carsten Langgaard, carstenl@mips.com
- * Copyright (C) 2001 MIPS Technologies, Inc.  All rights reserved.
- *
- * ########################################################################
- *
- * This file is the original bonito.h from Algorithmics with minor changes
- * to fit into linux.
- */
-
-/*
- * Bonito Register Map 
- * Copyright (c) 1999 Algorithmics Ltd
- *
- * Algorithmics gives permission for anyone to use and modify this file
- * without any obligation or license condition except that you retain
- * this copyright message in any source redistribution in whole or part.
- *
- * Updated copies of this and other files can be found at
- * ftp://ftp.algor.co.uk/pub/bonito/
- * 
- * Users of the Bonito controller are warmly recommended to contribute
- * any useful changes back to Algorithmics (mail to bonito@algor.co.uk).
- */
-
-/* Revision 1.48 autogenerated on 08/17/99 15:20:01 */
-/* This bonito64 version editted from bonito.h Revision 1.48 on 11/09/00 */
-
-#ifndef _ASM_MIPS_BOARDS_BONITO64_H
-#define _ASM_MIPS_BOARDS_BONITO64_H
-
-#ifdef __ASSEMBLY__
-
-/* offsets from base register */
-#define BONITO(x)	(x)
-
-#else /* !__ASSEMBLY__ */
-
-/* offsets from base pointer, this construct allows optimisation */
-/* static char * const _bonito = PA_TO_KVA1(BONITO_BASE); */
-#define BONITO(x)		*(volatile u32 *)(_bonito + (x))
-
-#endif /* __ASSEMBLY__ */
-
-
-#define BONITO_BOOT_BASE		0x1fc00000
-#define BONITO_BOOT_SIZE		0x00100000
-#define BONITO_BOOT_TOP 		(BONITO_BOOT_BASE+BONITO_BOOT_SIZE-1)
-#define BONITO_FLASH_BASE		0x1c000000
-#define BONITO_FLASH_SIZE		0x03000000
-#define BONITO_FLASH_TOP		(BONITO_FLASH_BASE+BONITO_FLASH_SIZE-1)
-#define BONITO_SOCKET_BASE		0x1f800000
-#define BONITO_SOCKET_SIZE		0x00400000
-#define BONITO_SOCKET_TOP		(BONITO_SOCKET_BASE+BONITO_SOCKET_SIZE-1)
-#define BONITO_REG_BASE 		0x1fe00000
-#define BONITO_REG_SIZE 		0x00040000
-#define BONITO_REG_TOP			(BONITO_REG_BASE+BONITO_REG_SIZE-1)
-#define BONITO_DEV_BASE 		0x1ff00000
-#define BONITO_DEV_SIZE 		0x00100000
-#define BONITO_DEV_TOP			(BONITO_DEV_BASE+BONITO_DEV_SIZE-1)
-#define BONITO_PCILO_BASE		0x10000000
-#define BONITO_PCILO_SIZE		0x0c000000
-#define BONITO_PCILO_TOP		(BONITO_PCILO_BASE+BONITO_PCILO_SIZE-1)
-#define BONITO_PCILO0_BASE		0x10000000
-#define BONITO_PCILO1_BASE		0x14000000
-#define BONITO_PCILO2_BASE		0x18000000
-#define BONITO_PCIHI_BASE		0x20000000
-#define BONITO_PCIHI_SIZE		0x20000000
-#define BONITO_PCIHI_TOP		(BONITO_PCIHI_BASE+BONITO_PCIHI_SIZE-1)
-#define BONITO_PCIIO_BASE		0x1fd00000
-#define BONITO_PCIIO_SIZE		0x00100000
-#define BONITO_PCIIO_TOP		(BONITO_PCIIO_BASE+BONITO_PCIIO_SIZE-1)
-#define BONITO_PCICFG_BASE		0x1fe80000
-#define BONITO_PCICFG_SIZE		0x00080000
-#define BONITO_PCICFG_TOP		(BONITO_PCICFG_BASE+BONITO_PCICFG_SIZE-1)
- 
-
-/* Bonito Register Bases */
-
-#define BONITO_PCICONFIGBASE		0x00
-#define BONITO_REGBASE			0x100
-
-
-/* PCI Configuration  Registers */
-
-#define BONITO_PCI_REG(x)               BONITO(BONITO_PCICONFIGBASE + (x))
-#define BONITO_PCIDID			BONITO_PCI_REG(0x00)
-#define BONITO_PCICMD			BONITO_PCI_REG(0x04)
-#define BONITO_PCICLASS 		BONITO_PCI_REG(0x08)
-#define BONITO_PCILTIMER		BONITO_PCI_REG(0x0c)
-#define BONITO_PCIBASE0 		BONITO_PCI_REG(0x10)
-#define BONITO_PCIBASE1 		BONITO_PCI_REG(0x14)
-#define BONITO_PCIBASE2 		BONITO_PCI_REG(0x18)
-#define BONITO_PCIEXPRBASE		BONITO_PCI_REG(0x30)
-#define BONITO_PCIINT			BONITO_PCI_REG(0x3c)
-
-#define BONITO_PCICMD_PERR_CLR		0x80000000
-#define BONITO_PCICMD_SERR_CLR		0x40000000
-#define BONITO_PCICMD_MABORT_CLR	0x20000000
-#define BONITO_PCICMD_MTABORT_CLR	0x10000000
-#define BONITO_PCICMD_TABORT_CLR	0x08000000
-#define BONITO_PCICMD_MPERR_CLR 	0x01000000
-#define BONITO_PCICMD_PERRRESPEN	0x00000040
-#define BONITO_PCICMD_ASTEPEN		0x00000080
-#define BONITO_PCICMD_SERREN		0x00000100
-#define BONITO_PCILTIMER_BUSLATENCY	0x0000ff00
-#define BONITO_PCILTIMER_BUSLATENCY_SHIFT	8
-
-
-
-
-/* 1. Bonito h/w Configuration */
-/* Power on register */
-
-#define BONITO_BONPONCFG		BONITO(BONITO_REGBASE + 0x00)
-
-#define BONITO_BONPONCFG_SYSCONTROLLERRD	0x00040000
-#define BONITO_BONPONCFG_ROMCS1SAMP	0x00020000
-#define BONITO_BONPONCFG_ROMCS0SAMP	0x00010000
-#define BONITO_BONPONCFG_CPUBIGEND	0x00004000
-/* Added by RPF 11-9-00 */
-#define BONITO_BONPONCFG_BURSTORDER	0x00001000
-/* --- */
-#define BONITO_BONPONCFG_CPUPARITY	0x00002000
-#define BONITO_BONPONCFG_CPUTYPE	0x00000007
-#define BONITO_BONPONCFG_CPUTYPE_SHIFT	0
-#define BONITO_BONPONCFG_PCIRESET_OUT	0x00000008
-#define BONITO_BONPONCFG_IS_ARBITER	0x00000010
-#define BONITO_BONPONCFG_ROMBOOT	0x000000c0
-#define BONITO_BONPONCFG_ROMBOOT_SHIFT	6
-
-#define BONITO_BONPONCFG_ROMBOOT_FLASH	(0x0<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_SOCKET (0x1<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_SDRAM	(0x2<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-#define BONITO_BONPONCFG_ROMBOOT_CPURESET	(0x3<<BONITO_BONPONCFG_ROMBOOT_SHIFT)
-
-#define BONITO_BONPONCFG_ROMCS0WIDTH	0x00000100
-#define BONITO_BONPONCFG_ROMCS1WIDTH	0x00000200
-#define BONITO_BONPONCFG_ROMCS0FAST	0x00000400
-#define BONITO_BONPONCFG_ROMCS1FAST	0x00000800
-#define BONITO_BONPONCFG_CONFIG_DIS	0x00000020
-
-
-/* Other Bonito configuration */
-
-#define BONITO_BONGENCFG_OFFSET         0x4
-#define BONITO_BONGENCFG		BONITO(BONITO_REGBASE + BONITO_BONGENCFG_OFFSET)
-
-#define BONITO_BONGENCFG_DEBUGMODE	0x00000001
-#define BONITO_BONGENCFG_SNOOPEN	0x00000002
-#define BONITO_BONGENCFG_CPUSELFRESET	0x00000004
-
-#define BONITO_BONGENCFG_FORCE_IRQA	0x00000008
-#define BONITO_BONGENCFG_IRQA_ISOUT	0x00000010
-#define BONITO_BONGENCFG_IRQA_FROM_INT1 0x00000020
-#define BONITO_BONGENCFG_BYTESWAP	0x00000040
-
-#define BONITO_BONGENCFG_UNCACHED	0x00000080
-#define BONITO_BONGENCFG_PREFETCHEN	0x00000100
-#define BONITO_BONGENCFG_WBEHINDEN	0x00000200
-#define BONITO_BONGENCFG_CACHEALG	0x00000c00
-#define BONITO_BONGENCFG_CACHEALG_SHIFT 10
-#define BONITO_BONGENCFG_PCIQUEUE	0x00001000
-#define BONITO_BONGENCFG_CACHESTOP	0x00002000
-#define BONITO_BONGENCFG_MSTRBYTESWAP	0x00004000
-#define BONITO_BONGENCFG_BUSERREN	0x00008000
-#define BONITO_BONGENCFG_NORETRYTIMEOUT 0x00010000
-#define BONITO_BONGENCFG_SHORTCOPYTIMEOUT	0x00020000
-
-/* 2. IO & IDE configuration */
-
-#define BONITO_IODEVCFG 		BONITO(BONITO_REGBASE + 0x08)
-
-/* 3. IO & IDE configuration */
-
-#define BONITO_SDCFG			BONITO(BONITO_REGBASE + 0x0c)
-
-/* 4. PCI address map control */
-
-#define BONITO_PCIMAP			BONITO(BONITO_REGBASE + 0x10)
-#define BONITO_PCIMEMBASECFG		BONITO(BONITO_REGBASE + 0x14)
-#define BONITO_PCIMAP_CFG		BONITO(BONITO_REGBASE + 0x18)
-
-/* 5. ICU & GPIO regs */
- 
-/* GPIO Regs - r/w */
-
-#define BONITO_GPIODATA_OFFSET          0x1c
-#define BONITO_GPIODATA 		BONITO(BONITO_REGBASE + BONITO_GPIODATA_OFFSET)
-#define BONITO_GPIOIE			BONITO(BONITO_REGBASE + 0x20)
-
-/* ICU Configuration Regs - r/w */
-
-#define BONITO_INTEDGE			BONITO(BONITO_REGBASE + 0x24)
-#define BONITO_INTSTEER 		BONITO(BONITO_REGBASE + 0x28)
-#define BONITO_INTPOL			BONITO(BONITO_REGBASE + 0x2c)
-
-/* ICU Enable Regs - IntEn & IntISR are r/o. */
-
-#define BONITO_INTENSET 		BONITO(BONITO_REGBASE + 0x30)
-#define BONITO_INTENCLR 		BONITO(BONITO_REGBASE + 0x34)
-#define BONITO_INTEN			BONITO(BONITO_REGBASE + 0x38)
-#define BONITO_INTISR			BONITO(BONITO_REGBASE + 0x3c)
-
-/* PCI mail boxes */
-
-#define BONITO_PCIMAIL0_OFFSET          0x40
-#define BONITO_PCIMAIL1_OFFSET          0x44
-#define BONITO_PCIMAIL2_OFFSET          0x48
-#define BONITO_PCIMAIL3_OFFSET          0x4c
-#define BONITO_PCIMAIL0 		BONITO(BONITO_REGBASE + 0x40)
-#define BONITO_PCIMAIL1 		BONITO(BONITO_REGBASE + 0x44)
-#define BONITO_PCIMAIL2 		BONITO(BONITO_REGBASE + 0x48)
-#define BONITO_PCIMAIL3 		BONITO(BONITO_REGBASE + 0x4c)
-
-
-/* 6. PCI cache */
-
-#define BONITO_PCICACHECTRL		BONITO(BONITO_REGBASE + 0x50)
-#define BONITO_PCICACHETAG		BONITO(BONITO_REGBASE + 0x54)
-
-#define BONITO_PCIBADADDR		BONITO(BONITO_REGBASE + 0x58)
-#define BONITO_PCIMSTAT 		BONITO(BONITO_REGBASE + 0x5c)
-
-
-/*
-#define BONITO_PCIRDPOST		BONITO(BONITO_REGBASE + 0x60)
-#define BONITO_PCIDATA			BONITO(BONITO_REGBASE + 0x64)
-*/
-
-/* 7. IDE DMA & Copier */
- 
-#define BONITO_CONFIGBASE		0x000
-#define BONITO_BONITOBASE		0x100
-#define BONITO_LDMABASE 		0x200
-#define BONITO_COPBASE			0x300
-#define BONITO_REG_BLOCKMASK		0x300
-
-#define BONITO_LDMACTRL 		BONITO(BONITO_LDMABASE + 0x0)
-#define BONITO_LDMASTAT 		BONITO(BONITO_LDMABASE + 0x0)
-#define BONITO_LDMAADDR 		BONITO(BONITO_LDMABASE + 0x4)
-#define BONITO_LDMAGO			BONITO(BONITO_LDMABASE + 0x8)
-#define BONITO_LDMADATA 		BONITO(BONITO_LDMABASE + 0xc)
-
-#define BONITO_COPCTRL			BONITO(BONITO_COPBASE + 0x0)
-#define BONITO_COPSTAT			BONITO(BONITO_COPBASE + 0x0)
-#define BONITO_COPPADDR 		BONITO(BONITO_COPBASE + 0x4)
-#define BONITO_COPDADDR 		BONITO(BONITO_COPBASE + 0x8)
-#define BONITO_COPGO			BONITO(BONITO_COPBASE + 0xc)
-
-
-/* ###### Bit Definitions for individual Registers #### */
-
-/* Gen DMA. */
-
-#define BONITO_IDECOPDADDR_DMA_DADDR	0x0ffffffc
-#define BONITO_IDECOPDADDR_DMA_DADDR_SHIFT	2
-#define BONITO_IDECOPPADDR_DMA_PADDR	0xfffffffc
-#define BONITO_IDECOPPADDR_DMA_PADDR_SHIFT	2
-#define BONITO_IDECOPGO_DMA_SIZE	0x0000fffe
-#define BONITO_IDECOPGO_DMA_SIZE_SHIFT	0
-#define BONITO_IDECOPGO_DMA_WRITE	0x00010000
-#define BONITO_IDECOPGO_DMAWCOUNT	0x000f0000
-#define BONITO_IDECOPGO_DMAWCOUNT_SHIFT	16
-
-#define BONITO_IDECOPCTRL_DMA_STARTBIT	0x80000000
-#define BONITO_IDECOPCTRL_DMA_RSTBIT	0x40000000
-
-/* DRAM - sdCfg */
-
-#define BONITO_SDCFG_AROWBITS		0x00000003
-#define BONITO_SDCFG_AROWBITS_SHIFT	0
-#define BONITO_SDCFG_ACOLBITS		0x0000000c
-#define BONITO_SDCFG_ACOLBITS_SHIFT	2
-#define BONITO_SDCFG_ABANKBIT		0x00000010
-#define BONITO_SDCFG_ASIDES		0x00000020
-#define BONITO_SDCFG_AABSENT		0x00000040
-#define BONITO_SDCFG_AWIDTH64		0x00000080
-
-#define BONITO_SDCFG_BROWBITS		0x00000300
-#define BONITO_SDCFG_BROWBITS_SHIFT	8
-#define BONITO_SDCFG_BCOLBITS		0x00000c00
-#define BONITO_SDCFG_BCOLBITS_SHIFT	10
-#define BONITO_SDCFG_BBANKBIT		0x00001000
-#define BONITO_SDCFG_BSIDES		0x00002000
-#define BONITO_SDCFG_BABSENT		0x00004000
-#define BONITO_SDCFG_BWIDTH64		0x00008000
-
-#define BONITO_SDCFG_EXTRDDATA		0x00010000
-#define BONITO_SDCFG_EXTRASCAS		0x00020000
-#define BONITO_SDCFG_EXTPRECH		0x00040000
-#define BONITO_SDCFG_EXTRASWIDTH	0x00180000
-#define BONITO_SDCFG_EXTRASWIDTH_SHIFT	19
-/* Changed by RPF 11-9-00 */
-#define BONITO_SDCFG_DRAMMODESET	0x00200000
-/* --- */
-#define BONITO_SDCFG_DRAMEXTREGS	0x00400000
-#define BONITO_SDCFG_DRAMPARITY 	0x00800000
-/* Added by RPF 11-9-00 */
-#define BONITO_SDCFG_DRAMBURSTLEN 	0x03000000
-#define BONITO_SDCFG_DRAMBURSTLEN_SHIFT	24
-#define BONITO_SDCFG_DRAMMODESET_DONE 	0x80000000
-/* --- */
-
-/* PCI Cache - pciCacheCtrl */
-
-#define BONITO_PCICACHECTRL_CACHECMD	0x00000007
-#define BONITO_PCICACHECTRL_CACHECMD_SHIFT	0
-#define BONITO_PCICACHECTRL_CACHECMDLINE	0x00000018
-#define BONITO_PCICACHECTRL_CACHECMDLINE_SHIFT	3
-#define BONITO_PCICACHECTRL_CMDEXEC	0x00000020
-
-#define BONITO_IODEVCFG_BUFFBIT_CS0	0x00000001
-#define BONITO_IODEVCFG_SPEEDBIT_CS0	0x00000002
-#define BONITO_IODEVCFG_MOREABITS_CS0	0x00000004
-
-#define BONITO_IODEVCFG_BUFFBIT_CS1	0x00000008
-#define BONITO_IODEVCFG_SPEEDBIT_CS1	0x00000010
-#define BONITO_IODEVCFG_MOREABITS_CS1	0x00000020
-
-#define BONITO_IODEVCFG_BUFFBIT_CS2	0x00000040
-#define BONITO_IODEVCFG_SPEEDBIT_CS2	0x00000080
-#define BONITO_IODEVCFG_MOREABITS_CS2	0x00000100
-
-#define BONITO_IODEVCFG_BUFFBIT_CS3	0x00000200
-#define BONITO_IODEVCFG_SPEEDBIT_CS3	0x00000400
-#define BONITO_IODEVCFG_MOREABITS_CS3	0x00000800
-
-#define BONITO_IODEVCFG_BUFFBIT_IDE	0x00001000
-#define BONITO_IODEVCFG_SPEEDBIT_IDE	0x00002000
-#define BONITO_IODEVCFG_WORDSWAPBIT_IDE 0x00004000
-#define BONITO_IODEVCFG_MODEBIT_IDE	0x00008000
-#define BONITO_IODEVCFG_DMAON_IDE	0x001f0000
-#define BONITO_IODEVCFG_DMAON_IDE_SHIFT 16
-#define BONITO_IODEVCFG_DMAOFF_IDE	0x01e00000
-#define BONITO_IODEVCFG_DMAOFF_IDE_SHIFT	21
-#define BONITO_IODEVCFG_EPROMSPLIT	0x02000000
-/* Added by RPF 11-9-00 */
-#define BONITO_IODEVCFG_CPUCLOCKPERIOD	0xfc000000
-#define BONITO_IODEVCFG_CPUCLOCKPERIOD_SHIFT 26
-/* --- */
-
-/* gpio */
-#define BONITO_GPIO_GPIOW		0x000003ff
-#define BONITO_GPIO_GPIOW_SHIFT 	0
-#define BONITO_GPIO_GPIOR		0x01ff0000
-#define BONITO_GPIO_GPIOR_SHIFT 	16
-#define BONITO_GPIO_GPINR		0xfe000000
-#define BONITO_GPIO_GPINR_SHIFT 	25
-#define BONITO_GPIO_IOW(N)		(1<<(BONITO_GPIO_GPIOW_SHIFT+(N)))
-#define BONITO_GPIO_IOR(N)		(1<<(BONITO_GPIO_GPIOR_SHIFT+(N)))
-#define BONITO_GPIO_INR(N)		(1<<(BONITO_GPIO_GPINR_SHIFT+(N)))
-
-/* ICU */
-#define BONITO_ICU_MBOXES		0x0000000f
-#define BONITO_ICU_MBOXES_SHIFT 	0
-#define BONITO_ICU_DMARDY		0x00000010
-#define BONITO_ICU_DMAEMPTY		0x00000020
-#define BONITO_ICU_COPYRDY		0x00000040
-#define BONITO_ICU_COPYEMPTY		0x00000080
-#define BONITO_ICU_COPYERR		0x00000100
-#define BONITO_ICU_PCIIRQ		0x00000200
-#define BONITO_ICU_MASTERERR		0x00000400
-#define BONITO_ICU_SYSTEMERR		0x00000800
-#define BONITO_ICU_DRAMPERR		0x00001000
-#define BONITO_ICU_RETRYERR		0x00002000
-#define BONITO_ICU_GPIOS		0x01ff0000
-#define BONITO_ICU_GPIOS_SHIFT		16
-#define BONITO_ICU_GPINS		0x7e000000
-#define BONITO_ICU_GPINS_SHIFT		25
-#define BONITO_ICU_MBOX(N)		(1<<(BONITO_ICU_MBOXES_SHIFT+(N)))
-#define BONITO_ICU_GPIO(N)		(1<<(BONITO_ICU_GPIOS_SHIFT+(N)))
-#define BONITO_ICU_GPIN(N)		(1<<(BONITO_ICU_GPINS_SHIFT+(N)))
-
-/* pcimap */
-
-#define BONITO_PCIMAP_PCIMAP_LO0	0x0000003f
-#define BONITO_PCIMAP_PCIMAP_LO0_SHIFT	0
-#define BONITO_PCIMAP_PCIMAP_LO1	0x00000fc0
-#define BONITO_PCIMAP_PCIMAP_LO1_SHIFT	6
-#define BONITO_PCIMAP_PCIMAP_LO2	0x0003f000
-#define BONITO_PCIMAP_PCIMAP_LO2_SHIFT	12
-#define BONITO_PCIMAP_PCIMAP_2		0x00040000
-#define BONITO_PCIMAP_WIN(WIN,ADDR)	((((ADDR)>>26) & BONITO_PCIMAP_PCIMAP_LO0) << ((WIN)*6))
-
-#define BONITO_PCIMAP_WINSIZE           (1<<26)
-#define BONITO_PCIMAP_WINOFFSET(ADDR)	((ADDR) & (BONITO_PCIMAP_WINSIZE - 1))
-#define BONITO_PCIMAP_WINBASE(ADDR)	((ADDR) << 26)
-
-/* pcimembaseCfg */
-
-#define BONITO_PCIMEMBASECFG_MASK               0xf0000000
-#define BONITO_PCIMEMBASECFG_MEMBASE0_MASK	0x0000001f
-#define BONITO_PCIMEMBASECFG_MEMBASE0_MASK_SHIFT	0
-#define BONITO_PCIMEMBASECFG_MEMBASE0_TRANS	0x000003e0
-#define BONITO_PCIMEMBASECFG_MEMBASE0_TRANS_SHIFT	5
-#define BONITO_PCIMEMBASECFG_MEMBASE0_CACHED	0x00000400
-#define BONITO_PCIMEMBASECFG_MEMBASE0_IO	0x00000800
-
-#define BONITO_PCIMEMBASECFG_MEMBASE1_MASK	0x0001f000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_MASK_SHIFT	12
-#define BONITO_PCIMEMBASECFG_MEMBASE1_TRANS	0x003e0000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_TRANS_SHIFT	17
-#define BONITO_PCIMEMBASECFG_MEMBASE1_CACHED	0x00400000
-#define BONITO_PCIMEMBASECFG_MEMBASE1_IO	0x00800000
-
-#define BONITO_PCIMEMBASECFG_ASHIFT	23
-#define BONITO_PCIMEMBASECFG_AMASK              0x007fffff
-#define BONITO_PCIMEMBASECFGSIZE(WIN,SIZE)	(((~((SIZE)-1))>>(BONITO_PCIMEMBASECFG_ASHIFT-BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK)
-#define BONITO_PCIMEMBASECFGBASE(WIN,BASE)	(((BASE)>>(BONITO_PCIMEMBASECFG_ASHIFT-BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS_SHIFT)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS)
-
-#define BONITO_PCIMEMBASECFG_SIZE(WIN,CFG)  (((((~(CFG)) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK)) << (BONITO_PCIMEMBASECFG_ASHIFT - BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT)) | BONITO_PCIMEMBASECFG_AMASK)
-
-
-#define BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)  ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-#define BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)  ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_MASK_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-#define BONITO_PCIMEMBASECFG_ADDRTRANS(WIN,CFG) ((((CFG) & BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS) >> BONITO_PCIMEMBASECFG_MEMBASE##WIN##_TRANS_SHIFT) << BONITO_PCIMEMBASECFG_ASHIFT)
-
-#define BONITO_PCITOPHYS(WIN,ADDR,CFG)          ( \
-                                                  (((ADDR) & (~(BONITO_PCIMEMBASECFG_MASK))) & (~(BONITO_PCIMEMBASECFG_ADDRMASK(WIN,CFG)))) | \
-                                                  (BONITO_PCIMEMBASECFG_ADDRTRANS(WIN,CFG)) \
-                                                )
-
-/* PCICmd */
-
-#define BONITO_PCICMD_MEMEN		0x00000002
-#define BONITO_PCICMD_MSTREN		0x00000004
-
-
-#endif /* _ASM_MIPS_BOARDS_BONITO64_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips64/mips-boards/msc01_pci.h linuxppc64_2_4/include/asm-mips64/mips-boards/msc01_pci.h
--- linux-2.4.19/include/asm-mips64/mips-boards/msc01_pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips64/mips-boards/msc01_pci.h	Wed Dec 31 18:00:00 1969
@@ -1,244 +0,0 @@
-/*
- * mcs01_pci.h
- *
- * Carsten Langgaard, carstenl@mips.com
- * Copyright (C) 2002 MIPS Technologies, Inc.  All rights reserved.
- *
- * ########################################################################
- *
- * PCI Register definitions for the MIPS System Controller.
- */
-#ifndef MSC01_PCI_H
-#define MSC01_PCI_H
-
-/*****************************************************************************
- * Register offset addresses
- ****************************************************************************/
-
-#define MSC01_PCI_ID_OFS		0x0000
-#define MSC01_PCI_SC2PMBASL_OFS		0x0208
-#define MSC01_PCI_SC2PMMSKL_OFS		0x0218
-#define MSC01_PCI_SC2PMMAPL_OFS		0x0228
-#define MSC01_PCI_SC2PIOBASL_OFS	0x0248
-#define MSC01_PCI_SC2PIOMSKL_OFS	0x0258
-#define MSC01_PCI_SC2PIOMAPL_OFS	0x0268
-#define MSC01_PCI_P2SCMSKL_OFS		0x0308
-#define MSC01_PCI_P2SCMAPL_OFS		0x0318
-#define MSC01_PCI_INTCFG_OFS		0x0600
-#define MSC01_PCI_INTSTAT_OFS		0x0608
-#define MSC01_PCI_CFGADDR_OFS		0x0610
-#define MSC01_PCI_CFGDATA_OFS		0x0618
-#define MSC01_PCI_IACK_OFS		0x0620
-#define MSC01_PCI_HEAD0_OFS		0x2000  /* DevID, VendorID */
-#define MSC01_PCI_HEAD1_OFS		0x2008  /* Status, Command */
-#define MSC01_PCI_HEAD2_OFS		0x2010  /* Class code, RevID */
-#define MSC01_PCI_HEAD3_OFS		0x2018  /* bist, header, latency */
-#define MSC01_PCI_HEAD4_OFS		0x2020  /* BAR 0 */
-#define MSC01_PCI_HEAD5_OFS		0x2028  /* BAR 1 */
-#define MSC01_PCI_HEAD6_OFS		0x2030  /* BAR 2 */
-#define MSC01_PCI_HEAD7_OFS		0x2038  /* BAR 3 */
-#define MSC01_PCI_HEAD8_OFS		0x2040  /* BAR 4 */
-#define MSC01_PCI_HEAD9_OFS		0x2048  /* BAR 5 */
-#define MSC01_PCI_HEAD10_OFS		0x2050  /* CardBus CIS Ptr */
-#define MSC01_PCI_HEAD11_OFS		0x2058  /* SubSystem ID, -VendorID */
-#define MSC01_PCI_HEAD12_OFS		0x2060  /* ROM BAR */
-#define MSC01_PCI_HEAD13_OFS		0x2068  /* Capabilities ptr */
-#define MSC01_PCI_HEAD14_OFS		0x2070  /* reserved */
-#define MSC01_PCI_HEAD15_OFS		0x2078  /* Maxl, ming, intpin, int */
-#define MSC01_PCI_BAR0_OFS		0x2220
-#define MSC01_PCI_CFG_OFS		0x2380
-#define MSC01_PCI_SWAP_OFS		0x2388
-
-
-/*****************************************************************************
- * Register encodings
- ****************************************************************************/
-
-#define MSC01_PCI_ID_ID_SHF		16
-#define MSC01_PCI_ID_ID_MSK		0x00ff0000
-#define MSC01_PCI_ID_ID_HOSTBRIDGE	82
-#define MSC01_PCI_ID_MAR_SHF		8
-#define MSC01_PCI_ID_MAR_MSK		0x0000ff00
-#define MSC01_PCI_ID_MIR_SHF		0
-#define MSC01_PCI_ID_MIR_MSK		0x000000ff
-
-#define MSC01_PCI_SC2PMBASL_BAS_SHF	24
-#define MSC01_PCI_SC2PMBASL_BAS_MSK	0xff000000
-
-#define MSC01_PCI_SC2PMMSKL_MSK_SHF	24
-#define MSC01_PCI_SC2PMMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_SC2PMMAPL_MAP_SHF	24
-#define MSC01_PCI_SC2PMMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOBASL_BAS_SHF	24
-#define MSC01_PCI_SC2PIOBASL_BAS_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOMSKL_MSK_SHF	24
-#define MSC01_PCI_SC2PIOMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_SC2PIOMAPL_MAP_SHF	24
-#define MSC01_PCI_SC2PIOMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_P2SCMSKL_MSK_SHF	24
-#define MSC01_PCI_P2SCMSKL_MSK_MSK	0xff000000
-
-#define MSC01_PCI_P2SCMAPL_MAP_SHF	24
-#define MSC01_PCI_P2SCMAPL_MAP_MSK	0xff000000
-
-#define MSC01_PCI_INTCFG_RST_SHF        10
-#define MSC01_PCI_INTCFG_RST_MSK        0x00000400
-#define MSC01_PCI_INTCFG_RST_BIT        0x00000400
-#define MSC01_PCI_INTCFG_MWE_SHF        9
-#define MSC01_PCI_INTCFG_MWE_MSK        0x00000200
-#define MSC01_PCI_INTCFG_MWE_BIT        0x00000200
-#define MSC01_PCI_INTCFG_DTO_SHF        8
-#define MSC01_PCI_INTCFG_DTO_MSK        0x00000100
-#define MSC01_PCI_INTCFG_DTO_BIT        0x00000100 
-#define MSC01_PCI_INTCFG_MA_SHF         7
-#define MSC01_PCI_INTCFG_MA_MSK         0x00000080
-#define MSC01_PCI_INTCFG_MA_BIT         0x00000080
-#define MSC01_PCI_INTCFG_TA_SHF         6
-#define MSC01_PCI_INTCFG_TA_MSK         0x00000040
-#define MSC01_PCI_INTCFG_TA_BIT         0x00000040
-#define MSC01_PCI_INTCFG_RTY_SHF        5
-#define MSC01_PCI_INTCFG_RTY_MSK        0x00000020
-#define MSC01_PCI_INTCFG_RTY_BIT        0x00000020
-#define MSC01_PCI_INTCFG_MWP_SHF        4
-#define MSC01_PCI_INTCFG_MWP_MSK        0x00000010
-#define MSC01_PCI_INTCFG_MWP_BIT        0x00000010
-#define MSC01_PCI_INTCFG_MRP_SHF        3
-#define MSC01_PCI_INTCFG_MRP_MSK        0x00000008
-#define MSC01_PCI_INTCFG_MRP_BIT        0x00000008
-#define MSC01_PCI_INTCFG_SWP_SHF        2
-#define MSC01_PCI_INTCFG_SWP_MSK        0x00000004
-#define MSC01_PCI_INTCFG_SWP_BIT        0x00000004
-#define MSC01_PCI_INTCFG_SRP_SHF        1
-#define MSC01_PCI_INTCFG_SRP_MSK        0x00000002
-#define MSC01_PCI_INTCFG_SRP_BIT        0x00000002
-#define MSC01_PCI_INTCFG_SE_SHF         0
-#define MSC01_PCI_INTCFG_SE_MSK         0x00000001
-#define MSC01_PCI_INTCFG_SE_BIT         0x00000001
-
-#define MSC01_PCI_INTSTAT_RST_SHF       10
-#define MSC01_PCI_INTSTAT_RST_MSK       0x00000400
-#define MSC01_PCI_INTSTAT_RST_BIT       0x00000400
-#define MSC01_PCI_INTSTAT_MWE_SHF       9
-#define MSC01_PCI_INTSTAT_MWE_MSK       0x00000200
-#define MSC01_PCI_INTSTAT_MWE_BIT       0x00000200
-#define MSC01_PCI_INTSTAT_DTO_SHF       8
-#define MSC01_PCI_INTSTAT_DTO_MSK       0x00000100
-#define MSC01_PCI_INTSTAT_DTO_BIT       0x00000100 
-#define MSC01_PCI_INTSTAT_MA_SHF        7
-#define MSC01_PCI_INTSTAT_MA_MSK        0x00000080
-#define MSC01_PCI_INTSTAT_MA_BIT        0x00000080
-#define MSC01_PCI_INTSTAT_TA_SHF        6
-#define MSC01_PCI_INTSTAT_TA_MSK        0x00000040
-#define MSC01_PCI_INTSTAT_TA_BIT        0x00000040
-#define MSC01_PCI_INTSTAT_RTY_SHF       5
-#define MSC01_PCI_INTSTAT_RTY_MSK       0x00000020
-#define MSC01_PCI_INTSTAT_RTY_BIT       0x00000020
-#define MSC01_PCI_INTSTAT_MWP_SHF       4
-#define MSC01_PCI_INTSTAT_MWP_MSK       0x00000010
-#define MSC01_PCI_INTSTAT_MWP_BIT       0x00000010
-#define MSC01_PCI_INTSTAT_MRP_SHF       3
-#define MSC01_PCI_INTSTAT_MRP_MSK       0x00000008
-#define MSC01_PCI_INTSTAT_MRP_BIT       0x00000008
-#define MSC01_PCI_INTSTAT_SWP_SHF       2
-#define MSC01_PCI_INTSTAT_SWP_MSK       0x00000004
-#define MSC01_PCI_INTSTAT_SWP_BIT       0x00000004
-#define MSC01_PCI_INTSTAT_SRP_SHF       1
-#define MSC01_PCI_INTSTAT_SRP_MSK       0x00000002
-#define MSC01_PCI_INTSTAT_SRP_BIT       0x00000002
-#define MSC01_PCI_INTSTAT_SE_SHF        0
-#define MSC01_PCI_INTSTAT_SE_MSK        0x00000001
-#define MSC01_PCI_INTSTAT_SE_BIT        0x00000001
-
-#define MSC01_PCI_CFGADDR_BNUM_SHF	16
-#define MSC01_PCI_CFGADDR_BNUM_MSK	0x00ff0000
-#define MSC01_PCI_CFGADDR_DNUM_SHF	11
-#define MSC01_PCI_CFGADDR_DNUM_MSK	0x0000f800
-#define MSC01_PCI_CFGADDR_FNUM_SHF	8
-#define MSC01_PCI_CFGADDR_FNUM_MSK	0x00000700
-#define MSC01_PCI_CFGADDR_RNUM_SHF	2
-#define MSC01_PCI_CFGADDR_RNUM_MSK	0x000000fc
-
-#define MSC01_PCI_CFGDATA_DATA_SHF	0
-#define MSC01_PCI_CFGDATA_DATA_MSK	0xffffffff
-
-/* The defines below are ONLY valid for a MEM bar! */
-#define MSC01_PCI_BAR0_SIZE_SHF	        4
-#define MSC01_PCI_BAR0_SIZE_MSK	        0xfffffff0
-#define MSC01_PCI_BAR0_P_SHF	        3
-#define MSC01_PCI_BAR0_P_MSK	        0x00000008
-#define MSC01_PCI_BAR0_P_BIT	        MSC01_PCI_BAR0_P_MSK
-#define MSC01_PCI_BAR0_D_SHF	        1
-#define MSC01_PCI_BAR0_D_MSK	        0x00000006
-#define MSC01_PCI_BAR0_T_SHF	        0
-#define MSC01_PCI_BAR0_T_MSK	        0x00000001
-#define MSC01_PCI_BAR0_T_BIT	        MSC01_PCI_BAR0_T_MSK
-
-
-#define MSC01_PCI_CFG_RA_SHF	        17
-#define MSC01_PCI_CFG_RA_MSK	        0x00020000
-#define MSC01_PCI_CFG_RA_BIT	        MSC01_PCI_CFG_RA_MSK
-#define MSC01_PCI_CFG_G_SHF	        16
-#define MSC01_PCI_CFG_G_MSK	        0x00010000
-#define MSC01_PCI_CFG_G_BIT	        MSC01_PCI_CFG_G_MSK
-#define MSC01_PCI_CFG_EN_SHF	        15
-#define MSC01_PCI_CFG_EN_MSK	        0x00008000
-#define MSC01_PCI_CFG_EN_BIT	        MSC01_PCI_CFG_EN_MSK
-#define MSC01_PCI_CFG_MAXRTRY_SHF       0
-#define MSC01_PCI_CFG_MAXRTRY_MSK       0x000000ff
-
-#define MSC01_PCI_SWAP_IO_SHF		18
-#define MSC01_PCI_SWAP_IO_MSK		0x000c0000
-#define MSC01_PCI_SWAP_MEM_SHF		16
-#define MSC01_PCI_SWAP_MEM_MSK		0x00030000
-#define MSC01_PCI_SWAP_BAR0_SHF		0
-#define MSC01_PCI_SWAP_BAR0_MSK		0x00000003
-#define MSC01_PCI_SWAP_NOSWAP		0
-#define MSC01_PCI_SWAP_BYTESWAP		1
-
-/*****************************************************************************
- * Registers absolute addresses
- ****************************************************************************/
-
-#define MSC01_PCI_ID            (MSC01_PCI_REG_BASE + MSC01_PCI_ID_OFS)
-#define MSC01_PCI_SC2PMBASL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMBASL_OFS)
-#define MSC01_PCI_SC2PMMSKL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMMSKL_OFS)
-#define MSC01_PCI_SC2PMMAPL     (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PMMAPL_OFS)
-#define MSC01_PCI_SC2PIOBASL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOBASL_OFS)
-#define MSC01_PCI_SC2PIOMSKL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOMSKL_OFS)
-#define MSC01_PCI_SC2PIOMAPL    (MSC01_PCI_REG_BASE + MSC01_PCI_SC2PIOMAPL_OFS)
-#define MSC01_PCI_P2SCMSKL      (MSC01_PCI_REG_BASE + MSC01_PCI_P2SCMSKL_OFS)
-#define MSC01_PCI_P2SCMAPL      (MSC01_PCI_REG_BASE + MSC01_PCI_P2SCMAPL_OFS)
-#define MSC01_PCI_INTCFG        (MSC01_PCI_REG_BASE + MSC01_PCI_INTCFG_OFS)
-#define MSC01_PCI_INTSTAT       (MSC01_PCI_REG_BASE + MSC01_PCI_INTSTAT_OFS)
-#define MSC01_PCI_CFGADDR       (MSC01_PCI_REG_BASE + MSC01_PCI_CFGADDR_OFS)
-#define MSC01_PCI_CFGDATA       (MSC01_PCI_REG_BASE + MSC01_PCI_CFGDATA_OFS)
-#define MSC01_PCI_IACK		(MSC01_PCI_REG_BASE + MSC01_PCI_IACK_OFS)
-#define MSC01_PCI_HEAD0		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD0_OFS)
-#define MSC01_PCI_HEAD1		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD1_OFS)
-#define MSC01_PCI_HEAD2		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD2_OFS)
-#define MSC01_PCI_HEAD3		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD3_OFS)
-#define MSC01_PCI_HEAD4		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD4_OFS)
-#define MSC01_PCI_HEAD5		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD5_OFS)
-#define MSC01_PCI_HEAD6		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD6_OFS)
-#define MSC01_PCI_HEAD7		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD7_OFS)
-#define MSC01_PCI_HEAD8		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD8_OFS)
-#define MSC01_PCI_HEAD9		(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD9_OFS)
-#define MSC01_PCI_HEAD10	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD10_OFS)
-#define MSC01_PCI_HEAD11	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD12	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD13	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD14	(MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_HEAD15        (MSC01_PCI_REG_BASE + MSC01_PCI_HEAD11_OFS)
-#define MSC01_PCI_BAR0		(MSC01_PCI_REG_BASE + MSC01_PCI_BAR0_OFS)
-#define MSC01_PCI_CFG		(MSC01_PCI_REG_BASE + MSC01_PCI_CFG_OFS)
-#define MSC01_PCI_SWAP		(MSC01_PCI_REG_BASE + MSC01_PCI_SWAP_OFS)
-
-#endif
-/*****************************************************************************
- *  End of msc01_pci.h
- *****************************************************************************/
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-mips64/traps.h linuxppc64_2_4/include/asm-mips64/traps.h
--- linux-2.4.19/include/asm-mips64/traps.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-mips64/traps.h	Wed Dec 31 18:00:00 1969
@@ -1,27 +0,0 @@
-/*
- *	include/asm-mips64/traps.h
- *
- *	Trap handling definitions.
- *
- *	Copyright (C) 2002  Maciej W. Rozycki
- *
- *	This program is free software; you can redistribute it and/or
- *	modify it under the terms of the GNU General Public License
- *	as published by the Free Software Foundation; either version
- *	2 of the License, or (at your option) any later version.
- */
-#ifndef __ASM_MIPS64_TRAPS_H
-#define __ASM_MIPS64_TRAPS_H
-
-/*
- * Possible status responses for a be_board_handler backend.
- */
-#define MIPS_BE_DISCARD	0		/* return with no action */
-#define MIPS_BE_FIXUP	1		/* return to the fixup code */
-#define MIPS_BE_FATAL	2		/* treat as an unrecoverable error */
-
-extern int (*be_board_handler)(struct pt_regs *regs, int is_fixup);
-
-extern void bus_error_init(void);
-
-#endif /* __ASM_MIPS64_TRAPS_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/byteorder.h linuxppc64_2_4/include/asm-ppc64/byteorder.h
--- linux-2.4.19/include/asm-ppc64/byteorder.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/byteorder.h	Tue Jul 23 02:38:54 2002
@@ -2,8 +2,6 @@
 #define _PPC64_BYTEORDER_H
 
 /*
- *  
- *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
  * as published by the Free Software Foundation; either version
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/delay.h linuxppc64_2_4/include/asm-ppc64/delay.h
--- linux-2.4.19/include/asm-ppc64/delay.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/delay.h	Mon Jun 24 09:24:15 2002
@@ -13,6 +13,7 @@
  * Anton Blanchard.
  */
 
+#ifndef __ASSEMBLY__
 extern unsigned long tb_ticks_per_usec;
 
 /* define these here to prevent circular dependencies */ 
@@ -42,5 +43,6 @@
 	__delay(loops);
 	__HMT_medium();
 }
+#endif /* !__ASSEMBLY__ */
 
 #endif /* _PPC64_DELAY_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/dump.h linuxppc64_2_4/include/asm-ppc64/dump.h
--- linux-2.4.19/include/asm-ppc64/dump.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/dump.h	Mon Aug  5 14:58:16 2002
@@ -0,0 +1,112 @@
+/*
+ * Kernel header file for Linux crash dumps.
+ *
+ * Created by: Todd Inglett <tinglett@vnet.ibm.com>
+ *
+ * Copyright 2002 International Business Machines
+ *
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/* This header file holds the architecture specific crash dump header */
+#ifndef _ASM_DUMP_H
+#define _ASM_DUMP_H
+
+/* necessary header files */
+#include <asm/ptrace.h>                          /* for pt_regs             */
+#include <linux/threads.h>
+
+/* definitions */
+#define DUMP_ASM_MAGIC_NUMBER     0xdeaddeadULL  /* magic number            */
+#define DUMP_ASM_VERSION_NUMBER   0x1            /* version number          */
+
+
+/*
+ * Structure: dump_header_asm_t
+ *  Function: This is the header for architecture-specific stuff.  It
+ *            follows right after the dump header.
+ */
+typedef struct _dump_header_asm_s {
+
+        /* the dump magic number -- unique to verify dump is valid */
+        uint64_t             dha_magic_number;
+
+        /* the version number of this dump */
+        uint32_t             dha_version;
+
+        /* the size of this header (in case we can't read it) */
+        uint32_t             dha_header_size;
+
+	/* the dump registers */
+	struct pt_regs       dha_regs;
+
+	/* smp specific */
+	uint32_t	     dha_smp_num_cpus;
+	int		     dha_dumping_cpu;	
+	struct pt_regs	     dha_smp_regs[NR_CPUS];
+	void *		     dha_smp_current_task[NR_CPUS];
+	void *		     dha_stack[NR_CPUS];
+} dump_header_asm_t;
+
+#ifdef __KERNEL__
+static inline void get_current_regs(struct pt_regs *regs)
+{
+	__asm__ __volatile__ (
+		"std	0,0(%0)\n"
+		"std	1,8(%0)\n"
+		"std	2,16(%0)\n"
+		"std	3,24(%0)\n"
+		"std	4,32(%0)\n"
+		"std	5,40(%0)\n"
+		"std	6,48(%0)\n"
+		"std	7,56(%0)\n"
+		"std	8,64(%0)\n"
+		"std	9,72(%0)\n"
+		"std	10,80(%0)\n"
+		"std	11,88(%0)\n"
+		"std	12,96(%0)\n"
+		"std	13,104(%0)\n"
+		"std	14,112(%0)\n"
+		"std	15,120(%0)\n"
+		"std	16,128(%0)\n"
+		"std	17,136(%0)\n"
+		"std	18,144(%0)\n"
+		"std	19,152(%0)\n"
+		"std	20,160(%0)\n"
+		"std	21,168(%0)\n"
+		"std	22,176(%0)\n"
+		"std	23,184(%0)\n"
+		"std	24,192(%0)\n"
+		"std	25,200(%0)\n"
+		"std	26,208(%0)\n"
+		"std	27,216(%0)\n"
+		"std	28,224(%0)\n"
+		"std	29,232(%0)\n"
+		"std	30,240(%0)\n"
+		"std	31,248(%0)\n"
+		"mfmsr	0\n"
+		"std	0, 264(%0)\n"
+		"mfctr	0\n"
+		"std	0, 280(%0)\n"
+		"mflr	0\n"
+		"std	0, 288(%0)\n"
+		"bl	1f\n"
+	"1:	 mflr	5\n"
+		"std	5, 256(%0)\n"
+		"mtlr	0\n"
+		"mfxer	0\n"
+		"std	0, 296(%0)\n"
+			      : : "b" (&regs));
+}
+
+extern volatile int dump_in_progress;
+extern dump_header_asm_t dump_header_asm;
+
+#ifdef CONFIG_SMP
+extern void dump_send_ipi(int (*dump_ipi_callback)(struct pt_regs *));
+#else
+#define dump_send_ipi()
+#endif
+#endif /* __KERNEL__ */
+
+#endif /* _ASM_DUMP_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/eeh.h linuxppc64_2_4/include/asm-ppc64/eeh.h
--- linux-2.4.19/include/asm-ppc64/eeh.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/eeh.h	Mon Aug 19 09:57:01 2002
@@ -149,4 +149,38 @@
 	memcpy(vdest, src, n);
 }
 
+static inline void eeh_insb(volatile u8 *addr, void *buf, int n) {
+	volatile u8 *vaddr = (volatile u8 *)IO_TOKEN_TO_ADDR(addr);
+	_insb(vaddr, buf, n);
+	/* ToDo: look for ff's in buf[n] */
+}
+
+static inline void eeh_outsb(volatile u8 *addr, const void *buf, int n) {
+	volatile u8 *vaddr = (volatile u8 *)IO_TOKEN_TO_ADDR(addr);
+	_outsb(vaddr, buf, n);
+}
+
+static inline void eeh_insw_ns(volatile u16 *addr, void *buf, int n) {
+	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
+	_insw_ns(vaddr, buf, n);
+	/* ToDo: look for ffff's in buf[n] */
+}
+
+static inline void eeh_outsw_ns(volatile u16 *addr, const void *buf, int n) {
+	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
+	_outsw_ns(vaddr, buf, n);
+}
+
+static inline void eeh_insl_ns(volatile u32 *addr, void *buf, int n) {
+	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
+	_insl_ns(vaddr, buf, n);
+	/* ToDo: look for ffffffff's in buf[n] */
+}
+
+static inline void eeh_outsl_ns(volatile u32 *addr, const void *buf, int n) {
+	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
+	_outsl_ns(vaddr, buf, n);
+}
+
+
 #endif /* _EEH_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/fcntl.h linuxppc64_2_4/include/asm-ppc64/fcntl.h
--- linux-2.4.19/include/asm-ppc64/fcntl.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/fcntl.h	Fri May 10 23:42:16 2002
@@ -26,7 +26,7 @@
 #define O_DIRECTORY      040000	/* must be a directory */
 #define O_NOFOLLOW      0100000	/* don't follow links */
 #define O_LARGEFILE     0200000
-#define O_DIRECT	0400000	/* direct disk access hint - currently ignored */
+#define O_DIRECT	0400000	/* direct disk access hint */
 
 #define F_DUPFD		0	/* dup */
 #define F_GETFD		1	/* get close_on_exec */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/hvcall.h linuxppc64_2_4/include/asm-ppc64/hvcall.h
--- linux-2.4.19/include/asm-ppc64/hvcall.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/hvcall.h	Fri Aug  9 08:58:58 2002
@@ -0,0 +1,85 @@
+
+#define H_Success	0
+#define H_Busy		1	/* Hardware busy -- retry later */
+#define H_Hardware	-1	/* Hardware error */
+#define H_Function	-2	/* Function not supported */
+#define H_Privilege	-3	/* Caller not privileged */
+#define H_Parameter	-4	/* Parameter invalid, out-of-range or conflicting */
+#define H_Bad_Mode	-5	/* Illegal msr value */
+#define H_PTEG_Full	-6	/* PTEG is full */
+#define H_Not_Found	-7	/* PTE was not found" */
+#define H_Reserved_DABR	-8	/* DABR address is reserved by the hypervisor on this processor" */
+
+/* Flags */
+#define H_LARGE_PAGE		(1UL<<(63-16))
+#define H_EXACT		    (1UL<<(63-24))	/* Use exact PTE or return H_PTEG_FULL */
+#define H_R_XLATE		(1UL<<(63-25))	/* include a valid logical page num in the pte if the valid bit is set */
+#define H_READ_4		(1UL<<(63-26))	/* Return 4 PTEs */
+#define H_AVPN			(1UL<<(63-32))	/* An avpn is provided as a sanity test */
+#define H_ANDCOND		(1UL<<(63-33))
+#define H_ICACHE_INVALIDATE	(1UL<<(63-40))	/* icbi, etc.  (ignored for IO pages) */
+#define H_ICACHE_SYNCHRONIZE	(1UL<<(63-41))	/* dcbst, icbi, etc (ignored for IO pages */
+#define H_ZERO_PAGE		(1UL<<(63-48))	/* zero the page before mapping (ignored for IO pages) */
+#define H_COPY_PAGE		(1UL<<(63-49))
+#define H_N			(1UL<<(63-61))
+#define H_PP1			(1UL<<(63-62))
+#define H_PP2			(1UL<<(63-63))
+
+
+
+/* pSeries hypervisor opcodes */
+#define H_REMOVE		0x04
+#define H_ENTER			0x08
+#define H_READ			0x0c
+#define H_CLEAR_MOD		0x10
+#define H_CLEAR_REF		0x14
+#define H_PROTECT		0x18
+#define H_GET_TCE		0x1c
+#define H_PUT_TCE		0x20
+#define H_SET_SPRG0		0x24
+#define H_SET_DABR		0x28
+#define H_PAGE_INIT		0x2c
+#define H_SET_ASR		0x30
+#define H_ASR_ON		0x34
+#define H_ASR_OFF		0x38
+#define H_LOGICAL_CI_LOAD	0x3c
+#define H_LOGICAL_CI_STORE	0x40
+#define H_LOGICAL_CACHE_LOAD	0x44
+#define H_LOGICAL_CACHE_STORE	0x48
+#define H_LOGICAL_ICBI		0x4c
+#define H_LOGICAL_DCBF		0x50
+#define H_GET_TERM_CHAR		0x54
+#define H_PUT_TERM_CHAR		0x58
+#define H_REAL_TO_LOGICAL	0x5c
+#define H_HYPERVISOR_DATA	0x60
+#define H_EOI			0x64
+#define H_CPPR			0x68
+#define H_IPI			0x6c
+#define H_IPOLL			0x70
+#define H_XIRR			0x74
+
+#define HSC			".long 0x44000022\n"
+#define H_ENTER_r3		"li	3, 0x08\n"
+
+/* plpar_hcall() -- Generic call interface using above opcodes
+ *
+ * The actual call interface is a hypervisor call instruction with
+ * the opcode in R3 and input args in R4-R7.
+ * Status is returned in R3 with variable output values in R4-R11.
+ * Only H_PTE_READ with H_READ_4 uses R6-R11 so we ignore it for now
+ * and return only two out args which MUST ALWAYS BE PROVIDED.
+ */
+long plpar_hcall(unsigned long opcode,
+		 unsigned long arg1,
+		 unsigned long arg2,
+		 unsigned long arg3,
+		 unsigned long arg4,
+		 unsigned long *out1,
+		 unsigned long *out2,
+		 unsigned long *out3);
+
+/* Same as plpar_hcall but for those opcodes that return no values
+ * other than status.  Slightly more efficient.
+ */
+long plpar_hcall_norets(unsigned long opcode, ...);
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/iSeries/HvCall.h linuxppc64_2_4/include/asm-ppc64/iSeries/HvCall.h
--- linux-2.4.19/include/asm-ppc64/iSeries/HvCall.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/iSeries/HvCall.h	Fri May 17 17:41:21 2002
@@ -130,6 +130,9 @@
 #define HvCallBaseRouter28				HvCallBase + 28
 #define HvCallBaseRouter29				HvCallBase + 29
 #define HvCallBaseRouter30				HvCallBase + 30
+
+#define HvCallCcSetDABR  				HvCallCc + 7
+
 //=====================================================================================
 static inline void		HvCall_setVirtualDecr(void)
 {
@@ -197,6 +200,10 @@
 	HvCall0( HvCallBaseTerminateMachineSrc );
 }
 
+static inline void HvCall_setDABR(unsigned long val)
+{
+	HvCall1(HvCallCcSetDABR, val);
+}
 
 #endif // _HVCALL_H
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/iSeries/HvCallSc.h linuxppc64_2_4/include/asm-ppc64/iSeries/HvCallSc.h
--- linux-2.4.19/include/asm-ppc64/iSeries/HvCallSc.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/iSeries/HvCallSc.h	Fri May 17 17:41:21 2002
@@ -25,6 +25,7 @@
 #define _HVCALLSC_H
 
 #define HvCallBase		0x8000000000000000
+#define HvCallCc		0x8001000000000000
 #define HvCallCfg		0x8002000000000000
 #define HvCallEvent		0x8003000000000000
 #define HvCallHpt		0x8004000000000000
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/iSeries/ItExtVpdPanel.h linuxppc64_2_4/include/asm-ppc64/iSeries/ItExtVpdPanel.h
--- linux-2.4.19/include/asm-ppc64/iSeries/ItExtVpdPanel.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/iSeries/ItExtVpdPanel.h	Mon Jul  8 10:33:21 2002
@@ -0,0 +1,60 @@
+/*
+ * ItExtVpdPanel.h
+ * Copyright (C) 2002  Dave Boutcher IBM Corporation
+ * 
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ * 
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ */
+
+/*
+ *
+ *	This struct maps the panel information 
+ *
+ * Warning:
+ *	This data must match the architecture for the panel information
+ *
+ */
+
+
+/*-------------------------------------------------------------------
+ * Standard Includes
+ *------------------------------------------------------------------- 
+*/
+#ifndef	_PPC_TYPES_H
+#include	<asm/types.h>
+#endif
+
+#ifndef _ITEXTVPDPANEL_H
+#define _ITEXTVPDPANEL_H
+struct ItExtVpdPanel
+{
+  // Definition of the Extended Vpd On Panel Data Area
+  char                      systemSerial[8];
+  char                      mfgID[4];
+  char                      reserved1[24];
+  char                      machineType[4];
+  char                      systemID[6];
+  char                      somUniqueCnt[4];
+  char                      serialNumberCount;
+  char                      reserved2[7];
+  u16                       bbu3;
+  u16                       bbu2;
+  u16                       bbu1;
+  char                      xLocationLabel[8];
+  u8                        xRsvd1[6];
+  u16                       xFrameId;
+  u8                        xRsvd2[48];
+};
+
+#endif /* _ITEXTVPDPANEL_H  */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/iSeries/LparData.h linuxppc64_2_4/include/asm-ppc64/iSeries/LparData.h
--- linux-2.4.19/include/asm-ppc64/iSeries/LparData.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/iSeries/LparData.h	Sun Jul  7 14:47:22 2002
@@ -37,6 +37,7 @@
 #include <asm/iSeries/LparMap.h>
 #include <asm/iSeries/ItVpdAreas.h>
 #include <asm/iSeries/ItIplParmsReal.h>
+#include <asm/iSeries/ItExtVpdPanel.h>
 #include <asm/iSeries/ItLpQueue.h>
 #include <asm/iSeries/IoHriProcessorVpd.h>
 #include <asm/page.h>
@@ -45,6 +46,7 @@
 extern struct HvReleaseData hvReleaseData;
 extern struct ItLpNaca	itLpNaca;
 extern struct ItIplParmsReal xItIplParmsReal;
+extern struct ItExtVpdPanel xItExtVpdPanel;
 extern struct IoHriProcessorVpd xIoHriProcessorVpd[];
 extern struct ItLpQueue xItLpQueue;
 extern struct ItVpdAreas itVpdAreas;
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/iSeries/iSeries_pci.h linuxppc64_2_4/include/asm-ppc64/iSeries/iSeries_pci.h
--- linux-2.4.19/include/asm-ppc64/iSeries/iSeries_pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/iSeries/iSeries_pci.h	Fri Jul 26 13:26:14 2002
@@ -89,7 +89,8 @@
 	int              Flags;          /* Possible flags(disable/bist)*/
 	u16              Vendor;         /* Vendor ID                   */
 	u8               LogicalSlot;    /* Hv Slot Index for Tces      */
-	struct TceTable* DevTceTable;    /* Device TCE Table            */ 
+	struct TceTable* DevTceTable;    /* Device TCE Table            */
+	spinlock_t       IoLock;	 /* Lock to single thread device*/
 	u8               PhbId;          /* Phb Card is on.             */
 	u16              Board;          /* Board Number                */
 	u8               FrameId;	 /* iSeries spcn Frame Id       */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/io.h linuxppc64_2_4/include/asm-ppc64/io.h
--- linux-2.4.19/include/asm-ppc64/io.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/io.h	Thu Aug 22 11:29:34 2002
@@ -40,7 +40,7 @@
 #define writeb(data, addr)	iSeries_Write_Byte(data,((void*)(addr)))
 #define writew(data, addr)	iSeries_Write_Word(data,((void*)(addr)))
 #define writel(data, addr)	iSeries_Write_Long(data,((void*)(addr)))
-#define memset_io(a,b,c)	iSeries_memset((void *)(a),(b),(c))
+#define memset_io(a,b,c)	iSeries_memset_io((void *)(a),(b),(c))
 #define memcpy_fromio(a,b,c)	iSeries_memcpy_fromio((void *)(a), (void *)(b), (c))
 #define memcpy_toio(a,b,c)	iSeries_memcpy_toio((void *)(a), (void *)(b), (c))
 #define inb(addr)		readb(((unsigned long)(addr)))  
@@ -57,7 +57,7 @@
 #define writeb(data, addr)	eeh_writeb((data), ((void*)(addr)))
 #define writew(data, addr)	eeh_writew((data), ((void*)(addr)))
 #define writel(data, addr)	eeh_writel((data), ((void*)(addr)))
-#define memset_io(a,b,c)	eeh_memset((void *)(a),(b),(c))
+#define memset_io(a,b,c)	eeh_memset_io((void *)(a),(b),(c))
 #define memcpy_fromio(a,b,c)	eeh_memcpy_fromio((a),(void *)(b),(c))
 #define memcpy_toio(a,b,c)	eeh_memcpy_toio((void *)(a),(b),(c))
 #define inb(port)		_inb((unsigned long)port)
@@ -66,6 +66,18 @@
 #define outw(val, port)		_outw(val, (unsigned long)port)
 #define inl(port)		_inl((unsigned long)port)
 #define outl(val, port)		_outl(val, (unsigned long)port)
+
+/*
+ * The insw/outsw/insl/outsl macros don't do byte-swapping.
+ * They are only used in practice for transferring buffers which
+ * are arrays of bytes, and byte-swapping is not appropriate in
+ * that case.  - paulus */
+#define insb(port, buf, ns)	eeh_insb((u8 *)(port), (buf), (ns))
+#define outsb(port, buf, ns)	eeh_outsb((u8 *)(port), (buf), (ns))
+#define insw(port, buf, ns)	eeh_insw_ns((u16 *)(port), (buf), (ns))
+#define outsw(port, buf, ns)	eeh_outsw_ns((u16 *)(port), (buf), (ns))
+#define insl(port, buf, nl)	eeh_insl_ns((u32 *)(port), (buf), (nl))
+#define outsl(port, buf, nl)	eeh_outsl_ns((u32 *)(port), (buf), (nl))
 #endif
 
 
@@ -80,18 +92,6 @@
 #define inl_p(port)             inl(port)
 #define outl_p(val, port)       (udelay(1), outl((val, (port)))
 
-/*
- * The insw/outsw/insl/outsl macros don't do byte-swapping.
- * They are only used in practice for transferring buffers which
- * are arrays of bytes, and byte-swapping is not appropriate in
- * that case.  - paulus */
-#define _IOMAP_VADDR(port) (IS_MAPPED_VADDR(port) ? (port) : (port)+_IO_BASE)
-#define insb(port, buf, ns)	_insb((u8 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define outsb(port, buf, ns)	_outsb((u8 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define insw(port, buf, ns)	_insw_ns((u16 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define outsw(port, buf, ns)	_outsw_ns((u16 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define insl(port, buf, nl)	_insl_ns((u32 *)(_IOMAP_VADDR(port)), (buf), (nl))
-#define outsl(port, buf, nl)	_outsl_ns((u32 *)(_IOMAP_VADDR(port)), (buf), (nl))
 
 extern void _insb(volatile u8 *port, void *buf, int ns);
 extern void _outsb(volatile u8 *port, const void *buf, int ns);
@@ -109,10 +109,10 @@
  * Neither do the standard versions now, these are just here
  * for older code.
  */
-#define insw_ns(port, buf, ns)	_insw_ns((u16 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define outsw_ns(port, buf, ns)	_outsw_ns((u16 *)(_IOMAP_VADDR(port)), (buf), (ns))
-#define insl_ns(port, buf, nl)	_insl_ns((u32 *)(_IOMAP_VADDR(port)), (buf), (nl))
-#define outsl_ns(port, buf, nl)	_outsl_ns((u32 *)(_IOMAP_VADDR(port)), (buf), (nl))
+#define insw_ns(port, buf, ns)	insw(port, buf, ns)
+#define outsw_ns(port, buf, ns)	outsw(port, buf, ns)
+#define insl_ns(port, buf, nl)	insl(port, buf, nl)
+#define outsl_ns(port, buf, nl)	outsl(port, buf, nl)
 
 
 #define IO_SPACE_LIMIT ~(0UL)
@@ -134,7 +134,7 @@
  * Change virtual addresses to physical addresses and vv, for
  * addresses in the area where the kernel has the RAM mapped.
  */
-extern inline unsigned long virt_to_phys(volatile void * address)
+static inline unsigned long virt_to_phys(volatile void * address)
 {
 #ifdef __IO_DEBUG
 	printk("virt_to_phys: 0x%08lx -> 0x%08lx\n", 
@@ -144,7 +144,7 @@
 	return __pa((unsigned long)address);
 }
 
-extern inline void * phys_to_virt(unsigned long address)
+static inline void * phys_to_virt(unsigned long address)
 {
 #ifdef __IO_DEBUG
 	printk("phys_to_virt: 0x%08lx -> 0x%08lx\n", address, __va(address));
@@ -154,7 +154,7 @@
 
 #endif /* __KERNEL__ */
 
-extern inline void iosync(void)
+static inline void iosync(void)
 {
         __asm__ __volatile__ ("sync" : : : "memory");
 }
@@ -172,7 +172,7 @@
  * excess of syncs before the MMIO operations will make things work.  On 
  * sstar, sync time is << than mmio time, so this should not be a big impact.
  */
-extern inline int in_8(volatile unsigned char *addr)
+static inline int in_8(volatile unsigned char *addr)
 {
 	int ret;
 
@@ -180,12 +180,12 @@
 	return ret;
 }
 
-extern inline void out_8(volatile unsigned char *addr, int val)
+static inline void out_8(volatile unsigned char *addr, int val)
 {
 	__asm__ __volatile__("sync; stb%U0%X0 %1,%0; sync" : "=m" (*addr) : "r" (val));
 }
 
-extern inline int in_le16(volatile unsigned short *addr)
+static inline int in_le16(volatile unsigned short *addr)
 {
 	int ret;
 
@@ -194,7 +194,7 @@
 	return ret;
 }
 
-extern inline int in_be16(volatile unsigned short *addr)
+static inline int in_be16(volatile unsigned short *addr)
 {
 	int ret;
 
@@ -202,18 +202,18 @@
 	return ret;
 }
 
-extern inline void out_le16(volatile unsigned short *addr, int val)
+static inline void out_le16(volatile unsigned short *addr, int val)
 {
 	__asm__ __volatile__("sync; sthbrx %1,0,%2; sync" : "=m" (*addr) :
 			      "r" (val), "r" (addr));
 }
 
-extern inline void out_be16(volatile unsigned short *addr, int val)
+static inline void out_be16(volatile unsigned short *addr, int val)
 {
 	__asm__ __volatile__("sync; sth%U0%X0 %1,%0; sync" : "=m" (*addr) : "r" (val));
 }
 
-extern inline unsigned in_le32(volatile unsigned *addr)
+static inline unsigned in_le32(volatile unsigned *addr)
 {
 	unsigned ret;
 
@@ -222,7 +222,7 @@
 	return ret;
 }
 
-extern inline unsigned in_be32(volatile unsigned *addr)
+static inline unsigned in_be32(volatile unsigned *addr)
 {
 	unsigned ret;
 
@@ -230,13 +230,13 @@
 	return ret;
 }
 
-extern inline void out_le32(volatile unsigned *addr, int val)
+static inline void out_le32(volatile unsigned *addr, int val)
 {
 	__asm__ __volatile__("sync; stwbrx %1,0,%2; sync" : "=m" (*addr) :
 			     "r" (val), "r" (addr));
 }
 
-extern inline void out_be32(volatile unsigned *addr, int val)
+static inline void out_be32(volatile unsigned *addr, int val)
 {
 	__asm__ __volatile__("sync; stw%U0%X0 %1,%0; sync" : "=m" (*addr) : "r" (val));
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/kdb.h linuxppc64_2_4/include/asm-ppc64/kdb.h
--- linux-2.4.19/include/asm-ppc64/kdb.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/kdb.h	Wed May 29 08:54:50 2002
@@ -0,0 +1,72 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ */
+#if !defined(_ASM_KDB_H)
+#define _ASM_KDB_H
+	/*
+	 * KDB_ENTER() is a macro which causes entry into the kernel
+	 * debugger from any point in the kernel code stream.  If it 
+	 * is intended to be used from interrupt level, it must  use
+	 * a non-maskable entry method.
+	 */
+#define KDB_ENTER()	asm("\ttrap\n")
+
+	/*
+	 * Define the exception frame for this architecture
+	 */
+struct pt_regs;
+typedef struct pt_regs	*kdb_eframe_t;
+
+	/*
+	 * Needed for exported symbols.
+	 */
+typedef unsigned long kdb_machreg_t;
+
+#define kdb_machreg_fmt		"0x%016lx"
+#define kdb_machreg_fmt0	"0x%016lx"
+#define kdb_bfd_vma_fmt		"0x%016lx"
+#define kdb_bfd_vma_fmt0	"0x%016lx"
+#define kdb_elfw_addr_fmt	"0x%016lx"
+#define kdb_elfw_addr_fmt0	"0x%016lx"
+
+	/*
+	 * Per cpu arch specific kdb state.  Must be in range 0xff000000.
+	 */
+#define KDB_STATE_A_IF		0x01000000	/* Saved IF flag */
+
+	 /*
+	  * Interface from kernel trap handling code to kernel debugger.
+	  */
+extern int	kdba_callback_die(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_bp(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_debug(struct pt_regs *, int, long, void *);
+
+#include <linux/types.h>
+extern int kdba_putarea_size(unsigned long to_xxx, void *from, size_t size);
+extern int kdba_getarea_size(void *to, unsigned long from_xxx, size_t size);
+
+static inline int
+kdba_verify_rw(unsigned long addr, size_t size)
+{
+	unsigned char data[size];
+	return(kdba_getarea_size(data, addr, size) || kdba_putarea_size(addr, data, size));
+}
+
+#endif	/* ASM_KDB_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/kdbprivate.h linuxppc64_2_4/include/asm-ppc64/kdbprivate.h
--- linux-2.4.19/include/asm-ppc64/kdbprivate.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/kdbprivate.h	Wed May 29 08:54:50 2002
@@ -0,0 +1,117 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+#if !defined(_ASM_KDBPRIVATE_H)
+#define _ASM_KDBPRIVATE_H
+
+typedef unsigned char kdb_machinst_t;
+
+	/*
+	 * KDB_MAXBPT describes the total number of breakpoints
+	 * supported by this architecure.  
+	 */
+#define KDB_MAXBPT	16
+	/*
+	 * KDB_MAXHARDBPT describes the total number of hardware
+	 * breakpoint registers that exist.
+	 */
+#define KDB_MAXHARDBPT	 4
+        /*
+         * Provide space for KDB_MAX_COMMANDS commands.
+         */
+#define KDB_MAX_COMMANDS        125
+
+	/*
+	 * Platform specific environment entries
+	 */
+#define KDB_PLATFORM_ENV	"IDMODE=PPC", "BYTESPERWORD=8", "IDCOUNT=16"
+
+	/*
+	 * Define the direction that the stack grows
+	 */
+#define KDB_STACK_DIRECTION	-1	/* Stack grows down */
+
+	/*
+	 * Support for ia32 debug registers 
+	 */
+typedef struct _kdbhard_bp {
+	kdb_machreg_t	bph_reg;	/* Register this breakpoint uses */
+
+	unsigned int	bph_free:1;	/* Register available for use */
+	unsigned int	bph_data:1;	/* Data Access breakpoint */
+
+	unsigned int	bph_write:1;	/* Write Data breakpoint */
+	unsigned int	bph_mode:2;	/* 0=inst, 1=write, 2=io, 3=read */
+	unsigned int	bph_length:2;	/* 0=1, 1=2, 2=BAD, 3=4 (bytes) */
+} kdbhard_bp_t;
+
+extern kdbhard_bp_t	kdb_hardbreaks[/* KDB_MAXHARDBPT */];
+
+#define KDB_HAVE_LONGJMP 
+#ifdef KDB_HAVE_LONGJMP
+typedef struct __kdb_jmp_buf {
+	unsigned int regs[100];
+} kdb_jmp_buf;
+extern int kdb_setjmp(kdb_jmp_buf *);
+extern void kdba_longjmp(kdb_jmp_buf *, int);
+extern kdb_jmp_buf  kdbjmpbuf[];
+#endif	/* KDB_HAVE_LONGJMP */
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+typedef struct {
+    unsigned long	flags;		/* flags: */
+#define KDBTBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define KDBTBTAB_FLAGSISEPROL		(1L<<46)
+#define KDBTBTAB_FLAGSHASTBOFF		(1L<<45)
+#define KDBTBTAB_FLAGSINTPROC		(1L<<44)
+#define KDBTBTAB_FLAGSHASCTL		(1L<<43)
+#define KDBTBTAB_FLAGSTOCLESS		(1L<<42)
+#define KDBTBTAB_FLAGSFPPRESENT		(1L<<41)
+#define KDBTBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define KDBTBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define KDBTBTAB_FLAGSSAVESCR		(1L<<33)
+#define KDBTBTAB_FLAGSSAVESLR		(1L<<32)
+#define KDBTBTAB_FLAGSSTORESBC		(1L<<31)
+#define KDBTBTAB_FLAGSFIXUP		(1L<<30)
+#define KDBTBTAB_FLAGSPARMSONSTK	(1L<<0)
+    unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+    unsigned char	gpr_saved;	/* num gpr's saved */
+    unsigned char	fixedparms;	/* num fixed point parms */
+    unsigned char	floatparms;	/* num float parms */
+    unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define KDBTBTAB_PARMFIXED 1
+#define KDBTBTAB_PARMSFLOAT 2
+#define KDBTBTAB_PARMDFLOAT 3
+    unsigned int	tb_offset;	/* offset from start of func */
+    unsigned long	funcstart;	/* addr of start of function */
+    char		name[64];	/* name of function (null terminated)*/
+    kdb_symtab_t	symtab;		/* fake symtab entry */
+} kdbtbtable_t;
+int kdba_find_tb_table(kdb_machreg_t eip, kdbtbtable_t *tab);
+
+#endif	/* !_ASM_KDBPRIVATE_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/lmb.h linuxppc64_2_4/include/asm-ppc64/lmb.h
--- linux-2.4.19/include/asm-ppc64/lmb.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/lmb.h	Wed Aug  7 15:37:07 2002
@@ -27,6 +27,9 @@
 #define LMB_MEMORY_AREA	1
 #define LMB_IO_AREA	2
 
+#define LMB_ALLOC_ANYWHERE	0
+#define LMB_ALLOC_FIRST4GBYTE	(1UL<<32)
+
 struct lmb_property {
 	unsigned long base;
 	unsigned long physbase;
@@ -44,6 +47,7 @@
 
 struct lmb {
 	unsigned long debug;
+	unsigned long rmo_size;
 	struct lmb_region memory;
 	struct lmb_region reserved;
 };
@@ -58,6 +62,7 @@
 #endif /* CONFIG_MSCHUNKS */
 extern long lmb_reserve(unsigned long, unsigned long);
 extern unsigned long lmb_alloc(unsigned long, unsigned long);
+extern unsigned long lmb_alloc_base(unsigned long, unsigned long, unsigned long);
 extern unsigned long lmb_phys_mem_size(void);
 extern unsigned long lmb_end_of_DRAM(void);
 extern unsigned long lmb_abs_to_phys(unsigned long);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/machdep.h linuxppc64_2_4/include/asm-ppc64/machdep.h
--- linux-2.4.19/include/asm-ppc64/machdep.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/machdep.h	Fri Aug  9 08:58:58 2002
@@ -10,7 +10,6 @@
  */
 
 #include <linux/config.h>
-#include <linux/seq_file.h>
 
 struct pt_regs;
 struct pci_bus;	
@@ -21,29 +20,24 @@
 struct rtc_time;
 
 struct machdep_calls {
-	/* High use functions in the first cachelines, low use functions
-	 * follow.  DRENG collect profile data.
-	 */
-	void            (*hpte_invalidate)(unsigned long slot);
-
-	void            (*hpte_updatepp)(long slot, 
+	void            (*hpte_invalidate)(unsigned long slot,
+					   unsigned long secondary,  
+					   unsigned long va, 
+					   int large, int local);
+	long            (*hpte_updatepp)(unsigned long slot,
+					 unsigned long secondary,  
 					 unsigned long newpp, 
-					 unsigned long va);
+					 unsigned long va,
+					 int large);
 	void            (*hpte_updateboltedpp)(unsigned long newpp, 
 					       unsigned long ea);
-	unsigned long	(*hpte_getword0)(unsigned long slot);
-
-	long		(*hpte_find)( unsigned long vpn );
-
-	long		(*hpte_selectslot)(unsigned long vpn); 
-
-	void		(*hpte_create_valid)(unsigned long slot,
-					     unsigned long vpn,
-					     unsigned long prpn,
-					     unsigned hash, 
-					     void * ptep,
-					     unsigned hpteflags, 
-					     unsigned bolted);
+	long		(*hpte_insert)(unsigned long vpn,
+				       unsigned long prpn,
+				       unsigned long hpteflags, 
+				       int bolted,
+				       int large);
+	long		(*hpte_remove)(unsigned long hpte_group);
+	
 	void		(*tce_build)(struct TceTable * tbl,
 				     long tcenum,
 				     unsigned long uaddr,
@@ -69,7 +63,6 @@
 	void		(*init_IRQ)(void);
 	void		(*init_ras_IRQ)(void);
 	int		(*get_irq)(struct pt_regs *);
-	void		(*post_irq)( struct pt_regs *, int );
 	
 	/* A general init function, called by ppc_init in init/main.c.
 	   May be NULL. */
@@ -87,6 +80,7 @@
 
   	void		(*progress)(char *, unsigned short);
 
+
 	unsigned char 	(*nvram_read_val)(int addr);
 	void		(*nvram_write_val)(int addr, unsigned char val);
 
@@ -145,6 +139,23 @@
 extern char cmd_line[512];
 
 extern void setup_pci_ptrs(void);
+
+
+/* Functions to produce codes on the leds.
+ * The SRC code should be unique for the message category and should
+ * be limited to the lower 24 bits (the upper 8 are set by these funcs),
+ * and (for boot & dump) should be sorted numerically in the order
+ * the events occur.
+ */
+/* Print a boot progress message. */
+void ppc64_boot_msg(unsigned int src, const char *msg);
+/* Print a termination message (print only -- does not stop the kernel) */
+void ppc64_terminate_msg(unsigned int src, const char *msg);
+/* Print something that needs attention (device error, etc) */
+void ppc64_attention_msg(unsigned int src, const char *msg);
+/* Print a dump progress message. */
+void ppc64_dump_msg(unsigned int src, const char *msg);
+
 
 #endif /* _PPC_MACHDEP_H */
 #endif /* __KERNEL__ */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/mmu.h linuxppc64_2_4/include/asm-ppc64/mmu.h
--- linux-2.4.19/include/asm-ppc64/mmu.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/mmu.h	Tue Aug 20 14:45:19 2002
@@ -107,7 +107,7 @@
 	unsigned long avpn:57; /* vsid | api == avpn  */
 	unsigned long :     2; /* Software use */
 	unsigned long bolted: 1; /* HPTE is "bolted" */
-	unsigned long :     1; /* Software use */
+	unsigned long lock: 1; /* lock on pSeries SMP */
 	unsigned long l:    1; /* Virtual page is large (L=1) or 4 KB (L=0) */
 	unsigned long h:    1; /* Hash function identifier */
 	unsigned long v:    1; /* Valid (v=1) or invalid (v=0) */
@@ -128,7 +128,7 @@
 
 typedef struct {
 	unsigned long pp0:  1; /* Page protection bit 0 */
-	unsigned long :     1; /* Reserved */
+	unsigned long ts:   1; /* Tag set bit */
 	unsigned long rpn: 50; /* Real page number */
 	unsigned long :     2; /* Reserved */
 	unsigned long ac:   1; /* Address compare */ 
@@ -156,21 +156,8 @@
 
 	union {
 		unsigned long dword1;
-		struct {
-			unsigned long pp0:  1; /* Page protection bit 0 */
-			unsigned long ts:   1; /* Tag set bit */ 
-			unsigned long rpn: 50; /* Real page number */
-			unsigned long :     2; /* Unused */
-			unsigned long ac:   1; /* Address compare bit */
-			unsigned long r:    1; /* Referenced */
-			unsigned long c:    1; /* Changed */
-			unsigned long w:    1; /* Write-thru cache mode */
-			unsigned long i:    1; /* Cache inhibited */
-			unsigned long m:    1; /* Memory coherence */
-			unsigned long g:    1; /* Guarded */
-			unsigned long n:    1; /* No-execute page if N=1 */
-			unsigned long pp:   2; /* Page protection bit 1:2 */
-		} dw1;
+		Hpte_dword1 dw1;
+		Hpte_dword1_flags flags;
 	} dw1;
 } HPTE; 
 
@@ -204,6 +191,8 @@
 #define PT_SHIFT (12)			/* Page Table */
 #define PT_MASK  0x02FF
 
+#define LARGE_PAGE_SHIFT 24
+
 static inline unsigned long hpt_hash(unsigned long vpn, int large)
 {
 	unsigned long vsid;
@@ -222,16 +211,24 @@
 
 #define PG_SHIFT (12)			/* Page Entry */
 
-extern __inline__ void _tlbie( unsigned long va )
+/*
+ * Invalidate a TLB entry.  Assumes a context syncronizing 
+ * instruction preceeded this call (for example taking the
+ * TLB lock).
+ */
+static inline void _tlbie(unsigned long va, int large)
 {
-	__asm__ __volatile__ ( " \n\
-		clrldi	%0,%0,16 \n\
-		ptesync		 \n\
-		tlbie	%0	 \n\
-		eieio		 \n\
-		tlbsync		 \n\
-		ptesync"
-		: : "r" (va) : "memory" );
+	asm volatile("ptesync": : :"memory");
+
+	if (large) {
+		asm volatile("clrldi	%0,%0,16\n\
+			      tlbie	%0,1" : : "r"(va) : "memory");
+	} else {
+		asm volatile("clrldi	%0,%0,16\n\
+			      tlbie	%0,0" : : "r"(va) : "memory");
+	}
+
+	asm volatile("eieio; tlbsync; ptesync": : :"memory");
 }
  
 #endif /* __ASSEMBLY__ */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/naca.h linuxppc64_2_4/include/asm-ppc64/naca.h
--- linux-2.4.19/include/asm-ppc64/naca.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/naca.h	Fri May 31 11:21:01 2002
@@ -13,27 +13,62 @@
 #include <asm/types.h>
 
 struct naca_struct {
-	void *xItVpdAreas;
-	void *xRamDisk;
-	u64 xRamDiskSize;		/* In pages */
-	struct paca_struct *paca;	/* Ptr to an array of pacas */
-	u64 debug_switch;		/* Bits to control debug printing */
-	u16 processorCount;		/* # of physical processors */
-	u16 dCacheL1LineSize;		/* Line size of L1 DCache in bytes */
-	u16 dCacheL1LogLineSize;	/* Log-2 of DCache line size */
-	u16 dCacheL1LinesPerPage;	/* DCache lines per page */
-	u16 iCacheL1LineSize;		/* Line size of L1 ICache in bytes */
-	u16 iCacheL1LogLineSize;	/* Log-2 of ICache line size */
-	u16 iCacheL1LinesPerPage;	/* ICache lines per page */
-	u16 slb_size;			/* SLB size in entries */
-	u64 physicalMemorySize;		/* Size of real memory in bytes */
-	u64 pftSize;			/* Log base 2 of page table size */
-	u64 serialPortAddr;		/* Phyical address of serial port */
-	u8 interrupt_controller;	/* Type of interrupt controller */ 
-	u8 resv0;    			/* Type of interrupt controller */
-	u16 platform;			/* Platform flags */
-	u8 resv1[12];			/* Padding */
+	/*==================================================================
+	 * Cache line 1: 0x0000 - 0x007F
+	 * Kernel only data - undefined for user space
+	 *==================================================================
+	 */
+	void *xItVpdAreas;              /* VPD Data                  0x00 */
+	void *xRamDisk;                 /* iSeries ramdisk           0x08 */
+	u64   xRamDiskSize;		/* In pages                  0x10 */
+	struct paca_struct *paca;	/* Ptr to an array of pacas  0x18 */
+	u64 debug_switch;		/* Debug print control       0x20 */
+	u64 banner;                     /* Ptr to banner string      0x28 */
+	u64 log;                        /* Ptr to log buffer         0x30 */
+	u64 serialPortAddr;		/* Phy addr of serial port   0x38 */
+	u64 interrupt_controller;	/* Type of int controller    0x40 */ 
+	u64 slb_size;			/* SLB size in entries       0x48 */
+	u64 pftSize;			/* Log 2 of page table size  0x50 */
+	u64 resv0[5];                   /* Reserved           0x58 - 0x7F */
+
+	/*==================================================================
+	 * Cache line 2: 0x0080 - 0x00FF
+	 * Kernel / User data
+	 *==================================================================
+	 */
+	u8  eye_catcher[6];             /* Eyecatcher: PPC64         0x00 */
+	u16 version;                    /* Version number            0x06 */
+	u16 platform;			/* Platform flags            0x08 */
+	u16 processor;			/* Processor type            0x0A */
+	u32 processorCount;		/* # of physical processors  0x0C */
+	u64 physicalMemorySize;		/* Size of real memory(B)    0x10 */
+
+	u16 dCacheL1Size;	        /* L1 d-cache size           0x18 */
+	u16 dCacheL1LineSize;		/* L1 d-cache line size      0x1A */
+	u16 dCacheL1LogLineSize;	/* L1 d-cache line size Log2 0x1C */
+	u16 dCacheL1LinesPerPage;	/* L1 d-cache lines / page   0x1E */
+	u16 dCacheL1Assoc;              /* L1 d-cache associativity  0x20 */
+
+	u16 iCacheL1Size;	        /* L1 i-cache size           0x22 */
+	u16 iCacheL1LineSize;		/* L1 i-cache line size      0x24 */
+	u16 iCacheL1LogLineSize;	/* L1 i-cache line size Log2 0x26 */
+	u16 iCacheL1LinesPerPage;	/* L1 i-cache lines / page   0x28 */
+	u16 iCacheL1Assoc;              /* L1 i-cache associativity  0x2A */
+
+	u16 cacheL2Size;	        /* L2 cache size             0x2C */
+	u16 cacheL2Assoc;	        /* L2 cache associativity    0x2E */
+
+	u64 tb_orig_stamp;              /* Timebase at boot          0x30 */
+	u64 tb_ticks_per_sec;           /* Timebase tics / sec       0x38 */
+	u64 tb_to_xs;                   /* Inverse of TB to 2^20     0x40 */
+	u64 stamp_xsec;                 /*                           0x48 */
+	volatile u64 tb_update_count;   /* Timebase atomicity ctr    0x50 */
+	u32 tz_minuteswest;             /* Minutes west of Greenwich 0x58 */
+	u32 tz_dsttime;                 /* Type of dst correction    0x5C */
+
+	u64 resv1[4];                   /* Reserverd          0x60 - 0x7F */
 };
+
 
 extern struct naca_struct *naca;
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/paca.h linuxppc64_2_4/include/asm-ppc64/paca.h
--- linux-2.4.19/include/asm-ppc64/paca.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/paca.h	Mon Aug 12 05:49:02 2002
@@ -84,8 +84,7 @@
 	u8 xSegments[STAB_CACHE_SIZE];	/* Cache of used stab entries		0x68,0x70 */
 	u8 xProcEnabled;		/* 1=soft enabled			0x78 */
 	u8 xHrdIntCount;		/* Count of active hardware interrupts  0x79  */
-	u8 prof_enabled;		/* 1=iSeries profiling enabled          0x7A */
-	u8 resv1[5];			/*					0x7B-0x7F */
+	u8 resv1[6];			/*					0x7B-0x7F */
 
 /*=====================================================================================
  * CACHE_LINE_2 0x0080 - 0x00FF
@@ -97,13 +96,7 @@
 	u64 pgtable_cache_sz;		/*					0x18 */
 	u64 next_jiffy_update_tb;	/* TB value for next jiffy update	0x20 */
 	u32 lpEvent_count;		/* lpEvents processed			0x28 */
-	u32 prof_multiplier;		/*					0x2C */
-	u32 prof_counter;		/*					0x30 */
-	u32 prof_shift;			/* iSeries shift for profile bucket size0x34 */
-	u32 *prof_buffer;		/* iSeries profiling buffer		0x38 */
-	u32 *prof_stext;		/* iSeries start of kernel text		0x40 */
-	u32 prof_len;			/* iSeries length of profile buffer -1	0x48 */
-	u8  rsvd2[128-76];		/*					0x4C */
+	u8  rsvd2[128-5*8-1*4];		/*					0x68 */
 
 /*=====================================================================================
  * CACHE_LINE_3 0x0100 - 0x017F
@@ -135,10 +128,29 @@
 	u8 rsvd5[256-16-sizeof(struct rtas_args)];
 
 /*=====================================================================================
- * CACHE_LINE_19-30 0x0800 - 0x0EFF Reserved
+ * CACHE_LINE_19 - 20 Profile Data
  *=====================================================================================
  */
-	u8 rsvd6[0x600];
+	u32 pmc[12];                    /* Default pmc value		*/	
+	u64 pmcc[8];                    /* Cumulative pmc counts        */
+	u64 rsvd5a[2];
+
+	u32 prof_multiplier;		/*					 */
+	u32 prof_shift;			/* iSeries shift for profile bucket size */
+	u32 *prof_buffer;		/* iSeries profiling buffer		 */
+	u32 *prof_stext;		/* iSeries start of kernel text		 */
+	u32 *prof_etext;		/* iSeries start of kernel text		 */
+	u32 prof_len;			/* iSeries length of profile buffer -1	 */
+	u8  prof_mode;                  /* */
+	u8  rsvv5b[3];
+	u64 prof_counter;		/*					 */
+	u8  rsvd5c[128-8*6];
+
+/*=====================================================================================
+ * CACHE_LINE_20-30
+ *=====================================================================================
+ */
+	u8 rsvd6[0x500];
 
 /*=====================================================================================
  * CACHE_LINE_31 0x0F00 - 0x0F7F Exception stack
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/page.h linuxppc64_2_4/include/asm-ppc64/page.h
--- linux-2.4.19/include/asm-ppc64/page.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/page.h	Wed May 29 08:54:50 2002
@@ -123,6 +123,14 @@
 	printk("kernel BUG at %s:%d!\n", __FILE__, __LINE__); \
 	xmon(0); \
 } while (0)
+#elif defined(CONFIG_KDB)
+#include <asm/ptrace.h>
+#include <linux/kdb.h>
+/* extern void kdb(kdb_reason_t reason, int error, kdb_eframe_t ef); */
+#define BUG() do { \
+      printk("kernel BUG at %s:%d!\n", __FILE__, __LINE__); \
+      kdb(KDB_REASON_OOPS, 0, (kdb_eframe_t) 0); \
+} while (0)
 #else
 #define BUG() do { \
 	printk("kernel BUG at %s:%d!\n", __FILE__, __LINE__); \
@@ -132,15 +140,8 @@
 
 #define PAGE_BUG(page) do { BUG(); } while (0)
 
-/*
- * XXX A bug in the current ppc64 compiler prevents an optimisation
- * where a divide is replaced by a multiply by shifted inverse. For
- * the moment use page->virtaul
- */
-#define WANT_PAGE_VIRTUAL 1
-
 /* Pure 2^n version of get_order */
-extern __inline__ int get_order(unsigned long size)
+static inline int get_order(unsigned long size)
 {
 	int order;
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/pci.h linuxppc64_2_4/include/asm-ppc64/pci.h
--- linux-2.4.19/include/asm-ppc64/pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/pci.h	Sat May 11 00:46:04 2002
@@ -25,12 +25,12 @@
 #define PCIBIOS_MIN_IO		0x1000
 #define PCIBIOS_MIN_MEM		0x10000000
 
-extern inline void pcibios_set_master(struct pci_dev *dev)
+static inline void pcibios_set_master(struct pci_dev *dev)
 {
 	/* No special bus mastering setup handling */
 }
 
-extern inline void pcibios_penalize_isa_irq(int irq)
+static inline void pcibios_penalize_isa_irq(int irq)
 {
 	/* We don't do dynamic PCI IRQ allocation */
 }
@@ -78,7 +78,7 @@
 
 extern void pSeries_pcibios_init_early(void);
 
-extern inline void pci_dma_sync_single(struct pci_dev *hwdev,
+static inline void pci_dma_sync_single(struct pci_dev *hwdev,
 				       dma_addr_t dma_handle,
 				       size_t size, int direction)
 {
@@ -87,7 +87,7 @@
 	/* nothing to do */
 }
 
-extern inline void pci_dma_sync_sg(struct pci_dev *hwdev,
+static inline void pci_dma_sync_sg(struct pci_dev *hwdev,
 				   struct scatterlist *sg,
 				   int nelems, int direction)
 {
@@ -101,7 +101,7 @@
  * only drive the low 24-bits during PCI bus mastering, then
  * you would pass 0x00ffffff as the mask to this function.
  */
-extern inline int pci_dma_supported(struct pci_dev *hwdev, u64 mask)
+static inline int pci_dma_supported(struct pci_dev *hwdev, u64 mask)
 {
 	return 1;
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/perfmon.h linuxppc64_2_4/include/asm-ppc64/perfmon.h
--- linux-2.4.19/include/asm-ppc64/perfmon.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/perfmon.h	Wed Jul  3 15:10:07 2002
@@ -0,0 +1,94 @@
+/*
+ * This file contains the code to configure and utilize the ppc64 pmc hardware
+ * Copyright (C) 2002 David Engebretsen <engebret@us.ibm.com>
+ */
+
+#ifndef __KERNEL__
+#define INLINE_SYSCALL(arg1, arg2)       \
+  ({                                            \
+    register long r0 __asm__ ("r0");     \
+    register long r3 __asm__ ("r3"); \
+    register long r4 __asm__ ("r4"); \
+    long ret, err;                              \
+    r0 = 208; \
+    r3 = (long) (arg1); \
+    r4 = (long) (arg2); \
+    __asm__ ("sc\n\t"                           \
+             "mfcr      %1\n\t"                 \
+             : "=r" (r3), "=r" (err)            \
+             : "r" (r0), "r" (r3), "r" (r4) \
+             : "cc", "memory");                 \
+    ret = r3;                                   \
+  })
+#endif
+
+#ifndef __ASSEMBLY__
+struct perfmon_base_struct {
+	u64 profile_buffer;
+	u64 profile_length;
+	u64 trace_buffer;
+	u64 trace_length;
+	u64 trace_end;
+	u64 state;
+};
+
+struct pmc_header {
+	int type;
+	int pid;
+	int resv[30];
+};
+
+struct pmc_struct {
+        int pmc[11];
+};
+
+struct pmc_info_struct {
+	unsigned int mode, cpu;
+
+	unsigned int  pmc_base[11];
+	unsigned long pmc_cumulative[8];
+};
+
+struct perfmon_struct {
+	struct pmc_header header;
+
+	union {
+		struct pmc_struct      pmc;
+		struct pmc_info_struct pmc_info;
+ 	} vdata;
+};
+
+enum {
+	PMC_OP_ALLOC         = 1,
+	PMC_OP_FREE          = 2,
+	PMC_OP_CLEAR         = 4,
+	PMC_OP_DUMP          = 5,
+	PMC_OP_DUMP_HARDWARE = 6,
+	PMC_OP_DECR_PROFILE  = 20,
+	PMC_OP_PMC_PROFILE   = 21,
+	PMC_OP_SET           = 30,
+	PMC_OP_SET_USER      = 31,
+	PMC_OP_END           = 30
+};
+
+
+#define	PMC_TRACE_CMD 0xFF
+
+enum {
+	PMC_TYPE_DERC_PROFILE  = 1,
+	PMC_TYPE_CYCLE         = 2,
+	PMC_TYPE_PROFILE       = 3,
+	PMC_TYPE_DCACHE        = 4,
+	PMC_TYPE_L2_MISS       = 5,
+	PMC_TYPE_LWARCX        = 6,
+	PMC_TYPE_END           = 6
+};
+#endif
+
+#define	PMC_STATE_INITIAL         0x00
+#define	PMC_STATE_READY           0x01
+#define	PMC_STATE_DECR_PROFILE    0x10
+#define	PMC_STATE_PROFILE_KERN    0x11
+#define	PMC_STATE_TRACE_KERN      0x20
+#define	PMC_STATE_TRACE_USER      0x21
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/pgtable.h linuxppc64_2_4/include/asm-ppc64/pgtable.h
--- linux-2.4.19/include/asm-ppc64/pgtable.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/pgtable.h	Tue Jul 23 02:38:54 2002
@@ -12,11 +12,6 @@
 #include <asm/page.h>
 #endif /* __ASSEMBLY__ */
 
-/* Certain architectures need to do special things when pte's
- * within a page table are directly modified.  Thus, the following
- * hook is made available.
- */
-
 /* PMD_SHIFT determines what a second-level page table entry can map */
 #define PMD_SHIFT	(PAGE_SHIFT + PAGE_SHIFT - 3)
 #define PMD_SIZE	(1UL << PMD_SHIFT)
@@ -232,47 +227,41 @@
 /* to find an entry in the ioremap page-table-directory */
 #define pgd_offset_i(address) (ioremap_pgd + pgd_index(address))
 
-/*
- * Given a pointer to an mem_map[] entry, return the kernel virtual
- * address corresponding to that page.
- */
-#define page_address(page) ((page)->virtual)
-
 #define pages_to_mb(x)		((x) >> (20-PAGE_SHIFT))
 
 /*
  * The following only work if pte_present() is true.
  * Undefined behaviour if not..
  */
-extern inline int pte_read(pte_t pte)  { return pte_val(pte) & _PAGE_USER;}
-extern inline int pte_write(pte_t pte) { return pte_val(pte) & _PAGE_RW;}
-extern inline int pte_exec(pte_t pte)  { return pte_val(pte) & _PAGE_EXEC;}
-extern inline int pte_dirty(pte_t pte) { return pte_val(pte) & _PAGE_DIRTY;}
-extern inline int pte_young(pte_t pte) { return pte_val(pte) & _PAGE_ACCESSED;}
+static inline int pte_read(pte_t pte)  { return pte_val(pte) & _PAGE_USER;}
+static inline int pte_write(pte_t pte) { return pte_val(pte) & _PAGE_RW;}
+static inline int pte_exec(pte_t pte)  { return pte_val(pte) & _PAGE_EXEC;}
+static inline int pte_dirty(pte_t pte) { return pte_val(pte) & _PAGE_DIRTY;}
+static inline int pte_young(pte_t pte) { return pte_val(pte) & _PAGE_ACCESSED;}
 
-extern inline void pte_uncache(pte_t pte) { pte_val(pte) |= _PAGE_NO_CACHE; }
-extern inline void pte_cache(pte_t pte)   { pte_val(pte) &= ~_PAGE_NO_CACHE; }
+static inline void pte_uncache(pte_t pte) { pte_val(pte) |= _PAGE_NO_CACHE; }
+static inline void pte_cache(pte_t pte)   { pte_val(pte) &= ~_PAGE_NO_CACHE; }
 
-extern inline pte_t pte_rdprotect(pte_t pte) {
+static inline pte_t pte_rdprotect(pte_t pte) {
 	pte_val(pte) &= ~_PAGE_USER; return pte; }
-extern inline pte_t pte_exprotect(pte_t pte) {
+static inline pte_t pte_exprotect(pte_t pte) {
 	pte_val(pte) &= ~_PAGE_EXEC; return pte; }
-extern inline pte_t pte_wrprotect(pte_t pte) {
+static inline pte_t pte_wrprotect(pte_t pte) {
 	pte_val(pte) &= ~(_PAGE_RW); return pte; }
-extern inline pte_t pte_mkclean(pte_t pte) {
+static inline pte_t pte_mkclean(pte_t pte) {
 	pte_val(pte) &= ~(_PAGE_DIRTY); return pte; }
-extern inline pte_t pte_mkold(pte_t pte) {
+static inline pte_t pte_mkold(pte_t pte) {
 	pte_val(pte) &= ~_PAGE_ACCESSED; return pte; }
 
-extern inline pte_t pte_mkread(pte_t pte) {
+static inline pte_t pte_mkread(pte_t pte) {
 	pte_val(pte) |= _PAGE_USER; return pte; }
-extern inline pte_t pte_mkexec(pte_t pte) {
+static inline pte_t pte_mkexec(pte_t pte) {
 	pte_val(pte) |= _PAGE_USER | _PAGE_EXEC; return pte; }
-extern inline pte_t pte_mkwrite(pte_t pte) {
+static inline pte_t pte_mkwrite(pte_t pte) {
 	pte_val(pte) |= _PAGE_RW; return pte; }
-extern inline pte_t pte_mkdirty(pte_t pte) {
+static inline pte_t pte_mkdirty(pte_t pte) {
 	pte_val(pte) |= _PAGE_DIRTY; return pte; }
-extern inline pte_t pte_mkyoung(pte_t pte) {
+static inline pte_t pte_mkyoung(pte_t pte) {
 	pte_val(pte) |= _PAGE_ACCESSED; return pte; }
 
 /* Atomic PTE updates */
@@ -349,8 +338,8 @@
 #define flush_tlb_page local_flush_tlb_page
 #define flush_tlb_range local_flush_tlb_range
 
-extern inline void flush_tlb_pgtables(struct mm_struct *mm,
-				unsigned long start, unsigned long end)
+static inline void flush_tlb_pgtables(struct mm_struct *mm,
+				      unsigned long start, unsigned long end)
 {
 	/* PPC has hw page tables. */
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/pmc.h linuxppc64_2_4/include/asm-ppc64/pmc.h
--- linux-2.4.19/include/asm-ppc64/pmc.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/pmc.h	Tue Jun 25 14:20:33 2002
@@ -98,18 +98,6 @@
 #define PMC_SW_SYSTEM(F)      do {;} while (0)
 #endif
 
-#define MMCR0 795
-#define MMCR1 798
-#define MMCRA 786
-#define PMC1  787
-#define PMC2  788
-#define PMC3  789
-#define PMC4  790
-#define PMC5  791
-#define PMC6  792
-#define PMC7  793
-#define PMC8  794
-
 #define PMC_CONTROL_CPI 1
 #define PMC_CONTROL_TLB 2
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/ppcdebug.h linuxppc64_2_4/include/asm-ppc64/ppcdebug.h
--- linux-2.4.19/include/asm-ppc64/ppcdebug.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/ppcdebug.h	Fri Aug 16 14:34:23 2002
@@ -51,6 +51,8 @@
 #define PPCDBG_SMP           PPCDBG_BITVAL(19)
 #define PPCDBG_BOOT          PPCDBG_BITVAL(20)
 #define PPCDBG_BUSWALK       PPCDBG_BITVAL(21)
+#define PPCDBG_PROM	     PPCDBG_BITVAL(22)
+#define PPCDBG_RTAS	     PPCDBG_BITVAL(23)
 #define PPCDBG_HTABSTRESS    PPCDBG_BITVAL(62)
 #define PPCDBG_HTABSIZE      PPCDBG_BITVAL(63)
 #define PPCDBG_NONE          (0UL)
@@ -74,7 +76,8 @@
 	"signal",	"signal_xmon",
 	"binfmt32",	"binfmt64",	"binfmt_xmon",	"binfmt_32addr",
 	"alignfixup",   "tceinit",      "tce",          "phb_init",     
-	"smp",          "boot",         "buswalk"
+	"smp",          "boot",         "buswalk",	"prom",
+	"rtas"
 };
 #else
 extern char *trace_names[64];
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/processor.h linuxppc64_2_4/include/asm-ppc64/processor.h
--- linux-2.4.19/include/asm-ppc64/processor.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/processor.h	Wed Jul 31 17:30:20 2002
@@ -18,6 +18,7 @@
 #endif
 #include <asm/ptrace.h>
 #include <asm/types.h>
+#include <asm/delay.h>
 
 /*
  * Default implementation of macro that returns current
@@ -97,7 +98,7 @@
 #define FPSCR_VX	0x20000000	/* Invalid operation summary */
 #define FPSCR_OX	0x10000000	/* Overflow exception summary */
 #define FPSCR_UX	0x08000000	/* Underflow exception summary */
-#define FPSCR_ZX	0x04000000	/* Zero-devide exception summary */
+#define FPSCR_ZX	0x04000000	/* Zero-divide exception summary */
 #define FPSCR_XX	0x02000000	/* Inexact exception summary */
 #define FPSCR_VXSNAN	0x01000000	/* Invalid op for SNaN */
 #define FPSCR_VXISI	0x00800000	/* Invalid op for Inv - Inv */
@@ -247,8 +248,6 @@
 #define	SPRN_IMMR	0x27E  	/* Internal Memory Map Register */
 #define	SPRN_L2CR	0x3F9	/* Level 2 Cache Control Regsiter */
 #define	SPRN_LR		0x008	/* Link Register */
-#define	SPRN_MMCR0	0x3B8	/* Monitor Mode Control Register 0 */
-#define	SPRN_MMCR1	0x3BC	/* Monitor Mode Control Register 1 */
 #define	SPRN_PBL1	0x3FC	/* Protection Bound Lower 1 */
 #define	SPRN_PBL2	0x3FE	/* Protection Bound Lower 2 */
 #define	SPRN_PBU1	0x3FD	/* Protection Bound Upper 1 */
@@ -256,18 +255,12 @@
 #define	SPRN_PID	0x3B1	/* Process ID */
 #define	SPRN_PIR	0x3FF	/* Processor Identification Register */
 #define	SPRN_PIT	0x3DB	/* Programmable Interval Timer */
-#define	SPRN_PMC1	0x3B9	/* Performance Counter Register 1 */
-#define	SPRN_PMC2	0x3BA	/* Performance Counter Register 2 */
-#define	SPRN_PMC3	0x3BD	/* Performance Counter Register 3 */
-#define	SPRN_PMC4	0x3BE	/* Performance Counter Register 4 */
 #define	SPRN_PVR	0x11F	/* Processor Version Register */
 #define	SPRN_RPA	0x3D6	/* Required Physical Address Register */
-#define	SPRN_SDA	0x3BF	/* Sampled Data Address Register */
 #define	SPRN_SDR1	0x019	/* MMU Hash Base Register */
 #define	SPRN_SGR	0x3B9	/* Storage Guarded Register */
 #define	  SGR_NORMAL		0
 #define	  SGR_GUARDED		1
-#define	SPRN_SIA	0x3BB	/* Sampled Instruction Address Register */
 #define	SPRN_SPRG0	0x110	/* Special Purpose Register General 0 */
 #define	SPRN_SPRG1	0x111	/* Special Purpose Register General 1 */
 #define	SPRN_SPRG2	0x112	/* Special Purpose Register General 2 */
@@ -324,13 +317,6 @@
 #define	    WRS_SYSTEM		3		/* WDT forced system reset */
 #define	  TSR_PIS		0x08000000	/* PIT Interrupt Status */
 #define	  TSR_FIS		0x04000000	/* FIT Interrupt Status */
-#define	SPRN_UMMCR0	0x3A8	/* User Monitor Mode Control Register 0 */
-#define	SPRN_UMMCR1	0x3AC	/* User Monitor Mode Control Register 0 */
-#define	SPRN_UPMC1	0x3A9	/* User Performance Counter Register 1 */
-#define	SPRN_UPMC2	0x3AA	/* User Performance Counter Register 2 */
-#define	SPRN_UPMC3	0x3AD	/* User Performance Counter Register 3 */
-#define	SPRN_UPMC4	0x3AE	/* User Performance Counter Register 4 */
-#define	SPRN_USIA	0x3AB	/* User Sampled Instruction Address Register */
 #define	SPRN_XER	0x001	/* Fixed Point Exception Register */
 #define	SPRN_ZPR	0x3B0	/* Zone Protection Register */
 
@@ -396,7 +382,19 @@
 #define	THRM2	SPRN_THRM2	/* Thermal Management Register 2 */
 #define	THRM3	SPRN_THRM3	/* Thermal Management Register 3 */
 #define	XER	SPRN_XER
-
+#define PMC1	0x313
+#define PMC2	0x314
+#define PMC3	0x315
+#define PMC4	0x316
+#define PMC5	0x317
+#define PMC6	0x318
+#define PMC7	0x319
+#define PMC8	0x31a
+#define MMCR0	0x31b
+#define MMCR1	0x31e
+#define MMCRA	0x312
+#define SIAR	0x30c
+#define SDAR	0x30d
 
 /* Device Control Registers */
 
@@ -483,10 +481,12 @@
 #define	PVR_REV(pvr)  (((pvr) >>   0) & 0xFFFF)	/* Revison field */
 
 /* Processor Version Numbers */
+#define	PV_NORTHSTAR	0x0033
 #define	PV_PULSAR	0x0034
 #define	PV_POWER4	0x0035
 #define	PV_ICESTAR	0x0036
 #define	PV_SSTAR	0x0037
+#define	PV_POWER4p	0x0038
 #define	PV_630        	0x0040
 #define	PV_630p	        0x0041
 
@@ -717,7 +717,7 @@
 #define init_task	(init_task_union.task)
 #define init_stack	(init_task_union.stack)
 
-#define cpu_relax()     do { } while (0)
+#define cpu_relax()     udelay(1)
 
 /*
  * Prefetch macros.
@@ -726,17 +726,29 @@
 #define ARCH_HAS_PREFETCHW
 #define ARCH_HAS_SPINLOCK_PREFETCH
 
-extern inline void prefetch(const void *x)
+static inline void prefetch(const void *x)
 {
 	__asm__ __volatile__ ("dcbt 0,%0" : : "r" (x));
 }
 
-extern inline void prefetchw(const void *x)
+static inline void prefetchw(const void *x)
 {
 	__asm__ __volatile__ ("dcbtst 0,%0" : : "r" (x));
 }
 
 #define spin_lock_prefetch(x)	prefetchw(x)
+
+#define cpu_has_largepage()	(__is_processor(PV_POWER4) || \
+				 __is_processor(PV_POWER4p))
+
+#define cpu_has_slb()		(__is_processor(PV_POWER4) || \
+				 __is_processor(PV_POWER4p))
+
+#define cpu_has_tlbiel()	(__is_processor(PV_POWER4) || \
+				 __is_processor(PV_POWER4p))
+
+#define cpu_has_noexecute()	(__is_processor(PV_POWER4) || \
+				 __is_processor(PV_POWER4p))
 
 #endif /* ASSEMBLY */
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/prom.h linuxppc64_2_4/include/asm-ppc64/prom.h
--- linux-2.4.19/include/asm-ppc64/prom.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/prom.h	Mon Aug 19 02:24:08 2002
@@ -128,7 +128,6 @@
 	int	busno;			/* for pci devices */
 	int	devfn;			/* for pci devices */
 	struct  pci_controller *phb;	/* for pci devices */
-	int	status;			/* current status of device */
 	struct	TceTable *tce_table;	/* for phb's or bridges */
 #define DN_STATUS_BIST_FAILED (1<<0)
 	struct	property *properties;
@@ -195,8 +194,5 @@
 extern int prom_n_addr_cells(struct device_node* np);
 extern int prom_n_size_cells(struct device_node* np);
 extern void prom_get_irq_senses(unsigned char *senses, int off, int max);
-extern void prom_drawstring(const char *c);
-extern void prom_drawhex(unsigned long v);
-extern void prom_drawchar(char c);
 
 #endif /* _PPC64_PROM_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/rtas.h linuxppc64_2_4/include/asm-ppc64/rtas.h
--- linux-2.4.19/include/asm-ppc64/rtas.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/rtas.h	Wed Jul 24 14:08:34 2002
@@ -2,6 +2,7 @@
 #define _PPC64_RTAS_H
 
 #include <linux/spinlock.h>
+#include <asm/page.h>
 
 /*
  * Definitions for talking to the RTAS on CHRP machines.
@@ -16,6 +17,8 @@
  */
 
 #define RTAS_UNKNOWN_SERVICE (-1)
+#define RTAS_INSTANTIATE_MAX (1UL<<30) /* Don't instantiate rtas at/above this value */
+
 /*
  * In general to call RTAS use rtas_token("string") to lookup
  * an RTAS token for the given string (e.g. "event-scan").
@@ -128,6 +131,29 @@
 	unsigned char buffer[1];		/* allocated by klimit bump */
 };
 
+struct flash_block {
+	char *data;
+	unsigned long length;
+};
+
+/* This struct is very similar but not identical to
+ * that needed by the rtas flash update.
+ * All we need to do for rtas is rewrite num_blocks
+ * into a version/length and translate the pointers
+ * to absolute.
+ */
+#define FLASH_BLOCKS_PER_NODE ((PAGE_SIZE - 16) / sizeof(struct flash_block))
+struct flash_block_list {
+	unsigned long num_blocks;
+	struct flash_block_list *next;
+	struct flash_block blocks[FLASH_BLOCKS_PER_NODE];
+};
+struct flash_block_list_header { /* just the header of flash_block_list */
+	unsigned long num_blocks;
+	struct flash_block_list *next;
+};
+extern struct flash_block_list_header rtas_firmware_flash_list;
+
 extern struct rtas_t rtas;
 
 extern void enter_rtas(struct rtas_args *);
@@ -139,5 +165,8 @@
 extern void rtas_restart(char *cmd);
 extern void rtas_power_off(void);
 extern void rtas_halt(void);
+
+extern struct proc_dir_entry *rtas_proc_dir;
+
 
 #endif /* _PPC64_RTAS_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/semaphore.h linuxppc64_2_4/include/asm-ppc64/semaphore.h
--- linux-2.4.19/include/asm-ppc64/semaphore.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/semaphore.h	Tue Aug 20 06:04:56 2002
@@ -2,14 +2,8 @@
 #define _PPC64_SEMAPHORE_H
 
 /*
- * Swiped from asm-sparc/semaphore.h and modified
- * -- Cort (cort@cs.nmt.edu)
- *
- * Stole some rw spinlock-based semaphore stuff from asm-alpha/semaphore.h
- * -- Ani Joshi (ajoshi@unixbox.com)
- *
  * Remove spinlock-based RW semaphores; RW semaphore definitions are
- * now in rwsem.h and we use the the generic lib/rwsem.c implementation.
+ * now in rwsem.h and we use the generic lib/rwsem.c implementation.
  * Rework semaphores to use atomic_dec_if_positive.
  * -- Paul Mackerras (paulus@samba.org)
  */
@@ -78,7 +72,7 @@
 extern int  __down_interruptible(struct semaphore * sem);
 extern void __up(struct semaphore * sem);
 
-extern inline void down(struct semaphore * sem)
+static inline void down(struct semaphore * sem)
 {
 #if WAITQUEUE_DEBUG
 	CHECK_MAGIC(sem->__magic);
@@ -92,7 +86,7 @@
 	smp_wmb();
 }
 
-extern inline int down_interruptible(struct semaphore * sem)
+static inline int down_interruptible(struct semaphore * sem)
 {
 	int ret = 0;
 
@@ -106,7 +100,7 @@
 	return ret;
 }
 
-extern inline int down_trylock(struct semaphore * sem)
+static inline int down_trylock(struct semaphore * sem)
 {
 	int ret;
 
@@ -119,7 +113,7 @@
 	return ret;
 }
 
-extern inline void up(struct semaphore * sem)
+static inline void up(struct semaphore * sem)
 {
 #if WAITQUEUE_DEBUG
 	CHECK_MAGIC(sem->__magic);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/siginfo.h linuxppc64_2_4/include/asm-ppc64/siginfo.h
--- linux-2.4.19/include/asm-ppc64/siginfo.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/siginfo.h	Sat May 11 00:46:04 2002
@@ -224,7 +224,7 @@
 #ifdef __KERNEL__
 #include <linux/string.h>
 
-extern inline void copy_siginfo(siginfo_t *to, siginfo_t *from)
+static inline void copy_siginfo(siginfo_t *to, siginfo_t *from)
 {
 	if (from->si_code < 0)
 		memcpy(to, from, sizeof(siginfo_t));
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/smplock.h linuxppc64_2_4/include/asm-ppc64/smplock.h
--- linux-2.4.19/include/asm-ppc64/smplock.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/smplock.h	Sat May 11 00:46:04 2002
@@ -43,13 +43,13 @@
  * so we only need to worry about other
  * CPU's.
  */
-extern __inline__ void lock_kernel(void)
+static inline void lock_kernel(void)
 {
 	if (!++current->lock_depth)
 		spin_lock(&kernel_flag);
 }
 
-extern __inline__ void unlock_kernel(void)
+static inline void unlock_kernel(void)
 {
 	if (--current->lock_depth < 0)
 		spin_unlock(&kernel_flag);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/softirq.h linuxppc64_2_4/include/asm-ppc64/softirq.h
--- linux-2.4.19/include/asm-ppc64/softirq.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/softirq.h	Tue Jul 23 02:37:23 2002
@@ -8,21 +8,19 @@
  * 2 of the License, or (at your option) any later version.
  */
 
-#include <asm/atomic.h>
 #include <asm/hardirq.h>
 
-
 #define local_bh_disable()	do { local_bh_count(smp_processor_id())++; barrier(); } while (0)
 #define __local_bh_enable()	do { barrier(); local_bh_count(smp_processor_id())--; } while (0)
 
 #define local_bh_enable()  \
 do {                                                    \
+	barrier();					\
         if (!--local_bh_count(smp_processor_id())       \
             && softirq_pending(smp_processor_id())) {   \
                 do_softirq();                           \
         }                                               \
 } while (0)
-
 
 #define in_softirq() (local_bh_count(smp_processor_id()) != 0)
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/system.h linuxppc64_2_4/include/asm-ppc64/system.h
--- linux-2.4.19/include/asm-ppc64/system.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/system.h	Tue Jul 23 02:37:23 2002
@@ -16,13 +16,6 @@
 #include <asm/memory.h>
 
 /*
- * System defines.
- */
-#define KERNEL_START_PHYS	0x800000 
-#define KERNEL_START	        (PAGE_OFFSET+KERNEL_START_PHYS)
-#define START_ADDR	        (PAGE_OFFSET+KERNEL_START_PHYS+0x00000)
-
-/*
  * Memory barrier.
  * The sync instruction guarantees that all memory accesses initiated
  * by this processor have been performed (with respect to all other
@@ -68,13 +61,8 @@
 extern int _get_PVR(void);
 extern long _get_L2CR(void);
 extern void _set_L2CR(unsigned long);
-extern void via_cuda_init(void);
-extern void pmac_nvram_init(void);
-extern void pmac_find_display(void);
 extern void giveup_fpu(struct task_struct *);
 extern void enable_kernel_fp(void);
-extern void giveup_altivec(struct task_struct *);
-extern void load_up_altivec(struct task_struct *);
 extern void cvt_fd(float *from, double *to, unsigned long *fpscr);
 extern void cvt_df(double *from, float *to, unsigned long *fpscr);
 extern int abs(int);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/time.h linuxppc64_2_4/include/asm-ppc64/time.h
--- linux-2.4.19/include/asm-ppc64/time.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/time.h	Fri May 31 10:41:01 2002
@@ -26,33 +26,12 @@
 extern unsigned long tb_ticks_per_usec;
 extern unsigned long tb_ticks_per_sec;
 extern unsigned long tb_to_xs;
-extern unsigned      tb_to_us;
 extern unsigned long tb_last_stamp;
 
 struct rtc_time;
 extern void to_tm(int tim, struct rtc_time * tm);
 extern time_t last_rtc_update;
 
-/*
- * By putting all of this stuff into a single struct we 
- * reduce the number of cache lines touched by do_gettimeofday.
- * Both by collecting all of the data in one cache line and
- * by touching only one TOC entry
- */
-struct gettimeofday_vars {
-	unsigned long tb_to_xs;
-	unsigned long stamp_xsec;
-};
-
-struct gettimeofday_struct {
-	unsigned long tb_orig_stamp;
-	unsigned long tb_ticks_per_sec;
-	struct gettimeofday_vars vars[2];
-	struct gettimeofday_vars * volatile varp;
-	unsigned      var_idx;
-	unsigned      tb_to_us;
-};
-
 struct div_result {
 	unsigned long result_high;
 	unsigned long result_low;
@@ -73,30 +52,28 @@
 
 static __inline__ void set_dec(int val)
 {
+#ifdef CONFIG_PPC_ISERIES
 	struct paca_struct *lpaca = get_paca();
 	int cur_dec;
 
-	if ( lpaca->xLpPaca.xSharedProc ) {
+	if (lpaca->xLpPaca.xSharedProc) {
 		lpaca->xLpPaca.xVirtualDecr = val;
 		cur_dec = get_dec();
-		if ( cur_dec > val )
+		if (cur_dec > val)
 			HvCall_setVirtualDecr();
-	} else {
+	} else
+#endif
 		mtspr(SPRN_DEC, val);
-	}
 }
 
-extern __inline__ unsigned long tb_ticks_since(unsigned long tstamp) {
+static inline unsigned long tb_ticks_since(unsigned long tstamp)
+{
 	return get_tb() - tstamp;
 }
 
-#define mulhwu(x,y) \
-({unsigned z; asm ("mulhwu %0,%1,%2" : "=r" (z) : "r" (x), "r" (y)); z;})
 #define mulhdu(x,y) \
 ({unsigned long z; asm ("mulhdu %0,%1,%2" : "=r" (z) : "r" (x), "r" (y)); z;})
 
-
-unsigned mulhwu_scale_factor(unsigned, unsigned);
 void div128_by_32( unsigned long dividend_high, unsigned long dividend_low,
 		   unsigned divisor, struct div_result *dr );
 #endif /* __KERNEL__ */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/uaccess.h linuxppc64_2_4/include/asm-ppc64/uaccess.h
--- linux-2.4.19/include/asm-ppc64/uaccess.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/uaccess.h	Tue Jul 23 02:37:23 2002
@@ -38,7 +38,7 @@
 #define __access_ok(addr,size) (__kernel_ok || __user_ok((addr),(size)))
 #define access_ok(type,addr,size) __access_ok((unsigned long)(addr),(size))
 
-extern inline int verify_area(int type, const void * addr, unsigned long size)
+static inline int verify_area(int type, const void * addr, unsigned long size)
 {
 	return access_ok(type,addr,size) ? 0 : -EFAULT;
 }
@@ -124,9 +124,6 @@
 	}							\
 } while (0)
 
-struct __large_struct { unsigned long buf[100]; };
-#define __m(x) (*(struct __large_struct *)(x))
-
 /*
  * We don't tell gcc that we are accessing memory, but this is OK
  * because we do not write to any memory gcc knows about, so there
@@ -200,7 +197,7 @@
 
 extern unsigned long __copy_tofrom_user(void *to, const void *from, unsigned long size);
 
-extern inline unsigned long
+static inline unsigned long
 copy_from_user(void *to, const void *from, unsigned long n)
 {
 	unsigned long over;
@@ -214,7 +211,7 @@
 	return n;
 }
 
-extern inline unsigned long
+static inline unsigned long
 copy_to_user(void *to, const void *from, unsigned long n)
 {
 	unsigned long over;
@@ -235,17 +232,21 @@
 
 extern unsigned long __clear_user(void *addr, unsigned long size);
 
-extern inline unsigned long
+static inline unsigned long
 clear_user(void *addr, unsigned long size)
 {
 	if (access_ok(VERIFY_WRITE, addr, size))
 		return __clear_user(addr, size);
-	return size? -EFAULT: 0;
+	if ((unsigned long)addr < TASK_SIZE) {
+		unsigned long over = (unsigned long)addr + size - TASK_SIZE;
+		return __clear_user(addr, size - over) + over;
+	}
+	return size;
 }
 
 extern int __strncpy_from_user(char *dst, const char *src, long count);
 
-extern inline long
+static inline long
 strncpy_from_user(char *dst, const char *src, long count)
 {
 	if (access_ok(VERIFY_READ, src, 1))
@@ -269,7 +270,7 @@
  * The `top' parameter to __strnlen_user is to make sure that
  * we can never overflow from the user area into kernel space.
  */
-extern __inline__ int strnlen_user(const char *str, long len)
+static inline int strnlen_user(const char *str, long len)
 {
 	unsigned long top = __kernel_ok? ~0UL: TASK_SIZE - 1;
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/udbg.h linuxppc64_2_4/include/asm-ppc64/udbg.h
--- linux-2.4.19/include/asm-ppc64/udbg.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/udbg.h	Thu Aug 22 11:27:57 2002
@@ -17,6 +17,8 @@
 void udbg_puts(const char *s);
 int udbg_write(const char *s, int n);
 int udbg_read(char *buf, int buflen);
+struct console;
+void udbg_console_write(struct console *con, const char *s, unsigned int n);
 void udbg_puthex(unsigned long val);
 void udbg_printSP(const char *s);
 void udbg_printf(const char *fmt, ...);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/user_exports.h linuxppc64_2_4/include/asm-ppc64/user_exports.h
--- linux-2.4.19/include/asm-ppc64/user_exports.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/asm-ppc64/user_exports.h	Fri May 31 14:16:20 2002
@@ -0,0 +1,82 @@
+#ifndef _USER_EXPORTS_H
+#define _USER_EXPORTS_H
+
+/* 
+ * Dave Engebretsen and Mike Corrigan {engebret|mikejc}@us.ibm.com
+ *   Copyright (C) 2002 Dave Engebretsen & Mike Corrigan
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+#ifdef __powerpc64__
+typedef unsigned long u64;
+#else
+typedef unsigned long long u64;
+#endif
+
+struct user_exports {
+	/*==================================================================
+	 * Cache line 1: 0x0000 - 0x007F
+	 * Kernel only data - undefined for user space
+	 *==================================================================
+	 */
+	u64 undefined[16]; 
+
+	/*==================================================================
+	 * Cache line 2: 0x0080 - 0x00FF
+	 * Kernel / User data
+	 *==================================================================
+	 */
+	u8  eye_catcher[6];      /* Eyecatcher: PPC64         0x00 */
+	u16 version;             /* Version number            0x06 */
+	u16 platform;	         /* Platform type             0x08 */
+	u16 processor;		 /* Processor type            0x0A */
+	u32 processorCount;	 /* # of physical processors  0x0C */
+	u64 physicalMemorySize;	 /* Size of real memory(B)    0x10 */
+
+	u16 dCacheL1Size;	 /* L1 d-cache size           0x18 */
+	u16 dCacheL1LineSize;	 /* L1 d-cache line size      0x1A */
+	u16 dCacheL1LogLineSize; /* L1 d-cache line size Log2 0x1C */
+	u16 dCacheL1LinesPerPage;/* L1 d-cache lines / page   0x1E */
+	u16 dCacheL1Assoc;       /* L1 d-cache associativity  0x20 */
+
+	u16 iCacheL1Size;	 /* L1 i-cache size           0x22 */
+	u16 iCacheL1LineSize;	 /* L1 i-cache line size      0x24 */
+	u16 iCacheL1LogLineSize; /* L1 i-cache line size Log2 0x26 */
+	u16 iCacheL1LinesPerPage;/* L1 i-cache lines / page   0x28 */
+	u16 iCacheL1Assoc;       /* L1 i-cache associativity  0x2A */
+
+	u16 cacheL2Size;	 /* L2 cache size             0x2C */
+	u16 cacheL2Assoc;	 /* L2 cache associativity    0x2E */
+
+	u64 tb_orig_stamp;       /* Timebase at boot          0x30 */
+	u64 tb_ticks_per_sec;    /* Timebase tics / sec       0x38 */
+	u64 tb_to_xs;            /* Inverse of TB to 2^20     0x40 */
+	u64 stamp_xsec;          /*                           0x48 */
+	volatile u64 tb_update_count; /* Timebase atomicity   0x50 */
+	u32 tz_minuteswest;      /* Minutes west of Greenwich 0x58 */
+	u32 tz_dsttime;          /* Type of dst correction    0x5C */
+
+	u64 resv1[4];            /* Reserverd          0x60 - 0x7F */
+};
+
+/* Platform types */
+#define PLATFORM_PSERIES      0x0100
+#define PLATFORM_PSERIES_LPAR 0x0101
+#define PLATFORM_ISERIES_LPAR 0x0201
+
+/* Processor types */
+#define PV_PULSAR       0x0034
+#define PV_POWER4       0x0035
+#define PV_ICESTAR      0x0036
+#define PV_SSTAR        0x0037
+#define PV_630          0x0040
+#define PV_630p         0x0041
+
+#endif /* USER_EXPORTS_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/asm-ppc64/vga.h linuxppc64_2_4/include/asm-ppc64/vga.h
--- linux-2.4.19/include/asm-ppc64/vga.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/asm-ppc64/vga.h	Sat May 11 00:46:04 2002
@@ -26,12 +26,12 @@
  *  <linux/vt_buffer.h> has already done the right job for us.
  */
 
-extern inline void scr_writew(u16 val, volatile u16 *addr)
+static inline void scr_writew(u16 val, volatile u16 *addr)
 {
     st_le16(addr, val);
 }
 
-extern inline u16 scr_readw(volatile const u16 *addr)
+static inline u16 scr_readw(volatile const u16 *addr)
 {
     return ld_le16(addr);
 }
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/blk.h linuxppc64_2_4/include/linux/blk.h
--- linux-2.4.19/include/linux/blk.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/linux/blk.h	Wed Jul  3 08:13:22 2002
@@ -313,6 +313,22 @@
 #define DEVICE_REQUEST i2ob_request
 #define DEVICE_NR(device) (MINOR(device)>>4)
 
+#elif (MAJOR_NR == VIODASD_MAJOR)
+
+#define DEVICE_NAME "viod"
+#define TIMEOUT_VALUE (25*HZ)
+#define DEVICE_REQUEST do_viodasd_request
+#define DEVICE_NR(device) (MINOR(device) >> 3)
+
+#elif (MAJOR_NR == VIOCD_MAJOR)
+
+#define DEVICE_NAME "viocd"
+#define TIMEOUT_VALUE (25*HZ)
+#define DEVICE_REQUEST do_viocd_request
+#define DEVICE_NR(device) (MINOR(device))
+#define DEVICE_ON(device)
+#define DEVICE_OFF(device)
+
 #elif (MAJOR_NR == COMPAQ_SMART2_MAJOR)
 
 #define DEVICE_NAME "ida"
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/dis-asm.h linuxppc64_2_4/include/linux/dis-asm.h
--- linux-2.4.19/include/linux/dis-asm.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/dis-asm.h	Tue May 28 16:13:07 2002
@@ -0,0 +1,307 @@
+/* Interface between the opcode library and its callers.
+   Written by Cygnus Support, 1993.
+
+   The opcode library (libopcodes.a) provides instruction decoders for
+   a large variety of instruction sets, callable with an identical
+   interface, for making instruction-processing programs more independent
+   of the instruction set being processed.  */
+
+/* Hacked by Scott Lurndal at SGI (02/1999) for linux kernel debugger */
+/* Upgraded to cygnus CVS Keith Owens <kaos@sgi.com> 30 Oct 2000 */
+
+#ifndef DIS_ASM_H
+#define DIS_ASM_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+	/*
+	 * Misc definitions
+	 */
+#ifndef PARAMS
+#define PARAMS(x)	x
+#endif
+#define PTR void *
+#define FILE int
+#if !defined(NULL)
+#define NULL 0
+#endif
+
+#define abort()		dis_abort(__LINE__)
+
+static inline void
+dis_abort(int line)
+{
+	panic("Aborting disassembler @ line %d\n", line);
+}
+
+#include <linux/slab.h>
+#include <asm/page.h>
+#define xstrdup(string) ({ char *res = kdb_strdup(string, GFP_ATOMIC); if (!res) BUG(); res; })
+#define xmalloc(size) ({ void *res = kmalloc(size, GFP_ATOMIC); if (!res) BUG(); res; })
+#define free(address) kfree(address)
+
+#include <bfd.h>
+
+typedef int (*fprintf_ftype) PARAMS((PTR, const char*, ...));
+
+enum dis_insn_type {
+  dis_noninsn,			/* Not a valid instruction */
+  dis_nonbranch,		/* Not a branch instruction */
+  dis_branch,			/* Unconditional branch */
+  dis_condbranch,		/* Conditional branch */
+  dis_jsr,			/* Jump to subroutine */
+  dis_condjsr,			/* Conditional jump to subroutine */
+  dis_dref,			/* Data reference instruction */
+  dis_dref2			/* Two data references in instruction */
+};
+
+/* This struct is passed into the instruction decoding routine, 
+   and is passed back out into each callback.  The various fields are used
+   for conveying information from your main routine into your callbacks,
+   for passing information into the instruction decoders (such as the
+   addresses of the callback functions), or for passing information
+   back from the instruction decoders to their callers.
+
+   It must be initialized before it is first passed; this can be done
+   by hand, or using one of the initialization macros below.  */
+
+typedef struct disassemble_info {
+  fprintf_ftype fprintf_func;
+  fprintf_ftype fprintf_dummy;
+  PTR stream;
+  PTR application_data;
+
+  /* Target description.  We could replace this with a pointer to the bfd,
+     but that would require one.  There currently isn't any such requirement
+     so to avoid introducing one we record these explicitly.  */
+  /* The bfd_flavour.  This can be bfd_target_unknown_flavour.  */
+  enum bfd_flavour flavour;
+  /* The bfd_arch value.  */
+  enum bfd_architecture arch;
+  /* The bfd_mach value.  */
+  unsigned long mach;
+  /* Endianness (for bi-endian cpus).  Mono-endian cpus can ignore this.  */
+  enum bfd_endian endian;
+
+  /* An array of pointers to symbols either at the location being disassembled
+     or at the start of the function being disassembled.  The array is sorted
+     so that the first symbol is intended to be the one used.  The others are
+     present for any misc. purposes.  This is not set reliably, but if it is
+     not NULL, it is correct.  */
+  asymbol **symbols;
+  /* Number of symbols in array.  */
+  int num_symbols;
+
+  /* For use by the disassembler.
+     The top 16 bits are reserved for public use (and are documented here).
+     The bottom 16 bits are for the internal use of the disassembler.  */
+  unsigned long flags;
+#define INSN_HAS_RELOC	0x80000000
+  PTR private_data;
+
+  /* Function used to get bytes to disassemble.  MEMADDR is the
+     address of the stuff to be disassembled, MYADDR is the address to
+     put the bytes in, and LENGTH is the number of bytes to read.
+     INFO is a pointer to this struct.
+     Returns an errno value or 0 for success.  */
+  int (*read_memory_func)
+    PARAMS ((bfd_vma memaddr, bfd_byte *myaddr, unsigned int length,
+	     struct disassemble_info *info));
+
+  /* Function which should be called if we get an error that we can't
+     recover from.  STATUS is the errno value from read_memory_func and
+     MEMADDR is the address that we were trying to read.  INFO is a
+     pointer to this struct.  */
+  void (*memory_error_func)
+    PARAMS ((int status, bfd_vma memaddr, struct disassemble_info *info));
+
+  /* Function called to print ADDR.  */
+  void (*print_address_func)
+    PARAMS ((bfd_vma addr, struct disassemble_info *info));
+
+  /* Function called to determine if there is a symbol at the given ADDR.
+     If there is, the function returns 1, otherwise it returns 0.
+     This is used by ports which support an overlay manager where
+     the overlay number is held in the top part of an address.  In
+     some circumstances we want to include the overlay number in the
+     address, (normally because there is a symbol associated with
+     that address), but sometimes we want to mask out the overlay bits.  */
+  int (* symbol_at_address_func)
+    PARAMS ((bfd_vma addr, struct disassemble_info * info));
+
+  /* These are for buffer_read_memory.  */
+  bfd_byte *buffer;
+  bfd_vma buffer_vma;
+  unsigned int buffer_length;
+
+  /* This variable may be set by the instruction decoder.  It suggests
+      the number of bytes objdump should display on a single line.  If
+      the instruction decoder sets this, it should always set it to
+      the same value in order to get reasonable looking output.  */
+  int bytes_per_line;
+
+  /* the next two variables control the way objdump displays the raw data */
+  /* For example, if bytes_per_line is 8 and bytes_per_chunk is 4, the */
+  /* output will look like this:
+     00:   00000000 00000000
+     with the chunks displayed according to "display_endian". */
+  int bytes_per_chunk;
+  enum bfd_endian display_endian;
+
+  /* Number of octets per incremented target address 
+     Normally one, but some DSPs have byte sizes of 16 or 32 bits
+   */
+  unsigned int octets_per_byte;
+
+  /* Results from instruction decoders.  Not all decoders yet support
+     this information.  This info is set each time an instruction is
+     decoded, and is only valid for the last such instruction.
+
+     To determine whether this decoder supports this information, set
+     insn_info_valid to 0, decode an instruction, then check it.  */
+
+  char insn_info_valid;		/* Branch info has been set. */
+  char branch_delay_insns;	/* How many sequential insn's will run before
+				   a branch takes effect.  (0 = normal) */
+  char data_size;		/* Size of data reference in insn, in bytes */
+  enum dis_insn_type insn_type;	/* Type of instruction */
+  bfd_vma target;		/* Target address of branch or dref, if known;
+				   zero if unknown.  */
+  bfd_vma target2;		/* Second target address for dref2 */
+
+  /* Command line options specific to the target disassembler.  */
+  char * disassembler_options;
+
+} disassemble_info;
+
+
+/* Standard disassemblers.  Disassemble one instruction at the given
+   target address.  Return number of bytes processed.  */
+typedef int (*disassembler_ftype)
+     PARAMS((bfd_vma, disassemble_info *));
+
+extern int print_insn_big_mips		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_little_mips	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_i386_att		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_i386_intel	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_ia64		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_i370		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_m68hc11		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_m68hc12		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_m68k		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_z8001		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_z8002		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_h8300		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_h8300h		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_h8300s		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_h8500		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_alpha		PARAMS ((bfd_vma, disassemble_info*));
+extern disassembler_ftype arc_get_disassembler PARAMS ((int, int));
+extern int print_insn_big_arm		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_little_arm	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_sparc		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_big_a29k		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_little_a29k	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_i860		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_i960		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_sh		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_shl		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_hppa		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_fr30		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_m32r		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_m88k		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_mcore		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_mn10200		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_mn10300		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_ns32k		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_big_powerpc	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_little_powerpc	PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_rs6000		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_w65		PARAMS ((bfd_vma, disassemble_info*));
+extern disassembler_ftype cris_get_disassembler PARAMS ((bfd *));
+extern int print_insn_d10v		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_d30v		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_v850		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_tic30		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_vax		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_tic54x		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_tic80		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_pj		PARAMS ((bfd_vma, disassemble_info*));
+extern int print_insn_avr		PARAMS ((bfd_vma, disassemble_info*));
+
+extern void print_arm_disassembler_options PARAMS ((FILE *));
+extern void parse_arm_disassembler_option  PARAMS ((char *));
+extern int  get_arm_regname_num_options    PARAMS ((void));
+extern int  set_arm_regname_option         PARAMS ((int));
+extern int  get_arm_regnames               PARAMS ((int, const char **, const char **, const char ***));
+
+/* Fetch the disassembler for a given BFD, if that support is available.  */
+extern disassembler_ftype disassembler	PARAMS ((bfd *));
+
+/* Document any target specific options available from the disassembler.  */
+extern void disassembler_usage          PARAMS ((FILE *));
+
+
+/* This block of definitions is for particular callers who read instructions
+   into a buffer before calling the instruction decoder.  */
+
+/* Here is a function which callers may wish to use for read_memory_func.
+   It gets bytes from a buffer.  */
+extern int buffer_read_memory
+  PARAMS ((bfd_vma, bfd_byte *, unsigned int, struct disassemble_info *));
+
+/* This function goes with buffer_read_memory.
+   It prints a message using info->fprintf_func and info->stream.  */
+extern void perror_memory PARAMS ((int, bfd_vma, struct disassemble_info *));
+
+
+/* Just print the address in hex.  This is included for completeness even
+   though both GDB and objdump provide their own (to print symbolic
+   addresses).  */
+extern void generic_print_address
+  PARAMS ((bfd_vma, struct disassemble_info *));
+
+/* Always true.  */
+extern int generic_symbol_at_address
+  PARAMS ((bfd_vma, struct disassemble_info *));
+
+/* Macro to initialize a disassemble_info struct.  This should be called
+   by all applications creating such a struct.  */
+#define INIT_DISASSEMBLE_INFO(INFO, STREAM, FPRINTF_FUNC) \
+  (INFO).flavour = bfd_target_unknown_flavour, \
+  (INFO).arch = bfd_arch_unknown, \
+  (INFO).mach = 0, \
+  (INFO).endian = BFD_ENDIAN_UNKNOWN, \
+  (INFO).octets_per_byte = 1, \
+  INIT_DISASSEMBLE_INFO_NO_ARCH(INFO, STREAM, FPRINTF_FUNC)
+
+/* Call this macro to initialize only the internal variables for the
+   disassembler.  Architecture dependent things such as byte order, or machine
+   variant are not touched by this macro.  This makes things much easier for
+   GDB which must initialize these things separately.  */
+
+#define INIT_DISASSEMBLE_INFO_NO_ARCH(INFO, STREAM, FPRINTF_FUNC) \
+  (INFO).fprintf_func = (fprintf_ftype)(FPRINTF_FUNC), \
+  (INFO).stream = (PTR)(STREAM), \
+  (INFO).symbols = NULL, \
+  (INFO).num_symbols = 0, \
+  (INFO).buffer = NULL, \
+  (INFO).buffer_vma = 0, \
+  (INFO).buffer_length = 0, \
+  (INFO).read_memory_func = buffer_read_memory, \
+  (INFO).memory_error_func = perror_memory, \
+  (INFO).print_address_func = generic_print_address, \
+  (INFO).symbol_at_address_func = generic_symbol_at_address, \
+  (INFO).flags = 0, \
+  (INFO).bytes_per_line = 0, \
+  (INFO).bytes_per_chunk = 0, \
+  (INFO).display_endian = BFD_ENDIAN_UNKNOWN, \
+  (INFO).insn_info_valid = 0
+
+#ifdef __cplusplus
+};
+#endif
+
+#endif /* ! defined (DIS_ASM_H) */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/dump.h linuxppc64_2_4/include/linux/dump.h
--- linux-2.4.19/include/linux/dump.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/dump.h	Mon Jul  8 14:13:43 2002
@@ -0,0 +1,376 @@
+/*
+ * Kernel header file for Linux crash dumps.
+ *
+ * Created by: Matt Robinson (yakker@sgi.com)
+ * Copyright 1999 - 2002 Silicon Graphics, Inc. All rights reserved.
+ *
+ * vmdump.h to dump.h by: Matt D. Robinson (yakker@sourceforge.net)
+ * Copyright 2001 - 2002 Matt D. Robinson.  All rights reserved.
+ * Copyright (C) 2002 Free Software Foundation, Inc. All rights reserved.
+ *
+ * Most of this is the same old stuff from vmdump.h, except now we're
+ * actually a stand-alone driver plugged into the block layer interface,
+ * with the exception that we now allow for compression modes externally
+ * loaded (e.g., someone can come up with their own).
+ *
+ * This code is released under version 2 of the GNU GPL.
+ */
+
+/* This header file includes all structure definitions for crash dumps. */
+#ifndef _DUMP_H
+#define _DUMP_H
+
+#include <linux/list.h>
+#include <linux/notifier.h>
+
+/* define TRUE and FALSE for use in our dump modules */
+#ifndef FALSE
+#define FALSE 0
+#endif
+
+#ifndef TRUE
+#define TRUE 1
+#endif
+
+
+/*
+ * DUMP_DEBUG: a debug level for the kernel dump code and
+ *             the supporting lkcd libraries in user space.
+ *
+ * 0: FALSE: No Debug Added
+ * 1: TRUE:  Break Points
+ * .
+ * .
+ * .
+ * 6: Add Debug Data to Structures
+ * .
+ * .
+ * 9: Max
+ */
+#define DUMP_DEBUG FALSE
+
+#if DUMP_DEBUG
+void dump_bp(void);			/* Called when something exceptional occures */
+#define DUMP_BP() dump_bp()			/* BreakPoint */
+#else
+#define DUMP_BP()
+#endif
+
+/* 
+ * Predefine default DUMP_PAGE constants, asm header may override.
+ *
+ * On ia64 discontinuous memory systems it's possible for the memory
+ * banks to stop at 2**12 page alignments, the smallest possible page
+ * size. But the system page size, PAGE_SIZE, is in fact larger.
+ */
+#define DUMP_PAGE_SHIFT 	PAGE_SHIFT
+#define DUMP_PAGE_SIZE		PAGE_SIZE
+#define DUMP_PAGE_MASK		PAGE_MASK
+#define DUMP_PAGE_ALIGN(addr)	PAGE_ALIGN(addr)
+#define DUMP_HEADER_OFFSET	PAGE_SIZE
+
+/* 
+ * Predefined default memcpy() to use when copying memory to the dump buffer.
+ *
+ * On ia64 there is a heads up function that can be called to let the prom
+ * machine check monitor know that the current activity is risky and it should
+ * ignore the fault (nofault). In this case the ia64 header will redefine this
+ * macro to __dump_memcpy() and use it's arch specific version.
+ */
+#define DUMP_memcpy		memcpy
+
+
+/* necessary header files */
+#include <asm/dump.h>                   /* for architecture-specific header */
+
+/* necessary header definitions in all cases */
+#define DUMP_KIOBUF_NUMBER  0xdeadbeef  /* special number for kiobuf maps   */
+
+/* 
+ * Size of the buffer that's used to hold:
+ *
+ *	1. the dump header (paded to fill the complete buffer)
+ *	2. the possibly compressed page headers and data
+ */
+#define DUMP_BUFFER_SIZE        (64 * 1024)  /* size of dump buffer (0x10000) */
+#define DUMP_HEADER_SIZE	 DUMP_BUFFER_SIZE
+
+/* header definitions for s390 dump */
+#define DUMP_MAGIC_S390     0xa8190173618f23fdULL  /* s390 magic number     */
+#define S390_DUMP_HEADER_SIZE     4096
+
+/* standard header definitions */
+#define DUMP_MAGIC_NUMBER   0xa8190173618f23edULL  /* dump magic number     */
+#define DUMP_MAGIC_LIVE     0xa8190173618f23cdULL  /* live magic number     */
+#define DUMP_VERSION_NUMBER   0x7       /* dump version number              */
+#define DUMP_PANIC_LEN        0x100     /* dump panic string length         */
+
+/* dump levels - type specific stuff added later -- add as necessary */
+#define DUMP_LEVEL_NONE        0x0      /* no dumping at all -- just bail   */
+#define DUMP_LEVEL_HEADER      0x1      /* kernel dump header only          */
+#define DUMP_LEVEL_KERN        0x2      /* dump header and kernel pages     */
+#define DUMP_LEVEL_USED        0x4      /* dump header, kernel/user pages   */
+#define DUMP_LEVEL_ALL_RAM     0x8      /* dump header, all RAM pages       */
+#define DUMP_LEVEL_ALL        0x10      /* dump all memory RAM and firmware */
+
+
+/* dump compression options -- add as necessary */
+#define DUMP_COMPRESS_NONE     0x0      /* don't compress this dump         */
+#define DUMP_COMPRESS_RLE      0x1      /* use RLE compression              */
+#define DUMP_COMPRESS_GZIP     0x2      /* use GZIP compression             */
+
+/* dump flags - any dump-type specific flags -- add as necessary */
+#define DUMP_FLAGS_NONE        0x0      /* no flags are set for this dump   */
+#define DUMP_FLAGS_NONDISRUPT  0x1      /* try to keep running after dump   */
+
+/* dump header flags -- add as necessary */
+#define DUMP_DH_FLAGS_NONE     0x0      /* no flags set (error condition!)  */
+#define DUMP_DH_RAW            0x1      /* raw page (no compression)        */
+#define DUMP_DH_COMPRESSED     0x2      /* page is compressed               */
+#define DUMP_DH_END            0x4      /* end marker on a full dump        */
+#define DUMP_DH_TRUNCATED      0x8	/* dump is incomplete               */
+#define DUMP_DH_TEST_PATTERN   0x10	/* dump page is a test pattern      */
+#define DUMP_DH_NOT_USED       0x20	/* 1st bit not used in flags        */
+
+/* names for various dump tunables (they are now all read-only) */
+#define DUMP_ROOT_NAME         "sys/dump"
+#define DUMP_DEVICE_NAME       "dump_device"
+#define DUMP_COMPRESS_NAME     "dump_compress"
+#define DUMP_LEVEL_NAME        "dump_level"
+#define DUMP_FLAGS_NAME        "dump_flags"
+
+/* page size for gzip compression -- buffered slightly beyond hardware PAGE_SIZE used by DUMP */
+#define DUMP_DPC_PAGE_SIZE     (DUMP_PAGE_SIZE + 512)
+
+/* dump ioctl() control options */
+#define DIOSDUMPDEV		1       /* set the dump device              */
+#define DIOGDUMPDEV		2       /* get the dump device              */
+#define DIOSDUMPLEVEL		3       /* set the dump level               */
+#define DIOGDUMPLEVEL		4       /* get the dump level               */
+#define DIOSDUMPFLAGS		5       /* set the dump flag parameters     */
+#define DIOGDUMPFLAGS		6       /* get the dump flag parameters     */
+#define DIOSDUMPCOMPRESS	7       /* set the dump compress level       */
+#define DIOGDUMPCOMPRESS	8       /* get the dump compress level        */
+#define DIODUMPTEST		99      /* test the dump facility (panic/dump) */
+
+/* the major number used for the dumping device */
+#ifndef DUMP_MAJOR
+#define DUMP_MAJOR              227
+#endif
+
+/*
+ * Structure: dump_header_t
+ *  Function: This is the header dumped at the top of every valid crash
+ *            dump.  
+ *            easy reassembly of each crash dump page.  The address bits
+ *            are split to make things easier for 64-bit/32-bit system
+ *            conversions.
+ */
+typedef struct _dump_header_s {
+	/* the dump magic number -- unique to verify dump is valid */
+	uint64_t             dh_magic_number;
+
+	/* the version number of this dump */
+	uint32_t             dh_version;
+
+	/* the size of this header (in case we can't read it) */
+	uint32_t             dh_header_size;
+
+	/* the level of this dump (just a header?) */
+	uint32_t             dh_dump_level;
+
+	/* 
+	 * The size of a hardware/physical memory page (DUMP_PAGE_SIZE).
+	 * NB: Not the configurable system page (PAGE_SIZE) (4K, 8K, 16K, etc.) 
+	 */
+	uint32_t             dh_dump_page_size;
+
+	/* the size of all physical memory */
+	uint64_t             dh_memory_size;
+
+	/* the start of physical memory */
+	uint64_t             dh_memory_start;
+
+	/* the end of physical memory */
+	uint64_t             dh_memory_end;
+
+#if DUMP_DEBUG >= 6
+	/* the number of bytes in this dump specifically */
+	uint64_t             dh_num_bytes;
+#endif
+
+	/* the number of hardware/physical pages in this dump specifically */
+	uint32_t             dh_num_dump_pages;
+
+	/* the panic string, if available */
+	char                 dh_panic_string[DUMP_PANIC_LEN];
+
+	/* the time of the system crash */
+	struct timeval       dh_time;
+
+	/* the NEW utsname (uname) information -- in character form */
+	/* we do this so we don't have to include utsname.h         */
+	/* plus it helps us be more architecture independent        */
+	/* now maybe one day soon they'll make the [65] a #define!  */
+	char                 dh_utsname_sysname[65];
+	char                 dh_utsname_nodename[65];
+	char                 dh_utsname_release[65];
+	char                 dh_utsname_version[65];
+	char                 dh_utsname_machine[65];
+	char                 dh_utsname_domainname[65];
+
+	/* the address of current task (OLD = task_struct *, NEW = void *) */
+	void                *dh_current_task;
+
+	/* what type of compression we're using in this dump (if any) */
+	uint32_t             dh_dump_compress;
+
+	/* any additional flags */
+	uint32_t             dh_dump_flags;
+
+	/* any additional flags */
+	uint32_t             dh_dump_device;
+
+} dump_header_t;
+
+/*
+ * Structure: dump_page_t
+ *  Function: To act as the header associated to each physical page of
+ *            memory saved in the system crash dump.  This allows for
+ *            easy reassembly of each crash dump page.  The address bits
+ *            are split to make things easier for 64-bit/32-bit system
+ *            conversions.
+ *
+ * dp_byte_offset and dp_page_index are landmarks that are helpfull when
+ * looking at a hexdump of /dev/vmdump,
+ */
+typedef struct _dump_page_s {
+
+#if DUMP_DEBUG >= 6
+	/* byte offset */
+	uint64_t		dp_byte_offset;
+
+	/* page index */
+	uint64_t		dp_page_index;
+#endif
+	/* the address of this dump page */
+	uint64_t             dp_address;
+
+	/* the size of this dump page */
+	uint32_t             dp_size;
+
+	/* flags (currently DUMP_COMPRESSED, DUMP_RAW or DUMP_END) */
+	uint32_t             dp_flags;
+} dump_page_t;
+
+/*
+ * This structure contains information needed for the lkcdutils
+ * package (particularly lcrash) to determine what information is
+ * associated to this kernel, specifically.
+ */
+typedef struct lkcdinfo_s {
+	int             arch;
+	int             ptrsz;
+	int             byte_order;
+	int             linux_release;
+	int             page_shift;
+	int             page_size;
+	uint64_t        page_mask;
+	uint64_t        page_offset;
+	int             stack_offset;
+} lkcdinfo_t;
+
+#ifdef __KERNEL__
+
+/*
+ * Structure: dump_compress_t
+ *  Function: This is what an individual compression mechanism can use
+ *            to plug in their own compression techniques.  It's always
+ *            best to build these as individual modules so that people
+ *            can put in whatever they want.
+ */
+typedef struct dump_compress_s {
+	/* the list_head structure for list storage */
+	struct list_head list;
+
+	/* the type of compression to use (DUMP_COMPRESS_XXX) */
+        int compress_type;
+
+	/* the compression function to call */
+        int (*compress_func)(char *, int, char *, int);
+} dump_compress_t;
+
+/* functions for dump compression registration */
+extern void dump_register_compression(dump_compress_t *);
+extern void dump_unregister_compression(int);
+
+/*
+ * Structure dump_mbank[]:
+ *
+ * For CONFIG_DISCONTIGMEM systems this array specifies the
+ * memory banks/chunks that need to be dumped after a panic.
+ *
+ * For clasic systems it specifies a single set of pages from
+ * 0 to max_mapnr.
+ */
+typedef struct dump_mbank {
+        u64 		start;
+        u64 		end;
+	int		type;
+	int		pad1;
+	long		pad2;
+} dump_mbank_t;
+
+#define DUMP_MBANK_TYPE_CONVENTIONAL_MEMORY		1
+#define DUMP_MBANK_TYPE_OTHER				2
+
+
+#define MAXCHUNKS 256
+extern int dump_mbanks;
+extern dump_mbank_t dump_mbank[MAXCHUNKS];
+
+extern struct notifier_block *dump_notifier_list;
+extern int register_dump_notifier(struct notifier_block *);
+extern int unregister_dump_notifier(struct notifier_block *);
+
+/* notification codes */
+#define DUMP_BEGIN	0x0001	/* Notify of dump beginning */
+#define DUMP_END	0x0002	/* Notify of dump ending */
+
+extern int dump_init(void);
+int dump_execute(char *, struct pt_regs *);
+extern volatile int dump_in_progress;
+extern volatile int dumping_cpu;
+extern int (*dump_function_ptr)(char *, struct pt_regs *);
+
+#if defined(CONFIG_X86) || defined(CONFIG_ALPHA)
+extern int page_is_ram(unsigned long);
+#endif
+
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+static inline void dump(char * str, struct pt_regs * regs)
+{
+	printk(KERN_ALERT "dump: dump_function_ptr=%p\n", dump_function_ptr);
+	if (dump_function_ptr) {
+		dump_function_ptr((char *)str, regs);
+	}
+}
+#else
+static inline void dump(char * str, struct pt_regs * regs)
+{
+}
+#endif /* CONFIG_DUMP */
+
+/*
+ * Common Arch Specific Functions should be declared here.
+ * This allows the C compiler to detect discrepancies.
+ */
+extern void 		__dump_open(void);
+extern void 		__dump_cleanup(void);
+extern void 		__dump_init(uint64_t);
+extern int 		__dump_configure_header(struct pt_regs *);
+extern unsigned int  	__dump_silence_system(unsigned int);
+extern unsigned int  	__dump_resume_system(unsigned int);
+#endif /* __KERNEL__ */
+
+#endif /* _DUMP_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/jfs_fs.h linuxppc64_2_4/include/linux/jfs_fs.h
--- linux-2.4.19/include/linux/jfs_fs.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/jfs_fs.h	Wed Nov 14 10:19:36 2001
@@ -0,0 +1,34 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/ 
+#ifndef _LINUX_JFS_FS_H
+#define _LINUX_JFS_FS_H
+
+#include <linux/version.h>
+
+#include <linux/jfs/jfs_types.h>
+#include <linux/jfs_fs_i.h>
+#include <linux/jfs_fs_sb.h>
+
+
+/* JFS magic number */
+
+#define JFS_SUPER_MAGIC 0x3153464a /* "JFS1" */
+
+#endif /* _LINUX_JFS_FS_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/jfs_fs_i.h linuxppc64_2_4/include/linux/jfs_fs_i.h
--- linux-2.4.19/include/linux/jfs_fs_i.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/jfs_fs_i.h	Wed Nov 14 10:19:36 2001
@@ -0,0 +1,80 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/ 
+#ifndef _JFS_FS_I
+#define _JFS_FS_I
+
+#include <linux/jfs/jfs_xtree.h>
+#include <linux/jfs/jfs_dtree.h>
+
+typedef struct	jfs_rwlock {
+	struct rw_semaphore rw_sem;
+	atomic_t in_use;	/* for hacked implementation of trylock */
+} jfs_rwlock_t;
+
+#define JFS_IP(ip)	((struct jfs_inode_info *)(ip)->u.generic_ip)
+
+struct jfs_inode_info {
+	int	fileset;	/* 4: fileset number (always 16)*/
+	uint	mode2;		/* 4: jfs-specific mode		*/
+        pxd_t   ixpxd;		/* 8: inode extent descriptor	*/
+	dxd_t	acl;		/* 16: dxd describing acl	*/
+	dxd_t	ea;		/* 16: dxd describing ea	*/
+	time_t	otime;		/* 4: time created	*/
+	uint	next_index;	/* 4: next available directory entry index */
+	int	acltype;	/* 4: Type of ACL	*/
+	short	btorder;	/* 2: access order	*/
+	short	btindex;	/* 2: btpage entry index*/
+	struct inode *ipimap;	/* 4: inode map			*/
+	ushort	flag;		/* 2: JFS in-memory flag*/
+	unchar	cflag;		/* 1: commit flags		*/
+	unchar	agno;		/* 1: ag number			*/
+	ushort	bxflag;		/* 2: xflag of pseudo buffer?	*/
+	short	blid;		/* 2: lid of pseudo buffer?	*/
+	ushort	atlhead;	/* 2: anonymous tlock list head	*/
+	ushort	atltail;	/* 2: anonymous tlock list tail	*/
+	struct inode *atlnext;	/* 4: next inode w/anonymous txn's */
+	struct inode *atlprev;	/* 4: previous inode w/anonymous txn's */
+	struct page *extent_page; /* 4: page containing extent  */
+	jfs_rwlock_t rdwrlock;	/* 12/20: read/write lock	*/
+	ushort	xtlid;		/* 2: lid of xtree lock on directory */
+	short	pad;		/* 2: pad			*/
+	union {
+		struct {
+			xtpage_t _xtroot;	/* 288: xtree root */
+			struct inomap *_imap;	/* 4: inode map header	*/
+		} file;
+		struct {
+			dir_table_slot_t _table[12]; /* 96: directory index */
+			dtroot_t _dtroot;	/* 288: dtree root */
+		} dir;
+		struct {
+			unchar _unused[16];	/* 16: */
+			dxd_t _dxd;		/* 16: */
+			unchar _inline[128];	/* 128: inline symlink */
+		} link;
+	} u;
+};
+#define i_xtroot u.file._xtroot
+#define i_imap u.file._imap
+#define i_dirtable u.dir._table
+#define i_dtroot u.dir._dtroot
+#define i_inline u.link._inline
+
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/jfs_fs_sb.h linuxppc64_2_4/include/linux/jfs_fs_sb.h
--- linux-2.4.19/include/linux/jfs_fs_sb.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/jfs_fs_sb.h	Wed Nov 14 10:19:36 2001
@@ -0,0 +1,53 @@
+/*
+ *
+ *   Copyright (c) International Business Machines  Corp., 2000
+ *
+ *   This program is free software;  you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or 
+ *   (at your option) any later version.
+ * 
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY;  without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+ *   the GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program;  if not, write to the Free Software 
+ *   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+*/ 
+#ifndef _JFS_FS_SB
+#define _JFS_FS_SB
+
+#define JFS_SBI(sb)	((struct jfs_sb_info *)(sb)->u.generic_sbp)
+
+struct jfs_sb_info {
+	unsigned long	mntflag;	/* 4: aggregate attributes	*/
+	struct inode	*ipbmap;	/* 4: block map inode		*/
+	struct inode	*ipaimap;	/* 4: aggregate inode map inode	*/
+	struct inode	*ipaimap2;	/* 4: secondary aimap inode	*/
+	struct inode	*ipimap;	/* 4: aggregate inode map inode	*/
+	struct jfs_log	*log;		/* 4: log			*/
+	short		bsize;		/* 2: logical block size	*/
+	short		l2bsize;	/* 2: log2 logical block size	*/
+	short		nbperpage;	/* 2: blocks per page		*/
+	short		l2nbperpage;	/* 2: log2 blocks per page	*/
+	short		l2niperblk;	/* 2: log2 inodes per page	*/
+	short		reserved;	/* 2: log2 inodes per page	*/
+	pxd_t		logpxd;		/* 8: pxd describing log	*/
+	pxd_t		ait2;		/* 8: pxd describing AIT copy	*/
+	/* Formerly in ipimap */
+	uint		gengen;		/* 4: inode generation generator*/
+	uint		inostamp;	/* 4: shows inode belongs to fileset*/
+
+        /* Formerly in ipbmap */
+	struct bmap	*bmap;		/* 4: incore bmap descriptor	*/
+	struct nls_table *nls_tab;	/* 4: current codepage		*/
+	struct inode	*direct_inode;	/* 4: inode for physical I/O	*/
+	struct address_space *direct_mapping; /* 4: mapping for physical I/O */
+};					/* (72)				*/
+
+#define isReadOnly(ip) ((JFS_SBI((ip)->i_sb)->log) ? 0 : 1)
+
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/kallsyms.h linuxppc64_2_4/include/linux/kallsyms.h
--- linux-2.4.19/include/linux/kallsyms.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/kallsyms.h	Tue May 28 16:13:07 2002
@@ -0,0 +1,139 @@
+/* kallsyms headers
+   Copyright 2000 Keith Owens <kaos@ocs.com.au>
+
+   This file is part of the Linux modutils.  It is exported to kernel
+   space so debuggers can access the kallsyms data.
+
+   The kallsyms data contains all the non-stack symbols from a kernel
+   or a module.  The kernel symbols are held between __start___kallsyms
+   and __stop___kallsyms.  The symbols for a module are accessed via
+   the struct module chain which is based at module_list.
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the
+   Free Software Foundation; either version 2 of the License, or (at your
+   option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software Foundation,
+   Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef MODUTILS_KALLSYMS_H
+#define MODUTILS_KALLSYMS_H 1
+
+/* Have to (re)define these ElfW entries here because external kallsyms
+ * code does not have access to modutils/include/obj.h.  This code is
+ * included from user spaces tools (modutils) and kernel, they need
+ * different includes.
+ */
+
+#ifndef ELFCLASS32
+#ifdef __KERNEL__
+#include <linux/elf.h>
+#else	/* __KERNEL__ */
+#include <elf.h>
+#endif	/* __KERNEL__ */
+#endif	/* ELFCLASS32 */
+
+#ifndef ELFCLASSM
+#define ELFCLASSM ELF_CLASS
+#endif
+
+#ifndef ElfW
+# if ELFCLASSM == ELFCLASS32
+#  define ElfW(x)  Elf32_ ## x
+#  define ELFW(x)  ELF32_ ## x
+# else
+#  define ElfW(x)  Elf64_ ## x
+#  define ELFW(x)  ELF64_ ## x
+# endif
+#endif
+
+/* Format of data in the kallsyms section.
+ * Most of the fields are small numbers but the total size and all
+ * offsets can be large so use the 32/64 bit types for these fields.
+ *
+ * Do not use sizeof() on these structures, modutils may be using extra
+ * fields.  Instead use the size fields in the header to access the
+ * other bits of data.
+ */  
+
+struct kallsyms_header {
+	int		size;		/* Size of this header */
+	ElfW(Word)	total_size;	/* Total size of kallsyms data */
+	int		sections;	/* Number of section entries */
+	ElfW(Off)	section_off;	/* Offset to first section entry */
+	int		section_size;	/* Size of one section entry */
+	int		symbols;	/* Number of symbol entries */
+	ElfW(Off)	symbol_off;	/* Offset to first symbol entry */
+	int		symbol_size;	/* Size of one symbol entry */
+	ElfW(Off)	string_off;	/* Offset to first string */
+	ElfW(Addr)	start;		/* Start address of first section */
+	ElfW(Addr)	end;		/* End address of last section */
+};
+
+struct kallsyms_section {
+	ElfW(Addr)	start;		/* Start address of section */
+	ElfW(Word)	size;		/* Size of this section */
+	ElfW(Off)	name_off;	/* Offset to section name */
+	ElfW(Word)	flags;		/* Flags from section */
+};
+
+struct kallsyms_symbol {
+	ElfW(Off)	section_off;	/* Offset to section that owns this symbol */
+	ElfW(Addr)	symbol_addr;	/* Address of symbol */
+	ElfW(Off)	name_off;	/* Offset to symbol name */
+};
+
+#define KALLSYMS_SEC_NAME "__kallsyms"
+#define KALLSYMS_IDX 2			/* obj_kallsyms creates kallsyms as section 2 */
+
+#define kallsyms_next_sec(h,s) \
+	((s) = (struct kallsyms_section *)((char *)(s) + (h)->section_size))
+#define kallsyms_next_sym(h,s) \
+	((s) = (struct kallsyms_symbol *)((char *)(s) + (h)->symbol_size))
+
+int kallsyms_symbol_to_address(
+	const char       *name,			/* Name to lookup */
+	unsigned long    *token,		/* Which module to start with */
+	const char      **mod_name,		/* Set to module name or "kernel" */
+	unsigned long    *mod_start,		/* Set to start address of module */
+	unsigned long    *mod_end,		/* Set to end address of module */
+	const char      **sec_name,		/* Set to section name */
+	unsigned long    *sec_start,		/* Set to start address of section */
+	unsigned long    *sec_end,		/* Set to end address of section */
+	const char      **sym_name,		/* Set to full symbol name */
+	unsigned long    *sym_start,		/* Set to start address of symbol */
+	unsigned long    *sym_end		/* Set to end address of symbol */
+	);
+
+int kallsyms_address_to_symbol(
+	unsigned long     address,		/* Address to lookup */
+	const char      **mod_name,		/* Set to module name */
+	unsigned long    *mod_start,		/* Set to start address of module */
+	unsigned long    *mod_end,		/* Set to end address of module */
+	const char      **sec_name,		/* Set to section name */
+	unsigned long    *sec_start,		/* Set to start address of section */
+	unsigned long    *sec_end,		/* Set to end address of section */
+	const char      **sym_name,		/* Set to full symbol name */
+	unsigned long    *sym_start,		/* Set to start address of symbol */
+	unsigned long    *sym_end		/* Set to end address of symbol */
+	);
+
+int kallsyms_sections(void *token,
+		      int (*callback)(void *,	/* token */
+		      	const char *,		/* module name */
+			const char *,		/* section name */
+			ElfW(Addr),		/* Section start */
+			ElfW(Addr),		/* Section end */
+			ElfW(Word)		/* Section flags */
+		      )
+		);
+
+#endif /* kallsyms.h */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/kdb.h linuxppc64_2_4/include/linux/kdb.h
--- linux-2.4.19/include/linux/kdb.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/kdb.h	Tue May 28 16:13:07 2002
@@ -0,0 +1,251 @@
+#ifndef _KDB_H
+#define _KDB_H
+
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ * Copyright (C) 2000 Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *	Stephane Eranian		2000/06/05
+ *		move to v1.2
+ *	Keith Owens			2000/09/16
+ *		KDB v1.4
+ *		kdb=on/off/early at boot, /proc/sys/kernel/kdb.
+ *		Env BTAPROMPT.
+ */
+
+#include <linux/config.h>
+#include <asm/kdb.h>
+
+#define KDB_MAJOR_VERSION	2
+#define KDB_MINOR_VERSION	1
+#define KDB_TEST_VERSION	""
+
+	/*
+	 * kdb_initial_cpu is initialized to -1, and is set to the cpu
+	 * number whenever the kernel debugger is entered.
+	 */
+extern volatile int kdb_initial_cpu;	
+#ifdef	CONFIG_KDB
+#define KDB_IS_RUNNING() (kdb_initial_cpu != -1)
+#else
+#define KDB_IS_RUNNING() (0)
+#endif	/* CONFIG_KDB */
+
+	/*
+	 * kdb_on
+	 *
+	 * 	Defines whether kdb is on or not.  Default value
+	 *	is set by CONFIG_KDB_OFF.  Boot with kdb=on/off
+	 *	or echo "[01]" > /proc/sys/kernel/kdb to change it.
+	 */
+extern int kdb_on;
+
+	/*
+	 * kdb_port is initialized to zero, and is set to the I/O port
+	 * address of the serial port when the console is setup in
+	 * serial_console_setup.
+	 */
+extern int kdb_port;
+
+	/*
+	 * kdb_diemsg
+	 *
+	 *	Contains a pointer to the last string supplied to the
+	 *	kernel 'die' panic function.
+	 */
+extern const char *kdb_diemsg;
+
+	/*
+	 * KDB_FLAG_EARLYKDB is set when the 'kdb' option is specified
+	 * as a boot parameter (e.g. via lilo).   It indicates that the
+	 * kernel debugger should be entered as soon as practical.
+	 */
+#define KDB_FLAG_EARLYKDB	0x00000001
+
+	/*
+	 * Internal debug flags
+	 */
+#define KDB_DEBUG_FLAG_BT	0x0001		/* Stack traceback debug */
+#define KDB_DEBUG_FLAG_BP	0x0002		/* Breakpoint subsystem debug */
+#define KDB_DEBUG_FLAG_LBR	0x0004		/* Print last branch register */
+#define KDB_DEBUG_FLAG_AR	0x0008		/* Activation record, generic */
+#define KDB_DEBUG_FLAG_ARA	0x0010		/* Activation record, arch specific */
+/*      KDB_DEBUG_FLAG_CALLBACK	0x0020		WAS Event callbacks to kdb */
+#define KDB_DEBUG_FLAG_STATE	0x0040		/* State flags */
+#define KDB_DEBUG_FLAG_MASK	0xffff		/* All debug flags */
+#define KDB_DEBUG_FLAG_SHIFT	16		/* Shift factor for dbflags */
+
+extern volatile int kdb_flags;			/* Global flags, see kdb_state for per cpu state */
+
+#define KDB_FLAG(flag)		(kdb_flags & KDB_FLAG_##flag)
+#define KDB_FLAG_SET(flag)	((void)(kdb_flags |= KDB_FLAG_##flag))
+#define KDB_FLAG_CLEAR(flag)	((void)(kdb_flags &= ~KDB_FLAG_##flag))
+#define KDB_DEBUG(flag)		(kdb_flags & (KDB_DEBUG_FLAG_##flag << KDB_DEBUG_FLAG_SHIFT))
+#define KDB_DEBUG_STATE(text,value)	if (KDB_DEBUG(STATE)) kdb_print_state(text, value)
+
+	/*
+	 * Per cpu kdb state.  A cpu can be under kdb control but outside kdb,
+	 * for example when doing single step.
+	 */
+volatile extern int kdb_state[ /*NR_CPUS*/ ];
+#define KDB_STATE_KDB		0x00000001	/* Cpu is inside kdb */
+#define KDB_STATE_LEAVING	0x00000002	/* Cpu is leaving kdb */
+#define KDB_STATE_CMD		0x00000004	/* Running a kdb command */
+#define KDB_STATE_KDB_CONTROL	0x00000008	/* This cpu is under kdb control */
+#define KDB_STATE_HOLD_CPU	0x00000010	/* Hold this cpu inside kdb */
+#define KDB_STATE_DOING_SS	0x00000020	/* Doing ss command */
+#define KDB_STATE_DOING_SSB	0x00000040	/* Doing ssb command, DOING_SS is also set */
+#define KDB_STATE_SSBPT		0x00000080	/* Install breakpoint after one ss, independent of DOING_SS */
+#define KDB_STATE_REENTRY	0x00000100	/* Valid re-entry into kdb */
+#define KDB_STATE_SUPPRESS	0x00000200	/* Suppress error messages */
+#define KDB_STATE_LONGJMP	0x00000400	/* longjmp() data is available */
+ /* Spare, was    NO_WATCHDOG	0x00000800 */
+#define KDB_STATE_PRINTF_LOCK	0x00001000	/* Holds kdb_printf lock */
+#define KDB_STATE_WAIT_IPI	0x00002000	/* Waiting for kdb_ipi() NMI */
+#define KDB_STATE_RECURSE	0x00004000	/* Recursive entry to kdb */
+#define KDB_STATE_IP_ADJUSTED	0x00008000	/* Restart IP has been adjusted */
+#define KDB_STATE_NO_BP_DELAY	0x00010000	/* No need to delay breakpoints */
+#define KDB_STATE_ARCH		0xff000000	/* Reserved for arch specific use */
+
+#define KDB_STATE_CPU(flag,cpu)		(kdb_state[cpu] & KDB_STATE_##flag)
+#define KDB_STATE_SET_CPU(flag,cpu)	((void)(kdb_state[cpu] |= KDB_STATE_##flag))
+#define KDB_STATE_CLEAR_CPU(flag,cpu)	((void)(kdb_state[cpu] &= ~KDB_STATE_##flag))
+
+#define KDB_STATE(flag)		KDB_STATE_CPU(flag,smp_processor_id())
+#define KDB_STATE_SET(flag)	KDB_STATE_SET_CPU(flag,smp_processor_id())
+#define KDB_STATE_CLEAR(flag)	KDB_STATE_CLEAR_CPU(flag,smp_processor_id())
+
+	/*
+	 * External entry point for the kernel debugger.  The pt_regs
+	 * at the time of entry are supplied along with the reason for
+	 * entry to the kernel debugger.
+	 */
+
+typedef enum {
+	KDB_REASON_CALL = 1,		/* Call kdb() directly - regs invalid */
+	KDB_REASON_FAULT,		/* Kernel fault - regs valid */
+	KDB_REASON_BREAK,		/* Breakpoint inst. - regs valid */
+	KDB_REASON_DEBUG,		/* Debug Fault - regs valid */
+	KDB_REASON_OOPS,		/* Kernel Oops - regs valid */
+	KDB_REASON_SWITCH,		/* CPU switch - regs valid*/
+	KDB_REASON_ENTER,		/* KDB_ENTER() trap/fault - regs valid */
+	KDB_REASON_KEYBOARD,		/* Keyboard entry - regs valid */
+	KDB_REASON_NMI,			/* Non-maskable interrupt; regs valid */
+	KDB_REASON_WATCHDOG,		/* Watchdog interrupt; regs valid */
+	KDB_REASON_RECURSE,		/* Recursive entry to kdb; regs probably valid */
+	KDB_REASON_SILENT,		/* Silent entry/exit to kdb; regs invalid */
+	KDB_REASON_PANIC,		/* From panic() routine; regs invalid */
+} kdb_reason_t;
+
+typedef enum {
+	KDB_REPEAT_NONE = 0,		/* Do not repeat this command */
+	KDB_REPEAT_NO_ARGS,		/* Repeat the command without arguments */
+	KDB_REPEAT_WITH_ARGS,		/* Repeat the command including its arguments */
+} kdb_repeat_t;
+
+#ifdef	CONFIG_KDB
+extern int   kdb(kdb_reason_t, int, kdb_eframe_t);
+#else
+#define kdb(reason,error_code,frame) (0)
+#endif
+
+typedef int (*kdb_func_t)(int, const char **, const char **, kdb_eframe_t);
+
+	/*
+	 * Symbol table format returned by kallsyms.
+	 */
+
+typedef struct __ksymtab {
+		unsigned long value;		/* Address of symbol */
+		const char *mod_name;		/* Module containing symbol or "kernel" */
+		unsigned long mod_start;
+		unsigned long mod_end;
+		const char *sec_name;		/* Section containing symbol */
+		unsigned long sec_start;
+		unsigned long sec_end;
+		const char *sym_name;		/* Full symbol name, including any version */
+		unsigned long sym_start;
+		unsigned long sym_end;
+		} kdb_symtab_t;
+
+	/*
+	 * Exported Symbols for kernel loadable modules to use.
+	 */
+extern int           kdb_register(char *, kdb_func_t, char *, char *, short);
+extern int           kdb_register_repeat(char *, kdb_func_t, char *, char *, short, kdb_repeat_t);
+extern int           kdb_unregister(char *);
+
+extern int	     kdb_getarea_size(void *, unsigned long, size_t);
+extern int	     kdb_putarea_size(unsigned long, void *, size_t);
+
+/* Like get_user and put_user, kdb_getarea and kdb_putarea take variable
+ * names, not pointers.  The underlying *_size functions take pointers.
+ */
+#define kdb_getarea(x,addr)	kdb_getarea_size(&(x), addr, sizeof((x)))
+#define kdb_putarea(addr,x)	kdb_putarea_size(addr, &(x), sizeof((x)))
+
+extern int	     kdb_getword(unsigned long *, unsigned long, size_t);
+extern int	     kdb_putword(unsigned long, unsigned long, size_t);
+
+extern int	     kdbgetularg(const char *, unsigned long *);
+extern char         *kdbgetenv(const char *);
+extern int	     kdbgetintenv(const char *, int *);
+extern int	     kdbgetaddrarg(int, const char**, int*, unsigned long *, 
+			           long *, char **, kdb_eframe_t);
+extern int	     kdbgetsymval(const char *, kdb_symtab_t *);
+extern int	     kdbnearsym(unsigned long, kdb_symtab_t *);
+extern void	     kdb_printf(const char *,...)
+		     __attribute__ ((format (printf, 1, 2)));
+extern void	     kdb_init(void);
+extern void	     kdb_symbol_print(kdb_machreg_t, const kdb_symtab_t *, unsigned int);
+extern char	    *kdb_read(char *buffer, size_t bufsize);
+extern char	    *kdb_strdup(const char *str, int type);
+
+#if defined(CONFIG_SMP)
+	/*
+	 * Kernel debugger non-maskable IPI handler.
+	 */
+extern int           kdb_ipi(kdb_eframe_t, void (*ack_interrupt)(void));
+extern void	     smp_kdb_stop(void);
+#else	/* CONFIG_SMP */
+#define	smp_kdb_stop()
+#endif	/* CONFIG_SMP */
+
+	/*
+	 * Interface from general kernel to enable any hardware
+	 * error reporting mechanisms.  Such as the Intel Machine
+	 * Check Architecture, for example.
+	 */
+extern void	     kdb_enablehwfault(void);
+
+	 /*
+	  * Determine if a kernel address is valid or not.
+	  */
+
+extern int kdb_vmlist_check(unsigned long, unsigned long);
+
+	 /*
+	  * Routine for debugging the debugger state.
+	  */
+
+extern void kdb_print_state(const char *, int);
+
+#endif	/* !_KDB_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/kdbprivate.h linuxppc64_2_4/include/linux/kdbprivate.h
--- linux-2.4.19/include/linux/kdbprivate.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/kdbprivate.h	Tue May 28 16:13:07 2002
@@ -0,0 +1,313 @@
+#ifndef _KDBPRIVATE_H
+#define _KDBPRIVATE_H
+
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/dis-asm.h>
+#include <asm/kdbprivate.h>
+
+#include <bfd.h>
+
+/*
+ * Kernel Debugger Error codes.  Must not overlap with command codes.
+ */
+
+#define KDB_NOTFOUND	(-1)
+#define KDB_ARGCOUNT	(-2)
+#define KDB_BADWIDTH	(-3)
+#define KDB_BADRADIX	(-4)
+#define KDB_NOTENV	(-5)
+#define KDB_NOENVVALUE	(-6)
+#define KDB_NOTIMP	(-7)
+#define KDB_ENVFULL	(-8)
+#define KDB_ENVBUFFULL	(-9 )
+#define KDB_TOOMANYBPT	(-10)
+#define KDB_TOOMANYDBREGS (-11)
+#define KDB_DUPBPT	(-12)
+#define KDB_BPTNOTFOUND	(-13)
+#define KDB_BADMODE	(-14)
+#define KDB_BADINT	(-15)
+#define KDB_INVADDRFMT  (-16)
+#define KDB_BADREG      (-17)
+#define KDB_BADCPUNUM   (-18)
+#define KDB_BADLENGTH	(-19)
+#define KDB_NOBP	(-20)
+#define KDB_BADADDR	(-21)
+
+/*
+ * Kernel Debugger Command codes.  Must not overlap with error codes.
+ */
+#define KDB_CMD_GO	(-1001)
+#define KDB_CMD_CPU	(-1002)
+#define KDB_CMD_SS	(-1003)
+#define KDB_CMD_SSB	(-1004)
+
+	/*
+	 * kdb_nextline
+	 *
+	 * 	Contains the current line number on the screen.  Used
+	 *	to handle the built-in pager (LINES env variable)
+	 */
+extern volatile int kdb_nextline;
+
+	/*
+	 * Breakpoint state
+	 *
+	 * 	Each active and inactive breakpoint is represented by
+	 * 	an instance of the following data structure.  
+	 */
+
+typedef struct _kdb_bp {
+	bfd_vma 	bp_addr;	/* Address breakpoint is present at */
+	kdb_machinst_t	bp_inst;	/* Replaced instruction */
+
+	unsigned int	bp_free:1;	/* This entry is available */
+
+	unsigned int	bp_enabled:1;	/* Breakpoint is active in register */
+	unsigned int	bp_global:1;	/* Global to all processors */
+
+	unsigned int	bp_hardtype:1;	/* Uses hardware register */
+	unsigned int	bp_forcehw:1;	/* Force hardware register */
+	unsigned int	bp_installed:1;	/* Breakpoint is installed */
+	unsigned int	bp_delay:1;	/* Do delayed bp handling */
+	unsigned int	bp_delayed:1;	/* Delayed breakpoint */
+
+	int		bp_cpu;		/* Cpu #  (if bp_global == 0) */
+	kdbhard_bp_t	bp_template;	/* Hardware breakpoint template */
+	kdbhard_bp_t	*bp_hard;	/* Hardware breakpoint structure */
+	int		bp_adjust;	/* Adjustment to PC for real instruction */
+} kdb_bp_t;
+
+	/*
+	 * Breakpoint handling subsystem global variables
+	 */
+extern kdb_bp_t		kdb_breakpoints[/* KDB_MAXBPT */];
+
+	/*
+	 * Breakpoint architecture dependent functions.  Must be provided
+	 * in some form for all architectures.
+	 */
+extern void 		kdba_initbp(void);
+extern void		kdba_printbp(kdb_bp_t *);
+extern void		kdba_printbpreg(kdbhard_bp_t *);
+extern kdbhard_bp_t	*kdba_allocbp(kdbhard_bp_t *, int *);
+extern void		kdba_freebp(kdbhard_bp_t *);
+extern int		kdba_parsebp(int, const char**, int *, kdb_bp_t*);
+extern char 		*kdba_bptype(kdbhard_bp_t *);
+extern void		kdba_setsinglestep(kdb_eframe_t);
+extern void		kdba_clearsinglestep(kdb_eframe_t);
+
+	/*
+	 * Adjust instruction pointer architecture dependent function.  Must be
+	 * provided in some form for all architectures.
+	 */
+extern void		kdba_adjust_ip(kdb_reason_t, int, kdb_eframe_t);
+
+	/*
+	 * KDB-only global function prototypes.
+	 */
+extern void	     kdb_id1(unsigned long);
+extern void	     kdb_id_init(void);
+
+	/*
+	 * Architecture dependent function to enable any
+	 * processor machine check exception handling modes.
+	 */
+extern void	     kdba_enable_mce(void);
+
+extern void	     kdba_enable_lbr(void);
+extern void	     kdba_disable_lbr(void);
+extern void	     kdba_print_lbr(void);
+
+	/*
+	 * Initialization functions.
+	 */
+extern void	     kdba_init(void);
+extern void	     kdb_io_init(void);
+
+	/*
+	 * Architecture specific function to read a string.
+	 */
+extern char *	     kdba_read(char *, size_t);
+
+	/*
+	 * Data for a single activation record on stack.
+	 */
+
+typedef struct __kdb_activation_record {
+	kdb_machreg_t	start;		/* -> start of activation record */
+	kdb_machreg_t	end;		/* -> end+1 of activation record */
+	kdb_machreg_t	ret;		/* Return address to caller */
+	kdb_machreg_t	oldfp;		/* Frame pointer for caller's frame */
+	kdb_machreg_t	fp;		/* Frame pointer for callee's frame */
+	kdb_machreg_t	arg0;		/* -> First argument on stack (in previous ar) */
+	unsigned long	locals;		/* Bytes allocated for local variables */
+	unsigned long	regs;		/* Bytes allocated for saved registers */
+	unsigned long	args;		/* Bytes allocated for arguments (in previous ar) */
+	unsigned long	setup;		/* Bytes allocated for setup data */
+} kdb_ar_t;
+
+	/* 
+	 * General Stack Traceback functions.
+	 */
+
+extern int	     kdb_get_next_ar(kdb_machreg_t, kdb_machreg_t,
+				     kdb_machreg_t, kdb_machreg_t,
+				     kdb_machreg_t,
+				     kdb_ar_t *, kdb_symtab_t *);
+
+	/* 
+	 * Architecture specific Stack Traceback functions.
+	 */
+
+struct task_struct;
+
+extern int	     kdba_bt_stack(struct pt_regs *, kdb_machreg_t *, 
+				   int, struct task_struct *);
+extern int	     kdba_bt_process(struct task_struct *, int);
+extern int	     kdba_prologue(const kdb_symtab_t *, kdb_machreg_t,
+				   kdb_machreg_t, kdb_machreg_t, kdb_machreg_t,
+				   int, kdb_ar_t *);
+	/*
+	 * KDB Command Table
+	 */
+
+typedef struct _kdbtab {
+        char    *cmd_name;		/* Command name */
+        kdb_func_t cmd_func;		/* Function to execute command */
+        char    *cmd_usage;		/* Usage String for this command */
+        char    *cmd_help;		/* Help message for this command */
+        short    cmd_flags;		/* Parsing flags */
+        short    cmd_minlen;		/* Minimum legal # command chars required */
+	kdb_repeat_t cmd_repeat;	/* Does command auto repeat on enter? */
+} kdbtab_t;
+
+	/*
+	 * External command function declarations
+	 */
+
+extern int kdb_id(int, const char **, const char **, kdb_eframe_t);
+extern int kdb_bp(int, const char **, const char **, kdb_eframe_t);
+extern int kdb_bc(int, const char **, const char **, kdb_eframe_t);
+extern int kdb_bt(int, const char **, const char **, kdb_eframe_t);
+extern int kdb_ss(int, const char **, const char **, kdb_eframe_t);
+
+	/*
+	 * External utility function declarations 
+	 */
+extern char* kdb_getstr(char *, size_t, char *);
+
+	/*
+	 * Register contents manipulation
+	 */
+extern int kdba_getregcontents(const char *, kdb_eframe_t, kdb_machreg_t *);
+extern int kdba_setregcontents(const char *, kdb_eframe_t, kdb_machreg_t);
+extern int kdba_dumpregs(struct pt_regs *, const char *, const char *);
+extern int kdba_setpc(kdb_eframe_t, kdb_machreg_t);
+extern kdb_machreg_t   kdba_getpc(kdb_eframe_t);
+
+	/*
+	 * Debug register handling. 
+	 */
+extern void kdba_installdbreg(kdb_bp_t*);
+extern void kdba_removedbreg(kdb_bp_t*);
+
+	/*
+	 * Breakpoint handling - External interfaces
+	 */
+extern void kdb_initbptab(void);
+extern void kdb_bp_install_global(kdb_eframe_t);
+extern void kdb_bp_install_local(kdb_eframe_t);
+extern void kdb_bp_remove_global(void);
+extern void kdb_bp_remove_local(void);
+
+	/*
+	 * Breakpoint handling - Internal to kdb_bp.c/kdba_bp.c
+	 */
+extern int kdba_installbp(kdb_eframe_t ef, kdb_bp_t *);
+extern int kdba_removebp(kdb_bp_t *);
+
+
+typedef enum {
+	KDB_DB_BPT,	/* Breakpoint */
+	KDB_DB_SS,	/* Single-step trap */
+	KDB_DB_SSB,	/* Single step to branch */
+	KDB_DB_SSBPT,	/* Single step over breakpoint */
+	KDB_DB_NOBPT	/* Spurious breakpoint */
+} kdb_dbtrap_t;
+
+extern kdb_dbtrap_t kdba_db_trap(kdb_eframe_t, int);	/* DEBUG trap/fault handler */
+extern kdb_dbtrap_t kdba_bp_trap(kdb_eframe_t, int);	/* Breakpoint trap/fault hdlr */
+
+	/*
+	 * Interrupt Handling
+	 */
+typedef int kdb_intstate_t;
+
+extern void kdba_disableint(kdb_intstate_t *);
+extern void kdba_restoreint(kdb_intstate_t *);
+
+	/*
+	 * SMP and process stack manipulation routines.
+	 */
+extern int	     kdba_ipi(kdb_eframe_t, void (*)(void));
+extern int	     kdba_main_loop(kdb_reason_t, kdb_reason_t, int, kdb_dbtrap_t, kdb_eframe_t);
+extern int           kdb_main_loop(kdb_reason_t, kdb_reason_t, int, kdb_dbtrap_t, kdb_eframe_t);
+
+	/*
+	 * General Disassembler interfaces
+	 */
+extern int kdb_dis_fprintf(PTR, const char *, ...) __attribute__ ((format (printf, 2, 3)));
+extern int kdb_dis_fprintf_dummy(PTR, const char *, ...) __attribute__ ((format (printf, 2, 3)));
+extern disassemble_info	kdb_di;
+
+	/*
+	 * Architecture Dependent Disassembler interfaces
+	 */
+extern void kdba_printaddress(kdb_machreg_t, disassemble_info *, int);
+extern int  kdba_id_printinsn(kdb_machreg_t, disassemble_info *);
+extern int  kdba_id_parsemode(const char *, disassemble_info*);
+extern void kdba_id_init(disassemble_info *);
+extern void kdba_check_pc(kdb_machreg_t *);
+
+	/*
+	 * Miscellaneous functions and data areas
+	 */
+#ifndef kdba_getcurrentframe
+extern int  kdba_getcurrentframe(kdb_eframe_t);
+#endif
+extern char *kdb_cmds[];
+
+	/*
+	 * Defines for kdb_symbol_print.
+	 */
+#define KDB_SP_SPACEB	0x0001		/* Space before string */
+#define KDB_SP_SPACEA	0x0002		/* Space after string */
+#define KDB_SP_PAREN	0x0004		/* Parenthesis around string */
+#define KDB_SP_VALUE	0x0008		/* Print the value of the address */
+#define KDB_SP_SYMSIZE	0x0010		/* Print the size of the symbol */
+#define KDB_SP_NEWLINE	0x0020		/* Newline after string */
+#define KDB_SP_DEFAULT (KDB_SP_VALUE|KDB_SP_PAREN)
+
+#endif	/* !_KDBPRIVATE_H */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/major.h linuxppc64_2_4/include/linux/major.h
--- linux-2.4.19/include/linux/major.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/linux/major.h	Wed Jul  3 08:13:22 2002
@@ -117,6 +117,9 @@
 #define COMPAQ_CISS_MAJOR6      110
 #define COMPAQ_CISS_MAJOR7      111
 
+#define VIODASD_MAJOR		112
+#define VIOCD_MAJOR	113
+
 #define ATARAID_MAJOR		114
 
 #define DASD_MAJOR      94	/* Official assignations from Peter */
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/pci.h linuxppc64_2_4/include/linux/pci.h
--- linux-2.4.19/include/linux/pci.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/linux/pci.h	Mon Apr 22 10:35:36 2002
@@ -416,10 +416,10 @@
 	void		*sysdata;	/* hook for sys-specific extension */
 	struct proc_dir_entry *procdir;	/* directory entry in /proc/bus/pci */
 
-	unsigned char	number;		/* bus number */
-	unsigned char	primary;	/* number of primary bridge */
-	unsigned char	secondary;	/* number of secondary bridge */
-	unsigned char	subordinate;	/* max number of subordinate buses */
+	unsigned int	number;		/* bus number */
+	unsigned int	primary;	/* number of primary bridge */
+	unsigned int	secondary;	/* number of secondary bridge */
+	unsigned int	subordinate;	/* max number of subordinate buses */
 
 	char		name[48];
 	unsigned short	vendor;
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/proc_fs.h linuxppc64_2_4/include/linux/proc_fs.h
--- linux-2.4.19/include/linux/proc_fs.h	Fri Aug  2 19:39:45 2002
+++ linuxppc64_2_4/include/linux/proc_fs.h	Mon Apr 22 10:35:36 2002
@@ -25,7 +25,11 @@
 /* Finally, the dynamically allocatable proc entries are reserved: */
 
 #define PROC_DYNAMIC_FIRST 4096
+#ifdef CONFIG_PPC64
+#define PROC_NDYNAMIC      16384
+#else
 #define PROC_NDYNAMIC      4096
+#endif
 
 #define PROC_SUPER_MAGIC 0x9fa0
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/sysctl.h linuxppc64_2_4/include/linux/sysctl.h
--- linux-2.4.19/include/linux/sysctl.h	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/include/linux/sysctl.h	Fri Jun 21 16:01:08 2002
@@ -124,6 +124,9 @@
 	KERN_CORE_USES_PID=52,		/* int: use core or core.%pid */
 	KERN_TAINTED=53,	/* int: various kernel tainted flags */
 	KERN_CADPID=54,		/* int: PID of the process to notify on CAD */
+#ifdef	CONFIG_KDB
+	KERN_KDB=55,		/* int: kdb on/off */
+#endif	/* CONFIG_KDB */
 };
 
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/include/linux/vethdevice.h linuxppc64_2_4/include/linux/vethdevice.h
--- linux-2.4.19/include/linux/vethdevice.h	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/include/linux/vethdevice.h	Tue Jun 19 11:06:08 2001
@@ -0,0 +1,16 @@
+/* File vethdevice.h created by Kyle A. Lucke on Wed Aug  9 2000. */
+
+/* Change Activity: */
+/* End Change Activity */
+
+#ifndef _LINUX_VETHDEVICE_H
+#define _LINUX_VETHDEVICE_H
+
+#include <linux/etherdevice.h>
+
+#ifdef __KERNEL__
+extern struct net_device	* init_vethdev(struct net_device *, int, int);
+#endif
+
+#endif /* _LINUX_VETHDEVICE_H */
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/init/do_mounts.c linuxppc64_2_4/init/do_mounts.c
--- linux-2.4.19/init/do_mounts.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/init/do_mounts.c	Thu Jul 18 09:03:56 2002
@@ -530,7 +530,8 @@
 
 #ifdef CONFIG_BLK_DEV_RAM
 	int in_fd, out_fd;
-	int nblocks, rd_blocks, devblocks, i;
+	int nblocks, devblocks, i;
+	unsigned long rd_blocks;
 	char *buf;
 	unsigned short rotate = 0;
 #if !defined(CONFIG_ARCH_S390) && !defined(CONFIG_PPC_ISERIES)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/init/kerntypes.c linuxppc64_2_4/init/kerntypes.c
--- linux-2.4.19/init/kerntypes.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/init/kerntypes.c	Fri Aug  9 12:47:05 2002
@@ -0,0 +1,32 @@
+/*
+ * kerntypes.c
+ *
+ * Copyright (C) 2000 Tom Morano (tjm@sgi.com) and
+ *                    Matt D. Robinson (yakker@alacritech.com)
+ *
+ * Dummy module that includes headers for all kernel types of interest. 
+ * The kernel type information is used by the lcrash utility when 
+ * analyzing system crash dumps or the live system. Using the type 
+ * information for the running system, rather than kernel header files,
+ * makes for a more flexible and robust analysis tool.
+ *
+ * This source code is released under version 2 of the GNU GPL.
+ */
+#ifndef __KERNEL__
+#define __KERNEL__
+#endif
+#include <linux/module.h>
+#include <linux/autoconf.h>
+#include <linux/mm.h>
+#include <linux/config.h>
+#include <linux/utsname.h>
+#if defined(CONFIG_DUMP)
+#include <linux/dump.h>
+#endif
+
+#ifndef __powerpc64__
+void
+kerntypes_dummy(void)
+{
+}
+#endif
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/init/main.c linuxppc64_2_4/init/main.c
--- linux-2.4.19/init/main.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/init/main.c	Fri Aug  9 12:47:05 2002
@@ -25,6 +25,9 @@
 #include <linux/blk.h>
 #include <linux/hdreg.h>
 #include <linux/iobuf.h>
+#if defined(CONFIG_DUMP)
+#include <linux/dump.h>
+#endif
 #include <linux/bootmem.h>
 #include <linux/tty.h>
 
@@ -69,6 +72,10 @@
 #include <asm/smp.h>
 #endif
 
+#ifdef	CONFIG_KDB
+#include <linux/kdb.h>
+#endif	/* CONFIG_KDB */
+
 /*
  * Versions of gcc older than that listed below may actually compile
  * and link okay, but the end product can have subtle run time bugs.
@@ -118,6 +125,16 @@
 
 int rows, cols;
 
+/*
+ * The kernel_magic value represents the address of _end, which allows
+ * namelist tools to "match" each other respectively.  That way a tool
+ * that looks at /dev/mem can verify that it is using the right System.map
+ * file -- if kernel_magic doesn't equal the namelist value of _end,
+ * something's wrong.
+ */
+extern unsigned long _end;
+unsigned long *kernel_magic = &_end;
+
 char *execute_command;
 
 static char * argv_init[MAX_INIT_ARGS+2] = { "init", NULL, };
@@ -157,7 +174,7 @@
    better than 1% */
 #define LPS_PREC 8
 
-void __init calibrate_delay(void)
+void __init do_calibrate_delay(void)
 {
 	unsigned long ticks, loopbit;
 	int lps_precision = LPS_PREC;
@@ -198,6 +215,8 @@
 		(loops_per_jiffy/(5000/HZ)) % 100);
 }
 
+void (*calibrate_delay)(void) = do_calibrate_delay;
+
 static int __init debug_kernel(char *str)
 {
 	if (*str)
@@ -251,6 +270,34 @@
                 }
                 if (next != NULL)
                         *next++ = 0;
+#ifdef	CONFIG_KDB
+		/* kdb, kdb=on, kdb=off, kdb=early */
+		if (strncmp(line, "kdb", 3) == 0) {
+			if (line[3] == '\0') {
+				/* Backward compatibility, kdb with no option means early activation */
+				printk("Boot flag kdb with no options is obsolete, use kdb=early\n");
+				kdb_on = 1;
+				kdb_flags |= KDB_FLAG_EARLYKDB;
+				continue;
+			}
+			if (line[3] == '=') {
+				if (strcmp(line+4, "on") == 0) {
+					kdb_on = 1;
+					continue;
+				}
+				if (strcmp(line+4, "off") == 0) {
+					kdb_on = 0;
+					continue;
+				}
+				if (strcmp(line+4, "early") == 0) {
+					kdb_on = 1;
+					kdb_flags |= KDB_FLAG_EARLYKDB;
+					continue;
+				}
+				printk("Boot flag %s not recognised, assumed to be environment variable\n", line);
+			}
+		}
+#endif	/* CONFIG_KDB */
 		if (!strncmp(line,"init=",5)) {
 			line += 5;
 			execute_command = line;
@@ -295,7 +342,7 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 static void __init smp_init(void)
 {
-	APIC_init_uniprocessor();
+		APIC_init_uniprocessor();
 }
 #else
 #define smp_init()	do { } while (0)
@@ -398,6 +445,15 @@
 	kmem_cache_sizes_init();
 	pgtable_cache_init();
 
+
+#ifdef        CONFIG_KDB
+	kdb_init();
+	if (KDB_FLAG(EARLYKDB)) {
+	KDB_ENTER();
+      }
+#endif        /* CONFIG_KDB */
+
+
 	/*
 	 * For architectures that have highmem, num_mappedpages represents
 	 * the amount of memory the kernel can use.  For other architectures
@@ -422,6 +478,9 @@
 #endif
 #if defined(CONFIG_SYSVIPC)
 	ipc_init();
+#endif
+#if defined(CONFIG_DUMP)
+	dump_init();
 #endif
 	check_bugs();
 	printk("POSIX conformance testing by UNIFIX\n");
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/ChangeLog linuxppc64_2_4/kdb/ChangeLog
--- linux-2.4.19/kdb/ChangeLog	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/ChangeLog	Tue May 28 16:13:07 2002
@@ -0,0 +1,485 @@
+2002-03-01 Keith Owens <kaos@sgi.com>
+
+	* Sync with XFS 2.4.18.
+	* kdb v2.1-2.4.18-common-2.
+
+2002-02-26 Keith Owens <kaos@sgi.com>
+
+	* Upgrade to 2.4.18.
+	* Add Paul Dorwin (IBM) magicpoint slides on using kdb as
+	  Documentation/kdb/slides.
+	* kdb v2.1-2.4.18-common-1.
+
+2002-01-23 Keith Owens <kaos@sgi.com>
+
+	* Sync with XFS pagebuf changes.
+	* kdb v2.1-2.4.17-common-2.
+
+2002-01-18 Keith Owens <kaos@sgi.com>
+
+	* Ignore single stepping during panic.
+	* Remove kdba_getword, kdba_putword.  Replace with kdb_getword,
+	  kdb_putword that rely on copy_xx_user.  The new functions return
+	  an error code, like copy_xx_user.
+	* New functions kdb_getarea, kdb_putarea for copying areas of data
+	  such as structures.  These functions also return an error code.
+	* Change all common code to use the new functions.
+	* bp command checks that it can read and write the word at the
+	  breakpoint before accepting the address.
+	* Break points are now set FIFO and cleared LIFO so overlapping
+	  entries give sensible results.
+	* Verify address before disassembling code.
+	* Common changes for sparc64.  Ethan Solomita, Tom Duffy.
+	* Remove ss <count>, never supported.
+	* Remove kallsyms entries from arch vmlinux.lds files.
+	* Specify which commands auto repeat.
+	* kdb v2.1-2.4.17-common-1.
+
+2002-01-07 Keith Owens <kaos@sgi.com>
+
+	* Remove console semaphore code, not good in interrupt.
+	* Remove fragment of ia64 patch that had crept into kdb.
+	* Release as kdb v2.0-2.4.17-common-3.
+
+2002-01-04 Keith Owens  <kaos@sgi.com>
+
+	* Sync xfs <-> kdb common code.
+
+2001-12-22 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to 2.4.17.
+	* Clean up ifdef CONFIG_KDB.
+	* Add ifdef CONFIG_KDB around include kdb.h.
+	* Delete dummy kdb.h files for unsupported architectures.
+	* Delete arch i386 and ia64 specific files.  This changelog now
+	  applies to kdb common code only.
+	* Release as kdb v2.0-2.4.17-common-1.
+
+2001-12-03 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to 2.4.16.
+	* Add include/asm-um/kdb.h stub to allow XFS to be tested under UML.
+	* Check if an interrupt frame on i386 came from user space.
+	* Out of scope bug fix in kdb_id.c.  Ethan Solomita.
+	* Changes to common code to support sparc64.  Ethan Solomita.
+	* Change GFP_KERNEL to GFP_ATOMIC in disasm.  Ethan Solomita.
+
+2001-11-16 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to 2.4.15-pre5.
+	* Wrap () around #define expressions with unary operators.
+
+2001-11-13 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to 2.4.15-pre4.
+	* kbdm_pg.c patch from Hugh Dickins.
+
+2001-11-07 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to 2.4.14-ia64-011105.
+	* Change name of l1 serial I/O routine, add ia64 init command.  SGI.
+	* Sync kdbm_pg with XFS.
+
+2001-11-06 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to kernel 2.4.14.
+
+2001-11-02 Keith Owens  <kaos@sgi.com>
+
+	* Sync kdbm_pg.c with XFS.
+
+2001-10-24 Keith Owens  <kaos@sgi.com>
+
+	* Upgrade to kernel 2.4.13.
+
+2001-10-14 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* More use of TMPPREFIX in top level Makefile to speed up NFS compiles.
+
+	* Correct repeat calculations in md/mds commands.
+
+2001-10-10 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Copy bfd.h and ansidecl.h to arch/$(ARCH)/kdb, remove dependecies on
+	  user space includes.
+
+	* Update kdb v1.9 to kernel 2.4.11.
+
+2001-10-01 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update kdb v1.9 to kernel 2.4.11-pre1 and 2.4.10-ac1.
+
+	* Correct loop in kdb_parse, reported by Tachino Nobuhiro.
+
+2001-09-25 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update kdb v1.8 to kernel 2.4.10.
+
+	* kdbm_pg patch from Hugh Dickens.
+
+	* DProbes patch from Bharata B Rao.
+
+	* mdWcn and mmW patch from Vamsi Krishna S.
+
+	* i386 disasm layout patch from Jean-Marc Saffroy.
+
+	* Work around for 64 bit binutils, Simon Munton.
+
+	* kdb.mm doc correction by Chris Pascoe.
+
+	* Enter repeats the last command, IA64 disasm only prints one
+	  instruction.  Don Dugger.
+
+	* Allow kdb/modules to be linked into vmlinux.
+
+	* Remove obsolete code from kdb/modules/kdbm_{pg,vm}.c.
+
+	* Warn when commands are entered at more prompt.
+
+	* Add MODULE_AUTHOR, DESCRIPTION, LICENSE.
+
+	* Release as kdb v1.9.
+
+2001-02-27 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update kdb v1.8 to kernel 2.4.2, sync kdb/modules with XFS.
+
+	* Hook into panic() call.
+
+2000-12-18 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update kdb v1.7 to kernel 2.4.0-test13-pre3, sync kdb/modules with
+	XFS.
+
+2000-11-18 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update to kernel 2.4.0-test11-pre7, including forward port of
+	bug fixes from WIP 2.4.0-test9 tree.
+
+	* Update to Cygnus CVS trees for disassembly code.
+
+	* Bump to kdb v1.6.
+
+2000-10-19 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Update to kernel 2.4.0-test10-pre4.
+
+2000-10-15 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* kdb/kdbmain.c (kdb_parse): Correctly handle blank input.
+
+	* kdb/kdbmain.c (kdb_local, kdb): Reason SILENT can have NULL ef.
+
+2000-10-13 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* kdb/kdbmain.c: Reduce CMD_LEN to avoid overflowing kdb_printf buffer.
+
+2000-10-11 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* kdb/kdbmain.c (kdb): Test for userspace breakpoints before driving
+	  other cpus into kdb.  Speeds up gdb and avoids SMP race.
+
+	* arch/i386/kdb/kdba_io.c (get_serial_char, get_kbd_char): Ignore
+	  unprintable characters.
+
+	* arch/i386/kdb/kdba_io.c (kdba_read): Better handling of buffer size.  
+
+2000-10-04 Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kdb/kdba_bt.c (kdba_bt_process): Verify that esp is inside
+	task_struct.  Original patch by Mike Galbraith.
+
+	* kdb/kdb_io.c (kdb_getstr): Reset output line counter, remove
+	unnecessary prompts.
+
+	* arch/i386/kdb/kdbasupport.c (kdb_getregcontents): Change " cs" to
+	"xcs", ditto ss, ds, es.  gdb2kdb does not like leading spaces.
+
+	* include/asm-xxx/kdb.h: Add dummy kdb.h for all architectures except
+	ix86.  This allows #include <linux/kdb.h> to appear in arch independent
+	code without causing compile errors.
+
+	* kdb/modules/kdbm_pg: Sync with XFS.
+
+2000-10-03  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* kdb/kdb_io.c (kdb_read): Ignore NMI while waiting for input.
+
+	* kdb/kdb_io.c, kdb/Makefile: Export kdb_read.
+
+2000-10-02  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kernel/smpboot.c (do_boot_cpu): Set nmi_watchdog_source to 2
+	to avoid premature NMI oops during cpu bring up.  We have to assume that
+	a box with more than 1 cpu has a working IO-APIC.
+
+	* Documentation/kdb/{kdb.mm,kdb_md.man}: Add mdr command.
+
+	* kdb/kdbmain.c (kdb_md): Add mdr command.
+
+	* Release as kdb v1.5 against 2.4.0-test9-pre8.
+
+	* arch/i386/kdb/kdba_io.c, arch/i386/kdb/kdbasupport.c, kdb/kdbmain.c,
+	kdb/kdb_io.c, kdb/kdb_id.c: Remove zero initializers for static
+	variables.
+
+2000-09-28  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* various: Add nmi_watchdog_source, 1 local APIC, 2 IO-APIC.
+	Test nmi_watchdog_source instead of nr_ioapics so UP works on SMP hardware.
+
+	* arch/i386/kernel/io_apic.c: Rename setup_nmi to setup_nmi_io for clarity.
+
+	* kdb/kdbmain.c (kdb_parse): Only set NO_WATCHDOG if it was already set.
+
+	* kdb/kdbmain.c (kdb): Clear NO_WATCHDOG on all exit paths.
+
+	* include/linux/kdb.h: Add KDB_REASON_SILENT.
+
+	* kdb/kdbmain.c (kdb_local): Treat reason SILENT as immediate 'go'.
+
+	* kdb/kdbmain.c (kdb_init): Invoke kdb with reason SILENT to instantiate
+	any breakpoints on boot cpu. 
+
+	* arch/i386/kernel/smpboot.c (smp_callin): Invoke kdb with reason SILENT
+	to instantiate any global breakpoints on this cpu.
+
+	* kdb/kdb_cmds: Remove comment that said initial commands only worked on
+	boot cpu.
+
+2000-09-27  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kernel/msr.c: Move {rd,wr}msr_eio to include/asm-i386/apic.h.
+
+	* include/asm-i386/apic.h: Define NMI interfaces.
+
+	* kernel/sysctl.c (kern_table):
+	* kernel/sysctl.c (do_proc_set_nmi_watchdog):
+	Add /proc/sys/kernel/nmi_watchdog.
+
+	* arch/i386/kernel/apic.c: New routines set_nmi_counter_local,
+	setup_apic_nmi_watchdog.
+
+	* arch/i386/kernel/traps.c: New routine set_nmi_watchdog().  Call apic
+	routines to set/clear local apic timer.
+
+2000-09-26  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* include/linux/sysctl.h (enum): Add NMI_WATCHDOG.
+
+	* arch/i386/kernel/traps.c (nmi_watchdog_tick): Check nmi_watchdog is
+	still on.
+
+	* arch/i386/config.in: Add CONFIG_UP_NMI_WATCHDOG.
+
+	* Documentation/Configure.help: Add CONFIG_UP_NMI_WATCHDOG.
+
+	* Documentation/nmi_watchdog.txt: Update for UP NMI watchdog.
+
+2000-09-25  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kernel/apic.c (init_apic_mappings):
+	* arch/i386/kernel/io_apic.c (IO_APIC_init_uniprocessor):
+	Merge Keir Fraser's local APIC for uniprocessors patch.
+
+2000-09-24  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Various: Declare initialization routines as __init.
+
+	* Makefile: Define and export AWK.
+
+	* kdb/Makefile: Generate gen-kdb_cmds.c from kdb/kdb_cmds.
+
+	* kdb/kdbmain.c (kdb_init): Call new routine kdb_cmds_init to execute
+	whatever the user put in kdb/kdb_cmds.
+
+	* arch/i386/kdb/kdba_bt.c (kdba_bt_stack): New parameter to
+	indicate if esp in regs is known to be valid or not.
+
+	* kdb/kdb_bp.c, arch/i386/kdb/kdba_bp.c: More trace prints for
+	breakpoint handling.
+
+	* arch/i386/kdb/kdba_bp.c (kdba_installbp): Finally found and fixed the
+	annoying breakpoint bug where breakpoints where not always installed
+	after 'go'.
+
+	* Documentation/kdb: Update man pages kdb.mm, kdb_env.man, kdb_ss.man.
+
+	* Released as kdb-v1.5-beta1-2.4.0-test8.
+
+	* Sync to 2.4.0-test9-pre6 and release as kdb-v1.5-beta1-2.4.0-test9-pre6.
+
+2000-09-23  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kdb/kdbasupport.c (kdba_getregcontents): New pseudo
+	registers cesp and ceflags to help with debugging the debugger.
+
+	* kdb/kdbmain.c (kdb_local, kdb): Add KDB_REASON_RECURSE.  Add
+	environment variable RECURSE.  Add code to cope with some types of
+	recursion.
+
+	* kdb/kdbmain.c (kdb), arch/i386/kdba/kdba_bp.c: Add
+	kdba_clearsinglestep.
+
+2000-09-22  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* drivers/video/vgacon.c (write_vga): No cli() if kdb is running, avoid
+	console deadlock.
+
+	* arch/i386/kernel/irq.c (get_irqlock): Warn if kdb is running, may hang.
+
+	* include/linux/kdb.h: Define KDB_IS_RUNNING as (0) if no CONFIG_KDB.
+
+	* arch/i386/kdb/kdba_bt.c (kdba_bt_stack): Do not attempt a backtrace if
+	the code segment is not in the kernel.
+
+	* kdb/modules: Change modules from MX_OBJS to M_OBJS.  Remove EXPORT_NOSYMBOLS.
+
+2000-09-21  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* arch/i386/kernel/i386_ksyms.c: Move EXPORT_SYMBOLS for kdb to kdb/kdbmain.c.
+
+	* kdb/Makefile: Change kdb/kdbmain.o from O_OBJS to OX_OBJS.
+
+	* arch/i386/kernel/smp.c: Remove some #ifdef CONFIG_KDB.  Remove kdbprivate.h.
+
+	* include/linux/kdb.h: Add kdb_print_state.  Add KDB_STATE_WAIT_IPI.
+
+	* kdb/kdbmain.c (kdb): Only mark cpu as leaving if it is in KDB state.  Maintain
+	WAIT_IPI state so a cpu is only driven through NMI once.
+
+	* arch/i386/kernel/smp.c (smp_kdb_stop): All state fiddling moved to kdb().
+
+2000-09-20  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* include/linux/kdb.h: #define kdb() as (0) if kdb is not configured.
+
+	* arch/i386/kernel/traps.c: Remove some #ifdef CONFIG_KDB.
+
+	* include/linux/kdbprivate.h: Move per cpu state to kdb.h.
+
+	* include/linux/kdb.h: Add KDB_STATE_NO_WATCHDOG, KDB_STATE_PRINTF_LOCK.
+	Rename KDB_DEBUG_xxx to KDB_DEBUG_FLAG_xxx.  Clean up debug flag
+	definitions.
+
+	* arch/i386/kernel/traps.c (nmi_watchdog_tick): Check no watchdog.
+
+	* kdb/kdbmain.c (kdb): Set no watchdog in normal kdb code.
+
+	* kdb/kdbmain.c (kdb_parse): Allow watchdog in commands.
+
+	* kdb/kdb_io.c (kdb_printf): No watchdog during printing.  Clean up lock handling.
+
+	* kdb/kdbmain.c (kdb_set): Clean up debug flag handling.
+
+2000-09-19  Juan J. Quintela  <quintela@fi.udc.es>
+
+	* kdb/arch/i386/kdb/kdba_io.c: Allow kdb to compile without CONFIG_VT and/or
+	serial console.
+
+2000-09-19  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* include/linux/kdb.h: Define KDB_DEBUG_STATE().
+
+	* kdb/kdbmain.c (kdb): Add kdb_print_state(), calls to KDB_DEBUG_STATE().
+
+2000-09-16  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* Move to finer grained control over individual processors in kdb with
+	per cpu kdb state.  Needed to allow ss[b] to only release one processor,
+	previously ss[b] released all processors.  Also need to recover from
+	errors inside kdb commands, e.g. oops in kdbm_pg code.
+
+	* various:
+	  Move global flags KDB_FLAG_SSB, KDB_FLAG_SUPRESS, KDB_FLAG_FAULT,
+	  KDB_FLAG_SS, KDB_FLAG_SSBPT, kdb_active, to per cpu state and macros
+	  KDB_STATE(xxx).
+	  Replace kdb_flags & KDB_FLAG_xxx with KDB_FLAG(xxx).
+	  Replace kdb_flags & KDB_DEBUG_xxx with KDB_DEBUG(xxx).
+	  Replace specific tests with wrapper KDB_IS_RUNNING().
+
+	* various: Remove #ifdef CONFIG_SMP from kdb code wherever
+	possible.  Simplifies the code and makes it much more readable.
+
+	* arch/i386/kdb/kdbasupport.c (kdb_setjmp): Record if we have reliable
+	longjmp data instead of assuming it is always set.
+
+	* various: Replace smp_kdb_wait with per cpu state, HOLD_CPU.
+
+	* init/main.c : Replace #ifdef KDB_DEBUG with KDB_DEBUG(CALLBACK).
+
+	* include/linux/kdbprivate.h: Separate command return codes from error
+	codes.  Add more detailed command codes.
+
+	* arch/i386/kernel/traps.c (die): Change spin_lock_irq to
+	spin_lock_irqsave.  Why did I do this?
+
+	* kdb/kdbmain.c (kdb_parse): Set per cpu flag CMD before executing kdb
+	command.  More detailed return codes for commands that affect
+	processors.
+
+	* kdb/kdbmain.c (kdb_previous_event): New, check if any processors are
+	still executing the previous kdb event.  Removes a race window where a
+	second event could enter kdb before the first had completely ended.
+
+	* kdb/kdbmain.c (kdb): Document all the concurrency conditions and how
+	kdb handles them.  ss[b] now releases only the current cpu.  Do not set
+	breakpoints when releasing for ss[b].  Recover from errors in kdb
+	commands.  Check that we have reliable longjmp data before using it.
+
+	* various: Update return code documentation.
+
+	* kdb/kdb_bp.c (kdb_ss): Separate ss and ssb return codes.
+
+	* kdb/kdbsupport.c (kdb_ipi): Finer grained algorithm for deciding
+	whether to call send a stop signal to a cpu.
+
+	* arch/i386/kdb/kdba_bp.c (kdba_db_trap): Separate ss and ssb return
+	codes.  Reinstall delayed software breakpoints per cpu instead of
+	globally.  Changed algorithm for handling ss[b].
+
+	* arch/i386/kdb/kdba_bp.c (kdba_bp_trap): Match software breakpoints per
+	cpu instead of globally.
+
+	* include/linux/kdb.h: Bump version to kdb v1.5.
+
+2000-09-16  Keith Owens  <kaos@melbourne.sgi.com>
+
+	* kernel/sysctl.c (kern_table): add /proc/sys/kernel/kdb.
+
+	* init/main.c (parse_options): add boot flags kdb=on, kdb=off,
+	kdb=early.
+
+	* include/linux/sysctl.h (enum): add KERN_KDB.
+
+	* drivers/char/serial.c (receive_chars): check kdb_on.
+
+	* drivers/char/keyboard.c (handle_scancode): check kdb_on.
+
+	* arch/i386/kernel/traps.c (nmi_watchdog_tick): check kdb_on.
+
+	* arch/i386/config.in: add CONFIG_KDB_OFF.
+
+	* Documentation/Configure.help: add CONFIG_KDB_OFF.
+
+	* kdb/kdbmain.c: add kdb_initial_cpu, kdb_on.
+
+	* kdb/kdbmain.c (kdb): check kdb_on, set kdb_initial_cpu.
+
+	* kdb/kdbmain.c (kdb_init): add Keith Owens to kdb banner.
+
+	* kdb/kdb_io.c (kdb_printf): serialize kdb_printf output.
+
+	* kdb/kdb_bt.c (kdb_bt): check environment variable BTAPROMPT.
+
+	* kdb/kdbsupport.c (kdb_ipi): ignore NMI for kdb_initial_cpu.
+
+	* kdb/modules/kdbm_pg.c (kdbm_page): merge updates from 2.4.0-test5-xfs.
+
+	* kdb/kdb_bt.man: add btp, bta, BTAPROMPT.
+
+	* kdb/kdb.mm: add CONFIG_KDB_OFF, boot flags, btp, bta.
+
+	* include/linux/kdbprivate.h: add kdb_initial_cpu.
+
+	* include/linux/kdb.h: add kdb_on, bump version to kdb v1.4.
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/Makefile linuxppc64_2_4/kdb/Makefile
--- linux-2.4.19/kdb/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/Makefile	Tue May 28 16:13:07 2002
@@ -0,0 +1,20 @@
+O_TARGET	:= kdb.o
+export-objs	:= kdbmain.o kdb_io.o
+obj-y		:= kdb_bt.o kdb_bp.o kdb_id.o kdbsupport.o gen-kdb_cmds.o kdbmain.o kdb_io.o
+
+subdir-$(CONFIG_KDB_MODULES) := modules
+obj-y += $(addsuffix /vmlinux-obj.o, $(subdir-y))
+
+override CFLAGS := $(CFLAGS:%-pg=% )
+
+EXTRA_CFLAGS += -I $(TOPDIR)/arch/$(ARCH)/kdb
+
+include $(TOPDIR)/Rules.make
+
+gen-kdb_cmds.c:	kdb_cmds Makefile
+	$(AWK) 'BEGIN {print "#include <linux/init.h>"} \
+		/^ *#/{next} \
+		/^[ \t]*$$/{next} \
+		{print "static __initdata char kdb_cmd" cmds++ "[] = \"" $$0 "\\n\";"} \
+		END {print "char __initdata *kdb_cmds[] = {"; for (i = 0; i < cmds; ++i) {print "  kdb_cmd" i ","}; print("  0\n};");}' \
+		kdb_cmds > gen-kdb_cmds.c
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdb_bp.c linuxppc64_2_4/kdb/kdb_bp.c
--- linux-2.4.19/kdb/kdb_bp.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdb_bp.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,615 @@
+/*
+ * Kernel Debugger Breakpoint Handler
+ *
+ * Copyright 1999, Silicon Graphics, Inc.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <asm/system.h>
+
+/*
+ * Table of kdb_breakpoints
+ */
+kdb_bp_t	kdb_breakpoints[KDB_MAXBPT];
+
+/*
+ * kdb_bp_install_global
+ *
+ *	Install global kdb_breakpoints prior to returning from the
+ *	kernel debugger.  This allows the kdb_breakpoints to be set
+ *	upon functions that are used internally by kdb, such as
+ *	printk().
+ *
+ * Parameters:
+ *	ef	Execution frame.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	This function is only called once per kdb session.
+ */
+
+void
+kdb_bp_install_global(kdb_eframe_t ef)
+{
+	int i;
+
+	for(i=0; i<KDB_MAXBPT; i++) {
+		if (KDB_DEBUG(BP)) {
+			kdb_printf("kdb_bp_install_global bp %d bp_enabled %d bp_global %d\n",
+				i, kdb_breakpoints[i].bp_enabled, kdb_breakpoints[i].bp_global);
+		}
+		if (kdb_breakpoints[i].bp_enabled
+		 && kdb_breakpoints[i].bp_global) {
+			kdba_installbp(ef, &kdb_breakpoints[i]);
+		}
+	}
+}
+
+/*
+ * kdb_bp_install_local
+ *
+ *	Install local kdb_breakpoints prior to returning from the
+ *	kernel debugger.  This allows the kdb_breakpoints to be set
+ *	upon functions that are used internally by kdb, such as
+ *	printk().
+ *
+ * Parameters:
+ *	ef	Execution frame.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	This function is called once per processor.
+ */
+
+void
+kdb_bp_install_local(kdb_eframe_t ef)
+{
+	int i;
+
+	for(i=0; i<KDB_MAXBPT; i++) {
+		if (KDB_DEBUG(BP)) {
+			kdb_printf("kdb_bp_install_local bp %d bp_enabled %d bp_global %d cpu %d bp_cpu %d\n",
+				i, kdb_breakpoints[i].bp_enabled, kdb_breakpoints[i].bp_global,
+				smp_processor_id(), kdb_breakpoints[i].bp_cpu);
+		}
+		if (KDB_STATE(NO_BP_DELAY)) {
+			kdb_breakpoints[i].bp_delay = 0;
+		}
+		if (kdb_breakpoints[i].bp_enabled
+		 && kdb_breakpoints[i].bp_cpu == smp_processor_id()
+		 && !kdb_breakpoints[i].bp_global){
+			kdba_installbp(ef, &kdb_breakpoints[i]);
+		}
+	}
+}
+
+/*
+ * kdb_bp_remove_global
+ *
+ * 	Remove global kdb_breakpoints upon entry to the kernel debugger.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdb_bp_remove_global(void)
+{
+	int i;
+
+	for(i=KDB_MAXBPT-1; i>=0; i--) {
+		if (KDB_DEBUG(BP)) {
+			kdb_printf("kdb_bp_remove_global bp %d bp_enabled %d bp_global %d\n",
+				i, kdb_breakpoints[i].bp_enabled, kdb_breakpoints[i].bp_global);
+		}
+		if (kdb_breakpoints[i].bp_enabled
+		 && kdb_breakpoints[i].bp_global) {
+			kdba_removebp(&kdb_breakpoints[i]);
+		}
+	}
+}
+
+
+/*
+ * kdb_bp_remove_local
+ *
+ * 	Remove local kdb_breakpoints upon entry to the kernel debugger.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdb_bp_remove_local(void)
+{
+	int i;
+
+	for(i=KDB_MAXBPT-1; i>=0; i--) {
+		if (KDB_DEBUG(BP)) {
+			kdb_printf("kdb_bp_remove_local bp %d bp_enabled %d bp_global %d cpu %d bp_cpu %d\n",
+				i, kdb_breakpoints[i].bp_enabled, kdb_breakpoints[i].bp_global,
+				smp_processor_id(), kdb_breakpoints[i].bp_cpu);
+		}
+		if (kdb_breakpoints[i].bp_enabled
+		 && kdb_breakpoints[i].bp_cpu == smp_processor_id()
+		 && !kdb_breakpoints[i].bp_global){
+			kdba_removebp(&kdb_breakpoints[i]);
+		}
+	}
+}
+
+/*
+ * kdb_printbp
+ *
+ * 	Internal function to format and print a breakpoint entry.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+static void
+kdb_printbp(kdb_bp_t *bp, int i)
+{
+	if (bp->bp_forcehw) {
+		kdb_printf("Forced ");
+	}
+
+	if (!bp->bp_template.bph_free) {
+		kdb_printf("%s ", kdba_bptype(&bp->bp_template));
+	} else {
+		kdb_printf("Instruction(i) ");
+	}
+
+	kdb_printf("BP #%d at ", i);
+	kdb_symbol_print(bp->bp_addr, NULL, KDB_SP_DEFAULT);
+
+	if (bp->bp_enabled) {
+		kdba_printbp(bp);
+		if (bp->bp_global)
+			kdb_printf(" globally");
+		else
+			kdb_printf(" on cpu %d", bp->bp_cpu);
+		if (bp->bp_adjust)
+			kdb_printf(" adjust %d", bp->bp_adjust);
+	} else {
+		kdb_printf("\n    is disabled");
+	}
+
+	kdb_printf("\n");
+}
+
+/*
+ * kdb_bp
+ *
+ * 	Handle the bp, and bpa commands.
+ *
+ *	[bp|bpa|bph] <addr-expression> [DATAR|DATAW|IO [length]]
+ *
+ * Parameters:
+ *	argc	Count of arguments in argv
+ *	argv	Space delimited command line arguments
+ *	envp	Environment value
+ *	regs	Exception frame at entry to kernel debugger
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic if failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ * 	bp	Set breakpoint.  Only use hardware assist if necessary.
+ *	bpa	Set breakpoint on all cpus, only use hardware regs if necessary
+ *	bph	Set breakpoint - force hardware register
+ *	bpha	Set breakpoint on all cpus, force hardware register
+ */
+
+int
+kdb_bp(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int     i;
+	kdb_bp_t *bp;
+	int     diag;
+	int     free, same;
+	kdb_machreg_t addr;
+	char   *symname = NULL;
+	long    offset = 0ul;
+	int	nextarg;
+	int	hardware;
+	int	global;
+
+	if (argc == 0) {
+		/*
+		 * Display breakpoint table
+		 */
+		for(i=0,bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+			if (bp->bp_free) continue;
+
+			kdb_printbp(bp, i);
+		}
+
+		return 0;
+	}
+
+	global = ((strcmp(argv[0], "bpa") == 0)
+	       || (strcmp(argv[0], "bpha") == 0));
+	hardware = ((strcmp(argv[0], "bph") == 0)
+		 || (strcmp(argv[0], "bpha") == 0));
+
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, &symname, regs);
+	if (diag)
+		return diag;
+
+	/*
+	 * Allocate a new bp structure
+	 */
+	free = same = KDB_MAXBPT;
+	for(i=0,bp=kdb_breakpoints; i<KDB_MAXBPT; i++,bp++) {
+		if (bp->bp_free) {
+			break;
+		}
+	}
+
+	if (i == KDB_MAXBPT)
+		return KDB_TOOMANYBPT;
+
+	kdba_check_pc(&addr);
+	if (kdba_verify_rw(addr, sizeof(kdb_machinst_t))) {
+		kdb_printf("Invalid address for breakpoint, ignoring bp command\n");
+		return(0);
+	}
+	bp->bp_addr = addr;
+	bp->bp_adjust = 0;
+
+	bp->bp_forcehw = hardware;
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdb_bp: forcehw is %d hardware is %d\n", bp->bp_forcehw, hardware);
+
+	/*
+	 * Handle architecture dependent parsing
+	 */
+	diag = kdba_parsebp(argc, argv, &nextarg, bp);
+	if (diag) {
+		return diag;
+	}
+
+	bp->bp_enabled = 1;
+	bp->bp_free = 0;
+	bp->bp_global = 1;	/* Most breakpoints are global */
+
+	if (hardware && !global) {
+		bp->bp_global = 0;
+		bp->bp_cpu = smp_processor_id();
+	}
+
+	/*
+	 * Allocate a hardware breakpoint.  If one is not available,
+ 	 * disable the breakpoint, but leave it in the breakpoint
+	 * table.  When the breakpoint is re-enabled (via 'be'), we'll
+	 * attempt to allocate a hardware register for it.
+	 */
+	if (!bp->bp_template.bph_free) {
+		bp->bp_hard = kdba_allocbp(&bp->bp_template, &diag);
+		if (diag) {
+			bp->bp_enabled = 0;
+			return diag;
+		}
+		bp->bp_hardtype = 1;
+	}
+
+	kdb_printbp(bp, i);
+
+	return 0;
+}
+
+/*
+ * kdb_bc
+ *
+ * 	Handles the 'bc', 'be', and 'bd' commands
+ *
+ *	[bd|bc|be] <breakpoint-number>
+ *
+ * Parameters:
+ *	argc	Count of arguments in argv
+ *	argv	Space delimited command line arguments
+ *	envp	Environment value
+ *	regs	Exception frame at entry to kernel debugger
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+#define KDBCMD_BC	0
+#define KDBCMD_BE	1
+#define KDBCMD_BD	2
+
+int
+kdb_bc(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	kdb_machreg_t 	addr;
+	kdb_bp_t	*bp = 0;
+	int lowbp = KDB_MAXBPT;
+	int highbp = 0;
+	int done = 0;
+	int i;
+	int diag;
+	int cmd;			/* KDBCMD_B? */
+
+	if (strcmp(argv[0], "be") == 0) {
+		cmd = KDBCMD_BE;
+	} else if (strcmp(argv[0], "bd") == 0) {
+		cmd = KDBCMD_BD;
+	} else
+		cmd = KDBCMD_BC;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	if (strcmp(argv[1], "*") == 0) {
+		lowbp = 0;
+		highbp = KDB_MAXBPT;
+	} else {
+		diag = kdbgetularg(argv[1], &addr);
+		if (diag)
+			return diag;
+
+		/*
+		 * For addresses less than the maximum breakpoint number,
+		 * assume that the breakpoint number is desired.
+		 */
+		if (addr < KDB_MAXBPT) {
+			bp = &kdb_breakpoints[addr];
+			lowbp = highbp = addr;
+			highbp++;
+		} else {
+			for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+				if (bp->bp_addr == addr) {
+					lowbp = highbp = i;
+					highbp++;
+					break;
+				}
+			}
+		}
+	}
+
+	/*
+	 * Now operate on the set of breakpoints matching the input
+	 * criteria (either '*' for all, or an individual breakpoint).
+	 */
+	for(bp=&kdb_breakpoints[lowbp], i=lowbp;
+	    i < highbp;
+	    i++, bp++) {
+		if (bp->bp_free)
+			continue;
+
+		done++;
+
+		switch (cmd) {
+		case KDBCMD_BC:
+			if (bp->bp_hardtype) {
+				kdba_freebp(bp->bp_hard);
+				bp->bp_hard = 0;
+				bp->bp_hardtype = 0;
+			}
+
+			bp->bp_enabled = 0;
+			bp->bp_global = 0;
+
+			kdb_printf("Breakpoint %d at " kdb_bfd_vma_fmt " cleared\n",
+				i, bp->bp_addr);
+
+			bp->bp_addr = 0;
+			bp->bp_free = 1;
+
+			break;
+		case KDBCMD_BE:
+			/*
+			 * Allocate a hardware breakpoint.  If one is not
+			 * available, don't enable the breakpoint.
+			 */
+			if (!bp->bp_template.bph_free
+			 && !bp->bp_hardtype) {
+				bp->bp_hard = kdba_allocbp(&bp->bp_template, &diag);
+				if (diag) {
+					bp->bp_enabled = 0;
+					return diag;
+				}
+				bp->bp_hardtype = 1;
+			}
+
+			bp->bp_enabled = 1;
+
+			kdb_printf("Breakpoint %d at " kdb_bfd_vma_fmt " in enabled",
+				i, bp->bp_addr);
+
+			kdb_printf("\n");
+			break;
+		case KDBCMD_BD:
+			if (!bp->bp_enabled) {
+				return 0;
+			}
+
+			/*
+			 * Since this breakpoint is now disabled, we can
+			 * give up the hardware register which is allocated
+			 * to it.
+			 */
+			if (bp->bp_hardtype) {
+				kdba_freebp(bp->bp_hard);
+				bp->bp_hard = 0;
+				bp->bp_hardtype = 0;
+			}
+
+			bp->bp_enabled = 0;
+
+			kdb_printf("Breakpoint %d at " kdb_bfd_vma_fmt " disabled\n",
+				i, bp->bp_addr);
+
+			break;
+		}
+	}
+
+	return (!done)?KDB_BPTNOTFOUND:0;
+}
+
+/*
+ * kdb_ss
+ *
+ *	Process the 'ss' (Single Step) and 'ssb' (Single Step to Branch)
+ *	commands.
+ *
+ *	ss
+ *	ssb
+ *
+ * Parameters:
+ *	argc	Argument count
+ *	argv	Argument vector
+ *	envp	Environment vector
+ *	regs	Registers at time of entry to kernel debugger
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_CMD_SS[B] for success, a kdb error if failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	Set the arch specific option to trigger a debug trap after the next
+ *	instruction.
+ *
+ *	For 'ssb', set the trace flag in the debug trap handler
+ *	after printing the current insn and return directly without
+ *	invoking the kdb command processor, until a branch instruction
+ *	is encountered.
+ */
+
+int
+kdb_ss(int argc, const char **argv, const char **envp, kdb_eframe_t ef)
+{
+	int ssb = 0;
+
+	ssb = (strcmp(argv[0], "ssb") == 0);
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	/*
+	 * Set trace flag and go.
+	 */
+	KDB_STATE_SET(DOING_SS);
+	if (ssb)
+		KDB_STATE_SET(DOING_SSB);
+
+	kdba_setsinglestep(ef);		/* Enable single step */
+
+	if (ssb)
+		return KDB_CMD_SSB;
+	return KDB_CMD_SS;
+}
+
+/*
+ * kdb_initbptab
+ *
+ *	Initialize the breakpoint table.  Register breakpoint commands.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void __init
+kdb_initbptab(void)
+{
+	int i;
+	kdb_bp_t *bp;
+
+	/*
+	 * First time initialization.
+	 */
+	memset(&kdb_breakpoints, '\0', sizeof(kdb_breakpoints));
+
+	for (i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		bp->bp_free = 1;
+		/*
+		 * The bph_free flag is architecturally required.  It
+		 * is set by architecture-dependent code to false (zero)
+		 * in the event a hardware breakpoint register is required
+		 * for this breakpoint.
+		 *
+		 * The rest of the template is reserved to the architecture
+		 * dependent code and _must_ not be touched by the architecture
+		 * independent code.
+		 */
+		bp->bp_template.bph_free = 1;
+	}
+
+	kdb_register_repeat("bp", kdb_bp, "[<vaddr>]", "Set/Display breakpoints", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("bl", kdb_bp, "[<vaddr>]", "Display breakpoints", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("bpa", kdb_bp, "[<vaddr>]", "Set/Display global breakpoints", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("bph", kdb_bp, "[<vaddr>]", "Set hardware breakpoint", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("bpha", kdb_bp, "[<vaddr>]", "Set global hardware breakpoint", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("bc", kdb_bc, "<bpnum>",   "Clear Breakpoint", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("be", kdb_bc, "<bpnum>",   "Enable Breakpoint", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("bd", kdb_bc, "<bpnum>",   "Disable Breakpoint", 0, KDB_REPEAT_NONE);
+
+	kdb_register_repeat("ss", kdb_ss, "", "Single Step", 1, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("ssb", kdb_ss, "", "Single step to branch/call", 0, KDB_REPEAT_NO_ARGS);
+	/*
+	 * Architecture dependent initialization.
+	 */
+	kdba_initbp();
+}
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdb_bt.c linuxppc64_2_4/kdb/kdb_bt.c
--- linux-2.4.19/kdb/kdb_bt.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdb_bt.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,138 @@
+/*
+ * Minimalist Kernel Debugger - Architecture independent stack traceback
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *	Keith Owens			2000/09/16
+ *		KDB v1.4
+ *		Env BTAPROMPT.
+ *
+ */
+
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <asm/system.h>
+
+
+/*
+ * kdb_bt
+ *
+ *	This function implements the 'bt' command.  Print a stack
+ *	traceback.
+ *
+ *	bt [<address-expression>]   (addr-exp is for alternate stacks)
+ *	btp <pid>		     (Kernel stack for <pid>)
+ *
+ * 	address expression refers to a return address on the stack.  It
+ *	is expected to be preceeded by a frame pointer.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	ef	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	Backtrack works best when the code uses frame pointers.  But
+ *	even without frame pointers we should get a reasonable trace.
+ *
+ *	mds comes in handy when examining the stack to do a manual
+ *	traceback.
+ */
+
+int
+kdb_bt(int argc, const char **argv, const char **envp, kdb_eframe_t ef)
+{
+	int	diag;
+	int	argcount = 5;
+	int	btaprompt = 1;
+	char	buffer[80];
+	int 	nextarg;
+	unsigned long addr;
+	long	offset;
+
+	kdbgetintenv("BTARGS", &argcount);	/* Arguments to print */
+	kdbgetintenv("BTAPROMPT", &btaprompt);	/* Prompt after each proc in bta */
+
+	if (strcmp(argv[0], "bta") == 0) {
+		struct task_struct *p;
+
+		for_each_task(p) {
+			kdb_printf("Stack traceback for pid %d\n", p->pid);
+
+			diag = kdba_bt_process(p, argcount);
+
+			if (btaprompt) {
+				kdb_getstr(buffer, sizeof(buffer),
+					   "Enter <q> to end, <cr> to continue:");
+
+				if (buffer[0] == 'q') {
+					return 0;
+				}
+			}
+		}
+	} else if (strcmp(argv[0], "btp") == 0) {
+		struct task_struct *p;
+		unsigned long	   pid;
+		
+		if (argc < 1)
+			return KDB_ARGCOUNT;
+
+		diag = kdbgetularg((char *)argv[1], &pid);
+		if (diag)
+			return diag;
+
+		for_each_task(p) {
+			if (p->pid == (pid_t)pid) {
+				return kdba_bt_process(p, argcount);
+			}
+		}
+
+		kdb_printf("No process with pid == %ld found\n", pid);
+		return 0;
+	} else {
+		if (argc) {
+			nextarg = 1;
+			diag = kdbgetaddrarg(argc, argv, &nextarg, &addr,
+					     &offset, NULL, ef);
+			if (diag)
+				return diag;
+
+			return kdba_bt_stack(ef, &addr, argcount, current);
+		} else {
+			return kdba_bt_stack(ef, NULL, argcount, current);
+		}
+	}
+
+	/* NOTREACHED */
+	return 0;
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdb_cmds linuxppc64_2_4/kdb/kdb_cmds
--- linux-2.4.19/kdb/kdb_cmds	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdb_cmds	Tue May 28 16:13:07 2002
@@ -0,0 +1,6 @@
+# Initial commands for kdb, alter to suit your needs.
+# These commands are executed in kdb_init() context, no SMP, no
+# processes.  Commands that require process data (including stack or
+# registers) are not reliable this early.  set and bp commands should
+# be safe.  Global breakpoint commands affect each cpu as it is booted.
+
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdb_id.c linuxppc64_2_4/kdb/kdb_id.c
--- linux-2.4.19/kdb/kdb_id.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdb_id.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,257 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Independent Instruction Disassembly
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <stdarg.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+disassemble_info	kdb_di;
+
+/*
+ * kdb_id
+ *
+ * 	Handle the id (instruction display) command.
+ *
+ *	id  [<addr>]
+ *
+ * Parameters:
+ *	argc	Count of arguments in argv
+ *	argv	Space delimited command line arguments
+ *	envp	Environment value
+ *	regs	Exception frame at entry to kernel debugger
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic if failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdb_id(int argc, const char **argv, const char **envp, struct pt_regs* regs)
+{
+	kdb_machreg_t		pc;
+	int			icount;
+	int			diag;
+	int			i;
+	char *			mode;
+	int			nextarg;
+	long			offset = 0;
+	static kdb_machreg_t 	lastpc;
+	struct disassemble_info *dip = &kdb_di;
+	char			lastbuf[50];
+	unsigned long		word;
+
+	if (argc != 1)  {
+		if (lastpc == 0) {
+			return KDB_ARGCOUNT;
+		} else {
+			sprintf(lastbuf, "0x%lx", lastpc);
+			argv[1] = lastbuf;
+			argc = 1;
+		}
+	}
+
+
+	/*
+	 * Fetch PC.  First, check to see if it is a symbol, if not,
+	 * try address.
+	 */
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &pc, &offset, NULL, regs);
+	if (diag)
+		return diag;
+	kdba_check_pc(&pc);
+	if (kdb_getarea(word, pc))
+		return(0);
+
+	/*
+	 * Number of lines to display
+	 */
+	diag = kdbgetintenv("IDCOUNT", &icount);
+	if (diag)
+		return diag;
+
+	dip->fprintf_dummy = kdb_dis_fprintf;
+
+	mode = kdbgetenv("IDMODE");
+	diag = kdba_id_parsemode(mode, dip);
+	if (diag) {
+		return diag;
+	}
+
+	for(i=0; i<icount; i++) {
+		pc += kdba_id_printinsn(pc, &kdb_di);
+		kdb_printf("\n");
+	}
+
+	lastpc = pc;
+
+	return 0;
+}
+
+/*
+ * kdb_id1
+ *
+ * 	Disassemble a single instruction at 'pc'.
+ *
+ * Parameters:
+ *	pc	Address of instruction to disassemble
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic if failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdb_id1(unsigned long pc)
+{
+	char   *mode;
+	int	diag;
+
+	/*
+	 * Allow the user to specify that this instruction
+	 * should be treated differently.
+	 */
+
+	kdb_di.fprintf_dummy = kdb_dis_fprintf_dummy;
+
+	mode = kdbgetenv("IDMODE");
+	diag = kdba_id_parsemode(mode, &kdb_di);
+	if (diag) {
+		kdb_printf("kdb_id: bad value in 'IDMODE' environment variable ignored\n");
+	}
+
+	(void) kdba_id_printinsn(pc, &kdb_di);
+	kdb_printf("\n");
+}
+
+/*
+ * kdb_dis_fprintf
+ *
+ *	Format and print a string.
+ *
+ * Parameters:
+ *	file	Unused paramter.
+ *	fmt	Format string
+ *	...	Optional additional parameters.
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ * 	Result of format conversion cannot exceed 255 bytes.
+ */
+
+int
+kdb_dis_fprintf(PTR file, const char *fmt, ...)
+{
+	char buffer[256];
+	va_list ap;
+
+	va_start(ap, fmt);
+	vsprintf(buffer, fmt, ap);
+	va_end(ap);
+
+	kdb_printf("%s", buffer);
+
+	return 0;
+}
+
+/*
+ * kdb_dis_fprintf_dummy
+ *
+ *	A dummy printf function for the disassembler, it does nothing.
+ *	This lets code call the disassembler to step through
+ *	instructions without actually printing anything.
+ * Inputs:
+ *	Always ignored.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Always 0.
+ * Locking:
+ *	none.
+ * Remarks:
+ *	None.
+ */
+
+int
+kdb_dis_fprintf_dummy(PTR file, const char *fmt, ...)
+{
+	return(0);
+}
+
+/*
+ * kdb_disinit
+ *
+ * 	Initialize the disassembly information structure
+ *	for the GNU disassembler.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic if failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void __init
+kdb_id_init(void)
+{
+	kdb_di.stream	    = NULL;
+	kdb_di.application_data = NULL;
+	kdb_di.symbols	    = NULL;
+	kdb_di.num_symbols  = 0;
+	kdb_di.flags	    = 0;
+	kdb_di.private_data	    = NULL;
+	kdb_di.buffer	    = NULL;
+	kdb_di.buffer_vma       = 0;
+	kdb_di.buffer_length    = 0;
+	kdb_di.bytes_per_line   = 0;
+	kdb_di.bytes_per_chunk  = 0;
+	kdb_di.insn_info_valid  = 0;
+	kdb_di.branch_delay_insns = 0;
+	kdb_di.data_size	    = 0;
+	kdb_di.insn_type	    = 0;
+	kdb_di.target           = 0;
+	kdb_di.target2          = 0;
+	kdb_di.fprintf_func	= kdb_dis_fprintf;
+
+	kdba_id_init(&kdb_di);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdb_io.c linuxppc64_2_4/kdb/kdb_io.c
--- linux-2.4.19/kdb/kdb_io.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdb_io.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,316 @@
+/*
+ * Kernel Debugger Console I/O handler
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *	Chuck Fleckenstein		1999/07/20
+ *		Move kdb_info struct declaration to this file
+ *		for cases where serial support is not compiled into
+ *		the kernel.
+ *
+ *	Masahiro Adegawa		1999/07/20
+ *		Handle some peculiarities of japanese 86/106
+ *		keyboards.
+ *
+ *	marc@mucom.co.il		1999/07/20
+ *		Catch buffer overflow for serial input.
+ *
+ *      Scott Foehner
+ *              Port to ia64
+ *
+ *	Scott Lurndal			2000/01/03
+ *		Restructure for v1.0
+ *
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/kdev_t.h>
+#include <linux/console.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#ifdef CONFIG_SPARC64
+#include <asm/oplib.h>
+#else
+static struct console *kdbcons;
+#endif
+
+/*
+ * kdb_read
+ *
+ *	This function reads a string of characters, terminated by
+ *	a newline, or by reaching the end of the supplied buffer,
+ *	from the current kernel debugger console device.
+ * Parameters:
+ *	buffer	- Address of character buffer to receive input characters.
+ *	bufsize - size, in bytes, of the character buffer
+ * Returns:
+ *	Returns a pointer to the buffer containing the received
+ *	character string.  This string will be terminated by a
+ *	newline character.
+ * Locking:
+ *	No locks are required to be held upon entry to this
+ *	function.  It is not reentrant - it relies on the fact
+ *	that while kdb is running on any one processor all other
+ *	processors will be spinning at the kdb barrier.
+ * Remarks:
+ *
+ * Davidm asks, why doesn't kdb use the console abstraction;
+ * here are some reasons:
+ *      - you cannot debug the console abstraction with kdb if
+ *        kdb uses it.
+ *      - you rely on the correct functioning of the abstraction
+ *        in the presence of general system failures.
+ *      - You must acquire the console spinlock thus restricting
+ *        the usability - what if the kernel fails with the spinlock
+ *        held - one still wishes to debug such situations.
+ *      - How about debugging before the console(s) are registered?
+ *      - None of the current consoles (sercons, vt_console_driver)
+ *        have read functions defined.
+ *	- The standard pc keyboard and terminal drivers are interrupt
+ *	  driven.   We cannot enable interrupts while kdb is active,
+ *	  so the standard input functions cannot be used by kdb.
+ *
+ * An implementation could be improved by removing the need for
+ * lock acquisition - just keep a 'struct console *kdbconsole;' global
+ * variable which refers to the preferred kdb console.
+ *
+ * The bulk of this function is architecture dependent.
+ */
+
+char *
+kdb_read(char *buffer, size_t bufsize)
+{
+	return(kdba_read(buffer, bufsize));
+}
+
+/*
+ * kdb_getstr
+ *
+ *	Print the prompt string and read a command from the
+ *	input device.
+ *
+ * Parameters:
+ *	buffer	Address of buffer to receive command
+ *	bufsize Size of buffer in bytes
+ *	prompt	Pointer to string to use as prompt string
+ * Returns:
+ *	Pointer to command buffer.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	For SMP kernels, the processor number will be
+ *	substituted for %d, %x or %o in the prompt.
+ */
+
+char *
+kdb_getstr(char *buffer, size_t bufsize, char *prompt)
+{
+#if defined(CONFIG_SMP)
+	kdb_printf(prompt, smp_processor_id());
+#else
+	kdb_printf("%s", prompt);
+#endif
+	kdb_nextline = 1;	/* Prompt and input resets line number */
+	return kdb_read(buffer, bufsize);
+}
+
+/*
+ * kdb_printf
+ *
+ *	Print a string to the output device(s).
+ *
+ * Parameters:
+ *	printf-like format and optional args.
+ * Returns:
+ *	0
+ * Locking:
+ *	None.
+ * Remarks:
+ *	use 'kdbcons->write()' to avoid polluting 'log_buf' with
+ *	kdb output.
+ */
+
+void
+kdb_printf(const char *fmt, ...)
+{
+	char buffer[256];
+	va_list	ap;
+	int diag;
+	int linecount;
+	int logging, saved_loglevel = 0;
+	int do_longjmp = 0;
+	struct console *c = console_drivers;
+	static spinlock_t kdb_printf_lock = SPIN_LOCK_UNLOCKED;
+
+	/* Serialize kdb_printf if multiple cpus try to write at once.
+	 * But if any cpu goes recursive in kdb, just print the output,
+	 * even if it is interleaved with any other text.
+	 */
+	if (!KDB_STATE(PRINTF_LOCK)) {
+		KDB_STATE_SET(PRINTF_LOCK);
+		spin_lock(&kdb_printf_lock);
+	}
+
+	diag = kdbgetintenv("LINES", &linecount);
+	if (diag)
+		linecount = 22;
+
+	diag = kdbgetintenv("LOGGING", &logging);
+	if (diag)
+		logging = 0;
+
+	va_start(ap, fmt);
+	vsprintf(buffer, fmt, ap);
+	va_end(ap);
+
+	/*
+	 * Write to all consoles.
+	 */
+#ifdef CONFIG_SPARC64
+	if (c == NULL)
+		prom_printf("%s", buffer);
+	else
+#endif
+	while (c) {
+		c->write(c, buffer, strlen(buffer));
+		c = c->next;
+	}
+	if (logging) {
+		saved_loglevel = console_loglevel;
+		console_loglevel = 0;
+		printk("%s", buffer);
+	}
+
+	if (strchr(buffer, '\n') != NULL) {
+		kdb_nextline++;
+	}
+
+	if (kdb_nextline == linecount) {
+#ifdef KDB_HAVE_LONGJMP
+		char buf1[16];
+#if defined(CONFIG_SMP)
+		char buf2[32];
+#endif
+		char *moreprompt;
+
+		/* Watch out for recursion here.  Any routine that calls
+		 * kdb_printf will come back through here.  And kdb_read
+		 * uses kdb_printf to echo on serial consoles ...
+		 */
+		kdb_nextline = 1;	/* In case of recursion */
+
+		/*
+		 * Pause until cr.
+		 */
+		moreprompt = kdbgetenv("MOREPROMPT");
+		if (moreprompt == NULL) {
+			moreprompt = "more> ";
+		}
+
+#if defined(CONFIG_SMP)
+		if (strchr(moreprompt, '%')) {
+			sprintf(buf2, moreprompt, smp_processor_id());
+			moreprompt = buf2;
+		}
+#endif
+
+		c = console_drivers;
+#ifdef CONFIG_SPARC64
+		if (c == NULL)
+			prom_printf("%s", moreprompt);
+		else
+#endif
+		while (c) {
+			c->write(c, moreprompt, strlen(moreprompt));
+			c = c->next;
+		}
+		if (logging)
+			printk("%s", moreprompt);
+
+		kdb_read(buf1, sizeof(buf1));
+		kdb_nextline = 1;	/* Really set output line 1 */
+
+		if ((buf1[0] == 'q') || (buf1[0] == 'Q'))
+			do_longjmp = 1;
+		else if (buf1[0] && buf1[0] != '\n')
+			kdb_printf("Only 'q' or 'Q' are processed at more prompt, input ignored\n");
+#endif	/* KDB_HAVE_LONGJMP */
+	}
+
+	if (logging) {
+		console_loglevel = saved_loglevel;
+	}
+	if (KDB_STATE(PRINTF_LOCK)) {
+		spin_unlock(&kdb_printf_lock);
+		KDB_STATE_CLEAR(PRINTF_LOCK);
+	}
+	if (do_longjmp)
+#ifdef KDB_HAVE_LONGJMP
+		kdba_longjmp(&kdbjmpbuf[smp_processor_id()], 1);
+#else
+		;
+#endif	/* KDB_HAVE_LONGJMP */
+}
+
+/*
+ * kdb_io_init
+ *
+ *	Initialize kernel debugger output environment.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Select a console device.
+ */
+
+void __init
+kdb_io_init(void)
+{
+#ifndef CONFIG_SPARC64 /* we don't register serial consoles in time */
+	/*
+ 	 * Select a console.
+ 	 */
+	struct console *c = console_drivers;
+
+	while (c) {
+		if ((c->flags & CON_CONSDEV)) {
+			kdbcons = c;
+			break;
+		}
+		c = c->next;
+	}
+
+	if (kdbcons == NULL) {
+		long long i;
+
+		printk("kdb: Initialization failed - no console\n");
+		while (1) i++;
+	}
+#endif
+	return;
+}
+
+EXPORT_SYMBOL(kdb_read);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdbmain.c linuxppc64_2_4/kdb/kdbmain.c
--- linux-2.4.19/kdb/kdbmain.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdbmain.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,2842 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ * Copyright (C) 2000 Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *	Keith Owens			2000/06/09
+ *		KDB v1.3.
+ *		  Rewrite SMP handling.
+ *		  Add NMI watchdog from Ted Kline,
+ *		  lsmod/rmmod commands from Marc Esipovich <marc@mucom.co.il>
+ *	Stephane Eranian		2000/06/05
+ *		Enabled disassembler support. Added command history support.
+ *
+ *	Keith Owens			2000/09/16
+ *		KDB v1.4
+ *		kdb=on/off/early at boot, /proc/sys/kernel/kdb.
+ *		Env BTAPROMPT.
+ */
+
+#include <linux/config.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/reboot.h>
+#include <linux/sched.h>
+#include <linux/sysrq.h>
+#include <linux/smp.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kallsyms.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#include <asm/system.h>
+
+#if defined(CONFIG_MODULES)
+extern struct module *module_list;
+#endif
+
+	/*
+	 * Kernel debugger state flags
+	 */
+volatile int kdb_flags;
+
+	/*
+	 * kdb_lock protects updates to kdb_initial_cpu.  Used to
+	 * single thread processors through the kernel debugger.
+	 */
+spinlock_t kdb_lock = SPIN_LOCK_UNLOCKED;
+volatile int kdb_initial_cpu = -1;		/* cpu number that owns kdb */
+
+volatile int kdb_nextline = 1;
+static volatile int kdb_new_cpu;		/* Which cpu to switch to */
+
+volatile int kdb_state[NR_CPUS];		/* Per cpu state */
+
+#ifdef	CONFIG_KDB_OFF
+int kdb_on = 0;				/* Default is off */
+#else
+int kdb_on = 1;				/* Default is on */
+#endif	/* CONFIG_KDB_OFF */
+
+const char *kdb_diemsg;
+
+#ifdef KDB_HAVE_LONGJMP
+	/*
+	 * Must have a setjmp buffer per CPU.  Switching cpus will
+	 * cause the jump buffer to be setup for the new cpu, and
+	 * subsequent switches (and pager aborts) will use the
+	 * appropriate per-processor values.
+	 */
+kdb_jmp_buf	kdbjmpbuf[NR_CPUS];
+#endif	/* KDB_HAVE_LONGJMP */
+
+	/*
+	 * kdb_commands describes the available commands.
+	 */
+static kdbtab_t kdb_commands[KDB_MAX_COMMANDS];
+
+typedef struct _kdbmsg {
+	int	km_diag;	/* kdb diagnostic */
+	char	*km_msg;	/* Corresponding message text */
+} kdbmsg_t;
+
+#define KDBMSG(msgnum, text) \
+	{ KDB_##msgnum, text }
+
+static kdbmsg_t kdbmsgs[] = {
+	KDBMSG(NOTFOUND,"Command Not Found"),
+	KDBMSG(ARGCOUNT, "Improper argument count, see usage."),
+	KDBMSG(BADWIDTH, "Illegal value for BYTESPERWORD use 1, 2, 4 or 8, 8 is only allowed on 64 bit systems"),
+	KDBMSG(BADRADIX, "Illegal value for RADIX use 8, 10 or 16"),
+	KDBMSG(NOTENV, "Cannot find environment variable"),
+	KDBMSG(NOENVVALUE, "Environment variable should have value"),
+	KDBMSG(NOTIMP, "Command not implemented"),
+	KDBMSG(ENVFULL, "Environment full"),
+	KDBMSG(ENVBUFFULL, "Environment buffer full"),
+	KDBMSG(TOOMANYBPT, "Too many breakpoints defined"),
+	KDBMSG(TOOMANYDBREGS, "More breakpoints than db registers defined"),
+	KDBMSG(DUPBPT, "Duplicate breakpoint address"),
+	KDBMSG(BPTNOTFOUND, "Breakpoint not found"),
+	KDBMSG(BADMODE, "Invalid IDMODE"),
+	KDBMSG(BADINT, "Illegal numeric value"),
+	KDBMSG(INVADDRFMT, "Invalid symbolic address format"),
+	KDBMSG(BADREG, "Invalid register name"),
+	KDBMSG(BADCPUNUM, "Invalid cpu number"),
+	KDBMSG(BADLENGTH, "Invalid length field"),
+	KDBMSG(NOBP, "No Breakpoint exists"),
+	KDBMSG(BADADDR, "Invalid address"),
+};
+#undef KDBMSG
+
+static const int __nkdb_err = sizeof(kdbmsgs) / sizeof(kdbmsg_t);
+
+
+/*
+ * Initial environment.   This is all kept static and local to
+ * this file.   We don't want to rely on the memory allocation
+ * mechanisms in the kernel, so we use a very limited allocate-only
+ * heap for new and altered environment variables.  The entire
+ * environment is limited to a fixed number of entries (add more
+ * to __env[] if required) and a fixed amount of heap (add more to
+ * KDB_ENVBUFSIZE if required).
+ */
+
+static char *__env[] = {
+#if defined(CONFIG_SMP)
+ "PROMPT=[%d]kdb> ",
+ "MOREPROMPT=[%d]more> ",
+#else
+ "PROMPT=kdb> ",
+ "MOREPROMPT=more> ",
+#endif
+ "RADIX=16",
+ "LINES=24",
+ "COLUMNS=80",
+ "MDCOUNT=8",			/* lines of md output */
+ "BTARGS=5",			/* 5 possible args in bt */
+ KDB_PLATFORM_ENV,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+ (char *)0,
+};
+
+static const int __nenv = (sizeof(__env) / sizeof(char *));
+
+/*
+ * kdbgetenv
+ *
+ *	This function will return the character string value of
+ *	an environment variable.
+ *
+ * Parameters:
+ *	match	A character string representing an environment variable.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	NULL	No environment variable matches 'match'
+ *	char*	Pointer to string value of environment variable.
+ * Locking:
+ *	No locking considerations required.
+ * Remarks:
+ */
+char *
+kdbgetenv(const char *match)
+{
+	char **ep = __env;
+	int    matchlen = strlen(match);
+	int i;
+
+	for(i=0; i<__nenv; i++) {
+		char *e = *ep++;
+
+		if (!e) continue;
+
+		if ((strncmp(match, e, matchlen) == 0)
+		 && ((e[matchlen] == '\0')
+		   ||(e[matchlen] == '='))) {
+			char *cp = strchr(e, '=');
+			return (cp)?++cp:"";
+		}
+	}
+	return (char *)0;
+}
+
+/*
+ * kdballocenv
+ *
+ *	This function is used to allocate bytes for environment entries.
+ *
+ * Parameters:
+ *	match	A character string representing a numeric value
+ * Outputs:
+ *	*value  the unsigned long represntation of the env variable 'match'
+ * Returns:
+ *	Zero on success, a kdb diagnostic on failure.
+ * Locking:
+ *	No locking considerations required.  Must be called with all
+ *	processors halted.
+ * Remarks:
+ *	We use a static environment buffer (envbuffer) to hold the values
+ *	of dynamically generated environment variables (see kdb_set).  Buffer
+ *	space once allocated is never free'd, so over time, the amount of space
+ *	(currently 512 bytes) will be exhausted if env variables are changed
+ *	frequently.
+ */
+static char *
+kdballocenv(size_t bytes)
+{
+#define	KDB_ENVBUFSIZE	512
+	static char envbuffer[KDB_ENVBUFSIZE];
+	static int  envbufsize;
+	char *ep = (char *)0;
+
+	if ((KDB_ENVBUFSIZE - envbufsize) >= bytes) {
+		ep = &envbuffer[envbufsize];
+		envbufsize += bytes;
+	}
+	return ep;
+}
+
+/*
+ * kdbgetulenv
+ *
+ *	This function will return the value of an unsigned long-valued
+ *	environment variable.
+ *
+ * Parameters:
+ *	match	A character string representing a numeric value
+ * Outputs:
+ *	*value  the unsigned long represntation of the env variable 'match'
+ * Returns:
+ *	Zero on success, a kdb diagnostic on failure.
+ * Locking:
+ *	No locking considerations required.
+ * Remarks:
+ */
+
+int
+kdbgetulenv(const char *match, unsigned long *value)
+{
+	char *ep;
+
+	ep = kdbgetenv(match);
+	if (!ep) return KDB_NOTENV;
+	if (strlen(ep) == 0) return KDB_NOENVVALUE;
+
+	*value = simple_strtoul(ep, 0, 0);
+
+	return 0;
+}
+
+/*
+ * kdbgetintenv
+ *
+ *	This function will return the value of an integer-valued
+ *	environment variable.
+ *
+ * Parameters:
+ *	match	A character string representing an integer-valued env variable
+ * Outputs:
+ *	*value  the integer representation of the environment variable 'match'
+ * Returns:
+ *	Zero on success, a kdb diagnostic on failure.
+ * Locking:
+ *	No locking considerations required.
+ * Remarks:
+ */
+
+int
+kdbgetintenv(const char *match, int *value) {
+	unsigned long val;
+	int           diag;
+
+	diag = kdbgetulenv(match, &val);
+	if (!diag) {
+		*value = (int) val;
+	}
+	return diag;
+}
+
+/*
+ * kdbgetularg
+ *
+ *	This function will convert a numeric string
+ *	into an unsigned long value.
+ *
+ * Parameters:
+ *	arg	A character string representing a numeric value
+ * Outputs:
+ *	*value  the unsigned long represntation of arg.
+ * Returns:
+ *	Zero on success, a kdb diagnostic on failure.
+ * Locking:
+ *	No locking considerations required.
+ * Remarks:
+ */
+
+int
+kdbgetularg(const char *arg, unsigned long *value)
+{
+	char *endp;
+	unsigned long val;
+
+	val = simple_strtoul(arg, &endp, 0);
+
+	if (endp == arg) {
+		/*
+		 * Try base 16, for us folks too lazy to type the
+		 * leading 0x...
+		 */
+		val = simple_strtoul(arg, &endp, 16);
+		if (endp == arg)
+			return KDB_BADINT;
+	}
+
+	*value = val;
+
+	return 0;
+}
+
+/*
+ * kdbgetaddrarg
+ *
+ *	This function is responsible for parsing an
+ *	address-expression and returning the value of
+ *	the expression, symbol name, and offset to the caller.
+ *
+ *	The argument may consist of a numeric value (decimal or
+ *	hexidecimal), a symbol name, a register name (preceeded
+ *	by the percent sign), an environment variable with a numeric
+ *	value (preceeded by a dollar sign) or a simple arithmetic
+ *	expression consisting of a symbol name, +/-, and a numeric
+ *	constant value (offset).
+ *
+ * Parameters:
+ *	argc	- count of arguments in argv
+ *	argv	- argument vector
+ *	*nextarg - index to next unparsed argument in argv[]
+ *	regs	- Register state at time of KDB entry
+ * Outputs:
+ *	*value	- receives the value of the address-expression
+ *	*offset - receives the offset specified, if any
+ *	*name   - receives the symbol name, if any
+ *	*nextarg - index to next unparsed argument in argv[]
+ *
+ * Returns:
+ *	zero is returned on success, a kdb diagnostic code is
+ *      returned on error.
+ *
+ * Locking:
+ *	No locking requirements.
+ *
+ * Remarks:
+ *
+ */
+
+int
+kdbgetaddrarg(int argc, const char **argv, int *nextarg,
+	      kdb_machreg_t *value,  long *offset,
+	      char **name, kdb_eframe_t ef)
+{
+	kdb_machreg_t addr;
+	long	      off = 0;
+	int	      positive;
+	int	      diag;
+	int	      found = 0;
+	char	     *symname;
+	char	      symbol = '\0';
+	char	     *cp;
+	kdb_symtab_t   symtab;
+
+	/*
+	 * Process arguments which follow the following syntax:
+	 *
+	 *  symbol | numeric-address [+/- numeric-offset]
+	 *  %register
+	 *  $environment-variable
+	 */
+
+	if (*nextarg > argc) {
+		return KDB_ARGCOUNT;
+	}
+
+	symname = (char *)argv[*nextarg];
+
+	/*
+	 * If there is no whitespace between the symbol
+	 * or address and the '+' or '-' symbols, we
+	 * remember the character and replace it with a
+	 * null so the symbol/value can be properly parsed
+	 */
+	if ((cp = strpbrk(symname, "+-")) != NULL) {
+		symbol = *cp;
+		*cp++ = '\0';
+	}
+
+	if (symname[0] == '$') {
+		diag = kdbgetulenv(&symname[1], &addr);
+		if (diag)
+			return diag;
+	} else if (symname[0] == '%') {
+		diag = kdba_getregcontents(&symname[1], ef, &addr);
+		if (diag)
+			return diag;
+	} else {
+		found = kdbgetsymval(symname, &symtab);
+		if (found) {
+			addr = symtab.sym_start;
+		} else {
+			diag = kdbgetularg(argv[*nextarg], &addr);
+			if (diag)
+				return diag;
+		}
+	}
+
+	if (!found)
+		found = kdbnearsym(addr, &symtab);
+
+	(*nextarg)++;
+
+	if (name)
+		*name = symname;
+	if (value)
+		*value = addr;
+	if (offset && name && *name)
+		*offset = addr - symtab.sym_start;
+
+	if ((*nextarg > argc)
+	 && (symbol == '\0'))
+		return 0;
+
+	/*
+	 * check for +/- and offset
+	 */
+
+	if (symbol == '\0') {
+		if ((argv[*nextarg][0] != '+')
+		 && (argv[*nextarg][0] != '-')) {
+			/*
+			 * Not our argument.  Return.
+			 */
+			return 0;
+		} else {
+			positive = (argv[*nextarg][0] == '+');
+			(*nextarg)++;
+		}
+	} else
+		positive = (symbol == '+');
+
+	/*
+	 * Now there must be an offset!
+	 */
+	if ((*nextarg > argc)
+	 && (symbol == '\0')) {
+		return KDB_INVADDRFMT;
+	}
+
+	if (!symbol) {
+		cp = (char *)argv[*nextarg];
+		(*nextarg)++;
+	}
+
+	diag = kdbgetularg(cp, &off);
+	if (diag)
+		return diag;
+
+	if (!positive)
+		off = -off;
+
+	if (offset)
+		*offset += off;
+
+	if (value)
+		*value += off;
+
+	return 0;
+}
+
+static void
+kdb_cmderror(int diag)
+{
+	int i;
+
+	if (diag >= 0) {
+		kdb_printf("no error detected\n");
+		return;
+	}
+
+	for(i=0; i<__nkdb_err; i++) {
+		if (kdbmsgs[i].km_diag == diag) {
+			kdb_printf("diag: %d: %s\n", diag, kdbmsgs[i].km_msg);
+			return;
+		}
+	}
+
+	kdb_printf("Unknown diag %d\n", -diag);
+}
+
+/* The command history feature is not functional at the moment.  It
+ * will be replaced by something that understands editting keys,
+ * including left, right, insert, delete as well as up, down.
+ * Keith Owens, November 18 2000
+ */
+#define KDB_CMD_HISTORY_COUNT	32
+#define CMD_BUFLEN		200	/* kdb_printf: max printline size == 256 */
+static unsigned int cmd_head, cmd_tail;
+static unsigned int cmdptr;
+static char cmd_hist[KDB_CMD_HISTORY_COUNT][CMD_BUFLEN];
+
+/*
+ * kdb_parse
+ *
+ *	Parse the command line, search the command table for a
+ *	matching command and invoke the command function.
+ *
+ * Parameters:
+ *      cmdstr	The input command line to be parsed.
+ *	regs	The registers at the time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic if failure.
+ * Locking:
+ * 	None.
+ * Remarks:
+ *	Limited to 20 tokens.
+ *
+ *	Real rudimentary tokenization. Basically only whitespace
+ *	is considered a token delimeter (but special consideration
+ *	is taken of the '=' sign as used by the 'set' command).
+ *
+ *	The algorithm used to tokenize the input string relies on
+ *	there being at least one whitespace (or otherwise useless)
+ *	character between tokens as the character immediately following
+ *	the token is altered in-place to a null-byte to terminate the
+ *	token string.
+ */
+
+#define MAXARGC	20
+
+static int
+kdb_parse(char *cmdstr, kdb_eframe_t ef)
+{
+	static char *argv[MAXARGC];
+	static int  argc = 0;
+	static char cbuf[CMD_BUFLEN];
+	char *cp, *cpp;
+	kdbtab_t *tp;
+	int i;
+
+	/*
+	 * First tokenize the command string.
+	 */
+	cp = cmdstr;
+
+	if (*cp != '\n' && *cp != '\0') {
+		argc = 0;
+		cpp = cbuf;
+		while (*cp) {
+			/* skip whitespace */
+			while (isspace(*cp)) cp++;
+			if ((*cp == '\0') || (*cp == '\n'))
+				break;
+			argv[argc++] = cpp;
+			/* Copy to next whitespace or '=' */
+			while (*cp && !isspace(*cp)) {
+				if ((*cpp = *cp++) == '=')
+					break;
+				++cpp;
+			}
+			*cpp++ = '\0';	/* Squash a ws or '=' character */
+		}
+	}
+	if (!argc)
+		return 0;
+
+	for(tp=kdb_commands, i=0; i < KDB_MAX_COMMANDS; i++,tp++) {
+		if (tp->cmd_name) {
+			/*
+			 * If this command is allowed to be abbreviated,
+			 * check to see if this is it.
+			 */
+
+			if (tp->cmd_minlen
+			 && (strlen(argv[0]) <= tp->cmd_minlen)) {
+				if (strncmp(argv[0],
+					    tp->cmd_name,
+					    tp->cmd_minlen) == 0) {
+					break;
+				}
+			}
+
+			if (strcmp(argv[0], tp->cmd_name)==0) {
+				break;
+			}
+		}
+	}
+	
+	/* 
+	 * If we don't find a command by this name, see if the first 
+	 * few characters of this match any of the known commands.
+	 * e.g., md1c20 should match md.
+	 */
+	if (i == KDB_MAX_COMMANDS) {
+		for(tp=kdb_commands, i=0; i < KDB_MAX_COMMANDS; i++,tp++) {
+			if (tp->cmd_name) {
+				if (strncmp(argv[0], 
+					    tp->cmd_name,
+					    strlen(tp->cmd_name))==0) {
+					break;
+				}
+			}
+		}
+	}
+
+	if (i < KDB_MAX_COMMANDS) {
+		int result;
+		KDB_STATE_SET(CMD);
+		result = (*tp->cmd_func)(argc-1,
+				       (const char**)argv,
+				       (const char**)__env,
+				       ef);
+		KDB_STATE_CLEAR(CMD);
+		switch (tp->cmd_repeat) {
+		case KDB_REPEAT_NONE:
+			argc = 0;
+			if (argv[0])
+				*(argv[0]) = '\0';
+			break;
+		case KDB_REPEAT_NO_ARGS:
+			argc = 1;
+			if (argv[1])
+				*(argv[1]) = '\0';
+			break;
+		case KDB_REPEAT_WITH_ARGS:
+			break;
+		}
+		return result;
+	}
+
+	/*
+	 * If the input with which we were presented does not
+	 * map to an existing command, attempt to parse it as an
+	 * address argument and display the result.   Useful for
+	 * obtaining the address of a variable, or the nearest symbol
+	 * to an address contained in a register.
+	 */
+	{
+		kdb_machreg_t value;
+		char *name = NULL;
+		long offset;
+		int nextarg = 0;
+
+		if (kdbgetaddrarg(0, (const char **)argv, &nextarg,
+				  &value, &offset, &name, ef)) {
+			return KDB_NOTFOUND;
+		}
+
+		kdb_printf("%s = ", argv[0]);
+		kdb_symbol_print(value, NULL, KDB_SP_DEFAULT);
+		kdb_printf("\n");
+		return 0;
+	}
+}
+
+
+static int
+handle_ctrl_cmd(char *cmd)
+{
+#define CTRL_P	16
+#define CTRL_N	14
+
+	/* initial situation */
+	if (cmd_head == cmd_tail) return 1;
+
+	switch(*cmd) {
+		case '\n':
+		case CTRL_P:
+			if (cmdptr != cmd_tail)
+				cmdptr = (cmdptr-1) % KDB_CMD_HISTORY_COUNT;
+			strcpy(cmd, cmd_hist[cmdptr]);
+			return 0;	
+		case CTRL_N:
+			if (cmdptr != (cmd_head-1))
+				cmdptr = (cmdptr+1) % KDB_CMD_HISTORY_COUNT;
+			strcpy(cmd, cmd_hist[cmdptr]);
+			return 0;
+	}
+	return 1;
+}
+
+
+/*
+ * kdb_local
+ *
+ *	The main code for kdb.  This routine is invoked on a specific
+ *	processor, it is not global.  The main kdb() routine ensures
+ *	that only one processor at a time is in this routine.  This
+ *	code is called with the real reason code on the first entry
+ *	to a kdb session, thereafter it is called with reason SWITCH,
+ *	even if the user goes back to the original cpu.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	ef		The exception frame at time of fault/breakpoint.  NULL
+ *			for reason SILENT, otherwise valid.
+ *	db_result	Result code from the break or debug point.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ *	KDB_CMD_GO	User typed 'go'.
+ *	KDB_CMD_CPU	User switched to another cpu.
+ *	KDB_CMD_SS	Single step.
+ *	KDB_CMD_SSB	Single step until branch.
+ * Locking:
+ *	none
+ * Remarks:
+ *	none
+ */
+
+static int
+kdb_local(kdb_reason_t reason, int error, kdb_eframe_t ef, kdb_dbtrap_t db_result)
+{
+	char		*cmdbuf;
+	char		cmd[CMD_BUFLEN];
+	int		diag;
+	typeof (*ef)	local_ef;
+
+	if (reason != KDB_REASON_DEBUG &&
+	    reason != KDB_REASON_SILENT) {
+		kdb_printf("\nEntering kdb (current=0x%p, pid %d) ", (void *)current, current->pid);
+#if defined(CONFIG_SMP)
+		kdb_printf("on processor %d ", smp_processor_id());
+#endif
+	}
+
+	switch (reason) {
+	case KDB_REASON_DEBUG:
+	{
+		/*
+		 * If re-entering kdb after a single step
+		 * command, don't print the message.
+		 */
+		switch(db_result) {
+		case KDB_DB_BPT:
+			kdb_printf("\nEntering kdb (0x%p) ", (void *)current);
+#if defined(CONFIG_SMP)
+			kdb_printf("on processor %d ", smp_processor_id());
+#endif
+			kdb_printf("due to Debug @ " kdb_machreg_fmt "\n", kdba_getpc(ef));
+			break;
+		case KDB_DB_SSB:
+			/*
+			 * In the midst of ssb command. Just return.
+			 */
+			return KDB_CMD_SSB;	/* Continue with SSB command */
+
+			break;
+		case KDB_DB_SS:
+			break;
+		case KDB_DB_SSBPT:
+			return 1;	/* kdba_db_trap did the work */
+		default:
+			kdb_printf("kdb: Bad result from kdba_db_trap: %d\n",
+				   db_result);
+			break;
+		}
+
+	}
+		break;
+	case KDB_REASON_FAULT:
+		break;
+	case KDB_REASON_ENTER:
+		kdb_printf("due to KDB_ENTER()\n");
+		break;
+	case KDB_REASON_KEYBOARD:
+		kdb_printf("due to Keyboard Entry\n");
+		break;
+	case KDB_REASON_SWITCH:
+		kdb_printf("due to cpu switch\n");
+		break;
+	case KDB_REASON_CALL:	
+		if (ef) break; /* drop through if regs is not specified */
+	case KDB_REASON_PANIC:
+		if (reason == KDB_REASON_CALL)
+			kdb_printf("due to direct function call\n");
+		else
+			kdb_printf("due to panic\n");
+		/*
+		 *  Get a set of registers that defines the current
+		 * context (as of the call to kdb).
+		 */
+		memset(&local_ef, 0, sizeof(local_ef));
+		ef = &local_ef;
+		kdba_getcurrentframe(ef);
+		kdba_setpc(ef, (kdb_machreg_t)(&kdb));	/* for traceback */
+		break;
+	case KDB_REASON_OOPS:
+		kdb_printf("Oops: %s\n", kdb_diemsg);
+		kdb_printf("due to oops @ " kdb_machreg_fmt "\n", kdba_getpc(ef));
+		kdba_dumpregs(ef, NULL, NULL);
+		break;
+	case KDB_REASON_NMI:
+		kdb_printf("due to NonMaskable Interrupt @ " kdb_machreg_fmt "\n",
+			  kdba_getpc(ef));
+		kdba_dumpregs(ef, NULL, NULL);
+		break;
+	case KDB_REASON_WATCHDOG:
+		kdb_printf("due to WatchDog Interrupt @ " kdb_machreg_fmt "\n",
+			  kdba_getpc(ef));
+		kdba_dumpregs(ef, NULL, NULL);
+		break;
+	case KDB_REASON_BREAK:
+		kdb_printf("due to Breakpoint @ " kdb_machreg_fmt "\n", kdba_getpc(ef));
+		/*
+		 * Determine if this breakpoint is one that we
+		 * are interested in.
+		 */
+		if (db_result != KDB_DB_BPT) {
+			kdb_printf("kdb: error return from kdba_bp_trap: %d\n", db_result);
+			return 0;	/* Not for us, dismiss it */
+		}
+		break;
+	case KDB_REASON_RECURSE:
+		kdb_printf("due to Recursion @ " kdb_machreg_fmt "\n", kdba_getpc(ef));
+		break;
+	case KDB_REASON_SILENT:
+		return KDB_CMD_GO;	/* Silent entry, silent exit */
+		break;
+	default:
+		kdb_printf("kdb: unexpected reason code: %d\n", reason);
+		return 0;	/* Not for us, dismiss it */
+	}
+
+	while (1) {
+		/*
+		 * Initialize pager context.
+		 */
+		kdb_nextline = 1;
+		KDB_STATE_CLEAR(SUPPRESS);
+#ifdef KDB_HAVE_LONGJMP
+		/*
+		 * Use kdba_setjmp/kdba_longjmp to break out of
+		 * the pager early and to attempt to recover from kdb errors.
+		 */
+		KDB_STATE_CLEAR(LONGJMP);
+		if (kdba_setjmp(&kdbjmpbuf[smp_processor_id()])) {
+			/* Command aborted (usually in pager) */
+			continue;
+		}
+		else
+			KDB_STATE_SET(LONGJMP);
+#endif	/* KDB_HAVE_LONGJMP */
+
+do_full_getstr:
+#if defined(CONFIG_SMP)
+	kdb_printf(kdbgetenv("PROMPT"), smp_processor_id());
+#else
+	kdb_printf(kdbgetenv("PROMPT"));
+#endif
+
+
+		cmdbuf = cmd_hist[cmd_head];
+		*cmdbuf = '\0';
+		/*
+		 * Fetch command from keyboard
+		 */
+		cmdbuf = kdb_getstr(cmdbuf, CMD_BUFLEN,"");
+		if (*cmdbuf < 32 && *cmdbuf != '\n')
+			if (handle_ctrl_cmd(cmdbuf))
+				goto do_full_getstr;
+
+		if (*cmdbuf != '\n') {
+			cmd_head = (cmd_head+1) % KDB_CMD_HISTORY_COUNT;
+			if (cmd_head == cmd_tail) cmd_tail = (cmd_tail+1) % KDB_CMD_HISTORY_COUNT;
+
+		}
+
+		cmdptr = cmd_head;
+		strcpy(cmd, cmdbuf); /* copy because of destructive parsing */
+		diag = kdb_parse(cmd, ef);
+		if (diag == KDB_NOTFOUND) {
+			kdb_printf("Unknown kdb command: '%s'\n", cmd);
+			diag = 0;
+		}
+		if (diag == KDB_CMD_GO
+		 || diag == KDB_CMD_CPU
+		 || diag == KDB_CMD_SS
+		 || diag == KDB_CMD_SSB)
+			break;
+
+		if (diag)
+			kdb_cmderror(diag);
+	}
+
+	return(diag);
+}
+
+
+/*
+ * kdb_print_state
+ *
+ *	Print the state data for the current processor for debugging.
+ *
+ * Inputs:
+ *	text		Identifies the debug point
+ *	value		Any integer value to be printed, e.g. reason code.
+ * Returns:
+ *	None.
+ * Locking:
+ *	none
+ * Remarks:
+ *	none
+ */
+
+void kdb_print_state(const char *text, int value)
+{
+	kdb_printf("state: %s cpu %d value %d initial %d state %x\n",
+		text, smp_processor_id(), value, kdb_initial_cpu, kdb_state[smp_processor_id()]);
+}
+
+/*
+ * kdb_previous_event
+ *
+ *	Return a count of cpus that are leaving kdb, i.e. the number
+ *	of processors that are still handling the previous kdb event.
+ *
+ * Inputs:
+ *	None.
+ * Returns:
+ *	Count of cpus in previous event.
+ * Locking:
+ *	none
+ * Remarks:
+ *	none
+ */
+
+static int
+kdb_previous_event(void)
+{
+	int i, leaving = 0;
+	for (i = 0; i < NR_CPUS; ++i) {
+		if (KDB_STATE_CPU(LEAVING, i))
+			++leaving;
+	}
+	return(leaving);
+}
+
+/*
+ * kdb_main_loop
+ *
+ * The main kdb loop.  After initial setup and assignment of the controlling
+ * cpu, all cpus are in this loop.  One cpu is in control and will issue the kdb
+ * prompt, the others will spin until 'go' or cpu switch.
+ *
+ * To get a consistent view of the kernel stacks for all processes, this routine
+ * is invoked from the main kdb code via an architecture specific routine.
+ * kdba_main_loop is responsible for making the kernel stacks consistent for all
+ * processes, there should be no difference between a blocked process and a
+ * running process as far as kdb is concerned.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	reason2		kdb's current reason code.  Initially error but can change
+ *			acording to kdb state.
+ *	db_result	Result code from break or debug point.
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT or KDB_REASON_PANIC then ef is NULL,
+ *			otherwise it should always be valid.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ * Locking:
+ *	none
+ * Remarks:
+ *	none
+ */
+
+int
+kdb_main_loop(kdb_reason_t reason, kdb_reason_t reason2, int error,
+	      kdb_dbtrap_t db_result, kdb_eframe_t ef)
+{
+	int result = 1;
+	/* Stay in kdb() until 'go', 'ss[b]' or an error */
+	while (1) {
+		int i;
+		/*
+		 * All processors except the one that is in control
+		 * will spin here.
+		 */
+		KDB_DEBUG_STATE("kdb_main_loop 1", reason);
+		while (KDB_STATE(HOLD_CPU))
+			;
+		KDB_STATE_CLEAR(SUPPRESS);
+		KDB_DEBUG_STATE("kdb_main_loop 2", reason);
+		if (KDB_STATE(LEAVING))
+			break;	/* Another cpu said 'go' */
+
+		/* Still using kdb, this processor is in control */
+		result = kdb_local(reason2, error, ef, db_result);
+		KDB_DEBUG_STATE("kdb_main_loop 3", result);
+
+		if (result == KDB_CMD_CPU) {
+			/* Cpu switch, hold the current cpu, release the target one. */
+			reason2 = KDB_REASON_SWITCH;
+			KDB_STATE_SET(HOLD_CPU);
+			KDB_STATE_CLEAR_CPU(HOLD_CPU, kdb_new_cpu);
+			continue;
+		}
+
+		if (result == KDB_CMD_SS) {
+			KDB_STATE_SET(DOING_SS);
+			break;
+		}
+
+		if (result == KDB_CMD_SSB) {
+			KDB_STATE_SET(DOING_SS);
+			KDB_STATE_SET(DOING_SSB);
+			break;
+		}
+
+		if (result && result != 1 && result != KDB_CMD_GO)
+			kdb_printf("\nUnexpected kdb_local return code %d\n", result);
+
+		/*
+		 * All other return codes (including KDB_CMD_GO) from
+		 * kdb_local will end kdb().  Release all other cpus
+		 * which will see KDB_STATE(LEAVING) is set.
+		 */
+		for (i = 0; i < NR_CPUS; ++i) {
+			if (KDB_STATE_CPU(KDB, i))
+				KDB_STATE_SET_CPU(LEAVING, i);
+			KDB_STATE_CLEAR_CPU(WAIT_IPI, i);
+			KDB_STATE_CLEAR_CPU(HOLD_CPU, i);
+		}
+		KDB_DEBUG_STATE("kdb_main_loop 4", reason);
+		break;
+	}
+	return(result != 0);
+}
+
+/*
+ * kdb
+ *
+ * 	This function is the entry point for the kernel debugger.  It
+ *	provides a command parser and associated support functions to
+ *	allow examination and control of an active kernel.
+ *
+ * 	This function may be invoked directly from any
+ *	point in the kernel by calling with reason == KDB_REASON_CALL
+ *	(XXX - note that the regs aren't set up this way - could
+ *	       use a software interrupt to enter kdb to get regs...)
+ *
+ *	The breakpoint trap code should invoke this function with
+ *	one of KDB_REASON_BREAK (int 03) or KDB_REASON_DEBUG (debug register)
+ *
+ *	the die_if_kernel function should invoke this function with
+ *	KDB_REASON_OOPS.
+ *
+ *	the panic function should invoke this function with KDB_REASON_PANIC.
+ *
+ *	The kernel fault handler should invoke this function with
+ *	reason == KDB_REASON_FAULT and error == trap vector #.
+ *
+ *	In single step mode, one cpu is released to run without
+ *	breakpoints.   Interrupts and NMI are reset to their original values,
+ *	the cpu is allowed to do one instruction which causes a trap
+ *	into kdb with KDB_REASON_DEBUG.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT or KDB_REASON_PANIC then ef is NULL,
+ *			otherwise it should always be valid.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ * Locking:
+ *	none
+ * Remarks:
+ *	No assumptions of system state.  This function may be invoked
+ *	with arbitrary locks held.  It will stop all other processors
+ *	in an SMP environment, disable all interrupts and does not use
+ *	the operating systems keyboard driver.
+ *
+ *	This code is reentrant but only for cpu switch.  Any other
+ *	reentrancy is an error, although kdb will attempt to recover.
+ *
+ *	At the start of a kdb session the initial processor is running
+ *	kdb() and the other processors can be doing anything.  When the
+ *	initial processor calls smp_kdb_stop() the other processors are
+ *	driven through kdb_ipi which calls kdb() with reason SWITCH.
+ *	That brings all processors into this routine, one with a "real"
+ *	reason code, the other with SWITCH.
+ *
+ *	Because the other processors are driven via smp_kdb_stop(),
+ *	they enter here from the NMI handler.  Until the other
+ *	processors exit from here and exit from kdb_ipi, they will not
+ *	take any more NMI requests.  The initial cpu will still take NMI.
+ *
+ *	Multiple race and reentrancy conditions, each with different
+ *	advoidance mechanisms.
+ *
+ *	Two cpus hit debug points at the same time.
+ *
+ *	  kdb_lock and kdb_initial_cpu ensure that only one cpu gets
+ *	  control of kdb.  The others spin on kdb_initial_cpu until
+ *	  they are driven through NMI into kdb_ipi.  When the initial
+ *	  cpu releases the others from NMI, they resume trying to get
+ *	  kdb_initial_cpu to start a new event.
+ *
+ *	A cpu is released from kdb and starts a new event before the
+ *	original event has completely ended.
+ *
+ *	  kdb_previous_event() prevents any cpu from entering
+ *	  kdb_initial_cpu state until the previous event has completely
+ *	  ended on all cpus.
+ *
+ *      An exception occurs inside kdb.
+ *
+ *	  kdb_initial_cpu detects recursive entry to kdb and attempts
+ *	  to recover.  The recovery uses longjmp() which means that
+ *	  recursive calls to kdb never return.  Beware of assumptions
+ *	  like
+ *
+ *          ++depth;
+ *          kdb();
+ *          --depth;
+ *
+ *        If the kdb call is recursive then longjmp takes over and
+ *        --depth is never executed.
+ *
+ *      NMI handling.
+ *
+ *	  NMI handling is tricky.  The initial cpu is invoked by some kdb event,
+ *	  this event could be NMI driven but usually is not.  The other cpus are
+ *	  driven into kdb() via kdb_ipi which uses NMI so at the start the other
+ *	  cpus will not accept NMI.  Some operations such as SS release one cpu
+ *	  but hold all the others.  Releasing a cpu means it drops back to
+ *	  whatever it was doing before the kdb event, this means it drops out of
+ *	  kdb_ipi and hence out of NMI status.  But the software watchdog uses
+ *	  NMI and we do not want spurious watchdog calls into kdb.  kdba_read()
+ *	  resets the watchdog counters in its input polling loop, when a kdb
+ *	  command is running it is subject to NMI watchdog events.
+ *
+ *	  Another problem with NMI handling is the NMI used to drive the other
+ *	  cpus into kdb cannot be distinguished from the watchdog NMI.  State
+ *	  flag WAIT_IPI indicates that a cpu is waiting for NMI via kdb_ipi,
+ *	  if not set then software NMI is ignored by kdb_ipi.
+ *
+ *      Cpu switching.
+ *
+ *        All cpus are in kdb (or they should be), all but one are
+ *        spinning on KDB_STATE(HOLD_CPU).  Only one cpu is not in
+ *        HOLD_CPU state, only that cpu can handle commands.
+ *
+ */
+
+int
+kdb(kdb_reason_t reason, int error, kdb_eframe_t ef)
+{
+	kdb_intstate_t	int_state;	/* Interrupt state */
+	kdb_reason_t	reason2 = reason;
+	int		result = 1;	/* Default is kdb handled it */
+	int		ss_event;
+	kdb_dbtrap_t 	db_result=KDB_DB_NOBPT;
+
+	if (!kdb_on)
+		return 0;
+
+	KDB_DEBUG_STATE("kdb 1", reason);
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	/* Filter out userspace breakpoints first, no point in doing all
+	 * the kdb smp fiddling when it is really a gdb trap.
+	 * Save the single step status first, kdba_db_trap clears ss status.
+	 */
+	ss_event = reason != KDB_REASON_PANIC && (KDB_STATE(DOING_SS) || KDB_STATE(SSBPT));
+	if (reason == KDB_REASON_BREAK)
+		db_result = kdba_bp_trap(ef, error);	/* Only call this once */
+	if (reason == KDB_REASON_DEBUG)
+		db_result = kdba_db_trap(ef, error);	/* Only call this once */
+
+	if ((reason == KDB_REASON_BREAK || reason == KDB_REASON_DEBUG)
+	 && db_result == KDB_DB_NOBPT) {
+		KDB_DEBUG_STATE("kdb 2", reason);
+		return 0;	/* Not one of mine */
+	}
+
+	/* Turn off single step if it was being used */
+	if (ss_event) {
+		kdba_clearsinglestep(ef);
+		/* Single step after a breakpoint removes the need for a delayed reinstall */
+		if (reason == KDB_REASON_BREAK || reason == KDB_REASON_DEBUG) {
+			KDB_STATE_SET(NO_BP_DELAY);
+		}
+	}
+
+	/* kdb can validly reenter but only for certain well defined conditions */
+	if (reason == KDB_REASON_DEBUG
+	 && !KDB_STATE(HOLD_CPU)
+	 && ss_event)
+		KDB_STATE_SET(REENTRY);
+	else
+		KDB_STATE_CLEAR(REENTRY);
+
+	/* Wait for previous kdb event to completely exit before starting
+	 * a new event.
+	 */
+	while (kdb_previous_event())
+		;
+	KDB_DEBUG_STATE("kdb 3", reason);
+
+	/*
+	 * If kdb is already active, print a message and try to recover.
+	 * If recovery is not possible and recursion is allowed or
+	 * forced recursion without recovery is set then try to recurse
+	 * in kdb.  Not guaranteed to work but it makes an attempt at
+	 * debugging the debugger.
+	 */
+	if (reason != KDB_REASON_SWITCH) {
+		if (KDB_IS_RUNNING() && !KDB_STATE(REENTRY)) {
+			int recover = 1;
+			unsigned long recurse = 0;
+			kdb_printf("kdb: Debugger re-entered on cpu %d, new reason = %d\n",
+				smp_processor_id(), reason);
+			/* Should only re-enter from released cpu */
+			if (KDB_STATE(HOLD_CPU)) {
+				kdb_printf("     Strange, cpu %d should not be running\n", smp_processor_id());
+				recover = 0;
+			}
+			if (!KDB_STATE(CMD)) {
+				kdb_printf("     Not executing a kdb command\n");
+				recover = 0;
+			}
+			if (!KDB_STATE(LONGJMP)) {
+				kdb_printf("     No longjmp available for recovery\n");
+				recover = 0;
+			}
+			kdbgetulenv("RECURSE", &recurse);
+			if (recurse > 1) {
+				kdb_printf("     Forced recursion is set\n");
+				recover = 0;
+			}
+			if (recover) {
+				kdb_printf("     Attempting to abort command and recover\n");
+#ifdef KDB_HAVE_LONGJMP
+				kdba_longjmp(&kdbjmpbuf[smp_processor_id()], 0);
+#endif
+			}
+			if (recurse) {
+				if (KDB_STATE(RECURSE)) {
+					kdb_printf("     Already in recursive mode\n");
+				} else {
+					kdb_printf("     Attempting recursive mode\n");
+					KDB_STATE_SET(RECURSE);
+					KDB_STATE_SET(REENTRY);
+					reason2 = KDB_REASON_RECURSE;
+					recover = 1;
+				}
+			}
+			if (!recover) {
+				kdb_printf("     Cannot recover, allowing event to proceed\n");
+				return(0);
+			}
+		}
+	} else if (!KDB_IS_RUNNING()) {
+		kdb_printf("kdb: CPU switch without kdb running, I'm confused\n");
+		return(0);
+	}
+
+	/*
+	 * Disable interrupts, breakpoints etc. on this processor
+	 * during kdb command processing
+	 */
+	KDB_STATE_SET(KDB);
+	kdba_disableint(&int_state);
+	if (!KDB_STATE(KDB_CONTROL)) {
+		kdb_bp_remove_local();
+		kdba_disable_lbr();
+		KDB_STATE_SET(KDB_CONTROL);
+	}
+	else if (KDB_DEBUG(LBR))
+		kdba_print_lbr();
+
+	/*
+	 * If not entering the debugger due to CPU switch or single step
+	 * reentry, serialize access here.
+	 * The processors may race getting to this point - if,
+	 * for example, more than one processor hits a breakpoint
+	 * at the same time.   We'll serialize access to kdb here -
+	 * other processors will loop here, and the NMI from the stop
+	 * IPI will take them into kdb as switch candidates.  Once
+	 * the initial processor releases the debugger, the rest of
+	 * the processors will race for it.
+	 */
+	if (reason == KDB_REASON_SWITCH
+	 || KDB_STATE(REENTRY))
+		;	/* drop through */
+	else {
+		KDB_DEBUG_STATE("kdb 4", reason);
+		spin_lock(&kdb_lock);
+
+		while (KDB_IS_RUNNING() || kdb_previous_event()) {
+			spin_unlock(&kdb_lock);
+
+			while (KDB_IS_RUNNING() || kdb_previous_event())
+				;
+
+			spin_lock(&kdb_lock);
+		}
+		KDB_DEBUG_STATE("kdb 5", reason);
+
+		kdb_initial_cpu = smp_processor_id();
+		spin_unlock(&kdb_lock);
+	}
+
+	if (smp_processor_id() == kdb_initial_cpu
+	 && !KDB_STATE(REENTRY)) {
+		KDB_STATE_CLEAR(HOLD_CPU);
+		KDB_STATE_CLEAR(WAIT_IPI);
+		/*
+		 * Remove the global breakpoints.  This is only done
+		 * once from the initial processor on initial entry.
+		 */
+		kdb_bp_remove_global();
+
+		/*
+		 * If SMP, stop other processors.  The other processors
+		 * will enter kdb() with KDB_REASON_SWITCH and spin
+		 * below.
+		 */
+		KDB_DEBUG_STATE("kdb 6", reason);
+		if (smp_num_cpus > 1) {
+			int i;
+			for (i = 0; i < NR_CPUS; ++i) {
+				if (i != kdb_initial_cpu) {
+					KDB_STATE_SET_CPU(HOLD_CPU, i);
+					KDB_STATE_SET_CPU(WAIT_IPI, i);
+				}
+			}
+			KDB_DEBUG_STATE("kdb 7", reason);
+			smp_kdb_stop();
+			KDB_DEBUG_STATE("kdb 8", reason);
+		}
+	}
+
+	/* Set up a consistent set of process stacks before talking to the user */
+	KDB_DEBUG_STATE("kdb 9", result);
+	result = kdba_main_loop(reason, reason2, error, db_result, ef);
+
+	KDB_DEBUG_STATE("kdb 10", result);
+	kdba_adjust_ip(reason, error, ef);
+	KDB_STATE_CLEAR(LONGJMP);
+	KDB_DEBUG_STATE("kdb 11", result);
+
+	/* No breakpoints installed for SS */
+	if (!KDB_STATE(DOING_SS) &&
+	    !KDB_STATE(SSBPT) &&
+	    !KDB_STATE(RECURSE)) {
+		KDB_DEBUG_STATE("kdb 12", result);
+		kdba_enable_lbr();
+		kdb_bp_install_local(ef);
+		KDB_STATE_CLEAR(NO_BP_DELAY);
+		KDB_STATE_CLEAR(KDB_CONTROL);
+	}
+
+	KDB_DEBUG_STATE("kdb 13", result);
+	kdba_restoreint(&int_state);
+
+	KDB_STATE_CLEAR(KDB);		/* Main kdb state has been cleared */
+	KDB_STATE_CLEAR(LEAVING);	/* Elvis has left the building ... */
+	KDB_DEBUG_STATE("kdb 14", result);
+
+	if (smp_processor_id() == kdb_initial_cpu &&
+	  !KDB_STATE(DOING_SS) &&
+	  !KDB_STATE(RECURSE)) {
+		/*
+		 * (Re)install the global breakpoints.  This is only done
+		 * once from the initial processor on final exit.
+		 */
+		KDB_DEBUG_STATE("kdb 15", reason);
+		kdb_bp_install_global(ef);
+		/* Wait until all the other processors leave kdb */
+		while (kdb_previous_event())
+			;
+		kdb_initial_cpu = -1;	/* release kdb control */
+		KDB_DEBUG_STATE("kdb 16", reason);
+	}
+
+	KDB_STATE_CLEAR(RECURSE);
+	KDB_DEBUG_STATE("kdb 17", reason);
+	return(result != 0);
+}
+
+/*
+ * kdb_mdr
+ *
+ *	This function implements the guts of the 'mdr' command.
+ *
+ *	mdr  <addr arg>,<byte count>
+ *
+ * Inputs:
+ *	addr	Start address
+ *	count	Number of bytes
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Always 0.  Any errors are detected and printed by kdb_getarea.
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+static int
+kdb_mdr(kdb_machreg_t addr, unsigned int count)
+{
+	unsigned char c;
+	while (count--) {
+		if (kdb_getarea(c, addr))
+			return(0);
+		kdb_printf("%02x", c);
+		addr++;
+	}
+	kdb_printf("\n");
+	return(0);
+}
+
+/*
+ * kdb_md
+ *
+ *	This function implements the 'md', 'md1', 'md2', 'md4', 'md8'
+ *	'mdr' and 'mds' commands.
+ *
+ *	md|mds  [<addr arg> [<line count> [<radix>]]]
+ *	mdWcN	[<addr arg> [<line count> [<radix>]]] 
+ *		where W = is the width (1, 2, 4 or 8) and N is the count.
+ *		for eg., md1c20 reads 20 bytes, 1 at a time.
+ *	mdr  <addr arg>,<byte count>
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_md(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	static kdb_machreg_t last_addr;
+	static int last_radix, last_bytesperword, last_repeat;
+	int radix = 16, mdcount = 8, bytesperword = sizeof(kdb_machreg_t), repeat;
+	int nosect = 0;
+	char fmtchar, fmtstr[64];
+	kdb_machreg_t addr;
+	unsigned long word;
+	long offset = 0;
+	kdb_symtab_t symtab;
+	int symbolic = 0;
+	int valid = 0;
+
+	kdbgetintenv("MDCOUNT", &mdcount);
+	kdbgetintenv("RADIX", &radix);
+	kdbgetintenv("BYTESPERWORD", &bytesperword);
+
+	/* Assume 'md <addr>' and start with environment values */
+	repeat = mdcount * 16 / bytesperword;
+
+	if (strcmp(argv[0], "mdr") == 0) {
+		if (argc != 2) 
+			return KDB_ARGCOUNT;
+		valid = 1;
+	} else if (isdigit(argv[0][2])) {
+		bytesperword = (int)(argv[0][2] - '0');
+		last_bytesperword = bytesperword;
+		repeat = mdcount * 16 / bytesperword;
+		if (!argv[0][3])
+			valid = 1;
+		else if (argv[0][3] == 'c' && argv[0][4]) {
+			char *p;
+			repeat = simple_strtoul(argv[0]+4, &p, 10);
+			mdcount = ((repeat * bytesperword) + 15) / 16;
+			valid = !*p;
+		}
+		last_repeat = repeat;
+	} else if (strcmp(argv[0], "md") == 0)
+		valid = 1;
+	else if (strcmp(argv[0], "mds") == 0)
+		valid = 1;
+	if (!valid)
+		return KDB_NOTFOUND;
+
+	if (argc == 0) {
+		if (last_addr == 0)
+			return KDB_ARGCOUNT;
+		addr = last_addr;
+		radix = last_radix;
+		bytesperword = last_bytesperword;
+		repeat = last_repeat;
+		mdcount = ((repeat * bytesperword) + 15) / 16;
+	} 
+
+	if (argc) {
+		kdb_machreg_t val;
+		int diag, nextarg = 1;
+		diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+		if (diag)
+			return diag;
+		if (argc > nextarg+2)
+			return KDB_ARGCOUNT;
+
+		if (argc >= nextarg) {
+			diag = kdbgetularg(argv[nextarg], &val);
+			if (!diag) {
+				mdcount = (int) val;
+				repeat = mdcount * 16 / bytesperword;
+			}
+		}
+		if (argc >= nextarg+1) {
+			diag = kdbgetularg(argv[nextarg+1], &val);
+			if (!diag)
+				radix = (int) val;
+		}
+	}
+
+	if (strcmp(argv[0], "mdr") == 0) {
+		return(kdb_mdr(addr, mdcount));
+	}
+
+	switch (radix) {
+	case 10:
+		fmtchar = 'd';
+		break;
+	case 16:
+		fmtchar = 'x';
+		break;
+	case 8:
+		fmtchar = 'o';
+		break;
+	default:
+		return KDB_BADRADIX;
+	}
+
+	last_radix = radix;
+
+	if (bytesperword > sizeof(kdb_machreg_t))
+		return KDB_BADWIDTH;
+
+	switch (bytesperword) {
+	case 8:
+		sprintf(fmtstr, "%%16.16l%c ", fmtchar);
+		break;
+	case 4:
+		sprintf(fmtstr, "%%8.8l%c ", fmtchar);
+		break;
+	case 2:
+		sprintf(fmtstr, "%%4.4l%c ", fmtchar);
+		break;
+	case 1:
+		sprintf(fmtstr, "%%2.2l%c ", fmtchar);
+		break;
+	default:
+		return KDB_BADWIDTH;
+	}
+
+	last_repeat = repeat;
+	last_bytesperword = bytesperword;
+
+	if (strcmp(argv[0], "mds") == 0) {
+		symbolic = 1;
+		/* Do not save these changes as last_*, they are temporary mds
+		 * overrides.
+		 */
+		bytesperword = sizeof(kdb_machreg_t);
+		repeat = mdcount;
+		kdbgetintenv("NOSECT", &nosect);
+	}
+
+	/* Round address down modulo BYTESPERWORD */
+
+	addr &= ~(bytesperword-1);
+
+	while (repeat > 0) {
+		int	num = (symbolic?1 :(16 / bytesperword));
+		char	cbuf[32];
+		char	*c = cbuf;
+		int     i;
+
+		memset(cbuf, '\0', sizeof(cbuf));
+		kdb_printf(kdb_machreg_fmt0 " ", addr);
+
+		for(i = 0; i < num && repeat--; i++) {
+			if (kdb_getword(&word, addr, bytesperword))
+				return 0;
+
+			kdb_printf(fmtstr, word);
+			if (symbolic) {
+				kdbnearsym(word, &symtab);
+			}
+			else {
+				memset(&symtab, 0, sizeof(symtab));
+			}
+			if (symtab.sym_name) {
+				kdb_symbol_print(word, &symtab, 0);
+				if (!nosect) {
+					kdb_printf("\n");
+					kdb_printf("                       %s %s "
+						   kdb_machreg_fmt " " kdb_machreg_fmt " " kdb_machreg_fmt,
+						symtab.mod_name,
+						symtab.sec_name,
+						symtab.sec_start,
+						symtab.sym_start,
+						symtab.sym_end);
+				}
+				addr += bytesperword;
+			} else {
+#define printable_char(addr) ({char __c = '\0'; unsigned long __addr = (addr); kdb_getarea(__c, __addr); isprint(__c) ? __c : '.';})
+				switch (bytesperword) {
+				case 8:
+					*c++ = printable_char(addr++);
+					*c++ = printable_char(addr++);
+					*c++ = printable_char(addr++);
+					*c++ = printable_char(addr++);
+				case 4:
+					*c++ = printable_char(addr++);
+					*c++ = printable_char(addr++);
+				case 2:
+					*c++ = printable_char(addr++);
+				case 1:
+					*c++ = printable_char(addr++);
+					break;
+				}
+#undef printable_char
+			}
+		}
+		kdb_printf("%*s %s\n", (int)((num-i)*(2*bytesperword + 1)+1), " ", cbuf);
+	}
+	last_addr = addr;
+
+	return 0;
+}
+
+/*
+ * kdb_mm
+ *
+ *	This function implements the 'mm' command.
+ *
+ *	mm address-expression new-value
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	mm works on machine words, mmW works on bytes.
+ */
+
+int
+kdb_mm(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	kdb_machreg_t addr;
+	long 	      offset = 0;
+	unsigned long contents;
+	int nextarg;
+	int width;
+
+	if (argv[0][2] && !isdigit(argv[0][2]))
+		return KDB_NOTFOUND;
+
+	if (argc < 2) {
+		return KDB_ARGCOUNT;
+	}
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)))
+		return diag;
+
+	if (nextarg > argc)
+		return KDB_ARGCOUNT;
+
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &contents, NULL, NULL, regs)))
+		return diag;
+
+	if (nextarg != argc + 1)
+		return KDB_ARGCOUNT;
+
+	width = argv[0][2] ? (argv[0][2] - '0') : (sizeof(kdb_machreg_t));
+	if ((diag = kdb_putword(addr, contents, width)))
+		return(diag);
+
+	kdb_printf(kdb_machreg_fmt " = " kdb_machreg_fmt "\n", addr, contents);
+
+	return 0;
+}
+
+/*
+ * kdb_go
+ *
+ *	This function implements the 'go' command.
+ *
+ *	go [address-expression]
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_CMD_GO for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_go(int argc, const char **argv, const char **envp, kdb_eframe_t ef)
+{
+	kdb_machreg_t addr;
+	int diag;
+	int nextarg;
+	long offset;
+
+	if (argc == 1) {
+		nextarg = 1;
+		diag = kdbgetaddrarg(argc, argv, &nextarg,
+				     &addr, &offset, NULL, ef);
+		if (diag)
+			return diag;
+
+		kdba_setpc(ef, addr);
+	} else if (argc)
+		return KDB_ARGCOUNT;
+
+	return KDB_CMD_GO;
+}
+
+/*
+ * kdb_rd
+ *
+ *	This function implements the 'rd' command.
+ *
+ *	rd		display all general registers.
+ *	rd  c		display all control registers.
+ *	rd  d		display all debug registers.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_rd(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	/*
+	 */
+
+	if (argc == 0) {
+		return kdba_dumpregs(regs, NULL, NULL);
+	}
+
+	if (argc > 2) {
+		return KDB_ARGCOUNT;
+	}
+
+	return kdba_dumpregs(regs, argv[1], argv[2]);
+}
+
+/*
+ * kdb_rm
+ *
+ *	This function implements the 'rm' (register modify)  command.
+ *
+ *	rm register-name new-contents
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	Currently doesn't allow modification of control or
+ *	debug registers, nor does it allow modification
+ *	of model-specific registers (MSR).
+ */
+
+int
+kdb_rm(int argc, const char **argv, const char **envp, kdb_eframe_t ef)
+{
+	int diag;
+	int ind = 0;
+	kdb_machreg_t contents;
+
+	if (argc != 2) {
+		return KDB_ARGCOUNT;
+	}
+
+	/*
+	 * Allow presence or absence of leading '%' symbol.
+	 */
+
+	if (argv[1][0] == '%')
+		ind = 1;
+
+	diag = kdbgetularg(argv[2], &contents);
+	if (diag)
+		return diag;
+
+	diag = kdba_setregcontents(&argv[1][ind], ef, contents);
+	if (diag)
+		return diag;
+
+	return 0;
+}
+
+#if defined(CONFIG_MAGIC_SYSRQ)
+/*
+ * kdb_sr
+ *
+ *	This function implements the 'sr' (SYSRQ key) command which
+ *	interfaces to the soi-disant MAGIC SYSRQ functionality.
+ *
+ *	sr <magic-sysrq-code>
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	None.
+ */
+int
+kdb_sr(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	if (argc != 1) {
+		return KDB_ARGCOUNT;
+	}
+
+	handle_sysrq(*argv[1], regs, 0, 0);
+
+	return 0;
+}
+#endif	/* CONFIG_MAGIC_SYSRQ */
+
+/*
+ * kdb_ef
+ *
+ *	This function implements the 'ef' (display exception frame)
+ *	command.  This command takes an address and expects to find
+ *	an exception frame at that address, formats and prints it.
+ *
+ *	ef address-expression
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	Not done yet.
+ */
+
+int
+kdb_ef(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	kdb_machreg_t   addr;
+	long		offset;
+	int nextarg;
+
+	if (argc == 1) {
+		nextarg = 1;
+		diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+		if (diag)
+			return diag;
+
+		return kdba_dumpregs((struct pt_regs *)addr, NULL, NULL);
+	}
+
+	return KDB_ARGCOUNT;
+}
+
+/*
+ * kdb_reboot
+ *
+ *	This function implements the 'reboot' command.  Reboot the system
+ *	immediately.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	Shouldn't return from this function.
+ */
+
+int
+kdb_reboot(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	machine_restart(0);
+	/* NOTREACHED */
+	return 0;
+}
+
+
+#if defined(CONFIG_MODULES)
+extern struct module *find_module(const char *);
+extern void free_module(struct module *, int);
+
+/*
+ * kdb_lsmod
+ *
+ *	This function implements the 'lsmod' command.  Lists currently
+ *	loaded kernel modules.
+ *
+ *	Mostly taken from userland lsmod.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *
+ */
+
+int
+kdb_lsmod(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct module *mod;
+	struct module_ref *mr;
+
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	kdb_printf("Module                  Size  modstruct     Used by\n");
+	for (mod = module_list; mod && mod->next ;mod = mod->next) {
+		kdb_printf("%-20s%8lu  0x%p  %4ld ", mod->name, mod->size, (void *)mod,
+			(long)atomic_read(&mod->uc.usecount));
+
+		if (mod->flags & MOD_DELETED)
+			kdb_printf(" (deleted)");
+		else if (mod->flags & MOD_INITIALIZING)
+			kdb_printf(" (initializing)");
+		else if (!(mod->flags & MOD_RUNNING))
+			kdb_printf(" (uninitialized)");
+		else {
+			if (mod->flags &  MOD_AUTOCLEAN)
+				kdb_printf(" (autoclean)");
+			if (!(mod->flags & MOD_USED_ONCE))
+				kdb_printf(" (unused)");
+		}
+
+		if (mod->refs) {
+			kdb_printf(" [ ");
+
+			mr = mod->refs;
+			while (mr) {
+				kdb_printf("%s ", mr->ref->name);
+				mr = mr->next_ref;
+			}
+
+			kdb_printf("]");
+		}
+
+		kdb_printf("\n");
+	}
+
+	return 0;
+}
+
+/*
+ * kdb_rmmod
+ *
+ *	This function implements the 'rmmod' command.  Removes a given
+ *	kernel module.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	Danger: free_module() calls mod->cleanup().  If the cleanup routine
+ *	relies on interrupts then it will hang, kdb has interrupts disabled.
+ */
+
+int
+kdb_rmmod(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct module *mod;
+
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	kdb_printf("Attempting to remove module: [%s]\n", argv[1]);
+	if ((mod = find_module(argv[1])) == NULL) {
+		kdb_printf("Unable to find a module by that name\n");
+		return 0;
+	}
+
+	if (mod->refs != NULL || __MOD_IN_USE(mod)) {
+		kdb_printf("Module is in use, unable to unload\n");
+		return 0;
+	}
+
+	free_module(mod, 0);
+	kdb_printf("Module successfully unloaded\n");
+
+	return 0;
+}
+#endif	/* CONFIG_MODULES */
+
+/*
+ * kdb_env
+ *
+ *	This function implements the 'env' command.  Display the current
+ *	environment variables.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_env(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int i;
+
+	for(i=0; i<__nenv; i++) {
+		if (__env[i]) {
+			kdb_printf("%s\n", __env[i]);
+		}
+	}
+
+	if (KDB_DEBUG(MASK))
+		kdb_printf("KDBFLAGS=0x%x\n", kdb_flags);
+
+	return 0;
+}
+
+/*
+ * kdb_set
+ *
+ *	This function implements the 'set' command.  Alter an existing
+ *	environment variable or create a new one.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_set(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int i;
+	char *ep;
+	size_t varlen, vallen;
+
+	/*
+	 * we can be invoked two ways:
+	 *   set var=value    argv[1]="var", argv[2]="value"
+	 *   set var = value  argv[1]="var", argv[2]="=", argv[3]="value"
+	 * - if the latter, shift 'em down.
+	 */
+	if (argc == 3) {
+		argv[2] = argv[3];
+		argc--;
+	}
+
+	if (argc != 2)
+		return KDB_ARGCOUNT;
+
+	/*
+	 * Check for internal variables
+	 */
+	if (strcmp(argv[1], "KDBDEBUG") == 0) {
+		unsigned int debugflags;
+		char *cp;
+
+		debugflags = simple_strtoul(argv[2], &cp, 0);
+		if (cp == argv[2] || debugflags & ~KDB_DEBUG_FLAG_MASK) {
+			kdb_printf("kdb: illegal debug flags '%s'\n",
+				    argv[2]);
+			return 0;
+		}
+		kdb_flags = (kdb_flags & ~(KDB_DEBUG_FLAG_MASK << KDB_DEBUG_FLAG_SHIFT))
+			  | (debugflags << KDB_DEBUG_FLAG_SHIFT);
+
+		return 0;
+	}
+
+	/*
+	 * Tokenizer squashed the '=' sign.  argv[1] is variable
+	 * name, argv[2] = value.
+	 */
+	varlen = strlen(argv[1]);
+	vallen = strlen(argv[2]);
+	ep = kdballocenv(varlen + vallen + 2);
+	if (ep == (char *)0)
+		return KDB_ENVBUFFULL;
+
+	sprintf(ep, "%s=%s", argv[1], argv[2]);
+
+	ep[varlen+vallen+1]='\0';
+
+	for(i=0; i<__nenv; i++) {
+		if (__env[i]
+		 && ((strncmp(__env[i], argv[1], varlen)==0)
+		   && ((__env[i][varlen] == '\0')
+		    || (__env[i][varlen] == '=')))) {
+			__env[i] = ep;
+			return 0;
+		}
+	}
+
+	/*
+	 * Wasn't existing variable.  Fit into slot.
+	 */
+	for(i=0; i<__nenv-1; i++) {
+		if (__env[i] == (char *)0) {
+			__env[i] = ep;
+			return 0;
+		}
+	}
+
+	return KDB_ENVFULL;
+}
+
+/*
+ * kdb_cpu
+ *
+ *	This function implements the 'cpu' command.
+ *
+ *	cpu	[<cpunum>]
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_CMD_CPU for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	All cpu's should be spinning in kdb().  However just in case
+ *	a cpu did not take the smp_kdb_stop NMI, check that a cpu
+ *	entered kdb() before passing control to it.
+ */
+
+int
+kdb_cpu(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	unsigned long cpunum;
+	int diag;
+
+	if (argc == 0) {
+		int i;
+
+		kdb_printf("Currently on cpu %d\n", smp_processor_id());
+		kdb_printf("Available cpus: ");
+		for (i=0; i<NR_CPUS; i++) {
+			if (cpu_online_map & (1UL << i)) {
+				if (i) kdb_printf(", ");
+				kdb_printf("%d", i);
+				if (!KDB_STATE_CPU(KDB, i))
+					kdb_printf("*");
+			}
+		}
+		kdb_printf("\n");
+		return 0;
+	}
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetularg(argv[1], &cpunum);
+	if (diag)
+		return diag;
+
+	/*
+	 * Validate cpunum
+	 */
+	if ((cpunum > NR_CPUS)
+	 || !(cpu_online_map & (1UL << cpunum))
+	 || !KDB_STATE_CPU(KDB, cpunum))
+		return KDB_BADCPUNUM;
+
+	kdb_new_cpu = cpunum;
+
+	/*
+	 * Switch to other cpu
+	 */
+	return KDB_CMD_CPU;
+}
+
+/*
+ * kdb_ps
+ *
+ *	This function implements the 'ps' command which shows
+ *	a list of the active processes.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_ps(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct task_struct	*p;
+
+	kdb_printf("%-*s Pid      Parent   [*] cpu  State %-*s Command\n",
+		(int)(2*sizeof(void *))+2, "Task Addr",
+		(int)(2*sizeof(void *))+2, "Thread");
+	for_each_task(p) {
+		kdb_printf("0x%p %08d %08d  %1.1d  %3.3d  %s  0x%p%c%s\n",
+			   (void *)p, p->pid, p->p_pptr->pid,
+			   task_has_cpu(p), p->processor,
+			   (p->state == 0)?"run ":(p->state>0)?"stop":"unrn",
+			   (void *)(&p->thread),
+			   (p == current) ? '*': ' ',
+			   p->comm);
+	}
+
+	return 0;
+}
+
+/*
+ * kdb_ll
+ *
+ *	This function implements the 'll' command which follows a linked
+ *	list and executes an arbitrary command for each element.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_ll(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	kdb_machreg_t addr;
+	long 	      offset = 0;
+	kdb_machreg_t va;
+	unsigned long linkoffset;
+	int nextarg;
+
+	if (argc != 3) {
+		return KDB_ARGCOUNT;
+	}
+
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	diag = kdbgetularg(argv[2], &linkoffset);
+	if (diag)
+		return diag;
+
+	/*
+	 * Using the starting address as
+	 * the first element in the list, and assuming that
+	 * the list ends with a null pointer.
+	 */
+
+	va = addr;
+
+	while (va) {
+		char buf[80];
+
+		sprintf(buf, "%s " kdb_machreg_fmt "\n", argv[3], va);
+		diag = kdb_parse(buf, regs);
+		if (diag)
+			return diag;
+
+		addr = va + linkoffset;
+		if (kdb_getword(&va, addr, sizeof(va)))
+			return(0);
+	}
+
+	return 0;
+}
+
+/*
+ * kdb_sections_callback
+ *
+ *	Invoked from kallsyms_sections for each section.
+ *
+ * Inputs:
+ *	prevmod	Previous module name
+ *	modname	Module name
+ *	secname	Section name
+ *	secstart Start of section
+ *	secend	End of section
+ *	secflags Section flags
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Always zero
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+static int
+kdb_sections_callback(void *token, const char *modname, const char *secname,
+		      ElfW(Addr) secstart, ElfW(Addr) secend, ElfW(Word) secflags)
+{
+	const char **prevmod = (const char **)token;
+	if (*prevmod != modname) {
+		*prevmod = modname;
+		kdb_printf("\n%s", modname);
+	}
+	kdb_printf(" %s " kdb_elfw_addr_fmt0 " " kdb_elfw_addr_fmt0 " 0x%x",
+		secname, secstart, secend, secflags);
+	return(0);
+}
+
+/*
+ * kdb_sections
+ *
+ *	This function implements the 'sections' command which prints the
+ *	kernel and module sections.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Always zero
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_sections(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	char *prev_mod = NULL;
+	if (argc != 0) {
+		return KDB_ARGCOUNT;
+	}
+	kallsyms_sections(&prev_mod, kdb_sections_callback);
+	kdb_printf("\n");	/* End last module */
+	return(0);
+}
+
+/*
+ * kdb_help
+ *
+ *	This function implements the 'help' and '?' commands.
+ *
+ * Inputs:
+ *	argc	argument count
+ *	argv	argument vector
+ *	envp	environment vector
+ *	regs	registers at time kdb was entered.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ */
+
+int
+kdb_help(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	kdbtab_t *kt;
+
+	kdb_printf("%-15.15s %-20.20s %s\n", "Command", "Usage", "Description");
+	kdb_printf("----------------------------------------------------------\n");
+	for(kt=kdb_commands; kt->cmd_name; kt++) {
+		kdb_printf("%-15.15s %-20.20s %s\n", kt->cmd_name,
+			kt->cmd_usage, kt->cmd_help);
+	}
+	return 0;
+}
+
+/*
+ * kdb_register_repeat
+ *
+ *	This function is used to register a kernel debugger command.
+ *
+ * Inputs:
+ *	cmd	Command name
+ *	func	Function to execute the command
+ *	usage	A simple usage string showing arguments
+ *	help	A simple help string describing command
+ *	repeat	Does the command auto repeat on enter?
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, one if a duplicate command.
+ * Locking:
+ *	none.
+ * Remarks:
+ *
+ */
+
+int
+kdb_register_repeat(char *cmd,
+		    kdb_func_t func,
+		    char *usage,
+		    char *help,
+		    short minlen,
+		    kdb_repeat_t repeat)
+{
+	int i;
+	kdbtab_t *kp;
+
+	/*
+	 *  Brute force method to determine duplicates
+	 */
+	for (i=0, kp=kdb_commands; i<KDB_MAX_COMMANDS; i++, kp++) {
+		if (kp->cmd_name && (strcmp(kp->cmd_name, cmd)==0)) {
+			kdb_printf("Duplicate kdb command registered: '%s'\n",
+				   cmd);
+			return 1;
+		}
+	}
+
+	/*
+	 * Insert command into first available location in table
+	 */
+	for (i=0, kp=kdb_commands; i<KDB_MAX_COMMANDS; i++, kp++) {
+		if (kp->cmd_name == NULL) {
+			kp->cmd_name   = cmd;
+			kp->cmd_func   = func;
+			kp->cmd_usage  = usage;
+			kp->cmd_help   = help;
+			kp->cmd_flags  = 0;
+			kp->cmd_minlen = minlen;
+			kp->cmd_repeat = repeat;
+			break;
+		}
+	}
+	return 0;
+}
+
+/*
+ * kdb_register
+ *
+ *	Compatibility register function for commands that do not need to
+ *	specify a repeat state.  Equivalent to kdb_register_repeat with
+ *	KDB_REPEAT_NONE.
+ *
+ * Inputs:
+ *	cmd	Command name
+ *	func	Function to execute the command
+ *	usage	A simple usage string showing arguments
+ *	help	A simple help string describing command
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, one if a duplicate command.
+ * Locking:
+ *	none.
+ * Remarks:
+ *
+ */
+
+int
+kdb_register(char *cmd,
+	     kdb_func_t func,
+	     char *usage,
+	     char *help,
+	     short minlen)
+{
+	return kdb_register_repeat(cmd, func, usage, help, minlen, KDB_REPEAT_NONE);
+}
+
+/*
+ * kdb_unregister
+ *
+ *	This function is used to unregister a kernel debugger command.
+ *	It is generally called when a module which implements kdb
+ *	commands is unloaded.
+ *
+ * Inputs:
+ *	cmd	Command name
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, one command not registered.
+ * Locking:
+ *	none.
+ * Remarks:
+ *
+ */
+
+int
+kdb_unregister(char *cmd)
+{
+	int i;
+	kdbtab_t *kp;
+
+	/*
+	 *  find the command.
+	 */
+	for (i=0, kp=kdb_commands; i<KDB_MAX_COMMANDS; i++, kp++) {
+		if (kp->cmd_name && (strcmp(kp->cmd_name, cmd)==0)) {
+			kp->cmd_name = NULL;
+			return 0;
+		}
+	}
+
+	/*
+	 * Couldn't find it.
+	 */
+	return 1;
+}
+
+/*
+ * kdb_inittab
+ *
+ *	This function is called by the kdb_init function to initialize
+ *	the kdb command table.   It must be called prior to any other
+ *	call to kdb_register_repeat.
+ *
+ * Inputs:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+static void __init
+kdb_inittab(void)
+{
+	int i;
+	kdbtab_t *kp;
+
+	for(i=0, kp=kdb_commands; i < KDB_MAX_COMMANDS; i++,kp++) {
+		kp->cmd_name = NULL;
+	}
+
+	kdb_register_repeat("md", kdb_md, "<vaddr>",   "Display Memory Contents, also mdWcN, e.g. md8c1", 1, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("mdr", kdb_md, "<vaddr> <bytes>", 	"Display Raw Memory", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("mds", kdb_md, "<vaddr>", 	"Display Memory Symbolically", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("mm", kdb_mm, "<vaddr> <contents>",   "Modify Memory Contents", 0, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("id", kdb_id, "<vaddr>",   "Display Instructions", 1, KDB_REPEAT_NO_ARGS);
+	kdb_register_repeat("go", kdb_go, "[<vaddr>]", "Continue Execution", 1, KDB_REPEAT_NONE);
+	kdb_register_repeat("rd", kdb_rd, "",		"Display Registers", 1, KDB_REPEAT_NONE);
+	kdb_register_repeat("rm", kdb_rm, "<reg> <contents>", "Modify Registers", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("ef", kdb_ef, "<vaddr>",   "Display exception frame", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("bt", kdb_bt, "[<vaddr>]", "Stack traceback", 1, KDB_REPEAT_NONE);
+	kdb_register_repeat("btp", kdb_bt, "<pid>", 	"Display stack for process <pid>", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("bta", kdb_bt, "", 	"Display stack all processes", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("ll", kdb_ll, "<first-element> <linkoffset> <cmd>", "Execute cmd for each element in linked list", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("env", kdb_env, "", 	"Show environment variables", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("set", kdb_set, "", 	"Set environment variables", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("help", kdb_help, "", 	"Display Help Message", 1, KDB_REPEAT_NONE);
+	kdb_register_repeat("?", kdb_help, "",         "Display Help Message", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("cpu", kdb_cpu, "<cpunum>","Switch to new cpu", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("ps", kdb_ps, "", 		"Display active task list", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("reboot", kdb_reboot, "",  "Reboot the machine immediately", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("sections", kdb_sections, "",  "List kernel and module sections", 0, KDB_REPEAT_NONE);
+#if defined(CONFIG_MODULES)
+	kdb_register_repeat("lsmod", kdb_lsmod, "",	"List loaded kernel modules", 0, KDB_REPEAT_NONE);
+	kdb_register_repeat("rmmod", kdb_rmmod, "<modname>", "Remove a kernel module", 1, KDB_REPEAT_NONE);
+#endif
+#if defined(CONFIG_MAGIC_SYSRQ)
+	kdb_register_repeat("sr", kdb_sr, "<key>",	"Magic SysRq key", 0, KDB_REPEAT_NONE);
+#endif
+}
+
+/*
+ * kdb_cmd_init
+ *
+ *	This function is called by the kdb_init function to execute any
+ *	commands defined in kdb_cmds.
+ *
+ * Inputs:
+ *	Commands in *kdb_cmds[];
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+static void __init
+kdb_cmd_init(void)
+{
+	int i, diag;
+	for (i = 0; kdb_cmds[i]; ++i) {
+		kdb_printf("kdb_cmd[%d]: %s", i, kdb_cmds[i]);
+		diag = kdb_parse(kdb_cmds[i], NULL);
+		if (diag)
+			kdb_printf("command failed, kdb diag %d\n", diag);
+	}
+}
+
+/*
+ * kdb_panic
+ *
+ *	Invoked via the panic_notifier_list.
+ *
+ * Inputs:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	When this function is called from panic(), the other cpus have already
+ *	been stopped.
+ *
+ */
+
+static int
+kdb_panic(struct notifier_block *self, unsigned long command, void *ptr)
+{
+	kdb(KDB_REASON_PANIC, 0, NULL);
+	return(0);
+}
+
+static struct notifier_block kdb_block = { kdb_panic, NULL, 0 };
+
+/*
+ * kdb_init
+ *
+ * 	Initialize the kernel debugger environment.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+void __init
+kdb_init(void)
+{
+	/*
+	 * This must be called before any calls to kdb_printf.
+	 */
+	kdb_io_init();
+
+	kdb_inittab();		/* Initialize Command Table */
+	kdb_initbptab();	/* Initialize Breakpoint Table */
+	kdb_id_init();		/* Initialize Disassembler */
+	kdba_init();		/* Architecture Dependent Initialization */
+
+	/*
+	 * Use printk() to get message in log_buf[];
+	 */
+	printk("kdb version %d.%d%s by Scott Lurndal, Keith Owens. "\
+	       "Copyright SGI, All Rights Reserved\n",
+		KDB_MAJOR_VERSION, KDB_MINOR_VERSION, KDB_TEST_VERSION);
+
+	kdb_cmd_init();		/* Preset commands from kdb_cmds */
+	kdb(KDB_REASON_SILENT, 0, 0);	/* Activate any preset breakpoints on boot cpu */
+	notifier_chain_register(&panic_notifier_list, &kdb_block);
+}
+
+EXPORT_SYMBOL(kdb_register);
+EXPORT_SYMBOL(kdb_register_repeat);
+EXPORT_SYMBOL(kdb_unregister);
+EXPORT_SYMBOL(kdb_getarea_size);
+EXPORT_SYMBOL(kdb_putarea_size);
+EXPORT_SYMBOL(kdb_getword);
+EXPORT_SYMBOL(kdb_putword);
+EXPORT_SYMBOL(kdbgetularg);
+EXPORT_SYMBOL(kdbgetenv);
+EXPORT_SYMBOL(kdbgetintenv);
+EXPORT_SYMBOL(kdbgetaddrarg);
+EXPORT_SYMBOL(kdb);
+EXPORT_SYMBOL(kdb_on);
+EXPORT_SYMBOL(kdbgetsymval);
+EXPORT_SYMBOL(kdbnearsym);
+EXPORT_SYMBOL(kdb_printf);
+EXPORT_SYMBOL(kdb_symbol_print);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/kdbsupport.c linuxppc64_2_4/kdb/kdbsupport.c
--- linux-2.4.19/kdb/kdbsupport.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/kdbsupport.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,639 @@
+/*
+ * Kernel Debugger Architecture Independent Support Functions
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+#include <stdarg.h>
+#include <linux/config.h>
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/kallsyms.h>
+#include <linux/stddef.h>
+#include <linux/vmalloc.h>
+#include <asm/uaccess.h>
+
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+/*
+ * Symbol table functions.
+ */
+
+/*
+ * kdbgetsymval
+ *
+ *	Return the address of the given symbol.
+ *
+ * Parameters:
+ * 	symname	Character string containing symbol name
+ *      symtab  Structure to receive results
+ * Outputs:
+ * Returns:
+ *	0	Symbol not found, symtab zero filled
+ *	1	Symbol mapped to module/symbol/section, data in symtab
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdbgetsymval(const char *symname, kdb_symtab_t *symtab)
+{
+	memset(symtab, 0, sizeof(*symtab));
+	return(kallsyms_symbol_to_address(
+		symname,
+		NULL,
+		&symtab->mod_name,
+		&symtab->mod_start,
+		&symtab->mod_end,
+		&symtab->sec_name,
+		&symtab->sec_start,
+		&symtab->sec_end,
+		&symtab->sym_name,
+		&symtab->sym_start,
+		&symtab->sym_end));
+}
+
+/*
+ * kdbnearsym
+ *
+ *	Return the name of the symbol with the nearest address
+ *	less than 'addr'.
+ *
+ * Parameters:
+ * 	addr	Address to check for symbol near
+ *      symtab  Structure to receive results
+ * Outputs:
+ * Returns:
+ *	0	No sections contain this address, symtab zero filled
+ *	1	Address mapped to module/symbol/section, data in symtab
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdbnearsym(unsigned long addr, kdb_symtab_t *symtab)
+{
+	memset(symtab, 0, sizeof(*symtab));
+	return(kallsyms_address_to_symbol(
+		addr,
+		&symtab->mod_name,
+		&symtab->mod_start,
+		&symtab->mod_end,
+		&symtab->sec_name,
+		&symtab->sec_start,
+		&symtab->sec_end,
+		&symtab->sym_name,
+		&symtab->sym_start,
+		&symtab->sym_end));
+}
+
+#if defined(CONFIG_SMP)
+/*
+ * kdb_ipi
+ *
+ *	This function is called from the non-maskable interrupt
+ *	handler to handle a kdb IPI instruction.
+ *
+ * Inputs:
+ *	ef	= Exception frame pointer
+ * Outputs:
+ *	None.
+ * Returns:
+ *	0	- Did not handle NMI
+ *	1	- Handled NMI
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Initially one processor is invoked in the kdb() code.  That
+ *	processor sends an ipi which drives this routine on the other
+ *	processors.  All this does is call kdb() with reason SWITCH.
+ *	This puts all processors into the kdb() routine and all the
+ *	code for breakpoints etc. is in one place.
+ *	One problem with the way the kdb NMI is sent, the NMI has no
+ *	identification that says it came from kdb.  If the cpu's kdb state is
+ *	marked as "waiting for kdb_ipi" then the NMI is treated as coming from
+ *	kdb, otherwise it is assumed to be for another reason and is ignored.
+ */
+
+int
+kdb_ipi(kdb_eframe_t ef, void (*ack_interrupt)(void))
+{
+	/* Do not print before checking and clearing WAIT_IPI, IPIs are
+	 * going all the time.
+	 */
+	if (KDB_STATE(WAIT_IPI)) {
+		/*
+		 * Stopping other processors via smp_kdb_stop().
+		 */
+		if (ack_interrupt)
+			(*ack_interrupt)();	/* Acknowledge the interrupt */
+		KDB_STATE_CLEAR(WAIT_IPI);
+		KDB_DEBUG_STATE("kdb_ipi 1", 0);
+		kdb(KDB_REASON_SWITCH, 0, ef);	/* Spin in kdb() */
+		KDB_DEBUG_STATE("kdb_ipi 2", 0);
+		return 1;
+	}
+	return 0;
+}
+#endif	/* CONFIG_SMP */
+
+void
+kdb_enablehwfault(void)
+{
+	kdba_enable_mce();
+}
+
+/*
+ * kdb_get_next_ar
+ *
+ *	Get the next activation record from the stack.
+ *
+ * Inputs:
+ *	arend	Last byte +1 of the activation record.  sp for the first
+ *		frame, start of callee's activation record otherwise.
+ *	func	Start address of function.
+ *	pc	Current program counter within this function.  pc for
+ *		the first frame, caller's return address otherwise.
+ *	fp	Current frame pointer.  Register fp for the first
+ *		frame, oldfp otherwise.  0 if not known.
+ *	ss	Start of stack for the current process.
+ * Outputs:
+ *	ar	Activation record.
+ *	symtab	kallsyms symbol table data for the calling function.
+ * Returns:
+ *	1 if ar is usable, 0 if not.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Activation Record format, assuming a stack that grows down
+ *	(KDB_STACK_DIRECTION == -1).
+ *
+ *	+-----------------------------+   ^         =====================
+ *	| Return address, frame 3     |   |
+ *	+-----------------------------+   |
+ *	| Frame Pointer, frame 3      |>--'
+ *	+-----------------------------+<--.
+ *	| Locals and automatics,      |   |
+ *	| frame 2. (variable size)    |   |                 AR 2
+ *	+-----------------------------+   |
+ *	| Save registers,             |   |
+ *	| frame 2. (variable size)    |   |
+ *	+-----------------------------+   |
+ *	| Arguments to frame 1,       |   |
+ *	| (variable size)             |   |
+ *	+-----------------------------+   |         =====================
+ *	| Return address, frame 2     |   |
+ *	+-----------------------------+   |
+ *	| Frame Pointer, frame 2      |>--'
+ *	+-----------------------------+<--.
+ *	| Locals and automatics,      |   |
+ *	| frame 1. (variable size)    |   |                 AR 1
+ *	+-----------------------------+   |
+ *	| Save registers,             |   |
+ *	| frame 1. (variable size)    |   |
+ *	+-----------------------------+   |
+ *	| Arguments to frame 0,       |   |
+ *	| (variable size)             |   |
+ *	+-----------------------------+   |  -- (5) =====================
+ *	| Return address, frame 1     |   |
+ *	+-----------------------------+   |  -- (0)
+ *	| Frame Pointer, frame 1      |>--'
+ *	+-----------------------------+      -- (1), (2)
+ *	| Locals and automatics,      |
+ *	| frame 0. (variable size)    |                     AR 0
+ *	+-----------------------------+      -- (3)
+ *	| Save registers,             |
+ *	| frame 0. (variable size)    |
+ *	+-----------------------------+      -- (4) =====================
+ *
+ * The stack for the top frame can be in one of several states.
+ *  (0) Immediately on entry to the function, stack pointer (sp) is
+ *      here.
+ *  (1) If the function was compiled with frame pointers and the 'push
+ *      fp' instruction has been executed then the pointer to the
+ *      previous frame is on the stack.  However there is no guarantee
+ *      that this saved pointer is valid, the calling function might
+ *      not have frame pointers.  sp is adjusted by wordsize after
+ *      'push fp'.
+ *  (2) If the function was compiled with frame pointers and the 'copy
+ *      sp to fp' instruction has been executed then fp points here.
+ *  (3) If the function startup has 'adjust sp by 0xnn bytes' and that
+ *      instruction has been executed then sp has been adjusted by
+ *      0xnn bytes for local and automatic variables.
+ *  (4) If the function startup has one or more 'push reg' instructions
+ *      and any have been executed then sp has been adjusted by
+ *      wordsize bytes for each register saved.
+ *
+ * As the function exits it rewinds the stack, typically to (1) then (0).
+ *
+ * The stack entries for the lower frames is normally are in state (5).
+ *  (5) Arguments for the called frame are on to the stack.
+ * However lower frames can be incomplete if there is an interrupt in
+ * progress.
+ *
+ * An activation record runs from the return address for a function
+ * through to the return address for the next function or sp, whichever
+ * comes first.  For each activation record we extract :-
+ *
+ *   start    Address of the activation record.
+ *   end      Address of the last byte+1 in the activation record.
+ *   ret      Return address to caller.
+ *   oldfp    Frame pointer to previous frame, 0 if this function was
+ *            not compiled with frame pointers.
+ *   fp       Frame pointer for the current frame, 0 if this function
+ *            was not compiled with frame pointers or fp has not been
+ *            set yet.
+ *   arg0     Address of the first argument (in the previous activation
+ *            record).
+ *   locals   Bytes allocated to locals and automatics.
+ *   regs     Bytes allocated to saved registers.
+ *   args     Bytes allocated to arguments (in the previous activation
+ *            record).
+ *   setup    Bytes allocated to setup data on stack (return address,
+ *	      frame pointer).
+ *
+ * Although the kernel might be compiled with frame pointers, we still
+ * have to assume the worst and validate the frame.  Some calls from
+ * asm code to C code might not use frame pointers.  Third party binary
+ * only modules might be compiled without frame pointers, even when the
+ * rest of the kernel has frame pointers.  Some routines are always
+ * compiled with frame pointers, even if the overall kernel is not.  A
+ * routine compiled with frame pointers can be called from a routine
+ * without frame pointers, the previous "frame pointer" is saved on
+ * stack but it contains garbage.
+ *
+ * We check the object code to see if it saved a frame pointer and we
+ * validate that pointer.  Basically frame pointers are hints.
+ */
+
+#define FORCE_ARG(ar,n)	(ar)->setup = (ar)->locals = (ar)->regs = \
+			(ar)->fp = (ar)->oldfp = (ar)->ret = 0; \
+			(ar)->start = (ar)->end - KDB_STACK_DIRECTION*(n)*sizeof(unsigned long);
+
+int
+kdb_get_next_ar(kdb_machreg_t arend, kdb_machreg_t func,
+		kdb_machreg_t pc, kdb_machreg_t fp, kdb_machreg_t ss,
+		kdb_ar_t *ar, kdb_symtab_t *symtab)
+{
+	if (KDB_DEBUG(AR)) {
+		kdb_printf("kdb_get_next_ar: arend=0x%lx func=0x%lx pc=0x%lx fp=0x%lx\n",
+			arend, func, pc, fp);
+	}
+
+	memset(ar, 0, sizeof(*ar));
+	if (!kdbnearsym(pc, symtab)) {
+		symtab->sym_name = symtab->sec_name = "<unknown>";
+		symtab->mod_name = "kernel";
+		if (KDB_DEBUG(AR)) {
+			kdb_printf("kdb_get_next_ar: callee not in kernel\n");
+		}
+		pc = 0;
+	}
+
+	if (!kdba_prologue(symtab, pc, arend, fp, ss, 0, ar)) {
+		if (KDB_DEBUG(AR)) {
+			kdb_printf("kdb_get_next_ar: callee prologue failed\n");
+		}
+		return(0);
+	}
+	if (KDB_DEBUG(AR)) {
+		kdb_printf("kdb_get_next_ar: callee activation record\n");
+		kdb_printf("  start=0x%lx end=0x%lx ret=0x%lx oldfp=0x%lx fp=0x%lx\n",
+			ar->start, ar->end, ar->ret, ar->oldfp, ar->fp);
+		kdb_printf("  locals=%ld regs=%ld setup=%ld\n",
+			ar->locals, ar->regs, ar->setup);
+	}
+
+	if (ar->ret) {
+		/* Run the caller code to get arguments to callee function */
+		kdb_symtab_t	caller_symtab;
+		kdb_ar_t	caller_ar;
+		memset(&caller_ar, 0, sizeof(caller_ar));
+		if (!kdbnearsym(ar->ret, &caller_symtab)) {
+			if (KDB_DEBUG(AR)) {
+				kdb_printf("kdb_get_next_ar: caller not in kernel\n");
+			}
+		} else if (kdba_prologue(&caller_symtab, ar->ret,
+				ar->start, ar->oldfp, ss, 1, &caller_ar)) {
+				/* some caller data extracted */ ;
+		} else if (strcmp(symtab->sym_name, "do_exit") == 0) {
+			/* non-standard caller, force one argument */
+			FORCE_ARG(&caller_ar, 1);
+		} else if (KDB_DEBUG(AR)) {
+				kdb_printf("kdb_get_next_ar: caller prologue failed\n");
+		}
+		if (KDB_DEBUG(AR)) {
+			kdb_printf("kdb_get_next_ar: caller activation record\n");
+			kdb_printf("  start=0x%lx end=0x%lx ret=0x%lx"
+				   " oldfp=0x%lx fp=0x%lx\n",
+				caller_ar.start, caller_ar.end, caller_ar.ret,
+				caller_ar.oldfp, caller_ar.fp);
+			kdb_printf("  locals=%ld regs=%ld args=%ld setup=%ld\n",
+				caller_ar.locals, caller_ar.regs,
+				caller_ar.args, caller_ar.setup);
+		}
+		if (caller_ar.start) {
+			ar->args = KDB_STACK_DIRECTION*(caller_ar.end - caller_ar.start) -
+				(caller_ar.setup + caller_ar.locals + caller_ar.regs);
+			if (ar->args < 0)
+				ar->args = 0;
+			if (ar->args) {
+				ar->arg0 = ar->start -
+					KDB_STACK_DIRECTION*(ar->args - 4);
+				if (KDB_DEBUG(AR)) {
+					kdb_printf("  callee arg0=0x%lx args=%ld\n",
+						ar->arg0, ar->args);
+				}
+			}
+		}
+	}
+
+	return(1);
+}
+
+/*
+ * kdb_symbol_print
+ *
+ *	Standard method for printing a symbol name and offset.
+ * Inputs:
+ *	addr	Address to be printed.
+ *	symtab	Address of symbol data, if NULL this routine does its
+ *		own lookup.
+ *	punc	Punctuation for string, bit field.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Always 0.
+ * Locking:
+ *	none.
+ * Remarks:
+ *	The string and its punctuation is only printed if the address
+ *	is inside the kernel, except that the value is always printed
+ *	when requested.
+ */
+
+void
+kdb_symbol_print(kdb_machreg_t addr, const kdb_symtab_t *symtab_p, unsigned int punc)
+{
+	kdb_symtab_t symtab, *symtab_p2;
+	if (symtab_p) {
+		symtab_p2 = (kdb_symtab_t *)symtab_p;
+	}
+	else {
+		symtab_p2 = &symtab;
+		kdbnearsym(addr, symtab_p2);
+	}
+	if (symtab_p2->sym_name || (punc & KDB_SP_VALUE)) {
+		;	/* drop through */
+	}
+	else {
+		return;
+	}
+	if (punc & KDB_SP_SPACEB) {
+		kdb_printf(" ");
+	}
+	if (punc & KDB_SP_VALUE) {
+		kdb_printf(kdb_machreg_fmt0, addr);
+	}
+	if (!symtab_p2->sym_name) {
+		return;
+	}
+	if (punc & KDB_SP_VALUE) {
+		kdb_printf(" ");
+	}
+	if (punc & KDB_SP_PAREN) {
+		kdb_printf("(");
+	}
+	if (strcmp(symtab_p2->mod_name, "kernel")) {
+		kdb_printf("[%s]", symtab_p2->mod_name);
+	}
+	kdb_printf("%s", symtab_p2->sym_name);
+	if (addr != symtab_p2->sym_start) {
+		kdb_printf("+0x%lx", addr - symtab_p2->sym_start);
+	}
+	if (punc & KDB_SP_SYMSIZE) {
+		kdb_printf("/0x%lx", symtab_p2->sym_end - symtab_p2->sym_start);
+	}
+	if (punc & KDB_SP_PAREN) {
+		kdb_printf(")");
+	}
+	if (punc & KDB_SP_SPACEA) {
+		kdb_printf(" ");
+	}
+	if (punc & KDB_SP_NEWLINE) {
+		kdb_printf("\n");
+	}
+}
+
+/*
+ * kdb_strdup
+ *
+ *	kdb equivalent of strdup, for disasm code.
+ * Inputs:
+ *	str	The string to duplicate.
+ *	type	Flags to kmalloc for the new string.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Address of the new string, NULL if storage could not be allocated.
+ * Locking:
+ *	none.
+ * Remarks:
+ *	This is not in lib/string.c because it uses kmalloc which is not
+ *	available when string.o is used in boot loaders.
+ */
+
+char *kdb_strdup(const char *str, int type)
+{
+	int n = strlen(str)+1;
+	char *s = kmalloc(n, type);
+	if (!s) return NULL;
+	return strcpy(s, str);
+}
+
+/*
+ * kdb_getarea_size
+ *
+ *	Read an area of data.  The kdb equivalent of copy_from_user, with
+ *	kdb messages for invalid addresses.
+ * Inputs:
+ *	res	Pointer to the area to receive the result.
+ *	addr	Address of the area to copy.
+ *	size	Size of the area.
+ * Outputs:
+ *	none.
+ * Returns:
+ *	0 for success, < 0 for error.
+ * Locking:
+ *	none.
+ */
+
+int kdb_getarea_size(void *res, unsigned long addr, size_t size)
+{
+	int ret = kdba_getarea_size(res, addr, size);
+	if (ret) {
+		if (!KDB_STATE(SUPPRESS)) {
+			kdb_printf("kdb_getarea: Bad address 0x%lx\n", addr);
+			KDB_STATE_SET(SUPPRESS);
+		}
+		ret = KDB_BADADDR;
+	}
+	else {
+		KDB_STATE_CLEAR(SUPPRESS);
+	}
+	return(ret);
+}
+
+/*
+ * kdb_putarea_size
+ *
+ *	Write an area of data.  The kdb equivalent of copy_to_user, with
+ *	kdb messages for invalid addresses.
+ * Inputs:
+ *	addr	Address of the area to write to.
+ *	res	Pointer to the area holding the data.
+ *	size	Size of the area.
+ * Outputs:
+ *	none.
+ * Returns:
+ *	0 for success, < 0 for error.
+ * Locking:
+ *	none.
+ */
+
+int kdb_putarea_size(unsigned long addr, void *res, size_t size)
+{
+	int ret = kdba_putarea_size(addr, res, size);
+	if (ret) {
+		if (!KDB_STATE(SUPPRESS)) {
+			kdb_printf("kdb_putarea: Bad address 0x%lx\n", addr);
+			KDB_STATE_SET(SUPPRESS);
+		}
+		ret = KDB_BADADDR;
+	}
+	else {
+		KDB_STATE_CLEAR(SUPPRESS);
+	}
+	return(ret);
+}
+
+/*
+ * kdb_getword
+ *
+ * 	Read a binary value.  Unlike kdb_getarea, this treats data as numbers.
+ * Inputs:
+ *	word	Pointer to the word to receive the result.
+ *	addr	Address of the area to copy.
+ *	size	Size of the area.
+ * Outputs:
+ *	none.
+ * Returns:
+ *	0 for success, < 0 for error.
+ * Locking:
+ *	none.
+ */
+
+int kdb_getword(unsigned long *word, unsigned long addr, size_t size)
+{
+	int diag;
+	__u8  w1;
+	__u16 w2;
+	__u32 w4;
+	__u64 w8;
+	*word = 0;	/* Default value if addr or size is invalid */
+	switch (size) {
+	case 1:
+		if (!(diag = kdb_getarea(w1, addr)))
+			*word = w1;
+		break;
+	case 2:
+		if (!(diag = kdb_getarea(w2, addr)))
+			*word = w2;
+		break;
+	case 4:
+		if (!(diag = kdb_getarea(w4, addr)))
+			*word = w4;
+		break;
+	case 8:
+		if (size <= sizeof(*word)) {
+			if (!(diag = kdb_getarea(w8, addr)))
+				*word = w8;
+			break;
+		}
+		/* drop through */
+	default:
+		diag = KDB_BADWIDTH;
+		kdb_printf("kdb_getword: bad width %ld\n", (long) size);
+	}
+	return(diag);
+}
+
+/*
+ * kdb_putword
+ *
+ * 	Write a binary value.  Unlike kdb_putarea, this treats data as numbers.
+ * Inputs:
+ *	addr	Address of the area to write to..
+ *	word	The value to set.
+ *	size	Size of the area.
+ * Outputs:
+ *	none.
+ * Returns:
+ *	0 for success, < 0 for error.
+ * Locking:
+ *	none.
+ */
+
+int kdb_putword(unsigned long addr, unsigned long word, size_t size)
+{
+	int diag;
+	__u8  w1;
+	__u16 w2;
+	__u32 w4;
+	__u64 w8;
+	switch (size) {
+	case 1:
+		w1 = word;
+		diag = kdb_putarea(addr, w1);
+		break;
+	case 2:
+		w2 = word;
+		diag = kdb_putarea(addr, w2);
+		break;
+	case 4:
+		w4 = word;
+		diag = kdb_putarea(addr, w4);
+		break;
+	case 8:
+		if (size <= sizeof(word)) {
+			w8 = word;
+			diag = kdb_putarea(addr, w8);
+			break;
+		}
+		/* drop through */
+	default:
+		diag = KDB_BADWIDTH;
+		kdb_printf("kdb_putword: bad width %ld\n", (long) size);
+	}
+	return(diag);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/modules/Makefile linuxppc64_2_4/kdb/modules/Makefile
--- linux-2.4.19/kdb/modules/Makefile	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/modules/Makefile	Tue May 28 16:13:07 2002
@@ -0,0 +1,15 @@
+#
+# Makefile for i386-specific kdb files..
+#
+#  Copyright 1999, Silicon Graphics Inc.
+# 
+#  Written April 1999 by Scott Lurndal at Silicon Graphics, Inc.
+#
+
+O_TARGET := vmlinux-obj.o
+obj-$(CONFIG_KDB_MODULES) += kdbm_vm.o kdbm_pg.o
+CFLAGS_kdbm_vm.o	+= -I $(TOPDIR)/drivers/scsi
+
+EXTRA_CFLAGS += -I $(TOPDIR)/arch/$(ARCH)/kdb
+
+include $(TOPDIR)/Rules.make
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/modules/kdbm_pg.c linuxppc64_2_4/kdb/modules/kdbm_pg.c
--- linux-2.4.19/kdb/modules/kdbm_pg.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/modules/kdbm_pg.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,519 @@
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/pagemap.h>
+#include <linux/fs.h>
+#include <linux/iobuf.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/blkdev.h>
+#include <linux/ctype.h>
+
+MODULE_AUTHOR("SGI");
+MODULE_DESCRIPTION("Debug page information");
+MODULE_LICENSE("GPL");
+
+/* Standard Linux page stuff */
+
+static char	*pg_flag_vals[] = {
+	"PG_locked", "PG_error", "PG_referenced", "PG_uptodate",
+	"PG_dirty", "PG_unused_5", "PG_lru", "PG_active",
+	"PG_slab", "PG_unused_9", "PG_skip", "PG_highmem",
+	"PG_checked", "PG_arch_1", "PG_reserved", "PG_launder",
+	NULL };
+
+static char	*bh_state_vals[] = {
+	"Uptodate", "Dirty", "Lock", "Req",
+	"Mapped", "New", "Async", "Wait_IO",
+	"Launder", "JBD",
+	/*XFS*/ "Delay",
+	NULL };
+
+static char *inode_flag_vals[] = {
+	"I_DIRTY_SYNC", "I_DIRTY_DATASYNC", "I_DIRTY_PAGES", "I_LOCK",
+	"I_FREEING", "I_CLEAR",
+	/*XFS*/ "I_NEW",
+	NULL };
+
+static char	*map_flags(unsigned long flags, char *mapping[])
+{
+	static	char	buffer[256];
+	int	index;
+	int	offset = 12;
+
+	buffer[0] = '\0';
+
+	for (index = 0; flags && mapping[index]; flags >>= 1, index++) { 
+		if (flags & 1) {
+			if ((offset + strlen(mapping[index]) + 1) >= 80) {
+				strcat(buffer, "\n            ");
+				offset = 12;
+			} else if (offset > 12) {
+				strcat(buffer, " ");
+				offset++;
+			}
+			strcat(buffer, mapping[index]);
+			offset += strlen(mapping[index]);
+		}
+	}
+
+	return (buffer);
+}
+
+static char	*page_flags(unsigned long flags)
+{
+	return(map_flags(flags, pg_flag_vals));
+}
+
+static int
+kdbm_buffers(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct buffer_head	bh;
+	unsigned long addr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(bh, addr)))
+		return(diag);
+
+	kdb_printf("buffer_head at 0x%lx\n", addr);
+	kdb_printf("  next 0x%p bno %ld rsec %ld size %d dev 0x%x rdev 0x%x\n",
+		bh.b_next, bh.b_blocknr, bh.b_rsector,
+		bh.b_size, bh.b_dev, bh.b_rdev);
+	kdb_printf("  count %d state 0x%lx [%s] ftime 0x%lx b_list %d b_reqnext 0x%p b_data 0x%p\n",
+		bh.b_count.counter, bh.b_state, map_flags(bh.b_state, bh_state_vals),
+		bh.b_flushtime, bh.b_list, bh.b_reqnext, bh.b_data);
+	kdb_printf("  b_page 0x%p b_this_page 0x%p b_private 0x%p\n",
+		bh.b_page, bh.b_this_page, bh.b_private);
+
+	return 0;
+}
+
+static int
+kdbm_page(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct page	page;
+	unsigned long addr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag) 
+		return diag;
+
+	if (addr < PAGE_OFFSET) {
+		printk("Treating 0x%lx as page index, page at 0x%p\n",
+			addr, &mem_map[addr]);
+		addr = (unsigned long) &mem_map[addr];
+	}
+
+	if ((diag = kdb_getarea(page, addr)))
+		return(diag);
+
+	kdb_printf("struct page at 0x%lx\n", addr);
+	kdb_printf("  next 0x%p prev 0x%p addr space 0x%p index %lu (offset 0x%x)\n",
+		   page.list.next, page.list.prev, page.mapping, page.index,
+		   (int)(page.index << PAGE_CACHE_SHIFT));
+	kdb_printf("  count %d flags %s virtual 0x%p\n",
+		   page.count.counter, page_flags(page.flags),
+		   page.virtual);
+	kdb_printf("  buffers 0x%p\n", page.buffers);
+
+	return 0;
+}
+
+unsigned long
+print_request(unsigned long addr)
+{
+	struct request	rq;
+
+	if (kdb_getarea(rq, addr))
+		return(0);
+
+	kdb_printf("struct request at 0x%lx\n", addr);
+	kdb_printf("  rq_dev 0x%x cmd %d errors %d sector %ld nr_sectors %ld\n",
+			rq.rq_dev, rq.cmd, rq.errors, rq.sector,
+			rq.nr_sectors);
+
+	kdb_printf("  hsect %ld hnrsect %ld nrseg %d nrhwseg %d currnrsect %ld seq %d\n",
+			rq.hard_sector, rq.hard_nr_sectors,
+			rq.nr_segments, rq.nr_hw_segments,
+			rq.current_nr_sectors, rq.elevator_sequence);
+	kdb_printf("  ");
+	kdb_printf("bh 0x%p bhtail 0x%p req_q 0x%p\n\n",
+			rq.bh, rq.bhtail, rq.q);
+
+	return (unsigned long) rq.queue.next;
+}
+
+static int
+kdbm_request(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	long	offset=0;
+	unsigned long addr;
+	int nextarg;
+	int diag;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag) 
+		return diag;
+
+	print_request(addr);
+	return 0;
+}
+
+
+static int
+kdbm_rqueue(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct request_queue	rq;
+	unsigned long addr, head_addr, next;
+	long	offset=0;
+	int nextarg;
+	int i, diag;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(rq, addr)))
+		return(diag);
+
+	kdb_printf("struct request_queue at 0x%lx [%s]\n", addr,
+			rq.plugged ? "plugged" : "running");
+	kdb_printf(" read free_list [0x%p, 0x%p]\n",
+			rq.rq[READ].free.prev,
+			rq.rq[READ].free.next);
+	kdb_printf(" write free_list [0x%p, 0x%p]\n",
+			rq.rq[WRITE].free.prev,
+			rq.rq[WRITE].free.next);
+
+	i = 0;
+	next = (unsigned long)rq.queue_head.next;
+	head_addr = addr + offsetof(struct request_queue, queue_head);
+	kdb_printf(" request queue: %s\n", next == head_addr ?
+		"empty" : "");
+	while (next != head_addr) {
+		i++;
+		next = print_request(next);
+	}
+
+	if (i)
+		kdb_printf("%d requests found\n", i);
+
+	return 0;
+}
+
+
+static void
+do_buffer(unsigned long addr)
+{
+	struct buffer_head	bh;
+	
+	if (kdb_getarea(bh, addr))
+		return;
+
+	kdb_printf("bh 0x%lx bno %8ld [%s]\n", addr, bh.b_blocknr,
+		 map_flags(bh.b_state, bh_state_vals));
+}
+
+static int
+kdbm_inode_pages(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct inode	inode;
+	struct address_space ap;
+	unsigned long addr, addr1 = 0;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	int which=0;
+
+	struct list_head *head, *curr;
+	
+	if (argc < 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag) 
+		return diag;
+
+	if (argc == 2) {
+		nextarg = 2;
+		diag = kdbgetaddrarg(argc, argv, &nextarg, &addr1,
+					&offset, NULL, regs);
+		if (diag) 
+			return diag;
+		kdb_printf("Looking for page index 0x%lx ... \n", addr1);
+	}
+
+	if ((diag = kdb_getarea(inode, addr)) ||
+	    (diag = kdb_getarea(ap, (unsigned long) inode.i_mapping)))
+		return(diag);
+	
+	if (!&inode.i_mapping) goto out;
+ again:
+	if (which == 0){
+	  which=1;
+	  head = &inode.i_mapping->clean_pages;
+	  kdb_printf("CLEAN  page_struct   index  cnt  flags\n");
+	} else if (which == 1) {
+	  which=2;
+	  head = &inode.i_mapping->dirty_pages;
+	  kdb_printf("DIRTY  page_struct   index  cnt  flags\n");
+	} else if (which == 2) {
+	  which=3;
+	  head = &inode.i_mapping->locked_pages;
+	  kdb_printf("LOCKED page_struct   index  cnt  flags\n");
+	} else {
+	  goto out;
+	}
+	
+	if(!head) goto again;
+	curr = head->next;
+	while (curr != head) {
+		struct page 	 page;
+		struct list_head curr_struct;
+
+		if ((diag = kdb_getarea(page, (unsigned long) list_entry(curr, struct page, list))))
+			return(diag);
+
+		if (!addr1 || page.index == addr1 ||
+			(addr1 == -1 && (page.flags & ( 1 << PG_locked))))
+		{
+			kdb_printf("    0x%lx    %6lu    %5d    0x%lx ",
+				addr, page.index, page.count.counter,
+				page.flags);
+			if (page.buffers)
+				do_buffer((unsigned long) page.buffers);
+		}
+
+		if ((diag = kdb_getarea(curr_struct, (unsigned long) curr)))
+			return(diag);
+
+		curr = curr_struct.next;
+	}
+	goto again;
+ out:
+	return 0;
+}
+
+static int
+kdbm_inode(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct inode	inode;
+	unsigned long addr;
+	unsigned char *iaddr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(inode, addr)))
+		return(diag);
+
+	kdb_printf("struct inode at  0x%lx\n", addr);
+
+	kdb_printf(" i_ino = %lu i_count = %u i_dev = 0x%x i_size %Ld\n",
+					inode.i_ino, atomic_read(&inode.i_count),
+					inode.i_dev, inode.i_size);
+
+	kdb_printf(" i_mode = 0x%x  i_nlink = %d  i_rdev = 0x%x\n",
+					inode.i_mode, inode.i_nlink,
+					inode.i_rdev);
+
+	kdb_printf(" i_hash.nxt = 0x%p i_hash.prv = 0x%p\n",
+					inode.i_hash.next, inode.i_hash.prev);
+
+	kdb_printf(" i_list.nxt = 0x%p i_list.prv = 0x%p\n",
+					inode.i_list.next, inode.i_list.prev);
+
+	kdb_printf(" i_dentry.nxt = 0x%p i_dentry.prv = 0x%p\n",
+					inode.i_dentry.next,
+					inode.i_dentry.prev);
+
+	kdb_printf(" i_dirty_buffers.nxt = 0x%p i_dirty_buffers.prv = 0x%p\n",
+					inode.i_dirty_buffers.next,
+					inode.i_dirty_buffers.prev);
+
+	kdb_printf(" i_sb = 0x%p i_op = 0x%p i_data = 0x%lx nrpages = %lu\n",
+					inode.i_sb, inode.i_op,
+					addr + offsetof(struct inode, i_data),
+					inode.i_data.nrpages);
+	kdb_printf(" i_mapping = 0x%p\n i_flags 0x%x i_state 0x%lx [%s]",
+			   inode.i_mapping, inode.i_flags,
+			   inode.i_state,
+			   map_flags(inode.i_state, inode_flag_vals));
+	
+	iaddr  = (char *)addr;
+	iaddr += offsetof(struct inode, u);
+
+	kdb_printf("  fs specific info @ 0x%p\n", iaddr);
+
+	return (0);
+}
+
+static int
+kdbm_kiobuf(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	struct kiobuf kiobuf;
+	struct page	page;
+	struct page	*page_array[64];
+	unsigned long addr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	int i;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(kiobuf, addr)))
+		return(diag);
+
+	kdb_printf("kiobuf at 0x%lx\n", addr);
+	kdb_printf("  nr_pages %d array_len %d offset 0x%x length 0x%x\n",
+		   kiobuf.nr_pages, kiobuf.array_len,
+		   kiobuf.offset, kiobuf.length);
+	if (kiobuf.maplist != (((struct kiobuf *)addr)->map_array)) {
+		if ((diag = kdb_getarea_size(page_array, (unsigned long)kiobuf.maplist, kiobuf.nr_pages * sizeof(page_array[0]))))
+			return(diag);
+		kiobuf.maplist = page_array;
+	}
+	kdb_printf("  errno %d\n", kiobuf.errno);
+	kdb_printf("    page_struct   page_addr     cnt  flags\n");
+	for (i = 0; i < kiobuf.nr_pages; i++) {
+		if ((diag = kdb_getarea(page, (unsigned long) kiobuf.maplist[i])))
+			return(diag);
+		kdb_printf("    0x%p    0x%p    %d    0x%lx\n",
+			   kiobuf.maplist[i], page.virtual,
+			   page.count.counter, page.flags);
+	}
+
+	return (0);
+}
+
+#if defined(CONFIG_XFS_FS) || defined(CONFIG_XFS_FS_MODULE)
+#define HAVE_DELALLOCPAGE
+#endif
+
+static int
+kdbm_memmap(int argc, const char **argv, const char **envp,
+        struct pt_regs *regs)
+{
+	struct page	page;
+	int		i, page_count;
+	int		slab_count = 0;
+#ifdef	HAVE_DELALLOCPAGE
+	int		delalloc_count = 0;
+#endif	/* HAVE_DELALLOCPAGE */
+	int		dirty_count = 0;
+	int		locked_count = 0;
+	int		page_counts[9];
+	int		buffered_count = 0;
+	int		diag;
+	unsigned long addr;
+
+	addr = (unsigned long)mem_map;
+	page_count = max_mapnr;
+	memset(page_counts, 0, sizeof(page_counts));
+
+	for (i = 0; i < page_count; i++) {
+		if ((diag = kdb_getarea(page, addr)))
+			return(diag);
+		addr += sizeof(page);
+
+		if (PageSlab(&page))
+			slab_count++;
+#ifdef	HAVE_DELALLOCPAGE
+		if (DelallocPage(&page))
+			delalloc_count++;
+#endif	/* HAVE_DELALLOCPAGE */
+		if (PageDirty(&page))
+			dirty_count++;
+		if (PageLocked(&page))
+			locked_count++;
+		if (page.count.counter < 8)
+			page_counts[page.count.counter]++;
+		else
+			page_counts[8]++;
+		if (page.buffers)
+			buffered_count++;
+
+	}
+
+	kdb_printf("  Total pages:      %6d\n", page_count);
+	kdb_printf("  Slab pages:       %6d\n", slab_count);
+#ifdef	HAVE_DELALLOCPAGE
+	kdb_printf("  Delalloc pages:   %6d\n", delalloc_count);
+#endif	/* HAVE_DELALLOCPAGE */
+	kdb_printf("  Dirty pages:      %6d\n", dirty_count);
+	kdb_printf("  Locked pages:     %6d\n", locked_count);
+	kdb_printf("  Buffer pages:     %6d\n", buffered_count);
+	for (i = 0; i < 8; i++) {
+		kdb_printf("  %d page count:     %6d\n",
+			i, page_counts[i]);
+	}
+	kdb_printf("  high page count:  %6d\n", page_counts[8]);
+	return 0;
+}
+
+static int __init kdbm_pg_init(void)
+{
+	kdb_register("kiobuf", kdbm_kiobuf, "<vaddr>", "Display kiobuf", 0);
+	kdb_register("page", kdbm_page, "<vaddr>", "Display page", 0);
+	kdb_register("inode", kdbm_inode, "<vaddr>", "Display inode", 0);
+	kdb_register("bh", kdbm_buffers, "<buffer head address>", "Display buffer", 0);
+	kdb_register("inode_pages", kdbm_inode_pages, "<inode *>", "Display pages in an inode", 0);
+	kdb_register("req", kdbm_request, "<vaddr>", "dump request struct", 0);
+	kdb_register("rqueue", kdbm_rqueue, "<vaddr>", "dump request queue", 0);
+	kdb_register("memmap", kdbm_memmap, "", "page table summary", 0);
+
+	return 0;
+}
+
+
+static void __exit kdbm_pg_exit(void)
+{
+	kdb_unregister("kiobuf");
+	kdb_unregister("page");
+	kdb_unregister("inode");
+	kdb_unregister("bh");
+	kdb_unregister("inode_pages");
+	kdb_unregister("req");
+	kdb_unregister("rqueue");
+	kdb_unregister("memmap");
+}
+
+module_init(kdbm_pg_init) 
+module_exit(kdbm_pg_exit)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kdb/modules/kdbm_vm.c linuxppc64_2_4/kdb/modules/kdbm_vm.c
--- linux-2.4.19/kdb/modules/kdbm_vm.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kdb/modules/kdbm_vm.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,371 @@
+#include <linux/blkdev.h>
+#include <linux/types.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+
+#include <scsi.h>
+#include <hosts.h>
+
+MODULE_AUTHOR("SGI");
+MODULE_DESCRIPTION("Debug VM information");
+MODULE_LICENSE("GPL");
+
+struct __vmflags {
+	unsigned long mask;
+	char *name;
+} vmflags[] = {
+	{ VM_READ, "READ" },
+	{ VM_WRITE, "WRITE" },
+	{ VM_EXEC, "EXEC" },
+	{ VM_SHARED, "SHARED" },
+	{ VM_MAYREAD, "MAYREAD" },
+	{ VM_MAYWRITE, "MAYWRITE" },
+	{ VM_MAYEXEC, "MAYEXEC" },
+	{ VM_MAYSHARE, "MAYSHARE" },
+	{ VM_GROWSDOWN, "GROWSDOWN" },
+	{ VM_GROWSUP, "GROWSUP" },
+	{ VM_SHM, "SHM" },
+	{ VM_DENYWRITE, "DENYWRITE" },
+	{ VM_EXECUTABLE, "EXECUTABLE" },
+	{ VM_LOCKED, "LOCKED" },
+	{ VM_IO , "IO " },
+	{ 0, "" }
+};
+
+static int
+kdbm_vm(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct vm_area_struct vp;
+	unsigned long addr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+	struct __vmflags *tp;
+	
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(vp, addr)))
+		return(diag);
+
+	kdb_printf("struct vm_area_struct at 0x%lx for %d bytes\n", 
+		   addr, (int)sizeof(struct vm_area_struct));
+	kdb_printf("vm_start = 0x%lx   vm_end = 0x%lx\n", vp.vm_start, vp.vm_end);
+	kdb_printf("page_prot = 0x%lx\n", pgprot_val(vp.vm_page_prot));
+	kdb_printf("flags:  ");
+	for(tp=vmflags; tp->mask; tp++) {
+		if (vp.vm_flags & tp->mask) {
+			kdb_printf("%s ", tp->name);
+		}
+	}
+	kdb_printf("\n");
+
+	return 0;
+}
+
+static int
+kdbm_fp(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct file   f;
+	struct inode  i;
+	struct dentry d;
+	int	      nextarg;
+	unsigned long addr;
+	long	      offset;
+	int	      diag;
+
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(f, addr)) ||
+	    (diag = kdb_getarea(d, (unsigned long)f.f_dentry)) ||
+	    (diag = kdb_getarea(i, (unsigned long)d.d_inode)))
+		return(diag);
+	
+	kdb_printf("name.name 0x%p  name.len  %d\n",
+		    d.d_name.name, d.d_name.len);
+
+	kdb_printf("File Pointer at 0x%lx\n", addr);
+
+	kdb_printf(" f_list.nxt = 0x%p f_list.prv = 0x%p\n",
+					f.f_list.next, f.f_list.prev);
+
+	kdb_printf(" f_dentry = 0x%p f_op = 0x%p\n",
+					f.f_dentry, f.f_op);
+
+	kdb_printf(" f_count = %d f_flags = 0x%x f_mode = 0x%x\n",
+					f.f_count.counter, f.f_flags, f.f_mode);
+
+	kdb_printf(" f_pos = %Ld f_reada = %ld f_ramax = %ld\n",
+					f.f_pos, f.f_reada, f.f_ramax);
+
+	kdb_printf(" f_raend = %ld f_ralen = %ld f_rawin = %ld\n\n",
+					f.f_raend, f.f_ralen, f.f_rawin);
+
+
+	kdb_printf("\nDirectory Entry at 0x%p\n", f.f_dentry);
+	kdb_printf(" d_name.len = %d d_name.name = 0x%p>\n",
+					d.d_name.len, d.d_name.name);
+
+	kdb_printf(" d_count = %d d_flags = 0x%x d_inode = 0x%p\n",
+					atomic_read(&d.d_count), d.d_flags, d.d_inode);
+
+	kdb_printf(" d_hash.nxt = 0x%p d_hash.prv = 0x%p\n",
+					d.d_hash.next, d.d_hash.prev);
+
+	kdb_printf(" d_lru.nxt = 0x%p d_lru.prv = 0x%p\n",
+					d.d_lru.next, d.d_lru.prev);
+
+	kdb_printf(" d_child.nxt = 0x%p d_child.prv = 0x%p\n",
+					d.d_child.next, d.d_child.prev);
+
+	kdb_printf(" d_subdirs.nxt = 0x%p d_subdirs.prv = 0x%p\n",
+					d.d_subdirs.next, d.d_subdirs.prev);
+
+	kdb_printf(" d_alias.nxt = 0x%p d_alias.prv = 0x%p\n",
+					d.d_alias.next, d.d_alias.prev);
+
+	kdb_printf(" d_op = 0x%p d_sb = 0x%p\n\n",
+					d.d_op, d.d_sb);
+
+
+	kdb_printf("\nInode Entry at 0x%p\n", d.d_inode);
+
+	kdb_printf(" i_mode = 0x%x  i_nlink = %d  i_rdev = 0x%x\n",
+					i.i_mode, i.i_nlink, i.i_rdev);
+
+	kdb_printf(" i_ino = %ld i_count = %d i_dev = 0x%x\n",
+					i.i_ino, atomic_read(&i.i_count), i.i_dev);
+
+	kdb_printf(" i_hash.nxt = 0x%p i_hash.prv = 0x%p\n",
+					i.i_hash.next, i.i_hash.prev);
+
+	kdb_printf(" i_list.nxt = 0x%p i_list.prv = 0x%p\n",
+					i.i_list.next, i.i_list.prev);
+
+	kdb_printf(" i_dentry.nxt = 0x%p i_dentry.prv = 0x%p\n",
+					i.i_dentry.next, i.i_dentry.prev);
+
+	return 0;
+}
+
+static int
+kdbm_dentry(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	struct dentry d;
+	int	      nextarg;
+	unsigned long addr;
+	long	      offset;
+	int	      diag;
+
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(d, addr)))
+		return diag;
+	
+	
+	kdb_printf("Dentry at 0x%lx\n", addr);
+
+	kdb_printf(" d_name.len = %d d_name.name = 0x%p>\n",
+					d.d_name.len, d.d_name.name);
+	
+	kdb_printf(" d_count = %d d_flags = 0x%x d_inode = 0x%p\n",
+					atomic_read(&d.d_count), d.d_flags, d.d_inode);
+
+	kdb_printf(" d_hash.nxt = 0x%p d_hash.prv = 0x%p\n",
+					d.d_hash.next, d.d_hash.prev);
+
+	kdb_printf(" d_lru.nxt = 0x%p d_lru.prv = 0x%p\n",
+					d.d_lru.next, d.d_lru.prev);
+
+	kdb_printf(" d_child.nxt = 0x%p d_child.prv = 0x%p\n",
+					d.d_child.next, d.d_child.prev);
+
+	kdb_printf(" d_subdirs.nxt = 0x%p d_subdirs.prv = 0x%p\n",
+					d.d_subdirs.next, d.d_subdirs.prev);
+
+	kdb_printf(" d_alias.nxt = 0x%p d_alias.prv = 0x%p\n",
+					d.d_alias.next, d.d_alias.prev);
+
+	kdb_printf(" d_op = 0x%p d_sb = 0x%p\n\n",
+					d.d_op, d.d_sb);
+
+	return 0;
+}
+
+static int
+kdbm_sh(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	int nextarg;
+	unsigned long addr;
+	long	      offset =0L;
+	struct Scsi_Host sh;
+
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(sh, addr)))
+		return diag;
+
+	kdb_printf("Scsi_Host at 0x%lx\n", addr);
+	kdb_printf("next = 0x%p   host_queue = 0x%p\n",
+		   sh.next, sh.host_queue);
+	kdb_printf("ehandler = 0x%p eh_wait = 0x%p  en_notify = 0x%p eh_action = 0x%p\n", 
+		   sh.ehandler, sh.eh_wait, sh.eh_notify, sh.eh_action);
+	kdb_printf("eh_active = 0x%d host_wait = 0x%p hostt = 0x%p host_busy = %d\n",
+		   sh.eh_active, &sh.host_wait, sh.hostt, sh.host_active.counter);
+	kdb_printf("host_failed = %d  extra_bytes = %d  host_no = %d resetting = %d\n",
+		   sh.host_failed, sh.extra_bytes, sh.host_no, sh.resetting);
+	kdb_printf("max id/lun/channel = [%d/%d/%d]  this_id = %d\n",
+		   sh.max_id, sh.max_lun, sh.max_channel, sh.this_id);
+	kdb_printf("can_queue = %d cmd_per_lun = %d  sg_tablesize = %d u_isa_dma = %d\n",
+		   sh.can_queue, sh.cmd_per_lun, sh.sg_tablesize, sh.unchecked_isa_dma);
+	kdb_printf("host_blocked = %d  reverse_ordering = %d \n",
+		   sh.host_blocked, sh.reverse_ordering);
+
+	return 0;
+}
+
+static int
+kdbm_sd(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	int nextarg;
+	unsigned long addr;
+	long	      offset =0L;
+	struct scsi_device sd;
+
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(sd, addr)))
+		return diag;
+
+	kdb_printf("scsi_device at 0x%lx\n", addr);
+	kdb_printf("next = 0x%p   prev = 0x%p  host = 0x%p\n",
+		   sd.next, sd.prev, sd.host);
+	kdb_printf("device_busy = %d   device_queue 0x%p\n",
+		   sd.device_busy, sd.device_queue);
+	kdb_printf("id/lun/chan = [%d/%d/%d]  single_lun = %d  device_blocked = %d\n",
+		   sd.id, sd.lun, sd.channel, sd.single_lun, sd.device_blocked);
+	kdb_printf("queue_depth = %d current_tag = %d  scsi_level = %d\n",
+		   sd.queue_depth, sd.current_tag, sd.scsi_level);
+	kdb_printf("%8.8s %16.16s %4.4s\n", sd.vendor, sd.model, sd.rev);
+
+	return 0;
+}
+
+static char *
+str_rq_status(int rq_status) 
+{
+	switch (rq_status) {
+	case RQ_INACTIVE:
+		return "RQ_INACTIVE";
+	case RQ_ACTIVE:
+		return "RQ_ACTIVE";
+	case RQ_SCSI_BUSY:
+		return "RQ_SCSI_BUSY";
+	case RQ_SCSI_DONE:
+		return "RQ_SCSI_DONE";
+	case RQ_SCSI_DISCONNECTING:
+		return "RQ_SCSI_DISCONNECTING";
+	default:
+		return "UNKNOWN";
+	}
+}
+
+static int
+kdbm_sc(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	int diag;
+	int nextarg;
+	unsigned long addr;
+	long	      offset =0L;
+	struct scsi_cmnd sc;
+
+	if (argc != 1) 
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(sc, addr)))
+		return diag;
+
+	kdb_printf("scsi_cmnd at 0x%lx\n", addr);
+	kdb_printf("host = 0x%p  state = %d  owner = %d  device = 0x%p\nb",
+		    sc.host, sc.state, sc.owner, sc.device);
+	kdb_printf("next = 0x%p  reset_chain = 0x%p  eh_state = %d done = 0x%p\n",
+		   sc.next, sc.reset_chain, sc.eh_state, sc.done);
+	kdb_printf("serial_number = %ld  serial_num_at_to = %ld retries = %d timeout = %d\n",
+		   sc.serial_number, sc.serial_number_at_timeout, sc.retries, sc.timeout);
+	kdb_printf("id/lun/cmnd = [%d/%d/%d]  cmd_len = %d  old_cmd_len = %d\n",
+		   sc.target, sc.lun, sc.channel, sc.cmd_len, sc.old_cmd_len);
+	kdb_printf("cmnd = [%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x]\n",
+		   sc.cmnd[0], sc.cmnd[1], sc.cmnd[2], sc.cmnd[3], sc.cmnd[4], 
+		   sc.cmnd[5], sc.cmnd[6], sc.cmnd[7], sc.cmnd[8], sc.cmnd[9],
+		   sc.cmnd[10], sc.cmnd[11]);
+	kdb_printf("data_cmnd = [%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x/%2.2x]\n",
+		   sc.data_cmnd[0], sc.data_cmnd[1], sc.data_cmnd[2], sc.data_cmnd[3], sc.data_cmnd[4], 
+		   sc.data_cmnd[5], sc.data_cmnd[6], sc.data_cmnd[7], sc.data_cmnd[8], sc.data_cmnd[9],
+		   sc.data_cmnd[10], sc.data_cmnd[11]);
+	kdb_printf("request_buffer = 0x%p  bh_next = 0x%p  request_bufflen = %d\n",
+		   sc.request_buffer, sc.bh_next, sc.request_bufflen);
+	kdb_printf("use_sg = %d  old_use_sg = %d sglist_len = %d abore_reason = %d\n",
+		   sc.use_sg, sc.old_use_sg, sc.sglist_len, sc.abort_reason);
+	kdb_printf("bufflen = %d  buffer = 0x%p  underflow = %d transfersize = %d\n",
+		   sc.bufflen, sc.buffer, sc.underflow, sc.transfersize);
+	kdb_printf("tag = %d pid = %ld\n",
+		   sc.tag, sc.pid);
+	kdb_printf("request struct\n");
+	kdb_printf("rq_status = %s  rq_dev = [%d/%d]  errors = %d  cmd = %d\n",
+		   str_rq_status(sc.request.rq_status), 
+		   MAJOR(sc.request.rq_dev),
+		   MINOR(sc.request.rq_dev), sc.request.cmd, 
+		   sc.request.errors);
+	kdb_printf("sector = %ld  nr_sectors = %ld  current_nr_sectors = %ld\n",
+		   sc.request.sector, sc.request.nr_sectors, sc.request.current_nr_sectors);
+	kdb_printf("buffer = 0x%p bh = 0x%p bhtail = 0x%p\n",
+		   sc.request.buffer, sc.request.bh, sc.request.bhtail);
+
+	return 0;
+}
+
+static int __init kdbm_vm_init(void)
+{
+	kdb_register("vm", kdbm_vm, "<vaddr>", "Display vm_area_struct", 0);
+	kdb_register("dentry", kdbm_dentry, "<dentry>", "Display interesting dentry stuff", 0);
+	kdb_register("filp", kdbm_fp, "<filp>", "Display interesting filp stuff", 0);
+	kdb_register("sh", kdbm_sh, "<vaddr>", "Show scsi_host", 0);
+	kdb_register("sd", kdbm_sd, "<vaddr>", "Show scsi_device", 0);
+	kdb_register("sc", kdbm_sc, "<vaddr>", "Show scsi_cmnd", 0);
+	
+	return 0;
+}
+
+static void __exit kdbm_vm_exit(void)
+{
+	kdb_unregister("vm");
+	kdb_unregister("dentry");
+	kdb_unregister("filp");
+	kdb_unregister("sh");
+	kdb_unregister("sd");
+	kdb_unregister("sc");
+}
+
+module_init(kdbm_vm_init) 
+module_exit(kdbm_vm_exit)
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/Makefile linuxppc64_2_4/kernel/Makefile
--- linux-2.4.19/kernel/Makefile	Sun Sep 16 23:22:40 2001
+++ linuxppc64_2_4/kernel/Makefile	Wed May 29 08:30:50 2002
@@ -19,6 +19,7 @@
 obj-$(CONFIG_UID16) += uid16.o
 obj-$(CONFIG_MODULES) += ksyms.o
 obj-$(CONFIG_PM) += pm.o
+obj-$(CONFIG_KALLSYMS) += kallsyms.o
 
 ifneq ($(CONFIG_IA64),y)
 # According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/kallsyms.c linuxppc64_2_4/kernel/kallsyms.c
--- linux-2.4.19/kernel/kallsyms.c	Wed Dec 31 18:00:00 1969
+++ linuxppc64_2_4/kernel/kallsyms.c	Tue May 28 16:13:07 2002
@@ -0,0 +1,304 @@
+/* An example of using kallsyms data in a kernel debugger.
+
+   Copyright 2000 Keith Owens <kaos@ocs.com.au> April 2000
+
+   This file is part of the Linux modutils.
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the
+   Free Software Foundation; either version 2 of the License, or (at your
+   option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software Foundation,
+   Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+  */
+
+/*
+   This code uses the list of all kernel and module symbols to :-
+
+   * Find any non-stack symbol in a kernel or module.  Symbols do
+     not have to be exported for debugging.
+
+   * Convert an address to the module (or kernel) that owns it, the
+     section it is in and the nearest symbol.  This finds all non-stack
+     symbols, not just exported ones.
+
+   You need modutils >= 2.3.11 and a kernel with the kallsyms patch
+   which was compiled with CONFIG_KALLSYMS.
+ */
+
+#include <linux/elf.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/kallsyms.h>
+
+/* These external symbols are only set on kernels compiled with
+ * CONFIG_KALLSYMS.
+ */
+
+extern const char __start___kallsyms[];
+extern const char __stop___kallsyms[];
+
+static struct module **kallsyms_module_list;
+
+static void kallsyms_get_module_list(void)
+{
+	const struct kallsyms_header	*ka_hdr;
+	const struct kallsyms_section	*ka_sec;
+	const struct kallsyms_symbol	*ka_sym;
+	const char			*ka_str;
+	int i;
+	const char *p;
+
+	if (__start___kallsyms >= __stop___kallsyms)
+		return;
+	ka_hdr = (struct kallsyms_header *)__start___kallsyms;
+	ka_sec = (struct kallsyms_section *)
+		((char *)(ka_hdr) + ka_hdr->section_off);
+	ka_sym = (struct kallsyms_symbol *)
+		((char *)(ka_hdr) + ka_hdr->symbol_off);
+	ka_str = 
+		((char *)(ka_hdr) + ka_hdr->string_off);
+
+	for (i = 0; i < ka_hdr->symbols; kallsyms_next_sym(ka_hdr, ka_sym), ++i) {
+		p = ka_str + ka_sym->name_off;
+		if (strcmp(p, "module_list") == 0) {
+			if (ka_sym->symbol_addr)
+				kallsyms_module_list = (struct module **)(ka_sym->symbol_addr);
+			break;
+		}
+	}
+}
+
+static inline void kallsyms_do_first_time(void)
+{
+	static int first_time = 1;
+	if (first_time)
+		kallsyms_get_module_list();
+	first_time = 0;
+}
+
+/* A symbol can appear in more than one module.  A token is used to
+ * restart the scan at the next module, set the token to 0 for the
+ * first scan of each symbol.
+ */
+
+int kallsyms_symbol_to_address(
+	const char	 *name,		/* Name to lookup */
+	unsigned long 	 *token,	/* Which module to start at */
+	const char	**mod_name,	/* Set to module name */
+	unsigned long 	 *mod_start,	/* Set to start address of module */
+	unsigned long 	 *mod_end,	/* Set to end address of module */
+	const char	**sec_name,	/* Set to section name */
+	unsigned long 	 *sec_start,	/* Set to start address of section */
+	unsigned long 	 *sec_end,	/* Set to end address of section */
+	const char	**sym_name,	/* Set to full symbol name */
+	unsigned long 	 *sym_start,	/* Set to start address of symbol */
+	unsigned long 	 *sym_end	/* Set to end address of symbol */
+	)
+{
+	const struct kallsyms_header	*ka_hdr = NULL;	/* stupid gcc */
+	const struct kallsyms_section	*ka_sec;
+	const struct kallsyms_symbol	*ka_sym = NULL;
+	const char			*ka_str = NULL;
+	const struct module *m;
+	int i = 0, l;
+	const char *p, *pt_R;
+	char *p2;
+
+	kallsyms_do_first_time();
+	if (!kallsyms_module_list)
+		return(0);
+
+	/* Restart? */
+	m = *kallsyms_module_list;
+	if (token && *token) {
+		for (; m; m = m->next)
+			if ((unsigned long)m == *token)
+				break;
+		if (m)
+			m = m->next;
+	}
+
+	for (; m; m = m->next) {
+		if (!mod_member_present(m, kallsyms_start) || 
+		    !mod_member_present(m, kallsyms_end) ||
+		    m->kallsyms_start >= m->kallsyms_end)
+			continue;
+		ka_hdr = (struct kallsyms_header *)m->kallsyms_start;
+		ka_sym = (struct kallsyms_symbol *)
+			((char *)(ka_hdr) + ka_hdr->symbol_off);
+		ka_str = 
+			((char *)(ka_hdr) + ka_hdr->string_off);
+		for (i = 0; i < ka_hdr->symbols; ++i, kallsyms_next_sym(ka_hdr, ka_sym)) {
+			p = ka_str + ka_sym->name_off;
+			if (strcmp(p, name) == 0)
+				break;
+			/* Unversioned requests match versioned names */
+			if (!(pt_R = strstr(p, "_R")))
+				continue;
+			l = strlen(pt_R);
+			if (l < 10)
+				continue;	/* Not _R.*xxxxxxxx */
+			(void)simple_strtoul(pt_R+l-8, &p2, 16);
+			if (*p2)
+				continue;	/* Not _R.*xxxxxxxx */
+			if (strncmp(p, name, pt_R-p) == 0)
+				break;	/* Match with version */
+		}
+		if (i < ka_hdr->symbols)
+			break;
+	}
+
+	if (token)
+		*token = (unsigned long)m;
+	if (!m)
+		return(0);	/* not found */
+
+	ka_sec = (const struct kallsyms_section *)
+		((char *)ka_hdr + ka_hdr->section_off + ka_sym->section_off);
+	*mod_name = *(m->name) ? m->name : "kernel";
+	*mod_start = ka_hdr->start;
+	*mod_end = ka_hdr->end;
+	*sec_name = ka_sec->name_off + ka_str;
+	*sec_start = ka_sec->start;
+	*sec_end = ka_sec->start + ka_sec->size;
+	*sym_name = ka_sym->name_off + ka_str;
+	*sym_start = ka_sym->symbol_addr;
+	if (i < ka_hdr->symbols-1) {
+		const struct kallsyms_symbol *ka_symn = ka_sym;
+		kallsyms_next_sym(ka_hdr, ka_symn);
+		*sym_end = ka_symn->symbol_addr;
+	}
+	else
+		*sym_end = *sec_end;
+	return(1);
+}
+
+int kallsyms_address_to_symbol(
+	unsigned long	  address,	/* Address to lookup */
+	const char	**mod_name,	/* Set to module name */
+	unsigned long 	 *mod_start,	/* Set to start address of module */
+	unsigned long 	 *mod_end,	/* Set to end address of module */
+	const char	**sec_name,	/* Set to section name */
+	unsigned long 	 *sec_start,	/* Set to start address of section */
+	unsigned long 	 *sec_end,	/* Set to end address of section */
+	const char	**sym_name,	/* Set to full symbol name */
+	unsigned long 	 *sym_start,	/* Set to start address of symbol */
+	unsigned long 	 *sym_end	/* Set to end address of symbol */
+	)
+{
+	const struct kallsyms_header	*ka_hdr = NULL;	/* stupid gcc */
+	const struct kallsyms_section	*ka_sec = NULL;
+	const struct kallsyms_symbol	*ka_sym;
+	const char			*ka_str;
+	const struct module *m;
+	int i;
+	unsigned long end;
+
+	kallsyms_do_first_time();
+	if (!kallsyms_module_list)
+		return(0);
+
+	for (m = *kallsyms_module_list; m; m = m->next) {
+		if (!mod_member_present(m, kallsyms_start) || 
+		    !mod_member_present(m, kallsyms_end) ||
+		    m->kallsyms_start >= m->kallsyms_end)
+			continue;
+		ka_hdr = (struct kallsyms_header *)m->kallsyms_start;
+		ka_sec = (const struct kallsyms_section *)
+			((char *)ka_hdr + ka_hdr->section_off);
+		/* Is the address in any section in this module? */
+		for (i = 0; i < ka_hdr->sections; ++i, kallsyms_next_sec(ka_hdr, ka_sec)) {
+			if (ka_sec->start <= address &&
+			    (ka_sec->start + ka_sec->size) > address)
+				break;
+		}
+		if (i < ka_hdr->sections)
+			break;	/* Found a matching section */
+	}
+
+	if (!m)
+		return(0);	/* not found */
+
+	ka_sym = (struct kallsyms_symbol *)
+		((char *)(ka_hdr) + ka_hdr->symbol_off);
+	ka_str = 
+		((char *)(ka_hdr) + ka_hdr->string_off);
+	*mod_name = *(m->name) ? m->name : "kernel";
+	*mod_start = ka_hdr->start;
+	*mod_end = ka_hdr->end;
+	*sec_name = ka_sec->name_off + ka_str;
+	*sec_start = ka_sec->start;
+	*sec_end = ka_sec->start + ka_sec->size;
+	*sym_name = *sec_name;		/* In case we find no matching symbol */
+	*sym_start = *sec_start;
+	*sym_end = *sec_end;
+
+	for (i = 0; i < ka_hdr->symbols; ++i, kallsyms_next_sym(ka_hdr, ka_sym)) {
+		if (ka_sym->symbol_addr > address)
+			continue;
+		if (i < ka_hdr->symbols-1) {
+			const struct kallsyms_symbol *ka_symn = ka_sym;
+			kallsyms_next_sym(ka_hdr, ka_symn);
+			end = ka_symn->symbol_addr;
+		}
+		else
+			end = *sec_end;
+		if (end <= address)
+			continue;
+		if ((char *)ka_hdr + ka_hdr->section_off + ka_sym->section_off
+		    != (char *)ka_sec)
+			continue;	/* wrong section */
+		*sym_name = ka_str + ka_sym->name_off;
+		*sym_start = ka_sym->symbol_addr;
+		*sym_end = end;
+		break;
+	}
+	return(1);
+}
+
+/* List all sections in all modules.  The callback routine is invoked with
+ * token, module name, section name, section start, section end, section flags.
+ */
+int kallsyms_sections(void *token,
+		      int (*callback)(void *, const char *, const char *, ElfW(Addr), ElfW(Addr), ElfW(Word)))
+{
+	const struct kallsyms_header	*ka_hdr = NULL;	/* stupid gcc */
+	const struct kallsyms_section	*ka_sec = NULL;
+	const char			*ka_str;
+	const struct module *m;
+	int i;
+
+	kallsyms_do_first_time();
+	if (!kallsyms_module_list)
+		return(0);
+
+	for (m = *kallsyms_module_list; m; m = m->next) {
+		if (!mod_member_present(m, kallsyms_start) || 
+		    !mod_member_present(m, kallsyms_end) ||
+		    m->kallsyms_start >= m->kallsyms_end)
+			continue;
+		ka_hdr = (struct kallsyms_header *)m->kallsyms_start;
+		ka_sec = (const struct kallsyms_section *) ((char *)ka_hdr + ka_hdr->section_off);
+		ka_str = ((char *)(ka_hdr) + ka_hdr->string_off);
+		for (i = 0; i < ka_hdr->sections; ++i, kallsyms_next_sec(ka_hdr, ka_sec)) {
+			if (callback(
+				token,
+				*(m->name) ? m->name : "kernel",
+				ka_sec->name_off + ka_str,
+				ka_sec->start,
+				ka_sec->start + ka_sec->size,
+				ka_sec->flags))
+				return(0);
+		}
+	}
+	return(1);
+}
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/ksyms.c linuxppc64_2_4/kernel/ksyms.c
--- linux-2.4.19/kernel/ksyms.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/ksyms.c	Fri Aug  9 12:47:05 2002
@@ -40,6 +40,9 @@
 #include <linux/mmzone.h>
 #include <linux/mm.h>
 #include <linux/capability.h>
+#if defined(CONFIG_DUMP)
+#include <linux/dump.h>
+#endif
 #include <linux/highuid.h>
 #include <linux/brlock.h>
 #include <linux/fs.h>
@@ -55,6 +58,9 @@
 #ifdef CONFIG_KMOD
 #include <linux/kmod.h>
 #endif
+#ifdef CONFIG_KALLSYMS
+#include <linux/kallsyms.h>
+#endif
 
 extern void set_device_ro(kdev_t dev,int flag);
 
@@ -64,6 +70,7 @@
 extern int request_dma(unsigned int dmanr, char * deviceID);
 extern void free_dma(unsigned int dmanr);
 extern spinlock_t dma_spin_lock;
+extern int panic_timeout;
 
 #ifdef CONFIG_MODVERSIONS
 const struct module_symbol __export_Using_Versions
@@ -80,6 +87,15 @@
 EXPORT_SYMBOL(inter_module_put);
 EXPORT_SYMBOL(try_inc_mod_count);
 
+#ifdef CONFIG_KALLSYMS
+extern const char __start___kallsyms[];
+extern const char __stop___kallsyms[];
+EXPORT_SYMBOL(__start___kallsyms);
+EXPORT_SYMBOL(__stop___kallsyms);
+EXPORT_SYMBOL(kallsyms_symbol_to_address);
+EXPORT_SYMBOL(kallsyms_address_to_symbol);
+#endif
+
 /* process memory management */
 EXPORT_SYMBOL(do_mmap_pgoff);
 EXPORT_SYMBOL(do_munmap);
@@ -358,6 +374,21 @@
 EXPORT_SYMBOL(proc_dointvec_minmax);
 EXPORT_SYMBOL(proc_doulongvec_ms_jiffies_minmax);
 EXPORT_SYMBOL(proc_doulongvec_minmax);
+
+/* dump (system crash dump) functions and needed parameters */
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+EXPORT_SYMBOL(get_blkfops);
+EXPORT_SYMBOL(dump_function_ptr);
+#if defined(CONFIG_X86) || defined(CONFIG_ALPHA)
+EXPORT_SYMBOL(page_is_ram);
+#endif
+EXPORT_SYMBOL(dump_in_progress);
+EXPORT_SYMBOL(dumping_cpu);
+EXPORT_SYMBOL(panic_timeout);
+EXPORT_SYMBOL(register_dump_notifier);
+EXPORT_SYMBOL(unregister_dump_notifier);
+EXPORT_SYMBOL(dump_notifier_list);
+#endif
 
 /* interrupt handling */
 EXPORT_SYMBOL(add_timer);
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/panic.c linuxppc64_2_4/kernel/panic.c
--- linux-2.4.19/kernel/panic.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/panic.c	Fri Aug  9 12:47:05 2002
@@ -13,6 +13,9 @@
 #include <linux/delay.h>
 #include <linux/reboot.h>
 #include <linux/notifier.h>
+#if defined(CONFIG_DUMP)
+#include <linux/dump.h>
+#endif
 #include <linux/init.h>
 #include <linux/sysrq.h>
 #include <linux/interrupt.h>
@@ -20,6 +23,11 @@
 asmlinkage void sys_sync(void);	/* it's really int */
 
 int panic_timeout;
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+int (*dump_function_ptr)(char *, struct pt_regs *) = 0;
+volatile int dump_in_progress = 0;
+volatile int dumping_cpu = 0;
+#endif
 
 struct notifier_block *panic_notifier_list;
 
@@ -49,6 +57,11 @@
         unsigned long caller = (unsigned long) __builtin_return_address(0);
 #endif
 
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+	struct pt_regs regs;
+	get_current_regs(&regs);
+#endif
+
 	bust_spinlocks(1);
 	va_start(args, fmt);
 	vsprintf(buf, fmt, args);
@@ -62,11 +75,17 @@
 		sys_sync();
 	bust_spinlocks(0);
 
+#if !defined(CONFIG_DUMP) && !defined(CONFIG_DUMP_MODULE)
 #ifdef CONFIG_SMP
 	smp_send_stop();
 #endif
+#endif
 
 	notifier_call_chain(&panic_notifier_list, 0, NULL);
+
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+	dump(buf, &regs);
+#endif
 
 	if (panic_timeout > 0)
 	{
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/printk.c linuxppc64_2_4/kernel/printk.c
--- linux-2.4.19/kernel/printk.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/printk.c	Thu Aug 22 11:27:44 2002
@@ -91,7 +91,7 @@
 static unsigned long logged_chars;		/* Number of chars produced since last read+clear operation */
 
 struct console_cmdline console_cmdline[MAX_CMDLINECONSOLES];
-static int preferred_console = -1;
+/*static*/ int preferred_console = -1;
 
 /* Flag: console code may call schedule() */
 static int console_may_schedule;
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/sched.c linuxppc64_2_4/kernel/sched.c
--- linux-2.4.19/kernel/sched.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/sched.c	Fri Aug  9 12:47:05 2002
@@ -27,6 +27,9 @@
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>
 #include <linux/completion.h>
+#if defined(CONFIG_DUMP)
+#include <linux/dump.h>
+#endif
 #include <linux/prefetch.h>
 #include <linux/compiler.h>
 
@@ -554,6 +557,12 @@
 	int this_cpu, c;
 
 
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+	if (dump_in_progress) {
+		goto dump_scheduling_disabled;
+	}
+#endif
+
 	spin_lock_prefetch(&runqueue_lock);
 
 	BUG_ON(!current->active_mm);
@@ -701,6 +710,23 @@
 	reacquire_kernel_lock(current);
 	if (current->need_resched)
 		goto need_resched_back;
+	return;
+
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+dump_scheduling_disabled:
+
+	/* make sure we assign our this_cpu ... */
+	if (!current->active_mm) BUG();
+	this_cpu = current->processor;
+
+	/*
+	 * If this is not the dumping cpu, then spin right here
+	 * till the dump is complete
+	 */
+	if (this_cpu != dumping_cpu) {
+		while (dump_in_progress);
+	}
+#endif
 	return;
 }
 
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/sys.c linuxppc64_2_4/kernel/sys.c
--- linux-2.4.19/kernel/sys.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/sys.c	Mon Jul  8 14:15:05 2002
@@ -14,6 +14,9 @@
 #include <linux/prctl.h>
 #include <linux/init.h>
 #include <linux/highuid.h>
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+#include <linux/dump.h>
+#endif
 
 #include <asm/uaccess.h>
 #include <asm/io.h>
@@ -49,6 +52,9 @@
  */
 
 static struct notifier_block *reboot_notifier_list;
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+struct notifier_block *dump_notifier_list;
+#endif
 rwlock_t notifier_lock = RW_LOCK_UNLOCKED;
 
 /**
@@ -166,6 +172,39 @@
 {
 	return notifier_chain_unregister(&reboot_notifier_list, nb);
 }
+
+#if defined(CONFIG_DUMP) || defined(CONFIG_DUMP_MODULE)
+/**
+ *	register_dump_notifier - Register function to be called at dump time
+ *	@nb: Info about notifier function to be called
+ *
+ *	Registers a function with the list of functions
+ *	to be called at dump time.
+ *
+ *	Currently always returns zero, as notifier_chain_register
+ *	always returns zero.
+ */
+ 
+int register_dump_notifier(struct notifier_block * nb)
+{
+	return notifier_chain_register(&dump_notifier_list, nb);
+}
+
+/**
+ *	unregister_dump_notifier - Unregister previously registered dump notifier
+ *	@nb: Hook to be unregistered
+ *
+ *	Unregisters a previously registered dump
+ *	notifier function.
+ *
+ *	Returns zero on success, or %-ENOENT on failure.
+ */
+ 
+int unregister_dump_notifier(struct notifier_block * nb)
+{
+	return notifier_chain_unregister(&dump_notifier_list, nb);
+}
+#endif
 
 asmlinkage long sys_ni_syscall(void)
 {
diff -uNr -X /home/engebret/dontdiff linux-2.4.19/kernel/sysctl.c linuxppc64_2_4/kernel/sysctl.c
--- linux-2.4.19/kernel/sysctl.c	Fri Aug  2 19:39:46 2002
+++ linuxppc64_2_4/kernel/sysctl.c	Fri Jul 26 13:43:29 2002
@@ -30,6 +30,9 @@
 #include <linux/init.h>
 #include <linux/sysrq.h>
 #include <linux/highuid.h>
+#ifdef	CONFIG_KDB
+#include <linux/kdb.h>
+#endif	/* CONFIG_KDB */
 
 #include <asm/uaccess.h>
 
@@ -256,6 +259,10 @@
 	{KERN_S390_USER_DEBUG_LOGGING,"userprocess_debug",
 	 &sysctl_userprocess_debug,sizeof(int),0644,NULL,&proc_dointvec},
 #endif
+#ifdef	CONFIG_KDB
+	{KERN_KDB, "kdb", &kdb_on, sizeof(int),
+	 0644, NULL, &proc_dointvec},
+#endif	/* CONFIG_KDB */
 	{0}
 };
 
